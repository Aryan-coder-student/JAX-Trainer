[
    {
        "title": "Does `jax` compilation save runtime memory by recognizing array elements that are duplicated by indexing",
        "question": "Consider the example code:\nfrom functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnums=(0,))\ndef my_function(n):\n\n    idx = jnp.tile(jnp.arange(n, dtype=int),(1,n)) # Contains duplicate indices\n    A = jnp.ones((n**2,), dtype=float)\n    B = jnp.ones((n,100,100), dtype=float)\n\n    return jnp.sum(A[...,None,None]*B[idx]) # Will data in B be duplicated in memory here?\n\nmy_function(5)\nWhen compiling through B[idx], will jax compilation recognize that there are duplicate indices and thereby avoid unnecessarily duplicating the data in B?\nI suspect probably not because it's value dependent in general, but just want to understand better.",
        "answers": [
            "No, I don't believe this is an optimization that the compiler does. I'm basing this on the fact that XLA's computational model requires all array shapes to be known at compile-time, and the values in idx are not known until runtime.\nIf you're not convinced and want to see for yourself what the compiler is doing with this code, you can use JAX's Ahead of time compilation APIs to peek at the compiled HLO produced by XLA for this code (note that the compiler will perform different optimizations on different hardware).\nFor example:\nprint(my_function.lower(5).compile().as_text())\nHloModule jit_my_function, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%region_0.1.clone (reduce_sum.0: f32[], reduce_sum.1: f32[]) -> f32[] {\n  %reduce_sum.0 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.1 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.2 = f32[] add(%reduce_sum.0, %reduce_sum.1), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%fused_computation () -> f32[1,25,100,100] {\n  %constant.2 = f32[] constant(1)\n  %broadcast_in_dim.0 = f32[5,100,100]{2,1,0} broadcast(%constant.2), dimensions={}, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=10 source_end_line=10 source_column=8 source_end_column=42}\n  %iota.5 = s32[1,1,5,5]{3,2,1,0} iota(), iota_dimension=3, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %bitcast.5 = s32[1,25]{1,0} bitcast(%iota.5), metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %constant.1 = s32[] constant(0)\n  %broadcast.4 = s32[1,25]{1,0} broadcast(%constant.1), dimensions={}\n  %lt.0 = pred[1,25]{1,0} compare(%bitcast.5, %broadcast.4), direction=LT, metadata={op_name=\"jit(my_function)/lt\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %constant.0 = s32[] constant(5)\n  %broadcast.1 = s32[1,25]{1,0} broadcast(%constant.0), dimensions={}\n  %add.0 = s32[1,25]{1,0} add(%bitcast.5, %broadcast.1), metadata={op_name=\"jit(my_function)/add\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %select_n.0 = s32[1,25]{1,0} select(%lt.0, %add.0, %bitcast.5), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %bitcast.4 = s32[25,1]{1,0} bitcast(%select_n.0), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %gather.2 = f32[25,1,100,100]{3,2,1,0} gather(%broadcast_in_dim.0, %bitcast.4), offset_dims={1,2,3}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,100,100}, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  ROOT %bitcast.3 = f32[1,25,100,100]{3,2,1,0} bitcast(%gather.2), metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n}\n\nENTRY %main.2 () -> f32[] {\n  %constant.7 = f32[] constant(0)\n  %gather_bitcast_fusion = f32[1,25,100,100]{3,2,1,0} fusion(), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}, backend_config={\"outer_dimension_partitions\":[\"1\",\"2\"]}\n  %reduce-window = f32[1,1,4,4]{3,2,1,0} reduce-window(%gather_bitcast_fusion, %constant.7), window={size=1x25x32x32 stride=1x25x32x32 pad=0_0x0_0x14_14x14_14}, to_apply=%region_0.1, backend_config={\"outer_dimension_partitions\":[\"1\",\"1\",\"2\"]}\n  ROOT %reduce_sum.7 = f32[] reduce(%reduce-window, %constant.7), dimensions={0,1,2,3}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\nReading this output takes some practice, but the relevant piece to answer your question is the line that starts with %gather.2 = f32[25,1,100,100]{3,2,1,0}: the gather primitive is XLA's version of indexing, and you see that it's explicitly constructing the full 25x100x100 array, and not removing the duplicated indices."
        ],
        "link": "https://stackoverflow.com/questions/79798175/does-jax-compilation-save-runtime-memory-by-recognizing-array-elements-that-ar"
    },
    {
        "title": "How to Make Batching Rule for Multiple Outputs",
        "question": "I am still exploring how to make batching rule correctly. Right now, my code of batching rule doesn't work as expected for multiple outputs. Here is my code.\nimport jax\nimport jax.numpy as jnp\nfrom jax.extend import core\nfrom jax.interpreters import batching\n\nmy_sum_p = core.Primitive('my_sum')\nmy_sum_p.multiple_results = True\n\ndef my_sum(x):\n    return my_sum_p.bind(x)\n\n# primal evaluation rule\ndef my_sum_impl(x):\n    return jnp.sum(x), jnp.ones((3,))\nmy_sum_p.def_impl(my_sum_impl)\n\n# batching rule\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (bd_x, bd_x)\n    return output, dims\nbatching.primitive_batchers[my_sum_p] = my_sum_batching_rule\n\n# example usage\nif __name__ == \"__main__\":\n    x = jnp.array([[1.0, 2.0, 3.0],\n                   [4.0, 5.0, 6.0]])\n    \n    vms = jax.vmap(my_sum)\n    print('my_sum:', vms(x))\n\n    def original_sum(x):\n        return jnp.sum(x), jnp.ones((3,))\n    vos = jax.vmap(original_sum)\n    print('original sum:', vos(x))\nThe output is like this\nmy_sum: [Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\noriginal sum: (Array([ 6., 15.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\nThe second output of my_sum is not batched like the one of original_sum.\nOn another case, when I change the vmap setup of my_sum such as follows\nvms = jax.vmap(my_sum, out_axes=(0, None))\nit raises an error\nTraceback (most recent call last):\n  File \"/home/yahya/Projects/neuralnet/neuralnet/nn/playground.py\", line 33, in <module>\n    print('my_sum:', vms(x))\n                     ~~~^^^\nValueError: vmap out_axes specification must be a tree prefix of the corresponding value, got specification (0, None) for value tree PyTreeDef([*, *]).\nMeanwhile the same vmap setup for the original_sum makes a correct output such as follows\noriginal_sum: (Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32))\nHow should my batching rule be? I really appreciate for any help",
        "answers": [
            "Your batching rule should look like this:\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (0, None)\n    return output, dims\nThe operative change here is that dims = (bd_x, bd_x) became dims = (0, None). The output batch dimensions must reflect the batching characteristics of the output, not the input: in this case, the first output is batched along the leading axis (bdim = 0), and the second output is unbatched (bdim = None).\nYou can test the correctness of this further by comparing the results when vmapping over different input and output axes; for example:\nprint(jax.vmap(original_sum, in_axes=(1,))(x))\nprint(jax.vmap(my_sum, in_axes=(1,))(x))\n(Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)]\nprint(jax.vmap(original_sum, in_axes=(1,), out_axes=(0, None))(x))\nprint(jax.vmap(my_sum, in_axes=(1,), out_axes=[0, None])(x))\n(Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\n(notice that out_axes is a tuple in the first case, because original_fun returns a tuple, and a list in the second case, because primitives with multiple_outputs=True return a list rather than a tuple)."
        ],
        "link": "https://stackoverflow.com/questions/79790782/how-to-make-batching-rule-for-multiple-outputs"
    },
    {
        "title": "How to Make Batching Rule Properly",
        "question": "I am trying to make a custom primitive and following this tutorial. I think I have followed the tutorial correctly, but the result shows the code doesn't work properly. Here is my code:\nimport jax\nimport jax.numpy as jnp\nfrom jax.extend import core\nfrom jax.interpreters import batching\n\nmy_sum_p = core.Primitive('my_sum')\n\ndef my_sum(x):\n    return my_sum_p.bind(x)\n\n# primal evaluation rule\ndef my_sum_impl(x):\n    return jnp.sum(x)\nmy_sum_p.def_impl(my_sum_impl)\n\n# batching rule\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    return my_sum(x), bd_x\nbatching.primitive_batchers[my_sum_p] = my_sum_batching_rule\n\n# example usage\nif __name__ == \"__main__\":\n    x = jnp.array([[1.0, 2.0, 3.0],\n                   [4.0, 5.0, 6.0]])\n    \n    vms = jax.vmap(my_sum)\n    print('my_sum:', vms(x))\n\n    def original_sum(x):\n        return jnp.sum(x)\n    vos = jax.vmap(original_sum)\n    print('original_sum:', vos(x))\nThe result is as follow:\nmy_sum: 21.0\noriginal_sum: [ 6. 15.]\nI expect that the result of my_sum is the same with the result of original_sum. Any help, I really appreciate. Thank you in advance.",
        "answers": [
            "A batched sum means that you're summing along all axes except the batch axis. You can encode this in your batching rule like this:\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    return jnp.sum(x, axis=axis), 0\nWith this substitution, vmap(my_sum)(x) and vmap(jnp.sum)(x) should have the same behavior for any valid x.\nNotice that the returned batch dimension is 0 regardless of what the input batch dimension is: this is because the batched sum is always one-dimensional with batched values along axis 0."
        ],
        "link": "https://stackoverflow.com/questions/79790324/how-to-make-batching-rule-properly"
    },
    {
        "title": "Subtlety in initializing attributes with methods in modules from the `equinox` `jax` library",
        "question": "I have the following code that defines an abstract class and its final subclasse. The two classes are both subclasses of the equinox.Module class, which registers class attributes as the leaves of a PyTree container.\n# === IMPORTS ===\nfrom abc import ABC, abstractmethod\nimport jax\nfrom jax.typing import ArrayLike\nimport jax.numpy as jnp\nimport equinox as eqx\nfrom quadax import quadgk\n\njax.config.update(\"jax_enable_x64\", True)\n\n\nclass MyClass(eqx.Module): # Works if I toggle to MyClass(ABC)\n\n    rtol = 1e-12\n    atol = 1e-12\n\n    param: ArrayLike\n\n    def __init__(self):\n        self.param = self._integral_moment(3) # Fails, but works if I toggle to something like \"self.param = self.func(1.)\"\n\n    @abstractmethod \n    def func(self, tau):\n        pass\n\n    def func_abs(self, tau):\n        return jnp.abs(self.func(tau))\n    \n    def _integral_moment(self, order): \n        return quadgk(self._integrand_moment, [0, jnp.inf], args=(order,), epsrel=self.rtol, epsabs=self.atol)[0]\n\n    def _integrand_moment(self, tau, order):\n        return self.func_abs(tau) * jnp.abs(tau)**order\n \n\nclass MySubClass(MyClass):\n\n    gamma: ArrayLike\n    kappa: ArrayLike\n    w0: ArrayLike\n\n    def __init__(self, gamma, kappa, w0):\n        self.gamma = jnp.asarray(gamma)\n        self.kappa = jnp.asarray(kappa) \n        self.w0 = jnp.asarray(w0)\n        super().__init__()\n\n    def func(self, tau):\n        return self.gamma * jnp.exp(-1j * self.w0 * tau) * jnp.exp(-self.kappa*jnp.abs(tau)/2)\n    \n\n# Test    \ntest = MySubClass(gamma=1., kappa=1., w0=1.)\ntest.param\nThis code produces the AttributeError message:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[21], line 52\n     48         return self.gamma * jnp.exp(-1j * self.w0 * tau) * jnp.exp(-self.kappa*jnp.abs(tau)/2)\n     51 # Test    \n---> 52 test = MySubClass(gamma=1., kappa=1., w0=1.)\n     53 test.param\n\n    [... skipping hidden 2 frame]\n\nCell In[21], line 45\n     43 self.kappa = jnp.asarray(kappa) \n     44 self.w0 = jnp.asarray(w0)\n---> 45 super().__init__()\n\nCell In[21], line 19\n     18 def __init__(self):\n---> 19     self.param = self._integral_moment(3)\n\n    [... skipping hidden 1 frame]\n\nCell In[21], line 29\n     28 def _integral_moment(self, order): \n---> 29     return quadgk(self._integrand_moment, [0, jnp.inf], args=(order,), epsrel=self.rtol, epsabs=self.atol)[0]\n...\n    659         and isinstance(out, types.MethodType)\n    660         and out.__self__ is self\n    661     ):\n\nAttributeError: 'MySubClass' object has no attribute 'param'\nThis error clearly comes from a restriction of the equinox.Module, since if I change the parent class to ABC, the code runs fine.\nFirst, I thought that maybe equinox did not allow me to use methods to initialize attributes. But if I use the func() method instead of the _integral_moment() method to initialize param, the code works fine.\nSo I just don't understand what is going on here. I thought it would be better to ask here before asking the developers at equinox.\nThis uses equinox version 0.13.1 with jax version 0.7.2.",
        "answers": [
            "The issue here is that when traced, eqx.Module attempts to access all the declared attributes of the Module, so the module cannot be traced before those attributes are created. Here's a simpler repro of the same problem:\nimport jax\nimport equinox as eqx\n\nclass MyClass(eqx.Module):\n    param: ArrayLike\n\n    def __init__(self):\n      self.param = jax.jit(self.func)()\n\n    def func(self):\n      return 4\n\nMyClass()  # AttributeError: 'MyClass' object has no attribute 'param'\nThe quadgk function traces its input, and since you call it before setting param, you get this error. With this issue in mind, you can fix your problem by setting the missing param to a placeholder value before you call a function that traces the object's methods:\nclass MyClass(eqx.Module):\n\n    ...\n\n    def __init__(self):\n        self.param = 0  # set to a placeholder to allow tracing\n        self.param = self._integral_moment(3)\n\n    ...",
            "To follow up on @jakevdp's answer, a completely equivalent but perhaps slightly more elegant way of systematically pre-empting this issue in equinox is to assign a value directly in the attribute definition:\nclass MyClass(eqx.Module):\n\n    ...\n\n    param: float = 0 # set to a placeholder to allow tracing\n    \n    def __init__(self):\n        self.param = self._integral_moment(3)\n\n    ...\nEDIT: Importantly, not that it is NOT allowed to initialize attributes as mutables or jax arrays at the class level in dataclasses like equinox modules, which raises a ValueError: Use default_factory. For the above code, all instances of the class will initially share the same instance object for the field, which is not desired behavior in a dataclass if the attribute can later be modified in some way. This is probably why the previous answer made that choice of initializing in init, which will always work."
        ],
        "link": "https://stackoverflow.com/questions/79787529/subtlety-in-initializing-attributes-with-methods-in-modules-from-the-equinox"
    },
    {
        "title": "Wrap `jax.lax.fori_loop` to systematically override `upper<=lower` tracing behavior",
        "question": "This is a follow-up to a previous question about the jax.lax.fori_loop function, with a little bit of a challenge for you at the end.\nAs described in the documentation, the fori_loop is never executed at runtime for the case upper<=lower. However, as has been pointed out several times, it is still traced. This can cause issues with out-of-bound indexing. I understand that the consensus is that this is intended behavior for fori_loop.\nNevertheless, in my use cases, the python-like behavior makes things much, much easier conceptually. So in my previous question, I came up with the following wrapper that overrides the default behavior when the indexing issue occurs:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI TO HANDLE THE CASE UPPER<=LOWER SEPARATELY\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((3,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThe above works fine. However, I noticed that I can't replace all my fori_loops with this wrapper. Specifically, it fails when used with nested loops. For example, the following modification of the function part_binom_conv() fails:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# # WRAPPER FOR FORI TO HANDLE THE CASE UPPER<=LOWER SEPARATELY\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = wrapped_fori(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0] #<--- This causes an error\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((3,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThe error is a TracerBoolConversionError which I think is related to the tracing the condition in my wrapper:\n---------------------------------------------------------------------------\nTracerBoolConversionError                 Traceback (most recent call last)\nCell In[4], line 55\n     53 Hks = jnp.zeros((3,2,2), dtype=complex)\n     54 U = jnp.eye(2, dtype=complex)\n---> 55 build(U, Hks)\n\n    [... skipping hidden 13 frame]\n\nCell In[4], line 43\n     41 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     42 Uks = Uks.at[0].set(U)\n---> 43 Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n     44 return Uks\n\nCell In[4], line 10\n      8     out = init_val\n      9 else:\n---> 10     out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n     11 return out\n\n    [... skipping hidden 12 frame]\n\nCell In[4], line 48\n     46 def update_Uks(k, val):\n...\n-> 1806   raise TracerBoolConversionError(arg)\n\nTracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function update_Uks at /var/folders/x0/28x522xx1vb2xl75tn781lqr0000gn/T/ipykernel_54810/1590930335.py:46 for fori_loop. This concrete value was not available in Python because it depends on the value of the argument k.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\nMy question is a little bit of a challenge. Is it possible to modify this wrapper for the fori_loop so that it doesn't trace the body when upper<=lower, and that it never causes an error in nested loops?\nI understand that this will not be implemented in jax, but I was wondering if it is something I could do in my code.",
        "answers": [
            "Is it possible to modify this wrapper for the fori_loop so that it doesn't trace the body when upper<=lower...\nNo, I don't believe that is possible.\nThe problematic case you point out occurs when the fori_loop start and endpoints are traced, in which case their concrete values are by definition unknown at trace-time. You cannot condition tracing behavior on values that are not known at trace time.\n... and that it never causes an error in nested loops?\nI don't think you need to worry about this. The reason your previous question ran into an error is because you were in a situation where the array shapes were related to the loop length, and so for loop length zero, indexing into the array failed. With dynamic loop endpoints, the array shapes cannot be related to the loop length, because shapes cannot be dynamic. So I don't think you'd ever run into an issue where tracing a zero-length dynamic/inner loop causes problems, unless your code had a bug such that it would error in all cases."
        ],
        "link": "https://stackoverflow.com/questions/79784971/wrap-jax-lax-fori-loop-to-systematically-override-upper-lower-tracing-behav"
    },
    {
        "title": "`jax.lax.fori_loop` with equal `lower` and `upper` should produce no iteration, but body still executed",
        "question": "I have a code that uses a bunch of jax.lax.fori_loop. The documentation of fori_loop says that \"setting upper <= lower will produce no iterations\". So I was naively expecting the loop to just return its init_val unchanged. But in my case, it seems like it does attempt to execute the body.\nThe code is as follows:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis returns the following error:\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 47\n     45 Hks = jnp.zeros((0,2,2), dtype=complex)\n     46 U = jnp.eye(2, dtype=complex)\n---> 47 build(U, Hks)\n\nCell In[10], line 35\n     33 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     34 Uks = Uks.at[0].set(U)\n---> 35 Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n     36 return Uks\n\n    [... skipping hidden 12 frame]\n\nCell In[10], line 40\n     38 def update_Uks(k, val):\n     39     Uks, Hks = val\n---> 40     Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n     41     return Uks, Hks\n\nCell In[10], line 12\n     11 def binom_conv(n, Aks, Bks):\n---> 12     return part_binom_conv(n, 0, n, Aks, Bks)\n...\n--> 930     raise IndexError(f\"index is out of bounds for axis {x_axis} with size 0\")\n    931   i = _normalize_index(i, x_shape[x_axis]) if normalize_indices else i\n    932   i_converted = lax.convert_element_type(i, index_dtype)\n\nIndexError: index is out of bounds for axis 0 with size 0\nI'm not sure I understand what is going on here. Shouldn't the fori_loop just return its init_val and not cause this error?",
        "answers": [
            "JAX code has two phases of execution: tracing and runtime (see JAX Key Concepts: tracing for a description of this). A fori_loop with upper <= lower will have no iterations at runtime, but the code is still traced. The error you're seeing is coming up during tracing, which is necessary even for an empty loop in order to determine the shape and type of the output. You could work around this by specially handling the zero-length case in your fori_loop body function.\nA similar issue with while_loop is discussed at https://github.com/jax-ml/jax/issues/3285.",
            "Following the insight by @Obaskly and @jakevdp, I went with a wrapper for the fori_loop:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI [WORKS IN EXTERNAL LOOPS, BUT CAUSES ISSUE IN part_binom_conv()]\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis produces the correct behavior, and it seems to trace correctly under minimal testing.\nNote however that there is still an error if I replace the inner fori_loop in part_binom_conv by this wrapper. I think it causes an issue in tracing of nested loops.\nSorry for deleting and undeleting this answer a few times, this last point had me confused for a while."
        ],
        "link": "https://stackoverflow.com/questions/79783857/jax-lax-fori-loop-with-equal-lower-and-upper-should-produce-no-iteration"
    },
    {
        "title": "Sequential compilation times of a jax-jitted recursive function",
        "question": "I have a recursively defined function my_func that is jitted using jax.jit from the jax library. It is defined below:\n# Imports\n\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\nimport time\n\n\n# Constants and subroutines used in the core recursive routing below ...\n\nsx = jnp.asarray([[0,1.],[1.,0]], dtype=complex)\nsy = jnp.asarray([[0,-1j],[1j,0]], dtype=complex)\n\ndef conj_op(A):\n    return jnp.swapaxes(A, -1,-2).conj()\n\ndef commutator_herm(A, B):\n    comm = A @ B\n    comm = comm - conj_op(comm)\n    return comm\n\ndef H(t):\n    return jnp.cos(t) * sy\n\ndef X0(t):\n    return sx\n\n# Core recursive routine ...\n\n@partial(jax.jit, static_argnames=\"k\")\ndef my_func(t, k):\n    if k==0:\n        X_k = X0(t)\n        return X_k\n    else:\n        X_km1 = lambda t: my_func(t,k-1)\n        X_k = 1j * commutator_herm(H(t), X_km1(t)) + jax.jacfwd(X_km1, holomorphic=True)(t)\n        return X_k\nwith the relevant test:\n# Tests ...\n\nt = jnp.asarray(1, dtype=complex)\n\nseq_exec_times = []\n\nfor k in range(9,10): # or toggle to range(10) to compile sequentially\n    start = time.time()\n    my_func(t, k)\n    dur = time.time() - start\n    seq_exec_times.append(dur)\n\ntotal_seq_exec_time = sum(seq_exec_times)\n\nprint(\"Sequential execution times:\")\nprint([\"{:.3e} s\".format(x) for x in seq_exec_times])\nprint(\"Total execution time:\")\nprint(\"{:.3e} s\".format(total_seq_exec_time))\nIf I execute this function the first time only for k=9, then I get a quite long compilation time, which I figure is because tracing a recursive function like this one takes an effort that scales exponentially with recursion depth. The output is:\nSequential execution times:\n['6.306e+01 s']   # First execution time when calling directly with k=9\nTotal execution time:\n6.306e+01 s\nBut then I thought that in practice, I need to evaluate my_func for increasing values of k=0,1,2,3... anyway. And if the lower step has already been traced, then you only need to trace the next level of the tree, and that should be more efficient. And indeed, executing k=1,2,3...,8 before executing k=9 yields a slightly lower execution time the first time k=9 is evaluated:\nSequential execution times:\n['3.797e-03 s',\n'2.203e-02 s',\n'3.487e-02 s',\n'7.054e-02 s',\n'1.779e-01 s',\n'4.680e-01 s',\n'1.326e+00 s',\n'4.145e+00 s',\n'1.456e+01 s',\n'5.550e+01 s']    # First execution time of k=9 after calling k=0,1,2,3...,8 first\nTotal execution time:\n7.631e+01 s\nThat said, this still scales exponentially with recursion depth, and I was naively expecting the compilation of k=9 to be more efficient. If the lower levels k=1,2,3...,8 are already compiled, then I would naively expect the compilation at the next level k=9 to be relatively simple. I would think that you can simply trace the link between k=9 and k=8, and avoid going through the whole recursion tree again at the lower levels.\nBut it looks like I was wrong, and I'm curious to know why? And if I'm not wrong, how do I make this better?\nThis was run with jax - 0.4.33 on MacOS - 15.6.1.",
        "answers": [
            "In general, you should avoid a recursive coding style when using JAX code with JIT, autodiff, or other transformations.\nThere are three different things at play here that complicate the analysis of runtimes:\ntracing: this is the general process used in transforming JAX code, whether for jit or for autodiff like jacfwd . I believe the main reason you are seeing different timings depending on the sequence of executions is because of the trace cache: for each value of k, the function will be traced only once and subsequent calls will use the cached trace.\nautodiff: the jacfwd call in your function retraces the original function and generates a sequence of operations representing the forward-jacobian. I don't believe that there is any cache for this, so each time you call jacfwd the transformation will be recomputed from the cached trace.\ncompilation: I don't believe the that compilation pass currently makes use of previously-compiled units using the trace cache. Any control flow in JAX (loops, recursive calls, etc.) are effectively flattened before being passed to the compiler: in your case the number of operations looks to scale roughly as O[3^k]. Compilation cost is superlinear—and often roughly quadratic—in the number of operations, and so you'll find compilation will become very expensive as k gets larger.\nUnfortunately, there's not really any workaround for these issues. When using JAX, you should avoid deep Python control flow like for loops and recursion. You may be able to make progress by re-expressing your recursive function as an iterative function, using one of JAX's control flow operators like fori_loop to reduce the number of lines and cut down the compilation time."
        ],
        "link": "https://stackoverflow.com/questions/79769647/sequential-compilation-times-of-a-jax-jitted-recursive-function"
    },
    {
        "title": "Using select_n to define a piecewise function",
        "question": "I'm trying to use jax.lax.select_n to define a piecewise function, and I don't understand what it does. I thought that the behavior should be evaluating to evaluate each of the Boolean terms in the which argument, and then return the result of evaluating the function in cases corresponding to the first of the conditions to be true, but this doesn't seem to be what happens.\nFor example, I would like the expression\n(lambda i: jax.lax.select_n(\n    jnp.array([jnp.less(i, 10), jnp.less(i, 20), True]),\n    jnp.array([i, i+1, i+2]),\n))(15)\nto evaluate to 16, but instead it evaluates to the array [15,16,17]. How can I get that desired behavior?",
        "answers": [
            "What you're describing is the semantics of jnp.select , which you can use like this:\n>>> import jax\n\n>>> import jax.numpy as jnp\n\n>>> (lambda i: jnp.select(\n...    jnp.array([jnp.less(i, 10), jnp.less(i, 20), True]),\n...    jnp.array([i, i+1, i+2]),\n... ))(15)\nArray(16, dtype=int32)\nOn the other hand, the first argument of lax.select_n is not a sequence of masks, but rather an index used to select among the subsequent arguments; for example:\n>>> i = 1\n>>> jax.lax.select_n(i, jnp.array([15]), jnp.array([16]), jnp.array([17]))\nArray([16], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/79768929/using-select-n-to-define-a-piecewise-function"
    },
    {
        "title": "How to control hyperparameter within flax.nnx loss function using an optax.schedule?",
        "question": "from jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 )\n    \n    def f(x,m=2.234,b=-1.123):\n        return m*x+b\n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n\n    c = 0.9\n    learning_rate_schedule = optax.schedules.cosine_decay_schedule(\n        init_value=2e-1,\n        decay_steps = int(c*epochs),\n        alpha= 0.01,\n    )\n    ## how do I get the values from hyperparam_schedule into the loss function?\n    hyperparam_schedule = optax.schedules.linear_schedule(\n        init_value=12,\n        end_value=234,\n        transition_steps=int(c*epochs),\n    )\n    \n    params = nnx.Param\n    optimizer = nnx.Optimizer(\n        model,\n        tx=optax.adam(learning_rate_schedule),\n        wrt = params\n    )\n    \n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( carry, x, obs ):\n        def loss_function(model, inputs, obs):\n            prediction = model(inputs)\n            ## I want this to instead look like:\n            #prediction = model( my_hyperparameter, inputs )\n            error = obs - prediction\n            loss = jnp.mean(error ** 2)\n            mae = jnp.mean(jnp.abs(error ) )\n            return loss, mae\n\n        model, optimizer= carry\n        (loss,mae), grads = nnx.value_and_grad(loss_function,has_aux = True)( model, x, obs )\n        optimizer.update( model, grads )\n        return (model,optimizer), (loss,mae)\n        #return (model,optimizer), (loss,mae,foo)\n    (model,optimizer), (losses,maes) = optimizer_scan( (model, optimizer), x1, obs1 )\n    #(model,optimizer), (losses,maes,foos) = optimizer_scan( (model, optimizer), x1, obs1 )\n    \n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', model.kernel.value )\n    print( 'b approximation:', model.bias.value )\nI want to control an argument to my model's __call__() method with an optax schedule. This is a hyperparameter, and I want it to change slowly over training. I have marked the schedule that I have already defined for this purpose with a comment. I have also shown in the loss function what I want the forward pass call to look like.\nI see that there is an optax.schedules method to inject hyperparameters, but I couldn't get it to do something useful here.\nFor a practical example, let's fill the array foos with values from the hyperparameter schedule. See the commented out return in the optimizer_scan function and the commented out line where optimizer_scan could be called such that this array is filled.\nPS: This is updated for and tested on flax version 0.11",
        "answers": [
            "    def optimizer_scan( carry,  x, obs ):\n        def loss_function( model, inputs, obs ):\n            prediction = model( inputs )\n            error = obs - prediction\n            loss = jnp.mean( error ** 2 )\n            mae = jnp.mean( jnp.abs( error ) )\n            return loss, mae\n\n        model, optimizer= carry\n        foo=hyperparam_schedule( optimizer.step.value )\n        (loss,mae), grads = nnx.value_and_grad(loss_function,has_aux = True)( model, x, obs )\n        optimizer.update( model, grads )\n        return (model,optimizer), (loss,mae,foo)\n    (model,optimizer), (losses,maes,bar) = optimizer_scan( (model, optimizer), x1, obs1 )\nOK so the hyperparam_schedule is just a function. I had imagined that it needed to be embedded in the optimizer state somehow, but that is not the case.\nI have found that the optimizer state has a variable that indicates how many optimization iterations have occurred. In the snippet above, I show how we can use the optimizer.step attribute as the argument for the hyperparam_schedule call."
        ],
        "link": "https://stackoverflow.com/questions/79760266/how-to-control-hyperparameter-within-flax-nnx-loss-function-using-an-optax-sched"
    }
]