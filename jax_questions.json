[
    {
        "title": "Flax nnx / jax: tree.map for layers of incongruent size",
        "question": "I am trying to figure out how to use nnx.split_rngs. Can somebody give a version of the code below that uses nnx.split_rngs with jax.tree.map to produce an arbitrary number of Linear layers with different out_features?\nimport jax\nfrom flax import nnx\nfrom functools import partial\n\nif __name__ == '__main__':\n\n    session_sizes = {\n        'a':2,\n        'b':3,\n        'c':4,\n        'd':5,\n        'e':6,\n    }\n    dz = 2\n\n    rngs = nnx.Rngs(0)\n    \n    my_linear = partial(\n        nnx.Linear,\n        use_bias = False,\n        in_features = dz,\n        rngs=rngs )\n    \n    def my_linear_wrapper(a):\n        return my_linear( out_features=a )\n\n    q_s = jax.tree.map(my_linear_wrapper, session_sizes)\n\n    for k in session_sizes.keys():\n        print(q_s[k].kernel)\nSo in this case, we would need a tree of layers that will take our 2 in_features into spaces of 2, ..., 6 out_features.\nThe function my_linear_wrapper is sort of a workaround for the original solution we had in mind, which is to map in very much the same fashion as we're doing, but instead use (something like) the @nnx.split_rngs function decorator.\nIs there a way to use nnx.split_rngs on my_linear in order to map over the rng argument to nnx.Linear?",
        "answers": [
            "split_rngs is mostly useful when you are going to pass the Rngs through a transform like vmap, here you want to produce variable sized Modules so the current solution is the way to go. Because of how partial works you can simplify this to:\ndin = 2\nrngs = nnx.Rngs(0)\n\nmy_linear = functools.partial(\n  nnx.Linear, din, use_bias=False, rngs=rngs\n)\n\nq_s = jax.tree.map(my_linear, session_sizes)\n\nfor k in session_sizes.keys():\n  print(q_s[k].kernel)"
        ],
        "link": "https://stackoverflow.com/questions/79551198/flax-nnx-jax-tree-map-for-layers-of-incongruent-size"
    },
    {
        "title": "Why is Jax treating floating point values as tracers rather than concretizing them when nesting jitted functions?",
        "question": "I am doing some physics simulations using jax, and this involves a function called the Hamiltonian defined as follows:\n# Constructing the Hamiltonian\n@partial(jit, static_argnames=['n', 'omega'])\ndef hamiltonian(n: int, omega: float):\n    \"\"\"Construct the Hamiltonian for the system.\"\"\"\n    H = omega *  create(n) @ annhilate(n)\n    return H \nand then a bigger function def solve_diff(n, omega, kappa, alpha0): that is defined as follows:\n@partial(jit, static_argnames=['n', 'omega'])\ndef solve_diff(n, omega, kappa, alpha0):\n    # Some functionality that uses kappa and alpha0\n    \n    H = hamiltonian(n, omega)\n\n    # returns an expectation value\nWhen I try to compute the gradient of this function using jax.grad\nn = 16   \nomega = 1.0   \nkappa = 0.1  \nalpha0 = 1.0 \n\n# Compute gradients with respect to omega, kappa, and alpha0\ngrad_population = grad(solve_diff, argnums=(1, 2, 3))\ngrads = grad_population(n, omega, kappa, alpha0)\n\nprint(f\"Gradient w.r.t. omega: {grads[0]}\")\nprint(f\"Gradient w.r.t. kappa: {grads[1]}\")\nprint(f\"Gradient w.r.t. alpha0: {grads[2]}\")\nit outputs the following error:\nValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jax._src.interpreters.ad.JVPTracer'>, Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n  primal = 1.0\n  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n    pval = (ShapedArray(float32[], weak_type=True), None)\n    recipe = LambdaBinding(). The error was:\nTypeError: unhashable type: 'JVPTracer'\nThough, running solve_diff(16,1.0,0.1,1.0) on its own works as expected.\nNow if I remove omega from the list of static variables for both the hamiltonian function and the solve_diff, the grad is output as expected.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls, both n and omega are constants and indeed should not change between function calls.",
        "answers": [
            "The fundamental issue is that you cannot differentiate with respect to a static variable, and if you try to do so you will get the error you observed.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls\nIn JAX, the term \"static\" does not have to do with whether the variable is changed between function calls. Rather, a static variable is a variable that does not participate in tracing, which is the mechanism used to compute transformations like vmap, grad, jit, etc. When you differentiate with respect to a variable, it is no longer static because it is participating in the autodiff transformation, and trying to treat it as static later in the computation will lead to an error.\nFor a discussion of transformations, tracing, and related concepts, I'd start with JAX Key Concepts: transformations."
        ],
        "link": "https://stackoverflow.com/questions/79550040/why-is-jax-treating-floating-point-values-as-tracers-rather-than-concretizing-th"
    },
    {
        "title": "General way to define JAX functions with non-differentiable arguments",
        "question": "For a particular JAX function func, one can define non-differentiable arguments by using the decorator @partial(jax.custom_jvp, nondiff_argnums=...). However, in order to make it work, one must also explicitly define the differentiation rules in a custom jvp function by using the decorator @func.defjvp. I'm wondering if there is a generic way to define non-differentiable arguments for any given func, without defining a custom jvp (or vjp) function? This will be useful when the differentiation rules are too complicated to write out.",
        "answers": [
            "In JAX's design, non-differentiated arguments are a property of the gradient transformation being used, not a property of the function being differentiated. custom_jvp is fundamentally about customizing the gradient behavior, and using it to mark non-differentiable arguments without actually customizing the gradient is not an intended use.\nThe way to ensure that arguments do not participate in an autodiff transformation is to specify the arguments you want to differentiate against when you call the jax.grad, jax.jacobian, or other autodiff transformation; e.g.\njax.grad(func, argnums=(0,))  # differentiate with respect to argument 0.\nRegardless of what func is, this will attempt to differentiate with respect to the 0th argument, and if that argument is either explicitly or implicitly not differentiable due to how func is defined, an error will be raised.",
            "In JAX, the @partial(jax.custom_jvp, nondiff_argnums=...) decorator allows you to specify which arguments of a function should be treated as non-differentiable (static). However, this approach requires manually defining a custom JVP rule using @func.defjvp, which can be cumbersome if the differentiation rules are complex . But there are alternative approaches to achieve this without explicitly specifying custom differentiation rules.\n1.Wrapping the Function Using jax.jit with static_argnums, jit(static_argnums=(2,)) ensures mode is treated as a compile-time constant, avoiding differentiation issues.\n2. Use jax.named_call for better debugging, this adds a custom name in JAX’s computational graph, making debugging easier.\n3. Use jax.tree_util.Partial for Partial Function Application. tree_util.Partial locks in the mode argument, treating it as static.\n4. Use functools.partial to Predefine Static Arguments. functools.partial locks in the static argument before JAX's autodiff kicks in."
        ],
        "link": "https://stackoverflow.com/questions/79516990/general-way-to-define-jax-functions-with-non-differentiable-arguments"
    },
    {
        "title": "Why does JAX's grad not always print inside the cost function?",
        "question": "I am new to JAX and trying to use it with PennyLane and optax to optimize a simple quantum circuit. However, I noticed that my print statement inside the cost function does not execute in every iteration. Specifically, it prints only once at the beginning and then stops appearing.\nThe quantum circuit itself does not make sense; I just wanted to simplify the example as much as possible. I believe the circuit is not actually relevant to the question, but it's included as an example.\nHere is my code:\nimport pennylane as qml\nimport jax\nimport jax.numpy as jnp\nimport optax\n\njax.config.update(\"jax_enable_x64\", True)\n\ndevice = qml.device(\"default.qubit\", wires=1)\n\n\n@qml.qnode(device, interface='jax')\ndef circuit(params):\n    qml.RX(params, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(params):\n    print('Evaluating')\n    return circuit(params)\n\n# Define optimizer\nparams = jnp.array(0.1)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\n\n# JIT the gradient function\ngrad = jax.jit(jax.grad(cost))\n\nfor epoch in range(5):\n    print(f'{epoch = }')\n    grad_value = grad(params)\n    updates, opt_state = opt.update(grad_value, opt_state)\n    params = optax.apply_updates(params, updates)\nExpected output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nEvaluating\nepoch = 3\nEvaluating\nepoch = 4\nEvaluating\nActual output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nepoch = 3\nepoch = 4\nQuestion:\nWhy is the print statement inside cost not executed after the first iteration? Is JAX caching the function call or optimizing it in a way that skips execution? How can I ensure that cost is evaluated in every iteration?",
        "answers": [
            "When working with JAX it is important to understand the difference between \"trace time\" and \"runtime\". For JIT compilation JAX does an abstract evaluation of the function when it is called first. This is used to \"trace\" the computational graph of the function and then create a fully compiled replacement, which is cached and then invoked on the next calls (\"runtime\") of the function. Now, Python's print statements are only evaluated at trace time and not at runtime, because the code of the function has been effectively replaced by a compiled version.\nFor the case of printing during runtime, JAX has a special jax.debug.print function, you can use:\ndef cost(params):\n    jax.debug.print('Evaluating')\n    return circuit(params)\nMore on the jax.debug utilities: https://docs.jax.dev/en/latest/debugging/index.html\nAnd JIT compilation: https://docs.jax.dev/en/latest/jit-compilation.html"
        ],
        "link": "https://stackoverflow.com/questions/79498911/why-does-jaxs-grad-not-always-print-inside-the-cost-function"
    },
    {
        "title": "Jax numpy extracting non-nan values gives NonConcreteBooleanIndexError",
        "question": "I have a jax 2d array with some nan-values\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\nand want to get an array which contains for each row only the non-nan values. The resulting array has thus the same number of rows, and either less columns or the same number but with nan values padded at the end. So in this case, the result should be\narray_2d = jnp.array([\n    [1,   2,      3],\n    [10  20,jnp.nan]\n    ])\nThe order (among non-nan values) should stay the same.\nTo make things easier, I know that each row has at most k (in this case 3) non-nan values. Getting the indices for the non-nan values is very easy, but ``moving them to the front'' is harder.\nI tried to work on a row-by-row basis; the following function works indeed:\n# we want to vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\nHowever, I can't vmap this. Even though I payed attention that the returned array always has the same size, the line ret_vals = row_vals[row_mask] makes problems, since this has a dynamic size. Does anyone know how to circumvent this? I believe that functions like `jnp.where' etc don't help either.\nHere is the full MWE:\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n# we want to get -- efficiently -- all non-nan values per row.\n# we know that each row has at most 3 non-nan values\n\n# we will vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\n\n# we now vmap\nnon_nan_vals = jax.vmap(get_non_nan_values)(array_2d) # this gives error: NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[5])\nNB: The array will be very large in practice and have many nan values, while k (the number of non-nan values) is on the order of 10 or 100.\nThank you very much!",
        "answers": [
            "By padding the array with a fill value at the end of each row first, you can rely on jnp.nonzero and its size and fill_value arguments, which define a fixed output size and fill value index, when the size requirement is not met. Here is a minimal example:\nimport jax.numpy as jnp\nimport jax\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n\n@jax.vmap\ndef get_non_nan_values(row_vals, size=3):\n    padded = jnp.pad(row_vals, (0, 1), constant_values=jnp.nan)\n    non_nan = jnp.nonzero(~jnp.isnan(padded), size=size, fill_value=-1)\n    return padded[non_nan]\n\nget_non_nan_values(array_2d)\nWhich returns:\nArray([[ 1.,  2.,  3.],\n       [10., 20., nan]], dtype=float32)\nI think this solution is a bit more compact and clearer in intend, however I have not checked the performance.\nI hope this helps!",
            "I think you can do what you want with this function, which rather than sorting the array (as I commented), sorts and masks the indices of the non-nan values:\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(1,))\ndef func(array, k=3):\n    m, n = array.shape[-2:]\n    indices = jnp.broadcast_to(jnp.arange(n)[None, :], (m, n))\n    sorted_masked_indices = jnp.sort(jnp.where(jnp.isnan(array), jnp.nan, indices))\n    array_rearranged = array[jnp.arange(m)[:, None], sorted_masked_indices.astype(int)]\n    return jnp.where(jnp.isnan(sorted_masked_indices), jnp.nan, array_rearranged)[:, :k]\nTest:\nimport numpy as np\nrng = np.random.default_rng(0)\nk = 3\n\na = rng.random((12, 6))\na[np.arange(12)[:, None], rng.integers(0, 6, (12, 6))] = np.nan\n\nprint(a)\nprint(func(a, k=k))\nGives:\n[[0.63696169        nan        nan 0.01652764 0.81327024        nan]\n [       nan 0.72949656        nan        nan 0.81585355        nan]\n [       nan 0.03358558        nan        nan        nan        nan]\n [0.29971189        nan        nan        nan        nan 0.64718951]\n [       nan        nan        nan 0.98083534        nan 0.65045928]\n [       nan        nan 0.13509651 0.72148834        nan        nan]\n [       nan 0.88948783 0.93404352 0.3577952         nan        nan]\n [       nan 0.33791123 0.391619   0.89027435        nan        nan]\n [       nan 0.83264415        nan        nan 0.87648423        nan]\n [0.33611706        nan        nan 0.79632427        nan 0.0520213 ]\n [       nan        nan 0.09075305 0.58033239        nan        nan]\n [       nan 0.94211311        nan        nan 0.62910815        nan]]\n[[0.6369617  0.01652764 0.8132702 ]\n [0.72949654 0.81585354        nan]\n [0.03358557        nan        nan]\n [0.29971188 0.6471895         nan]\n [0.9808353  0.6504593         nan]\n [0.1350965  0.72148836        nan]\n [0.88948786 0.9340435  0.3577952 ]\n [0.33791122 0.391619   0.89027435]\n [0.83264416 0.8764842         nan]\n [0.33611706 0.79632425 0.0520213 ]\n [0.09075305 0.5803324         nan]\n [0.9421131  0.62910813        nan]]",
            "With the stable=True option, argsort on a boolean array is guaranteed to preserve the relative order between True and False elements. So this should do the trick:\ndef get_non_nan_values(row_vals):\n    return row_vals[jnp.argsort(jnp.isnan(rowvals), stable=True)[:3]]\nHowever, for wide rows, sorting the entire row seems unnecessary when we already know there are only at most 3 non-nan values. So another simple approach using jax.lax.top_k:\ndef get_top_3_non_nan(row_vals):\n  return row_vals[jax.lax.top_k(~jnp.isnan(row_vals), 3)[1]]",
            "I would do this using vmap of argsort of isnan:\nimport jax\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n])\n\nresult = jax.vmap(lambda x: x[jnp.argsort(jnp.isnan(x))])(array_2d)\nprint(result)\n# [[ 1.  2.  3. nan nan]\n#  [10. 20. nan nan nan]]\nThis approach uses static shapes, and thus will be compatible with jit."
        ],
        "link": "https://stackoverflow.com/questions/79443943/jax-numpy-extracting-non-nan-values-gives-nonconcretebooleanindexerror"
    },
    {
        "title": "Problems when boolean indexing in Jax, getting NonConcreteBooleanIndexError",
        "question": "I'm currently trying to create a CustomProblem inheriting from the BaseProblem class in TensorNEAT which is a Jax based library. In trying to implement the evaluate function of this class, I'm using a boolean mask, but I have problems getting it to work. My code results in jax.errors.NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[n,n]) which I think is due to some of my arrays not having a definite shape. How do I circumvent this?\nConsider this example in np:\nimport numpy as np\n\nran_int = np.random.randint(1, 5, size=(2, 2))\nprint(ran_int)\n\nran_bool = np.random.randint(0,2, size=(2,2), dtype=bool)\nprint(ran_bool)\n\na = (ran_int[ran_bool]>0).astype(int)\nprint(a)\nIt could give an output like this:\n[[2 2]\n [3 4]]\n[[ True False]\n [ True  True]]\n[1 1 1] #Is 1D and has less elements than before boolean mask was applied!\nBut in Jax, the same way of thinking results in the NonConcreteBooleanIndexError error I got.\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #ERROR HAPPENS HERE\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #WOULD HAPPEN HERE TOO IF THIS PART WAS FIRST\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI was considering using jnp.where to solve the issue, but the resulting pairwise_labels and pairwise_predictions have a different shape than what I expect (namely (n, n)) as seen in the code below:\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) # shape (n, n)\n\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) # shape (n, n)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n ,n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI fear that the differing shapes of pairwise_predictions and pairwise_labels after using jnp.where will result in a different loss than if I had just used the boolean mask as I would in np. There is also the fact that I get another error that happens later in the pipeline with the output ValueError: max() iterable argument is empty from line 143 in the pipeline.py file of TensorNeat. This is curiously circumvented by changing pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold to pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold, which probably also results in some loss that is incorrect.\nBelow is some code that should be enough to setup a minimal running example that is similar to my setup:\nfrom tensorneat import algorithm, genome, common\nfrom tensorneat.pipeline import Pipeline\nfrom tensorneat.genome.gene.node import DefaultNode\nfrom tensorneat.genome.gene.conn import DefaultConn\nfrom tensorneat.genome.operations import mutation\nimport jax, jax.numpy as jnp\nfrom tensorneat.problem import BaseProblem\n\ndef binary_cross_entropy(prediction, target):\n    return -(target * jnp.log(prediction) + (1 - target) * jnp.log(1 - prediction))\n\n# Define the custom Problem\nclass CustomProblem(BaseProblem):\n\n    jitable = True  # necessary\n\n    def __init__(self, inputs, labels, threshold):\n        self.inputs = jnp.array(inputs) #nb! already has shape (n, 768)\n        self.labels = jnp.array(labels).reshape((-1,1)) #nb! has shape (n), must be transformed to have shape (n, 1) \n        self.threshold = threshold\n\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold #this is the thing I actually want\n        #pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold #weird fix to circumvent ValueError: max() iterable argument is empty when using jnp.where for pairwise_labels and pairwise_predictions\n        print(pairs_to_keep.shape)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape)\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\n\n    @property\n    def input_shape(self):\n        # the input shape that the act_func expects\n        return (self.inputs.shape[1],)\n\n    @property\n    def output_shape(self):\n        # the output shape that the act_func returns\n        return (1,)\n\n    def show(self, state, randkey, act_func, params, *args, **kwargs):\n        # showcase the performance of one individual\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(state, params, self.inputs)\n\n        loss = jnp.mean(jnp.square(predict - self.labels))\n\n        n_elements = 5\n        if n_elements > len(self.inputs):\n            n_elements = len(self.inputs)\n\n        msg = f\"Looking at {n_elements} first elements of input\\n\"\n        for i in range(n_elements):\n            msg += f\"for input i: {i}, target: {self.labels[i]}, predict: {predict[i]}\\n\"\n        msg += f\"total loss: {loss}\\n\"\n        print(msg)\n\nalgorithm = algorithm.NEAT(\n    pop_size=10,\n    survival_threshold=0.2,\n    min_species_size=2,\n    compatibility_threshold=3.0,  \n    species_elitism=2,  \n    genome=genome.DefaultGenome(\n        num_inputs=768,\n        num_outputs=1,\n        max_nodes=769,  # must at least be same as inputs and outputs\n        max_conns=768,  # must be 768 connections for the network to be fully connected\n        output_transform=common.ACT.sigmoid,\n        mutation=mutation.DefaultMutation(\n            # no allowing adding or deleting nodes\n            node_add=0.0,\n            node_delete=0.0,\n            # set mutation rates for edges to 0.5\n            conn_add=0.5,\n            conn_delete=0.5,\n        ),\n        node_gene=DefaultNode(),\n        conn_gene=DefaultConn(),\n    ),\n)\n\n\nINPUTS = jax.random.uniform(jax.random.PRNGKey(0), (100, 768)) #the input data x\nLABELS = jax.random.uniform(jax.random.PRNGKey(0), (100)) #the annotated labels y\n\nproblem = CustomProblem(INPUTS, LABELS, 0.25)\n\nprint(\"Setting up pipeline and running it\")\nprint(\"-----------------------------------------------------------------------\")\npipeline = Pipeline(\n    algorithm,\n    problem,\n    generation_limit=1,\n    fitness_target=1,\n    seed=42,\n)\n\nstate = pipeline.setup()\n# run until termination\nstate, best = pipeline.auto_run(state)\n# show results\npipeline.show(state, best)",
        "answers": [
            "The solution I got from the authors of TensorNEAT was to update the evaluate() function to use jnp.nan instead of -jnp.inf in the first jnp.where() calls used on pairwise_labels and pairwise_predictions. I also had to make the loss take into consideration the nan values that would be present in the loss after running the bce. The new evaluate() function that has the same behavior as boolean indexing is pasted below.\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold\n\n        #finding only the labels to keep\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, jnp.nan) #use jnp.nan here\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n\n        #finding only the predictions to keep\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, jnp.nan) #use jnp.nan here\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # loss with shape (len(labels), len(labels)), we need to reduce it to a scalar\n        loss = jnp.mean(loss, where=~jnp.isnan(loss)) #only use number values in loss\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss        \n        return -loss",
            "Yes, the mask operation makes the shape of the resulting array dependent on the content of the array. And jax only supports static shapes. The workaround you propose looks reasonable, with using the value -inf as a placeholder. The missing part is ignoring the zero entries in the mean. This you could achieve by a custom “masked” mean function along the lines of:\nfrom jax import numpy as jnp\nfrom jax import random\nimport jax\n\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (4, 4))\n\nkey, subkey = random.split(key)\nmask = random.bernoulli(key, 0.5, (4, 4))\n\n@jax.jit\ndef masked_mean(x, mask):\n    return jnp.sum(jnp.where(mask, x, 0), axis=0) / jnp.sum(mask, axis=0)\n\n\nmasked_mean(x, mask)\nI have not checked other parts of the code in detail, but e.g. the statement jnp.where(pairwise_labels > 0, True, False) has no effect. And with the masked mean you might not need the placeholder values at all.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79423352/problems-when-boolean-indexing-in-jax-getting-nonconcretebooleanindexerror"
    },
    {
        "title": "How to use jax.vmap with a tuple of flax TrainStates as input?",
        "question": "I am setting up a Deep MARL framework and I need to assess my actor policies. Ideally, this would entail using jax.vmap over a tuple of actor flax TrainStates. I have tried the following:\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.linen.initializers import constant, orthogonal\nfrom flax.training.train_state import TrainState\nimport optax\nimport distrax\n\nclass PGActor_1(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 4\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(128, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0))(x)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nclass PGActor_2(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 2\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nstate= jnp.zeros((1, 5))\n\nnetwork_1 = PGActor_1()\nnetwork_1_init_rng = jax.random.PRNGKey(42)\nparams_1 = network_1.init(network_1_init_rng, state)\n\nnetwork_2 = PGActor_2()\nnetwork_2_init_rng = jax.random.PRNGKey(42)\nparams_2 = network_2.init(network_2_init_rng, state)\n\ntx = optax.chain(\noptax.clip_by_global_norm(1),\noptax.adam(lr=1e-3)\n)\nactor_trainstates= (\n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_1),             \n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_2)\n )\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(actor_trainstates)\nbut I recieve the following error:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nDoes anybody have any idea how to make this work?\nThank you in advance.",
        "answers": [
            "This is quite similar to other questions (e.g. Jax - vmap over batch of dataclasses). The key point is that JAX transformations like vmap require data in a struct of arrays pattern, whereas you are using an array of structs pattern.\nTo work directly with an array of structs pattern in JAX, you can use Python's built-in map function – due to JAX's asynchronous dispatch, the resulting operations will be executed in parallel where possible:\npis = map(lambda x: x.apply_fn(x.params, state), actor_trainstates)\nHowever, this doesn't take advantage of the automatic vectorization done by vmap. In order to do this, you can convert your data from an array of structs to a struct of arrays, although this requires that all entries have the same structure.\nFor compatible cases, the solution would look something like this, however it errors for your data:\ntrain_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(train_states_soa)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-36-da904fa40b9c> in <cell line: 0>()\n----> 1 train_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\n\nValueError: Dict key mismatch; expected keys: ['Dense_0', 'Dense_1', 'Dense_2']\nThe problem is that your two train states do not have matching structure, and so they cannot be transformed into a single struct of arrays. You can see the difference in structure by inspecting the params:\nprint(actor_trainstates[0].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1', 'Dense_2'])\nprint(actor_trainstates[1].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1'])\nThere is no way to use vmap in a context where your inputs have different structure, so you'll either have to change the problem to ensure the same structure, or stick with the map approach."
        ],
        "link": "https://stackoverflow.com/questions/79405049/how-to-use-jax-vmap-with-a-tuple-of-flax-trainstates-as-input"
    },
    {
        "title": "Serialization in JAX",
        "question": "What is the recommended way to do serialization/deserialization in JAX?\nIn the context of reinforcement learning my starting point in terms of data might be e.g. match replays that have to be pre-processed to obtain tuples of JAX arrays. This is a process that I would like to do just once, save to disk, then wrap around that a data loading interface like grain.DataLoader.\nBut I have no idea how to do the actual serialization.\nRight now I'm doing\ndef save_jax(path, x: jnp.array):\n    y = np.array(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.array(x)\n    return y\nIt works. My hope is there won't be any copies in wrapping jnp->np or the other way around, and then hopefully this will be mmapped.\nIs this approach bad for performance? What is a better way?",
        "answers": [
            "Both Jax and Numpy support the Python buffer protocol, allowing for zero-copy data sharing between the different types. That being said when using jnp.array or np.array, the default is to copy (see linked docs). So right now you do an unnecessary copy of the data. So I would suggest to use np.asarray instead (and the Jax equivalent), which only copies when needed. So your code would look like:\ndef save_jax(path, x: jnp.array):\n    y = np.asarray(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.asarray(x)\n    return y\nAside from the copy the native Numpy format might not be the best choice of format in neither I/O performance nor associated meta data. For a lightweight alternative you might want to look for example into safetensors.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79387670/serialization-in-jax"
    },
    {
        "title": "How to do jittable masked get?",
        "question": "How do I do a jax get from a masked index?\nThe code below works without jit.\nx = jnp.arange(25).reshape((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n    [1,2],\n])\ncoords_mask = jnp.array([True, True, False, True])\n\n@jax.jit\ndef masked_gather(x, coords, coords_mask):\n    coords_masked = coords[coords_mask]\n    return x.at[coords_masked[:, 0], coords_masked[:, 1]].get()\n\nmasked_gather(x, coords, coords_mask)\nFails with NonConcreteBooleanIndexError.\nShould return Array([ 7, 13,  7], dtype=int32)",
        "answers": [
            "There is no way to execute this function in a JIT-compatible way, because JAX does not support compilation of programs with dynamic shapes. In your case, the size of the returned array depends on the number of True elements in coords_mask, and so the shape is dynamic by definition.\nSee JAX Sharp Bits: Dynamic Shapes for more information.\nDepending on what you are doing with the resulting value, there are a number of available approaches to work around this: for example, if the shape is truly unknown, you could return an array padded with zeros; it might look something like this:\n@jax.jit\ndef masked_gather_padded(x, coords, coords_mask, fill_value=0):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='fill', fill_value=fill_value)\n  return result[order]\n\nmasked_gather_padded(x, coords, coords_mask)\n# Array([ 7, 13,  7,  0], dtype=int32)\nAlternatively, if the number of True entries in the mask is known a priori, you could modify the function to accept a static size argument and use that to construct an appropriate output. It might look something like this:\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['size'])\ndef masked_gather_with_size(x, coords, coords_mask, *, size):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='drop')\n  return result[order[:size]]\n\nmasked_gather_with_size(x, coords, coords_mask, size=3)\n# Array([ 7, 13,  7], dtype=int32)\nThe best approach will depend on your application."
        ],
        "link": "https://stackoverflow.com/questions/79375141/how-to-do-jittable-masked-get"
    },
    {
        "title": "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?",
        "question": "Is it possible to use vmap for auto-batching if your function isn't jittable?\nI have a function that's not jittable:\ndef testfunc(model, x1, x2, x2_mask):\n    ( ... non-jittable stuff with masks ... )\nI'm trying to wrap it in vmap so I can benefit from auto-batching as explained here.\nSo I do:\ntestfunc_batched = jax.vmap(testfunc, in_axes=(None, 0, 0, 0))\nThe intention is that in batched mode, each of x1, x2, and x2_mask will have an additional outter dimension, the batching dimension. The model shouldn't be treated differently in batched mode hence the None. Let me know if the syntax isn't right.\nI create batches of size one just to test, schematically:\nx1s = x1.reshape(1, ...)\nx2s = x2.reshape(1, ...)\nx2_masks = x2_mask.reshape(1, ...)\n\ntestfunc_batched(model, x1s, x2s, x2_masks)\nThe last line fails with ConcretizationTypeError.\nI've recently learned that stuff with masks makes functions not jittable. But does that mean that I also can't use vmap? Or am I doing something wrong?\n(There is further context in How to JIT code involving masked arrays without NonConcreteBooleanIndexError?, but you don't have to read that question to understand this one.)",
        "answers": [
            "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?\nNo. In general, functions which are incompatible with jit will also be incompatible with vmap, because both jit and vmap use the same JAX tracing mechanism to transform the program."
        ],
        "link": "https://stackoverflow.com/questions/79374152/is-it-possible-to-use-jax-vmap-for-auto-batching-if-your-function-isnt-jittable"
    },
    {
        "title": "Count onto 2D JAX coordinates of another 2D array",
        "question": "I have\nx = jnp.zeros((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\nI would like to count onto x how many times each of the individual (x,y) coordinates appear in coords. In other words, obtain the output:\nArray([[0., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI've tried x.at[coords].add(1) and this gives me:\nArray([[0., 0., 0., 0., 0.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI understand what it's doing, but not how to make it do the thing I want.\nThere's this related question[1], but I haven't been able to use it to solve my problem.\n[1] Update JAX array based on values in another array",
        "answers": [
            "For multiple indices, you should pass a tuple of index arrays:\nx = x.at[coords[:, 0], coords[:, 1]].add(1)\nprint(x)\n[[0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]",
            "The generalized operation is basically computing a histogram, especially when the coordinate arrays are float values. So depending on the context the code is used, the following alternative might communicate the intent a bit more clearly:\nfrom jax import numpy as jnp\n\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\n\nbins = jnp.arange(5 + 1) - 0.5 \nx, _ = jnp.histogramdd(coords, bins=(bins, bins))\nIt will also handle if coordinates are out of bounds. But I presume under the hood, it does the same operation as at[...].add(1). So I would not expect any relevant difference in performance."
        ],
        "link": "https://stackoverflow.com/questions/79370053/count-onto-2d-jax-coordinates-of-another-2d-array"
    },
    {
        "title": "Return a different class based on an optional flag in the arguments without factory",
        "question": "I am implementing a series of classes in Equinox to enable taking derivatives with respect to the class parameters. Most of the time, the user will be instantiating class A and using the fn function to generate some data, the details of which are unimportant. However, in cases where we are interested in gradients, it is beneficial to represent param_c in terms of a sigmoid function to ensure that it remains clamped in the range (0,1). However, I don't want the user to notice a difference in how the class behaves if they do this. As such, I implement another class A_sigmoid that has param_c as a property and use A_abstract to ensure that both classes inherit the fn method, which will call param_c in its logic. While I could simply have the user instantiate an A_sigmoid object with a _param_c_sigmoid instead of param_c I don't want to force the user to have to make this distinction. Rather, I would want them to pass in the same kwargs dictionary no matter the class and have conversion happen behind the scenes. I also wanted to make it so that when making a new A one could simply pass an optional flag to direct the program to use the sigmoid version of the code. To do so, I implemented the following MWE:\nclass A_abstract(eqx.Module):\n    param_a: jax.Array\n    param_b: jax.Array\n    param_c: eqx.AbstractVar[jax.Array]\n    \n    def fn(self,*args,**kwargs):\n        pass\n\nclass A_sigmoid(A_abstract):\n    _param_c_sigmoid: jax.Array\n\n    @property\n    def param_c(self):\n        return 1 / (1 + jnp.exp(-self._param_c_sigmoid))\n\nclass A(A_abstract):\n    param_c: jax.Array\n\n    def __new__(cls, **kwargs):\n        sigmoid_flag = kwargs.pop('use_sigmoid_c',False)\n        if sigmoid_flag == True:\n            param_c = kwargs.pop('param_c')\n            _param_c_sigmoid = jnp.log(param_c / (1 - param_c))\n            kwargs['_param_c_sigmoid'] = _param_c_sigmoid\n            instance = A_sigmoid.__new__(A_sigmoid)\n            instance.__init__(**kwargs)\n            print(type(instance))\n            return instance\n        else:\n            return super(A,cls).__new__(cls)\n\nclassA = A(param_a = 1.,param_b = 2.,param_c = 0.5,use_sigmoid_c=True)\nprint(type(classA))\nThe code correctly says that instance has type A_sigmoid when print is called in the __new__ method. However, when I print type(classA), it is of type A and has no attribute param_c, though it does have a value for _param_c_sigmoid. Why is this the case? Am I missing something in my use of __new__ that is causing this error? While I know that in principle a factory would be the best way to do this, there are other classes of types B, C, etc. that don't have this need for a sigmoid implementation and that I would like to behave exactly the same way as A to enable them to be easily swapped. Thus, I don't want some custom method to instantiate A that would be different from calling the default constructor on the other classes.\nI am running this on a Jupyter notebook with the following package versions:\nPython           : 3.12.4\nIPython          : 8.30.0\nipykernel        : 6.29.5\njupyter_client   : 8.6.3\njupyter_core     : 5.7.2",
        "answers": [
            "If you were using a normal class, what you did is perfectly reasonable:\nclass A_abstract:\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A_sigmoid'>\nHowever, eqx.Module includes a bunch of metaclass logic that overrides how __new__ works, and this seems to collide with the __new__ overrides that you're making. Notice here the only difference is that A_abstract inherits from eqx.Module, and the result is A rather than A_sigmoid:\nimport equinox as eqx\n\nclass A_abstract(eqx.Module):\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A'>\nI dug-in for a few minutes to try and find the exact cause of this change, but wasn't able to pin it down.\nIf you're trying to do metaprogramming during class construction, you'll have to modify it to work within the construction-time metaprogramming that equinox is already doing."
        ],
        "link": "https://stackoverflow.com/questions/79359839/return-a-different-class-based-on-an-optional-flag-in-the-arguments-without-fact"
    },
    {
        "title": "How Can I Use GPU to Accelerate Image Augmentation?",
        "question": "When setting up image augmentation pipelines using keras.layers.Random* or other augmentation or processing methods, we often integrate these pipelines with a data loader, such as the tf.data API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.\nTo address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?",
        "answers": [
            "We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in keras. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.\nClassification\nLet’s take a classification task as an example. If we use the tf.data API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.\nimport numpy as np\nfrom keras import layers\n\na = np.ones((4, 224, 224, 3)).astype(np.float32)\nb = np.ones((4, 2)).astype(np.float32)\n\naugmentation_layers = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.2),\n    ]\n)\n\ndataset = tf.data.Dataset.from_tensor_slices((a, b))\ndataset = dataset.batch(3, drop_remainder=True)\ndataset = dataset.map(\n    lambda x, y: (augmentation_layers(x), y), \n    num_parallel_calls=tf.data.AUTOTUNE\n)\nx.shape, y.shape\n(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))\nBut for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.\ninputs = keras.Input(shape=(224, 224, 3))\nprocessed = augmentation_layers(inputs)\nbackbone = keras.applications.EfficientNetB0(\n    include_top=True, pooling='avg'\n)(processed)\noutput = keras.layers.Dense(10)(backbone)\nmodel = keras.Model(inputs, output)\nmodel.count_params() / 1e6\n5.340581\nHere, we set the augmentation pipeline right after keras.Input. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.\nSegmentation\nI'll use this dataset for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using keras-3, which might allow for multi-backend support.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # torch, jax\n\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nkeras.__version__ # 3.4.1\n# ref https://keras.io/examples/vision/oxford_pets_image_segmentation/\n# u-net model\ndef get_model(img_size, num_classes, classifier_activation):\n    ...\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(\n        num_classes, \n        3, \n        activation=classifier_activation, \n        padding=\"same\", \n        dtype='float32'\n    )(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\nimg_size = (224, 224)\nnum_classes = 1\nclassifier_activation = 'sigmoid'\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nLet's define the augmentation pipelines.\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\")\n]\n\ndef augment_data(images, masks):\n    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = tf.cast(combined[..., 3:], tf.int32)\n    return images_augmented, masks_augmented\nLet’s define the tf.data API to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    \n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = tf.cast(image, tf.int32)\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.\n        \n    return image\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask  = read_image(mask_list, mask=True)\n    return image, mask\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE) \n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n    # Augmenting on CPU\n    dataset = dataset.map(\n        augment_data, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nprint(\"Train Dataset:\", train_dataset)\nTrain Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))>\nNow, let's compile it and run it.\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nmodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n\n%%time\nepochs = 2\nmodel.fit(\n    train_dataset, \n    epochs=epochs, \n)\nEpoch 1/2\n318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087\nEpoch 2/2\n318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338\nCPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s\nWall time: 1min 48s\nNext, we will remove the augmentation layers from the dataloader.\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE)\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nTo offload augmentation to the GPU, we’ll create a custom model class, override the train_step, and use the augment_data method that we defined earlier. Here's how to structure it:\nclass ExtendedModel(keras.Model):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def call(self, inputs):\n        return self.model(inputs)\n\n    def save(\n        self, filepath, \n        overwrite=True, \n        include_optimizer=True, \n        save_format=None, \n        add_loss=None, \n    ):\n        # Overriding this method will allow us to use the `ModelCheckpoint`\n        self.model.save(\n            filepath=filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            include_optimizer=include_optimizer,\n        )\nNow that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nemodel = ExtendedModel(model)\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nemodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n%%time\nepochs = 2\nemodel.fit(\n    train_dataset, \n    epochs=epochs, \n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            filepath='model.{epoch:02d}-{loss:.3f}.keras',\n            monitor='loss',\n            mode='min',\n            save_best_only=True\n        )\n    ]\n)\nEpoch 1/2\n318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748\nEpoch 2/2\n318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585\nCPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s\nWall time: 1min 29s\nSo, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around 18.35% improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.\nAs shown in the ExtendedModel class above, we override the save method, allowing the callbacks.ModelCheckpoint to save the full model. Inference can then be performed as shown below.\nloaded_model = keras.saving.load_model(\n    \"/kaggle/working/model.02-0.0585.keras\"\n)\nx, y = next(iter(train_dataset))\noutput = loaded_model.predict(x)\n1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step\nUpdate\nIn order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the augment_data that is used in ExtendedModel use the following backend agnostic keras.ops functions.\ndef augment_data(images, masks):\n    combined = keras.ops.concatenate(\n        [images, keras.ops.cast(masks, 'float32')], axis=-1\n    )\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')\n    return images_augmented, masks_augmented\nAdditionally, to make the pipeline flexible for all backend, we can update the ExtendedModel as follows. Now, this code can run with tensorflow, jax, and torch backends.\nclass ExtendedModel(keras.Model):\n    ...\n\n    def train_step(self, *args, **kwargs):\n        if keras.backend.backend() == \"jax\":\n            return self._jax_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"tensorflow\":\n            return self._tensorflow_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"torch\":\n            return self._torch_train_step(*args, **kwargs)\n\n    def _jax_train_step(self, state, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step(state, (x, y))\n\n    def _tensorflow_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def _torch_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    ..."
        ],
        "link": "https://stackoverflow.com/questions/79327723/how-can-i-use-gpu-to-accelerate-image-augmentation"
    },
    {
        "title": "jax and flax not playing nicely with each other",
        "question": "I want to implement a neural network with multiple LSTM gates stacked one after the other.I set the hidden states to 0, as suggested here. When I try to run the code, I get\nJaxTransformError: Jax transforms and Flax models cannot be mixed. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.JaxTransformError)\nWhen I try to replace jax.lax.scan by flax.linen.scan, it gives another error. Not quite sure how to proceed or what's actually going wrong here. Code attached below. Thanks!\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom typing import Sequence\n\n\nclass LSTMModel(nn.Module):\nlstm_hidden_size: int\nnum_lstm_layers: int\nlinear_layer_sizes: Sequence[int]\nmean_aggregation: bool\n\ndef initialize_carry(self, batch_size, feature_size=1):\n    \"\"\"Initialize carry states with zeros for all LSTM layers.\"\"\"\n    return [\n        (\n            # Hidden state (h)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n            # Cell state (c)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n        )\n        for _ in range(self.num_lstm_layers)\n    ]\n\n@nn.compact\ndef __call__(self, x, carry=None):\n    if carry is None:\n        raise ValueError(\n            \"Carry must be initialized explicitly using `initialize_carry`.\"\n        )\n\n    # Expand 2D input to 3D (if necessary)\n    if x.ndim == 2:\n        # [batch_size, sequence_length] -> [batch_size, sequence_length, 1]\n        x = jnp.expand_dims(x, axis=-1)\n\n    # Process through LSTM layers\n    for i in range(self.num_lstm_layers):\n        lstm_cell = nn.LSTMCell(\n            features=self.lstm_hidden_size, name=f'lstm_cell_{i}')\n\n        def step_fn(carry, xt):\n            new_carry, yt = lstm_cell(carry, xt)\n            return new_carry, yt\n\n        # Use lax.scan to process the sequence\n        carry[i], outputs = jax.lax.scan(step_fn, carry[i], x)\n        x = outputs  # Update x for the next layer\n\n    # Aggregate outputs\n    if self.mean_aggregation:\n        x = jnp.mean(x, axis=1)  # Average over the sequence\n    else:\n        x = x[:, -1, :]  # Use the last output\n\n    # Pass through linear layers\n    for size in self.linear_layer_sizes:\n        x = nn.Dense(features=size)(x)\n        x = nn.elu(x)\n\n    # Final output layer\n    x = nn.Dense(features=1)(x)\n    return x\n\n\n# Model hyperparameters\nlstm_hidden_size = 64\nnum_lstm_layers = 2\nlinear_layer_sizes = [32, 16]\nmean_aggregation = False\n\n# Initialize model\nmodel = LSTMModel(\n    lstm_hidden_size=lstm_hidden_size,\n    num_lstm_layers=num_lstm_layers,\n    linear_layer_sizes=linear_layer_sizes,\n    mean_aggregation=mean_aggregation\n)\n\n# Dummy input: batch of sequences with 10 timesteps\nkey = jax.random.PRNGKey(0)\n# [batch_size, sequence_length, feature_size]\ndummy_input = jax.random.normal(key, (32, 10, 1))\n\n# Initialize carry states\ncarry = model.initialize_carry(\n    batch_size=dummy_input.shape[0], feature_size=dummy_input.shape[-1])\n\n# Initialize parameters\nparams = model.init(key, dummy_input, carry)\n\n# Apply the model\noutputs = model.apply(params, dummy_input, carry)\n\n# Should print: [batch_size, 1]\nprint(\"Model output shape:\", outputs.shape)",
        "answers": [
            "Consider using nn.RNN to simplify your code:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  name=f'lstm_cell_{i}'\n)\noutputs = lstm(x)\nRNN will handle the carries for you. If you really want to handle the carries yourself you could use return_carry and initial_carry:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  return_carry=True, \n  name=f'lstm_cell_{i}'\n)\ncarry[i], outputs = lstm(x, initial_carry=carry[i])"
        ],
        "link": "https://stackoverflow.com/questions/79266328/jax-and-flax-not-playing-nicely-with-each-other"
    },
    {
        "title": "Efficiently custom array creation routines in JAX",
        "question": "I'm still getting a handle of best practices in jax. My broad question is the following:\nWhat are best practices for the implementation of custom array creation routines in jax?\nFor instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):\nimport numpy as np\nimport jax.numpy as jnp\n\ndef ones_at_col(shape_mat, idx):\n    idxs = jnp.arange(shape_mat[1])[None,:]\n    mat = jnp.where(idx==idxs, 1, 0)\n    mat = jnp.repeat(mat, shape_mat[0], axis=0)\n    return mat\n\nshape_mat = (5,10)\n\nprint(ones_at_col(shape_mat, 5))\n\n%timeit np.zeros(shape_mat)\n\n%timeit jnp.zeros(shape_mat)\n\n%timeit ones_at_col(shape_mat, 5)\nThe output is\n[[0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]]\n127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nMy function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.\nBut then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?\nIs there something better I can do or am I pushing the limits of what is possible in jax?",
        "answers": [
            "The best way to do this is probably something like this:\nmat = jnp.zeros(shape_mat).at[:, 5].set(1)\nRegarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.\nIf you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):\n@partial(jax.jit, static_argnames=['shape_mat', 'idx'])\ndef ones_at_col(shape_mat, idx):\n  return jnp.zeros(shape_mat).at[:, idx].set(1)\nYou could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.\nAnother side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects."
        ],
        "link": "https://stackoverflow.com/questions/79256001/efficiently-custom-array-creation-routines-in-jax"
    },
    {
        "title": "How to handle PRNG splitting in a jax.vmap context?",
        "question": "I have a function which simulates a stochastic differential equation. Currently, without stochastic noise, my invokation of simulating the process up to time t looks like this (and, yeah, I need to use jax):\ndef evolve(u, t):\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\n\ndef simulate(x, t):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt), u)\nNow, the pain comes with the noise. I'm a C++-guy who only occasionally needs to use Python for research/scientific work. And I really don't understand how I need (or should) implement PRNG splitting here. I guess I would change evolve to\ndef evolve(u, t, key):\n    noise = jax.random.multivariate_normal(key, jax.numpy.zeros(d), covariance_matrix, shape = (n,))\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\nBut that will not work properly I guess. If I got it right, I need to use jax.random.split to split the key. Cause if I don't, I end up with correlated samples. But how and where do I need to split?\nAlso: I guess I would need to modify simulate to def simulate(x, t, key). But then, should simulate also return the modified key?\nAnd to make it even more complicated: I actually wrap simulate into a batch_simulate function which uses jax.vmap to process a whole batch of x's and t's. How do I pass the PRNG to that batch_simulate function, how do I pass it (and broadcast it) to jax.vmap and what should batch_forward return? At first glance, it seems to me that it would take a single PRNG and split it into many (due to the vmap). But what does the caller of batch_forward do then ...\nCompletely lost on this. Any help is highly appreciated!",
        "answers": [
            "If I understand your setup correctly, you should make both evolve and simulate accept a key, and within simulate, use fold_in to generate unique keys for the loop:\ndef evolve(u, t, key):\n    ...\n\ndef simulate(x, t, key):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt, jax.random.fold_in(key, i)), u)\nThen if you want to vmap over simulate, you can split the key and map over it:\nx_batch = ...  # your batched x inputs\nt_batch = ...  # your batched t inputs\nkey_batch = jax.random.split(key, x_batch.shape[0])\n\nbatch_result = jax.vmap(simulate)(x_batch, t_batch, key_batch)"
        ],
        "link": "https://stackoverflow.com/questions/79238188/how-to-handle-prng-splitting-in-a-jax-vmap-context"
    },
    {
        "title": "Hello World for jaxtyping?",
        "question": "I can't find any instructions or tutorials for getting started with jaxtyping. I tried the simplest possible program and it fails to parse. I'm on Python 3.11. I don't see anything on GitHub jaxtyping project about an upper bound (lower bound is Python 3.9) and it looks like it's actively maintained (last commit was 8 hours ago). What step am I missing?\njaxtyping==0.2.36\nnumpy==2.1.3\ntorch==2.5.1\ntypeguard==4.4.1\n(It seems like numpy is required for some reason even though I'm not using it)\nfrom typeguard import typechecked\nfrom jaxtyping import Float\nfrom torch import Tensor\n\n\n@typechecked\ndef matmul(a: Float[Tensor, \"m n\"], b: Float[Tensor, \"n p\"]) -> Float[Tensor, \"m p\"]:\n    \"\"\"\n    Matrix multiplication of two 2D arrays.\n    \"\"\"\n    raise NotImplementedError(\"This function is not implemented yet.\")\n(venv) dspyz@dspyz-desktop:~/helloworld$ python matmul.py \nTraceback (most recent call last):\n  File \"/home/dspyz/helloworld/matmul.py\", line 6, in <module>\n    @typechecked\n     ^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 221, in typechecked\n    retval = instrument(target)\n             ^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 72, in instrument\n    instrumentor.visit(module_ast)\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 598, in visit_Module\n    self.generic_visit(node)\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 498, in generic_visit\n    node = super().generic_visit(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 494, in generic_visit\n    value = self.visit(value)\n            ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 672, in visit_FunctionDef\n    with self._use_memo(node):\n  File \"/usr/lib/python3.11/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 556, in _use_memo\n    new_memo.return_annotation = self._convert_annotation(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 582, in _convert_annotation\n    new_annotation = cast(expr, AnnotationTransformer(self).visit(annotation))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in visit_Subscript\n    [self.visit(item) for item in node.slice.elts],\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in <listcomp>\n    [self.visit(item) for item in node.slice.elts],\n     ^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 474, in visit_Constant\n    expression = ast.parse(node.value, mode=\"eval\")\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n    return compile(source, filename, mode, flags,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<unknown>\", line 1\n    m p\n      ^\nSyntaxError: invalid syntax",
        "answers": [
            "(jaxtyping author here)\nSadly this is a known bug in typeguard v4. It's been around forever and hasn't been fixed. (At a technical level: typeguard v4 attempts to load and reparse the source code of your function, but it doesn't properly parse all type annotations.)\nI use typeguard==2.13.3 myself, which seems to be pretty robust.\nEDIT: removed some other suggested workarounds. These turned out not to, well, work. For now I just recommend pinning to that earlier version of typeguard.",
            "You are running into the issue reported here: https://github.com/patrick-kidger/jaxtyping/issues/80\nYou can work around this by installing typeguard version 3.0, but given how long this bug has remained open without any real fix, I suspect the best conclusion is that jaxtyping should no longer be considered compatible with typeguard."
        ],
        "link": "https://stackoverflow.com/questions/79201839/hello-world-for-jaxtyping"
    },
    {
        "title": "Why is JAX's jit compilation slower on the second run in my example?",
        "question": "I am new to using JAX, and I’m still getting familiar with how it works. From what I understand, when using Just-In-Time (JIT) compilation (jax.jit), the first execution of a function might be slower due to the compilation overhead, but subsequent executions should be faster. However, I am seeing the opposite behavior.\nIn the following code snippet:\nfrom icecream import ic\nimport jax\nfrom time import time\nimport numpy as np\n\n\n@jax.jit\ndef my_function(x, y):\n    return x @ y\n\n\nvectorized_function = jax.vmap(my_function, in_axes=(0, None))\n\nshape = (1_000_000, 1_000)\n\nx = np.ones(shape)\ny = np.ones(shape[1])\n\nstart = time()\nvectorized_function(x, y)\nt_1 = time() - start\n\nstart = time()\nvectorized_function(x, y)\nt_2 = time() - start\n\nprint(f'{t_1 = }\\n{t_2 = }')\nI get the following results:\nt_1 = 13.106784582138062\nt_2 = 15.664098024368286\nAs you can see, the second run (t_2) is actually slower than the first one (t_1), which seems counterintuitive to me. I expected the second run to be faster due to JAX’s JIT caching.\nHas anyone encountered a similar situation or have any insights into why this might be happening?\nPS: I know I could have done x @ y directly without invoking vmap, but this is an easy example just to test its behaviour. My actual code is more complex, and the difference in runtime is even bigger (around 8x slower). I hope this simple example works similar.",
        "answers": [
            "For general tips on running JAX microbenchmarks effectively, see FAQ: Benchmarking JAX code.\nI cannot reproduce the timings from your snippet, but in your more complicated case, I suspect you are getting fooled by JAX's Asynchronous dispatch, which means that the timing method you're using will not actually reflect the time taken by the underlying computation. To address this, you can wrap your results in jax.block_until_ready:\nstart = time()\nvectorized_function(x, y).block_until_ready()\nt_1 = time() - start"
        ],
        "link": "https://stackoverflow.com/questions/79192549/why-is-jaxs-jit-compilation-slower-on-the-second-run-in-my-example"
    },
    {
        "title": "Restoring flax model checkpoints using orbax throws ValueError",
        "question": "The following code blocks are being utlized to save the train state of the model during training and to restore the state back into memory.\nfrom flax.training import orbax_utils\nimport orbax.checkpoint\n\ndirectory_gen_path = \"checkpoints_loc\"\norbax_checkpointer_gen = orbax.checkpoint.PyTreeCheckpointer()\ngen_options = orbax.checkpoint.CheckpointManagerOptions(save_interval_steps=5, create=True)\ngen_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n    directory_gen_path, orbax_checkpointer_gen, gen_options\n)\n\ndef save_model_checkpoints(step_, generator_state, generator_batch_stats):\n\n    gen_ckpt = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    save_args_gen = orbax_utils.save_args_from_target(gen_ckpt)\n    gen_checkpoint_manager.save(step_, gen_ckpt, save_kwargs={\"save_args\": save_args_gen})\n\ndef load_model_checkpoints(generator_state, generator_batch_stats):\n    gen_target = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    latest_step = gen_checkpoint_manager.latest_step()\n    gen_ckpt = gen_checkpoint_manager.restore(latest_step, items=gen_target)\n    generator_state = gen_ckpt[\"model\"]\n    generator_batch_stats = gen_ckpt[\"batch_stats\"]\n\n    return generator_state, generator_batch_stats\nThe training of the model was done on a GPU and loading the state onto GPU device works fine, however, when trying to load the model to cpu, the following error is being thrown by the orbax checkpoint manager's restore method\nValueError: SingleDeviceSharding with Device=cuda:0 was not found in jax.local_devices().\nI'm not quite sure what could be the reason, any thoughts folks?\nUpdate: Updated to the latest version of orbax-checkpoint, 0.8.0 traceback changed to the following error\nValueError: sharding passed to deserialization should be specified, concrete and an instance of `jax.sharding.Sharding`. Got None",
        "answers": [
            "What version of orbax.checkpoint are you using?\nIt looks like this issue was fixed in https://github.com/google/orbax/issues/678 – you should update to the most recent version of orbax-checkpoint, and try running your code again. If that doesn't work, I'd suggest reporting the problem at https://github.com/google/orbax/issues/new"
        ],
        "link": "https://stackoverflow.com/questions/79162665/restoring-flax-model-checkpoints-using-orbax-throws-valueerror"
    },
    {
        "title": "Storing and jax.vmap() over Pytrees",
        "question": "I've ran into an issue with Jax that will make me rewrite an entire 20000-line application if I don't solve it.\nI have a non-ML application which relies on pytrees to store data, and the pytrees are deep - about 6-7 layers of data storage (class1 stores class2, and that stores an array of class3 etc.)\nI've used python lists to store pytrees and hoped to vmap over them, but turns out jax can't vmap over lists.\n(So one solution is to rewrite literally every single dataclass to be a structured array and work from there, possibly putting all 6-7 layers of data into one mega-array)\nIs there a way to avoid the rewrite? Is there a way to store pytree classes in a vmappable state so that everything works as before?\nI have my classes marked with flax.struct.dataclass if that helps.",
        "answers": [
            "jax.vmap is designed to work with a struct-of-arrays pattern, and it sounds like you have an array-of-structs pattern. From your description, it sounds like you have a sequence of nested structs that look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom flax.struct import dataclass\n\n@dataclass\nclass Params:\n  x: jax.Array\n  y: jax.Array\n\n\n@dataclass\nclass AllParams:\n  p: list[Params]\n\n\nparams_list = [AllParams([Params(4, 2), Params(4, 3)]),\n               AllParams([Params(3, 5), Params(2, 4)]),\n               AllParams([Params(3, 2), Params(6, 3)])]\nThen you have a function that you want to apply to each element of the list; something like this:\ndef some_func(params):\n  a, b = params.p\n  return a.x * b.y - b.x * a.y\n\n[some_func(params) for params in params_list]\n[4, 2, -3]\nBut as you found, if you try to do this with vmap, you get an error:\njax.vmap(some_func)(params_list)\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nThe issue is that vmap operates separately over each entry of the list or pytree you pass to it, not over the elements of the list.\nTo address this, you can often transform your data structure from an array-of-structs into a struct-of-arrays, and then apply vmap over this. For example:\nparams_array = jax.tree.map(lambda *vals: jnp.array(vals), *params_list)\nprint(params_array)\nAllParams(p=[\n  Params(x=Array([4, 3, 3], dtype=int32), y=Array([2, 5, 2], dtype=int32)),\n  Params(x=Array([4, 2, 6], dtype=int32), y=Array([3, 4, 3], dtype=int32))\n])\nNotice that rather than a list of structures, this is now a single structure with the batching pushed all the way down to the leaves. This is the \"struct-of-arrays\" pattern that vmap is designed to work with, and so vmap will work correctly:\njax.vmap(some_func)(params_array)\nArray([ 4,  2, -3], dtype=int32)\nNow, this assumes that every dataclass in your list has identical structure: if not, then vmap will not be applicable, because by design it must map over computations with identical structure."
        ],
        "link": "https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees"
    },
    {
        "title": "JIT: partial or with static argnums? Non hashable input, but hashable partial",
        "question": "I am a bit lost on what exactly going on and what option to choose. Let's go trough an example:\nimport jax\nfrom functools import partial\nfrom typing import List\n\ndef dummy(a: int, b: List[str]):\n    return a + 1\nAs b argument is mutable, jitting with static argnames will be failed:\nj_dummy = jax.jit(dummy, static_argnames=['b'])\nj_dummy(2, ['kek'])\nValueError: Non-hashable static arguments are not supported\nHowever, if we do partial: jp_dummy = jax.jit(partial(dummy, b=['kek'])), we aim the goal. Somehow, partial object is indeed has __hash__ method, so we can check it with hash(partial(dummy, b=['kek'])).\nSo, I am a bit lost here: how I should proceed in a bigger picture? Should I produce partial functions with whatever arguments and then jit them or should I try to maintain my arguments hashable? What are situations when one approach is better than other? Is there any drawbacks?",
        "answers": [
            "When you use static_argnames, the static values passed to the function become part of the cache key, so if the value changes the function is re-compiled:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, s):\n  return x * len(s)\n\nf_jit = jax.jit(f, static_argnames=['s'])\n\nprint(f_jit(2, \"abc\"))  # 6\nprint(f_jit(2, \"abcd\"))  # 8\nThis is why the static arguments must be hashable: their hash is used as the JIT cache key.\nOn the other hand, when you wrap a static argument via closure, its value does not affect the cache key, and so it need not be hashable. On the other hand, since it's not part of the cache key, if the global value changes, it does not trigger a recompilation and so you may get unexpected results:\nf_closure = jax.jit(lambda x: f(x, s))\n\ns = \"abc\"\nprint(f_closure(2))  # 6\ns = \"abcd\"\nprint(f_closure(2))  # 6\nFor this reason, explicit static arguments can be safer. In your case, it may be best to change your list into a tuple, as tuples are hashable and can be used as explicit static arguments."
        ],
        "link": "https://stackoverflow.com/questions/79114391/jit-partial-or-with-static-argnums-non-hashable-input-but-hashable-partial"
    },
    {
        "title": "precision of JAX",
        "question": "I have a question regarding the precision of float in JAX. For the following code,\nimport numpy as np\nimport jax.numpy as jnp\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is:','%.60f' % np.arctan(10))\n\njnp.arctan(10) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\n\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is:','%.60f' % np.arctan(10+1e-7))\n\njnp.arctan(10+1e-7) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\njnp gave identical results for arctan(x) for a small change of input variable (1e-7), but np did not. My question is how to let jax.numpy get the right number for a small change of x?\nAny comments are appreciated.",
        "answers": [
            "JAX defaults to float32 computation, which has a relative precision of about 1E-7. This means that your two inputs are effectively identical:\n>>> np.float32(10) == np.float32(10 + 1E-7)\nTrue\nIf you want 64-bit precision like NumPy, you can enable it as discussed at JAX sharp bits: double precision, and then the results will match to 64-bit precision:\nimport jax\njax.config.update('jax_enable_x64', True)\n\nimport jax.numpy as jnp\nimport numpy as np\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is: ','%.60f' % np.arctan(10))\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is: ','%.60f' % np.arctan(10+1e-7))\njnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\nnp.arctan(10) is:  1.471127674303734700345103192375972867012023925781250000000000\njnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\nnp.arctan(10+1e-7) is:  1.471127675293833592107262120407540351152420043945312500000000\n(but please note that even the 64-bit precision used by Python and NumPy is only accurate to about one part in 10^16, so most of the digits in the representation you printed are inaccurate compared to the true arctan value)."
        ],
        "link": "https://stackoverflow.com/questions/79098013/precision-of-jax"
    },
    {
        "title": "jax register_pytree_node_class and register_dataclass returns non consistent datatype: list and tuple accordingly",
        "question": "I am writing custom class, which is basically a wrapper around list, with custom setitem method. I would like this class participate in jax.jit code, so during that I found a following problem: during jitting List field converted to tuple. However, this is case only when using\nregister_pytree_node_class When use register_dataclas , then List keep being list.\nI simplify example to highlight only this problem.\nimport jax\nfrom jax.tree_util import register_dataclass\nfrom jax.tree_util import register_pytree_node_class\nfrom functools import partial\nfrom dataclasses import dataclass\nfrom typing import List\n\n@partial(register_dataclass,\n         data_fields=['data'],\n         meta_fields=['shift'])\n@dataclass\nclass DecoratorFlatten:\n    data: List[int]\n    shift: int = 5\n\n@register_pytree_node_class\n@dataclass\nclass CustomFlatten:\n    data: List[int]\n    shift: int = 5\n\n    def tree_flatten(self):\n            children = self.data\n            aux_data = self.shift\n            return (children, aux_data)\n    \n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        obj = object.__new__(cls)\n        obj.data = children\n        setattr(obj, 'shift', aux_data)\n        return obj\nNow let's call a simple as this function over instances of this two class:\n@jax.jit\ndef get_value(a):\n    return a.data\ndf = DecoratorFlatten([0,1,2])\ncf = CustomFlatten([0,1,3])\nget_value(df), get_value(cf)\nIn first case we get list as output, but in second tuple. I thought maybe this is because of my implementation of the tree_flatten method, however:\ncf.tree_flatten()\nLeads to ([0, 1, 3], 5) as desirable.",
        "answers": [
            "In tree_unflatten, children is a tuple, and you are assigning this directly to obj.data. If you want it to be a list, you should use obj.data = list(children)."
        ],
        "link": "https://stackoverflow.com/questions/79093341/jax-register-pytree-node-class-and-register-dataclass-returns-non-consistent-dat"
    },
    {
        "title": "Batched matrix multiplication with JAX on GPU faster with larger matrices",
        "question": "I'm trying to perform batched matrix multiplication with JAX on GPU, and noticed that it is ~3x faster to multiply shapes (1000, 1000, 3, 35) @ (1000, 1000, 35, 1) than it is to multiply (1000, 1000, 3, 25) @ (1000, 1000, 25, 1) with f64 and ~5x with f32.\nWhat explains this difference, considering that on cpu neither JAX or NumPy show this behaviour, and on GPU CuPy doesn't show this behaviour?\nI'm running this with JAX: 0.4.32 on an NVIDIA RTX A5000 (and get similar results on a Tesla T4), code to reproduce:\nimport numpy as np\nimport cupy as cp\nfrom cupyx.profiler import benchmark\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng()\n\nx = np.arange(5, 55, 5)\nGPU timings:\ndtype = cp.float64\ntimings_cp = []\nfor i in range(5, 55, 5):\n    a = cp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n    b = cp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n    timings_cp.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ndtype = jnp.float64\ntimings_jax_gpu = []\nwith jax.default_device(jax.devices('gpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n        b = jnp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_gpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.gpu_times.mean() for i in timings_cp], label=\"CuPy\")\nplt.plot(x, [i.gpu_times.mean() for i in timings_jax_gpu], label=\"JAX GPU\")\nplt.legend()\nTimings with those specific shapes:\ndtype = jnp.float64\nwith jax.default_device(jax.devices('gpu')[0]):\n    a = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\n    func = jax.jit(lambda a, b: a@b)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\n\n    a = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\nGives\nf64:\n0.01453789699935913\n0.004859122595310211\n\nf32:\n\n0.005860503035545349\n0.001209742688536644\nCPU timings:\ntimings_np = []\nfor i in range(5, 55, 5):\n    a = rng.random((1000, 1000, 3, i))\n    b = rng.random((1000, 1000, i, 1))\n    timings_np.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ntimings_jax_cpu = []\nwith jax.default_device(jax.devices('cpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)))\n        b = jnp.array(rng.random((1000, 1000, i, 1)))\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_cpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.cpu_times.mean() for i in timings_np], label=\"NumPy\")\nplt.plot(x, [i.cpu_times.mean() for i in timings_jax_cpu], label=\"JAX CPU\")\nplt.legend()",
        "answers": [
            "The difference seems to come from the compiler emitting a kLoop fusion for smaller sizes, and a kInput fusion for larger sizes. You can read about the effect of these in this source comment: https://github.com/openxla/xla/blob/e6b6e61b29cc439350a6ad2f9d39535cb06011e5/xla/hlo/ir/hlo_instruction.h#L639-L656\nThe compiler likely uses some heuristic to choose between the two, and it appears that this heuristic is suboptimal at the boundary for your particular problem. You can see this by outputting the compiled HLO for your operation:\na = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\nHloModule jit__lambda_, is_scheduled=true, entry_computation_layout={(f64[1000,1000,3,25]{3,2,1,0}, f64[1000,1000,25,1]{3,2,1,0})->f64[1000,1000,3,1]{3,2,1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=\"a02cbfe0fda9d44e2bd23462363b6cc0\"}\n\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.7: f64[1000,1000,3,25], param_1.6: f64[1000,1000,25,1]) -> f64[1000,1000,3] {\n  %param_0.7 = f64[1000,1000,3,25]{3,2,1,0} parameter(0)\n  %param_1.6 = f64[1000,1000,25,1]{3,2,1,0} parameter(1)\n  %bitcast.28.5 = f64[1000,1000,25]{2,1,0} bitcast(f64[1000,1000,25,1]{3,2,1,0} %param_1.6)\n  %broadcast.2.5 = f64[1000,1000,3,25]{3,2,1,0} broadcast(f64[1000,1000,25]{2,1,0} %bitcast.28.5), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  %multiply.2.3 = f64[1000,1000,3,25]{3,2,1,0} multiply(f64[1000,1000,3,25]{3,2,1,0} %param_0.7, f64[1000,1000,3,25]{3,2,1,0} %broadcast.2.5)\n  %constant_4 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,25]{3,2,1,0} %multiply.2.3, f64[] %constant_4), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,25], Arg_1.2.0: f64[1000,1000,25,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,25,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,25]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %loop_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,25]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,25,1]{3,2,1,0} %Arg_1.2.0), kind=kLoop, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %loop_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\na = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.5: f64[1000,1000,3,35], param_1.2: f64[1000,1000,35,1]) -> f64[1000,1000,3] {\n  %param_0.5 = f64[1000,1000,3,35]{3,2,1,0} parameter(0)\n  %param_1.2 = f64[1000,1000,35,1]{3,2,1,0} parameter(1)\n  %bitcast.28.3 = f64[1000,1000,35]{2,1,0} bitcast(f64[1000,1000,35,1]{3,2,1,0} %param_1.2)\n  %broadcast.2.3 = f64[1000,1000,3,35]{3,2,1,0} broadcast(f64[1000,1000,35]{2,1,0} %bitcast.28.3), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  %multiply.2.1 = f64[1000,1000,3,35]{3,2,1,0} multiply(f64[1000,1000,3,35]{3,2,1,0} %param_0.5, f64[1000,1000,3,35]{3,2,1,0} %broadcast.2.3)\n  %constant_3 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,35]{3,2,1,0} %multiply.2.1, f64[] %constant_3), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,35], Arg_1.2.0: f64[1000,1000,35,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,35,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,35]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %input_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,35]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,35,1]{3,2,1,0} %Arg_1.2.0), kind=kInput, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %input_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\nHere's a script to observe this compiler decision with respect to size:\nfor size in range(10, 55, 5):\n  a = jnp.array(rng.random((1000, 1000, 3, size)), dtype=dtype)\n  b = jnp.array(rng.random((1000, 1000, size, 1)), dtype=dtype)\n  hlo_text = jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text()\n  print(f\"{size=} {'kLoop' in hlo_text=}\")\nsize=10 'kLoop' in hlo_text=True\nsize=15 'kLoop' in hlo_text=True\nsize=20 'kLoop' in hlo_text=True\nsize=25 'kLoop' in hlo_text=True\nsize=30 'kLoop' in hlo_text=True\nsize=35 'kLoop' in hlo_text=False\nsize=40 'kLoop' in hlo_text=False\nsize=45 'kLoop' in hlo_text=False\nsize=50 'kLoop' in hlo_text=False\nI don't have any suggestion beyond perhaps reporting this at https://github.com/openxla/xla; it may be that the compiler heuristic for choosing to emit kLoop vs. kInput needs some additional logic."
        ],
        "link": "https://stackoverflow.com/questions/79085795/batched-matrix-multiplication-with-jax-on-gpu-faster-with-larger-matrices"
    },
    {
        "title": "Computing gradient using JAX of a function that outputs a list of arrays",
        "question": "I have a function which returns a list of arrays, and I need to find its derivative with respect to a single parameter. For instance, let's say we have\ndef fun(x):\n...\nreturn [a,b,c]\nwhere a,b,c and d are multi-dimensional arrays (for example, 2 by 2 by 2 real arrays). Now I want to obtain [da/dx, db/dx, dc/dx]. By db/dx I mean I want to obtain derivative of each element in the a:222 array with respect to x, so da/dx, db/dx, dc/dx are all 222 arrays.\nThis is me using JAX differentiation for the first time, and most of the examples I find online are about functions that has scalar output.\nFrom my search, I understand one way to find this is basically get the gradient of each scalar in all these arrays one at a time (probably making it faster using vmap). Is there any other way that is faster? I think JAX.jacobian might do the trick, but I am having hard time finding its documentation to see what does the function does exactly. Any help is very much appreciated.\nNow, I have tried JAX.jacobian with simple examples, and it does give me the answer that I expect. This assures me a bit, but I would like to find official documentation or assurance from others that is the right way to do it, and it is doing what I expect it.",
        "answers": [
            "You can use jax.jacobian for what you describe. Here is an example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  a = jnp.full((2, 2), 2) * x\n  b = jnp.full((2, 2), 3) * x\n  c = jnp.full((2, 2), 4) * x\n  return [a, b, c]\n\nda_dx, db_dx, dc_dx = jax.jacobian(f)(1.0)\n\nprint(da_dx)\n# [[2. 2.]\n#  [2. 2.]]\n\nprint(db_dx)\n# [[3. 3.]\n#  [3. 3.]]\n\nprint(dc_dx)\n# [[4. 4.]\n#  [4. 4.]]\njax.jacobian is an alias of jax.jacrev, and you can find the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.jacrev.html"
        ],
        "link": "https://stackoverflow.com/questions/79025241/computing-gradient-using-jax-of-a-function-that-outputs-a-list-of-arrays"
    },
    {
        "title": "Modifying multiple dimensions of Jax array simultaneously",
        "question": "When using the jax_array.at[idx] function, I wish to be able to set values at both a set of specified rows and columns within the jax_array to another jax_array containing values in the same shape. For example, given a 5x5 jax array, I might want to set the values, jax_array.at[[0,3],:][:,[1,2]] to some 2x2 array of values. However, I am coming across an issue where the _IndexUpdateRef' object is not subscriptable. I understand the idea of the error (and I get a similar one when using 2 chained .at[]s), but I want to know if there is anyway to achieve the desired functionality within 1 line.",
        "answers": [
            "JAX follows the indexing semantics of NumPy, and NumPy's indexing semantics allow you to do this via broadcasted arrays of indices (this is discussed in Integer array indexing in the NumPy docs).\nSo for example, you could do something like this:\nimport jax.numpy as jnp\n\nx = jnp.zeros((4, 6), dtype=int)\ny = jnp.array([[1, 2],\n               [3, 4]])\ni = jnp.array([0, 3])\nj = jnp.array([1, 2])\n\n# reshape indices so they broadcast \ni = i[:, jnp.newaxis]\nj = j[jnp.newaxis, :]\n\nx = x.at[i, j].set(y)\nprint(x)\n[[0 1 2 0 0 0]\n [0 0 0 0 0 0]\n [0 0 0 0 0 0]\n [0 3 4 0 0 0]]\nHere the i index has shape (2, 1), and the j index has shape (1, 2), and via broadcasting rules they index a 2x2 noncontiguous subgrid of the array x, which you can then set to the contents of y in a single statement."
        ],
        "link": "https://stackoverflow.com/questions/78985089/modifying-multiple-dimensions-of-jax-array-simultaneously"
    },
    {
        "title": "Mapping Over Arrays of Functions in JAX",
        "question": "What is the most performant, idiomatic way of mapping over arrays of functions in JAX?\nContext: This GitHub issue shows a way to apply vmap to several functions using lax.switch. The example is reproduced below:\nfrom jax import lax, vmap\nimport jax.numpy as jnp\n\ndef func1(x):\n  return 2 * x\n\ndef func2(x):\n  return -2 * x\n\ndef func3(x):\n  return 0 * x\n\nfunctions = [func1, func2, func3]\nindex = jnp.arange(len(functions))\nx = jnp.ones((3, 5))\n\nvmap_functions = vmap(lambda i, x: lax.switch(i, functions, x))\nvmap_functions(index, x)\n# DeviceArray([[ 2.,  2.,  2.,  2.,  2.],\n#              [-2., -2., -2., -2., -2.],\n#              [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)\nMy specific questions are:\nIs this (currently) the most idiomatic way of mapping over arrays of functions in JAX?\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)",
        "answers": [
            "For the kind of operation you're doing, where the functions are applied over full axes of an array in a way that's known statically, you'll probably get the best performance via a simple Python loop:\ndef map_functions(functions: list[Callable[[Array], Array], x: Array) -> Array:\n  assert len(functions) == x.shape[0]\n  return jnp.array([f(row) for f, row in zip(functions, x)])\nThe method based on switch is designed for the more general case where the structure of the indices is not known statically.\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)\nvmap of switch is implemented via select, which will compute the output of each function for the full input array before selecting just the pieces needed to construct the output, so if the functions are expensive to compute, it may lead to longer runtimes."
        ],
        "link": "https://stackoverflow.com/questions/78980521/mapping-over-arrays-of-functions-in-jax"
    },
    {
        "title": "JAX TypeError: 'Device' object is not callable",
        "question": "I found a piece of JAX codes from few years ago.\nimport jax\nimport jax.random as rand\n\ndevice_cpu = None\n\ndef do_on_cpu(f):\n    global device_cpu\n    if device_cpu is None:\n        device_cpu = jax.devices('cpu')[0]\n\n    def inner(*args, **kwargs):\n        with jax.default_device(device_cpu):\n            return f(*args, **kwargs)\n    return inner\n\nseed2key = do_on_cpu(rand.PRNGKey)\nseed2key.__doc__ = '''Same as `jax.random.PRNGKey`, but always produces the result on CPU.'''\nand I call it with:\nkey = seed2key(42)\nBut it results in TypeError:\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 14\n---> 14 key = seed2key(42)\n\nFile ~/bert-tokenizer-cantonese/lib/seed2key.py:12, in do_on_cpu.<locals>.inner(*args, **kwargs)\n     11 def inner(*args, **kwargs):\n---> 12     with jax.default_device(device_cpu):\n     13         return f(*args, **kwargs)\n\nTypeError: 'Device' object is not callable\nI think the function has breaking changes after version upgrade.\nCurrent versions:\njax 0.4.31\njaxlib 0.4.31\n(latest version at the moment of writing)\nHow can I change the codes to avoid the error? Thanks.",
        "answers": [
            "This code works fine in all recent versions of JAX: jax.default_device is a configuration function designed to be used as a context manager.\nI can reproduce the error you're seeing if I add this to the top of your script:\njax.default_device = jax.devices('cpu')[0]  # wrong!\nI suspect you inadvertently executed something similar to this at some point earlier in your notebook session. Try restarting your notebook runtime and rerunning just your valid code."
        ],
        "link": "https://stackoverflow.com/questions/78951225/jax-typeerror-device-object-is-not-callable"
    },
    {
        "title": "Using Jax Jit on a method as decorator versus applying jit function directly",
        "question": "I guess most people familiar with jax have seen this example in the documentation and know that it does not work:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit  # <---- How to do this correctly?\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\nc.calc(3)  \n3 workarounds are mentioned, but it appears that applying jit as a function directly, rather than a decorator works fine as well. That is, JAX does not complain about not knowing how to deal with the CustomClass type of self:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  # No decorator here !\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\n6 # works fine!\nAlthough not documented (which it maybe should be?), this appears to function identical to marking self as static via @partial(jax.jit, static_argnums=0), in that changing self does nothing for subsequent calls, i.e.:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\n6\n6 # no update\nSo I originally assumed that decorators in general might just deal with self as a static parameter when applying them directly. Because the method might be saved to another variable with a specific instance (copy) of self. As a sanity check, I checked if non-jit decorators indeed do this as well, but this appears not to be the case, as the below non-jit \"decorated\" function happily deals with changes to self:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\n\ncustom = CustomClass(2, True)\ndecorated_calc = decorator(custom.calc)\nprint(decorated_calc(3))\ncustom.mul = False\nprint(decorated_calc(3))\n6\n3\nI saw some other questions about applying decorators directly as functions versus decorator style (e.g. here and here), and there it is mentioned there is a slight difference in the two versions, but this should almost never matter. I am left wondering what it is about the jit decorator that makes these versions behave so differently, in that JAX.jit cán deal with the self type if not in decorated style. If anyone has an answer, that would be much appreciated.",
        "answers": [
            "Decorators have nothing to do with static arguments: static arguments are a concept specific to jax.jit.\nBacking up, you should keep in mind that whenever jax.jit compiles a function, it caches the compilation artifact based on several quantites, including:\nthe ID of the function or callable being compiled\nthe static attributes of any non-static arguments, such as shape and dtype\nthe hash of any arguments marked static via static_argnums or static_argnames\nthe value of any global configurations that would affect outputs\nWith this in mind, let's examine this snippet:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\nthe reason that jitted_calc doesn't update when you update attributes of c is because nothing related to the cache key has changed: (1) the function ID is the same, (2) the shape and dtype of the argument is unchanged, (3) there are no static arguments, (4) no global configurations have changed. Thus the previous cached compilation artifact (with the previous value of mul) is executed again. This is the primary reason I didn't mention this strategy in the doc you linked to: it's rarely the behavior that users would want.\nThis approach of wrapping the bound method in JIT is incidentally similar to wrapping the method definition with @partial(jit, static_argnums=0), but the details are not the same: in the static_argnums version, self is marked as a static argument, and so its hash becomes part of the JIT cache. The default __hash__ method for a class is simply based on the ID of the instance, and so changing c.mul does not change the hash, and does not trigger re-compilation. You can see an example of how to rectify this under Strategy 2 in the doc you linked to: basically, define appropriate __hash__ and __eq__ methods for the class:\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def __hash__(self):\n    return hash((self.x, self.mul))\n\n  def __eq__(self, other):\n    return (isinstance(other, CustomClass) and\n            (self.x, self.mul) == (other.x, other.mul))\nIn your last example, you define this:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\nThis code does not use jax.jit at all. The fact that changes to c.mul lead to changes in outputs has nothing to do with decorator syntax, but rather has to do with the fact that there is no JIT cache in play here.\nI hope that's all clear!"
        ],
        "link": "https://stackoverflow.com/questions/78918066/using-jax-jit-on-a-method-as-decorator-versus-applying-jit-function-directly"
    },
    {
        "title": "Zero length error of non-zero length array",
        "question": "I'm writing environment for rl agent training.\nMy env.step method takes as action array with length 3\n    def scan(self, f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n\n    def step_env(\n        self,\n        key: chex.PRNGKey,\n        state: EnvState,\n        action: Union[int, float, chex.Array],\n        params: EnvParams,\n    ) -> Tuple[chex.Array, EnvState, jnp.ndarray, jnp.ndarray, Dict[Any, Any]]:\n        \n        c_action = jnp.clip(action,\n                          params.min_action, \n                          params.max_action)\n        \n        _, m1 = self.scan(self.Rx, 0, action[0])\n        _, m2 = self.scan(self.Rx, 0, action[1])\n        _, m3 = self.scan(self.Rx, 0, action[2])\nI vectorize the env.step using and then call it\nobsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(rng_step,\n                                                                                          env_state,\n                                                                                          action,\n                                                                                          env_params)\nBut I got error\nCell In[9], line 65, in PCJ1_0.scan(self, f, init, xs, length)\n     63 ys = []\n     64 print(xs)\n---> 65 for x in xs:\n     66     carry, y = f(carry, x)\n     67     ys.append(y)\n\n    [... skipping hidden 1 frame]\n\nFile ~/anaconda3/envs/jax/lib/python3.10/site-packages/jax/_src/lax/lax.py:1592, in _iter(tracer)\n   1590 def _iter(tracer):\n   1591   if tracer.ndim == 0:\n-> 1592     raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n   1593   else:\n   1594     n = int(tracer.shape[0])\n\nTypeError: iteration over a 0-d array\nHow is it possible? If I plot the action array in the scan function I got array with length 5 (I vectored env.step for 5 envs), the length!=0\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0",
        "answers": [
            "When you print your value, it gives this:\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0\nHere float32[] tells you that this is a tracer with dtype float32 and shape []: that is, your array is zero-dimensional within the context of the vmapped function.\nThe purpose of vmap is to efficiently map a function over an axis of an array, so that within the function evaluation the array has one less dimension than it does outside the vmapped context. You can see that this way:\n>>> import jax\n\n>>> def f(x):\n...  print(f\"{x.shape=}\")\n...  print(f\"{x=}\")\n...\n>>> x = jax.numpy.arange(4.0)\n\n>>> f(x)\nx.shape=(4,)\nx=Array([0., 1., 2., 3.], dtype=float32)\n\n>>> jax.vmap(f)(x)\nx.shape=()\nx=Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0., 1., 2., 3.], dtype=float32)\n  batch_dim = 0\nIf you're passing a 1D input into your function and you want to manipulate the full 1D array within your function (instead of evaluating the function element-by-element), then it sounds like you should remove the vmap."
        ],
        "link": "https://stackoverflow.com/questions/78838517/zero-length-error-of-non-zero-length-array"
    },
    {
        "title": "Jax jitting of kd-tree code taking an intractably long amount of time",
        "question": "I've written myself into a corner with the following situation:\nI'm running an optimiser which requires smooth gradients to work, and I'm using Jax for automatic differentiation. Since this code is Jax jitted, this means that anything connected to it has to be Jax jit traceable.\nI need to interpolate a function to use with the optimiser, but can't use the Scipy library as it isn't compatable with Jax (there's a jax.scipy.interpolate.RegularGridInterpolator implementation, but this isn't smooth - it only supports linear and nearest neighbour interpolation).\nThis means that I'm having to write my own Jax-compatible smooth interpolator, which I'm basing off the Scipy RBFInterpolator code. The implementation of this is very nice - it uses a kd-tree to find the nearest neighbours of a queried point in space, and then uses these to construct a local interpolation. This means that I also need to write a Jax-compatable kd-tree class (the Scipy one also isn't compatible with Jax), which I've done.\nThe problem comes with jit-compiling the kd-tree code. I've written it in the 'standard way', using objects for the tree nodes with left and right node fields for the children. At the leaf nodes, these fields have None values to signify the absense of children.\nThe code runs and is functionally correct, however jit-compiling it takes a long time: 72 seconds for a tree of 64 coordinates, 131 seconds for 343 coordinates, ... and my intended dataset has over 14 million points. I think internally Jax is tracing every single possible path through the tree, which is why it's taking so long. The results are that it's blazingly quick: 0.0075s for kd-tree 10-point retrieval vs 0.4s for a brute force search over all of the points (for 343 points). These are the kind of speeds I'm hoping to obtain for use in the optimiser (without jitting it will be too slow). However it doesn't seem possible if the compilation times are going to continue to grow as experienced.\nI thought that the problem might lie in the structure of the tree, with lots of different objects to be stored, so have also implemented a kd-tree search algorithm where the tree is represented by a set of Jax-numpy arrays (e.g. coord, value, left and right; where each index corresponds to a point in the tree) and iteration rather than recursion is used to do the tree search (this was a challenge but it works!). However, converting this to work with jit (changing if-statements for jax.lax.cond) is going to be complicated, and before I start I was wondering if it's going to be worth it - surely I'll have the same problem: Jax will trace all branches of the tree until the 'null terminators' (-1 values in the left and right arrays) are reached, and it will still take a very long time to compile. I've been investigating structures like jax.lax.while_loop, in case they might help?\n(I've also written a hybrid of the two approaches, with an array-based tree and a recursion-based algorithm. In this case the tracing goes into an infinite loop, I think because of the fact that the null-terminator is -1 rather than None. But the arrays should be known statically (they don't change after construction, and belong to an object which is marked as a static input), so maybe the solution lies in this and I'm doing something wrong.)\nI was wondering if I'm doing anything which is obviously wrong (or if my understanding is wrong), and if there is anything I can do to speed it up? Is it just to be expected that the compile time would be so high when there are so many code paths to trace? I don't suppose I could even build the jitted function only once and then save it?\nI'm concerned that the only solution may be to rewrite the optimiser code so that it doesn't use Jax (e.g. if I hard-code the derivatives, and rewrite some of the code so that it operates on arrays directly instead of being vectorised across the inputs).\nThe code is available here: https://github.com/FluffyCodeMonster/jax_kd_tree\nAll three varieties described are given: the node-based tree with recursion, the array-based tree with iteration, and the array-based tree with recursion. The former works, but is very slow to jit compile as the number of points in the tree increases; the second also works, but is not written in a jit-able way yet. The last is written to be jitted, but can't jit compile as it gets into an infinite recursion.\nI really need to get this working urgently so that I can obtain the optimisation results.",
        "answers": [
            "All python-level control flow, including if statements, for and while loops, and recursion, is traced in full and flattened into a linear set of commands that is then sent to the compiler. If you are attempting a tree traversal via Python-level control flow, you're going to end up with very large programs that take a very long time to compile. This issue is discussed broadly at JAX sharp bits: control flow.\nIf you want to traverse a KD tree under JIT without the long compilation, you'll have to use an iterative approach with XLA control-flow operators such as jax.lax.fori_loop and jax.lax.while_loop.\nAlternatively, you might think about instead using jax.pure_callback in order to run neighbors queries using scipy on the host. There is some discussion of this at Exploring pure_callback. It's not super efficient—each call will incur some host synchronization and data movement overhead—but it can be a pretty effective solution for things like this, particularly if you're running on CPU."
        ],
        "link": "https://stackoverflow.com/questions/78791013/jax-jitting-of-kd-tree-code-taking-an-intractably-long-amount-of-time"
    },
    {
        "title": "Execution of conditional branches causing errors in Jax (kd-tree implementation)",
        "question": "I'm writing a kd-tree in Jax, and using custom written Node objects for the tree elements. Each Node is very simple, with a single data field (for holding numeric values) and left and right fields which are references to other Nodes. A leaf Node is identified as one for which the left and right fields are None.\nThe code performs conditional checks on the values of left and right as part of the tree traversal process - e.g. it will only try to traverse down the left or right branch of a node's subtree if it actually exists. Doing checks like if (current_node.left is not None) (or does it have to be jax.numpy.logical_not(current_node.left is None) in Jax - I've tried both?) was fine for this, but since converting the if statements to jax.lax.cond(...) I've been getting the error AttributeError: 'NoneType' object has no attribute 'left'.\nI think the situation might be like in the following minimum working example:\nimport jax\nimport jax.numpy as jnp\n\ndef my_func(val):\n    return 2*val\n\n@jax.jit\ndef test_fn(a):\n    return jax.lax.cond(a is not None,\n                lambda: my_func(a),\n                lambda: 0)\n\nprint(test_fn(2))       # Prints 4\n# in test_fn(), a has type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\nprint(test_fn(None))    # TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n# in test_fn(), a has type <class 'NoneType'>\nIn this code, if the Jax cond statement were a regular if statement, my_func() wouldn't even be called when a is None, and no error would be raised. To the best of my understanding, Jax tries to trace the function, meaning that all branches are executed, and this leads to my_func() being called with None (when a is None), causing the error. I believe a similar situation is arising in my tree code, where conditional branches are being executed even though .left and /or .right are None, and a traditional if statement wouldn't lead to execution of the code branches.\nIs my understanding correct, and what could I do about this issue? Strangely, the minimum working example code also has the problem when the @jax.jit decorator is omitted, suggesting that both branches are still being traced.\nAs a related point, is the tree structure 'baked into' the Jax/XLA code? I have noticed that when using larger trees the code takes longer to be jit-compiled, which makes me concerned that this might not be a valid approach with the very large number of points I need to represent (about 14,000,000). I would use the regular Scipy kd-tree implementation, but this isn't compatible with Jax unfortunately, and the rest of my code requires it. I might ask this as a separate question for clarity.",
        "answers": [
            "If you are using jax.lax.cond, the input must have a valid type for both branches. When a is None, the first branch is invalid because None * 2 results in an error.\nIn this case, the condition a is not None is known statically, so rather than using lax.cond you can use a regular if statement:\n@jax.jit\ndef test_fn(a):\n  return my_func(a) if a is not None else 0"
        ],
        "link": "https://stackoverflow.com/questions/78784486/execution-of-conditional-branches-causing-errors-in-jax-kd-tree-implementation"
    },
    {
        "title": "JAX/Equinox pipeline slows down after adding an integer argument to a loss function",
        "question": "I have the following training pipeline in JAX and Equinox. I want to pass a batch index to the loss function in order to apply different logic depending on index. Without batch index training loop works for about 15 sec, but if I pass an index, then it slows down for about an hour. Could you explain, why this happens? I'm new to JAX, sorry.\ndef fit_cv(model: eqx.Module, \n           dataloader: jdl.DataLoader, \n           optimizer: optax.GradientTransformation, \n           loss: tp.Callable, \n           n_steps: int = 1000):\n    \n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    dloss = eqx.filter_jit(eqx.filter_value_and_grad(loss))\n    \n    @eqx.filter_jit\n    def step(model, data, opt_state, batch_index):\n        loss_score, grads = dloss(model, data, batch_index)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state, loss_score\n    \n    loss_history = []\n    for batch_index, batch in tqdm(zip(range(n_steps), dataloader), total=n_steps):\n        if batch_index >= n_steps:\n            break\n        batch = batch[0] # dataloader returns tuple of size (1,)\n        model, opt_state, loss_score = step(model, batch, opt_state, batch_index)\n        loss_history.append(loss_score)\n    return model, loss_history\nLoss function has the following signature\ndef loss(self, model: eqx.Module, data: jnp.ndarray, batch_index: int):\nIn particular, I want to switch between two loss functions after N steps. So, probably, I need to know the concrete value of a batch index.\nSolution:\nTo use jax.lax.cond\n        condition = (batch_index // self.switch_steps) % 2 == 1\n        ...\n        loss_value = jax.lax.cond(\n            jnp.all(condition),\n            lambda: loss1(inputs),\n            lambda: loss2(inputs),\n        )\n        return loss_value",
        "answers": [
            "I suspect the issue is excessive recompilation. You are using filter_jit, which according to the docs has the following property:\nAll JAX and NumPy arrays are traced, and all other types are held static.\nEach time a static argument to a JIT-compiled function changes, it triggers a re-compilation. This means that if batch_index is a Python int, then each time you call your function with a new value, the function will be recompiled.\nAs a fix, I would recommend using regular old jax.jit, which requires you to explicitly specify static arguments, instead of the function trying to make the choice for you (potential surprises like this are one of the reasons why JAX has made this design choice - as the Zen of Python says, explicit is better than implicit). If you use jax.jit and don't mark batch_index as static, you shouldn't see this recompilation penalty.\nAlternatively, if you want to keep using filter_jit, then you could change your step call to this:\nstep(model, batch, opt_state, jnp.asarray(batch_index))\nWith this change, filter_jit will no longer decide to make the batch index static. Of course, either of these suggestions would require that that loss is compatible with dynamic batch_index, which can't be determined from the information included in your question."
        ],
        "link": "https://stackoverflow.com/questions/78775635/jax-equinox-pipeline-slows-down-after-adding-an-integer-argument-to-a-loss-funct"
    },
    {
        "title": "Dictionary indexing with Numpy/Jax",
        "question": "I'm writing an interpolation routine and have a dictionary which stores the function values at the fitting points. Ideally, the dictionary keys would be 2D Numpy arrays of the fitting point coordinates, np.array([x, y]), but since Numpy arrays aren't hashable these are converted to tuples for the keys.\n# fit_pt_coords: (n_pts, n_dims) array\n# fn_vals: (n_pts,) array\ndef fit(fit_pt_coords, fn_vals):\n    pt_map = {tuple(k): v for k, v in zip(fit_pt_coords, fn_vals)}\n    ...\nLater in the code I need to get the function values using coordinates as keys in order to do the interpolation fitting. I'd like this to be within @jax.jited code, but the coordinate values are of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>, which can't be converted to a tuple. I've tried other things, like creating a dictionary key as (x + y, x - y), but again this requires concrete values, and calling .item() results in an ConcretizationTypeError.\nAt the moment I've @jax.jited all of the code I can, and have just left this code un-jitted. It would be great if I could jit this code as well however. Are there any better ways to do the dictionary indexing (or better Jax-compatible data structures) which would allow all of the code to be jitted? I am new to Jax and still understading how it works, so I'm sure there must be better ways of doing it...",
        "answers": [
            "There is no way to use traced JAX values as dictionary keys. The problem is that the key values will not be known until runtime within the XLA compiler, and XLA has no dictionary-like data structure that such lookups can be lowered to.\nThere are imperfect solutions, such as keeping the dictionary on the host and using something like io_callback to do the dict lookups on host, but this approach comes with performance penalties that will likely make it impractical.\nUnfortunately, your best approach for doing this efficiently under JIT would probably be to switch to a different interpolation algorithm that doesn't depend on hash table lookups.",
            "I agree with @jakevdp that this might not be the best solution. Python is not the quickest when built-ins are looped over.\nPython can do anything... Except for-loops. We use numpy for that.\nMaybe a pandas.DataFrame with columns [\"x\", \"y\", \"v\"] would be a way to go.\nCan you not use scipy.interpolate's functions?"
        ],
        "link": "https://stackoverflow.com/questions/78767142/dictionary-indexing-with-numpy-jax"
    },
    {
        "title": "Taking derivatives with multiple inputs in JAX",
        "question": "I am trying to take first and second derivatives of functions in JAX however, my ways of doing that give me the wrong number or zeros. I have an array with two columns for each variable and two rows for each input\nimport jax.numpy as jnp\nimport jax\n\nrng = rng = jax.random.PRNGKey(1234)\narray = jax.random.normal(rng, (2,2))\nTwo test functions\ndef F1(arr):\n    return 1/arr\n\ndef F2(arr):\n    return jnp.array([arr[0]**2 + arr[1]**3])\nand two methods of taking first and second derivatives, one using jax.grad()\ndef dF_m1(arr, F):\n    return jax.grad(lambda arr: F(arr)[0])(arr)\n\ndef ddF_m1(arr, F, dF):\n    return jax.grad(lambda arr: dF(arr, F)[0])(arr)\nand another using jax.jacobian()\ndef dF_m2(arr, F):\n    jac = jax.jacobian(lambda arr: F(arr))(arr)\n    return jnp.diag(jac)\n\ndef ddF_m2(arr, F, dF):\n    hess = jax.jacobian(lambda arr: dF(arr, F))(arr)\n    return jnp.diag(hess)\nComputing the first and second derivative (and error) of each function using both methods gives the following\nexact_dF1  = (-1/array**2)\nexact_ddF1 = (2/array**3)\n\nprint(\"Function 1 using all grad()\")\ndF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)\nddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)\nprint(dF1_m1  - exact_dF1,\"\\n\")\nprint(ddF1_m1 - exact_ddF1,\"\\n\")\n\nprint(\"Function 1 using all jacobian()\")\ndF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)\nddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)\nprint(dF1_m2  - exact_dF1,\"\\n\")\nprint(ddF1_m2 - exact_ddF1,\"\\n\")\nOutput\nFunction 1 using all grad()\n[[ 0.         48.43877   ]\n [ 0.          0.62903005]] \n\n[[  0.        674.248    ]\n [  0.          0.9977852]] \n\nFunction 1 using all jacobian()\n[[0. 0.]\n [0. 0.]] \n\n[[0. 0.]\n [0. 0.]] \nand\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\nprint(\"Function 2 using all grad()\")\ndF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)\nddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)\nprint(dF2_m1  - exact_dF2,\"\\n\")\nprint(ddF2_m1 - exact_ddF2,\"\\n\")\n\nprint(\"Function 2 using all jacobian()\")\ndF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)\nddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)\nprint(dF2_m2  - exact_dF2,\"\\n\")\nprint(ddF2_m2 - exact_ddF2,\"\\n\")\nOutput\nFunction 2 using all grad()\n[[0. 0.]\n [0. 0.]] \n\n[[0.         0.86209416]\n [0.         7.5651155 ]] \n\nFunction 2 using all jacobian()\n[[ 0.         -0.10149619]\n [ 0.         -6.925739  ]] \n\n[[0.        2.8620942]\n [0.        9.565115 ]] \nI would prefer only to use jax.grad() for something like F1 but it seems right now that only jax.jacobian is working. The whole reason for this is that I need to calculate higher-order derivatives of a neural network with respect to its inputs. Thank you for any help.",
        "answers": [
            "Assuming exact_* is what you're attempting to compute, you're going about it in the wrong way. Your indexing within the differentiated functions (i.e. ...[0]) is removing some of the elements that you're trying to compute.\nWhat exact_dF1 and exact_ddF1 are computing is element-wise first and second derivatives for 2D inputs. You can compute this using either grad or jacobian by applying vmap twice (once for each input dimension). For example:\nexact_dF1  = (-1/array**2)\ngrad_dF1 = jax.vmap(jax.vmap(jax.grad(F1)))(array)\njac_dF1 = jax.vmap(jax.vmap(jax.jacobian(F1)))(array)\nprint(jnp.allclose(exact_dF1, grad_dF1))  # True\nprint(jnp.allclose(exact_dF1, jac_dF1))  # True\n\nexact_ddF1 = (2/array**3)\ngrad_ddF1 = jax.vmap(jax.vmap(jax.grad(jax.grad(F1))))(array)\njac_ddF1 = jax.vmap(jax.vmap(jax.jacobian(jax.jacobian(F1))))(array)\nprint(jnp.allclose(exact_ddF1, grad_ddF1))  # True\nprint(jnp.allclose(exact_ddF1, jac_ddF1))  # True\nWhat exact_dF2 and exact_ddF2 are computing is a row-wise jacobian and hessian of a 2D->1D mapping. By its nature, this is difficult to compute using jax.grad, which is meant for functions with scalar output, but you can compute it using the jacobian this way:\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\njac_dF2 = jax.vmap(jax.jacobian(lambda a: F2(a)[0]))(array)\njac_ddF2_full = jax.vmap(jax.jacobian(jax.jacobian(lambda a: F2(a)[0])))(array)\njac_ddF2 = jax.vmap(jnp.diagonal)(jac_ddF2_full)\nprint(jnp.allclose(exact_dF2, jac_dF2))  # True\nprint(jnp.allclose(exact_ddF2, jac_ddF2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax"
    },
    {
        "title": "weird shape when indexing a jax array",
        "question": "I am experiencing a weird issue when indexing a Jax array using a list. If I place a debugger in the middle of my code, I have the following:\nThis array are created by convering a numpy array.\nHowever, when I try this in a new instance of Python, I have the correct behavior: [\nWhat is it happening?",
        "answers": [
            "This is working as expected. JAX follows the semantics of NumPy indexing, and in the case of advanced indexing with multiple scalars and integer arrays separated by slices, the indexed dimensions are combined via broadcasting and moved to the front of the output array. You can read more about the details of this kind of indexing in the NumPy documentation: https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing. In particular:\nTwo cases of index combination need to be distinguished:\nThe advanced indices are separated by a slice, Ellipsis or newaxis. For example x[arr1, :, arr2].\nThe advanced indices are all next to each other. For example x[..., arr1, arr2, :] but not x[arr1, :, 1] since 1 is an advanced index in this regard.\nIn the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array\nThe code in your program falls under the first case, while the code in your separate interpreter falls under the second case. This is why you're seeing different results.\nHere's a concise example of this difference:\n>>> import numpy as np\n>>> x = np.zeros((3, 4, 5))\n\n>>> x[0, :, [1, 2]].shape  # size-2 dimension moved to front\n(2, 4)\n\n>>> x[:, 0, [1, 2]].shape  # size-2 dimension not moved to front\n(3, 2)"
        ],
        "link": "https://stackoverflow.com/questions/78741064/weird-shape-when-indexing-a-jax-array"
    },
    {
        "title": "Jax vmap with lax scan having different sequence length in batch dimension",
        "question": "I have this following code , where my sim_timestep is in batch I am not able to run this since the lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) ) requires the concrete array , but since I have vmapped the state_predictor function the sim_timestep is being as a tracedArray . Any help would be greatly appreciated . Thanks all\nfrom jax import random\nfrom jax import lax\nimport jax\nimport jax.numpy as jnp\nimport pdb\n\n\ndef fwd_dynamics(x_u, xs):\n    x0,uk =  x_u\n    Delta_T = 0.001\n    lwb = 1.2\n    psi0=x0[2][0]\n    v0= x0[3][0]\n    vdot0 = uk[0][0]\n    delta0 = uk[1][0]\n    thetadot0 = uk[2][0]\n        \n    xdot= jnp.asarray([[v0*jnp.cos(psi0) ],\n        [v0*jnp.sin(psi0)] ,\n        [v0*jnp.tan(delta0)/(lwb)],\n        [vdot0],\n        [thetadot0]])\n    x_next = x0 + xdot*Delta_T\n    return (x_next,uk), x_next  # (\"carryover\", \"accumulated\")\n\n\ndef state_predictor( xk,uk ,sim_timestep):\n    (x_next,_), _ = lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) )\n    return x_next\n\nlow = 0  # Adjust minimum value as needed\nhigh = 100  # Adjust maximum value as needed\nkey = jax.random.PRNGKey(44)\n\nsim_time = jax.random.randint(key, shape=(10, 1), minval=low, maxval=high)\n\nxk = jax.random.uniform(key, shape=(10,5, 1))\nuk = jax.random.uniform(key, shape=(10,2, 1))\n\nstate_predictor_vmap = jax.jit(jax.vmap(state_predictor,in_axes= 0 ,out_axes=0 ))\nx_next = state_predictor_vmap( xk,uk ,sim_time)\nprint(x_next.shape)\nI tried to solve it by above code , hoping to get alternative way to achieve the same functionality.",
        "answers": [
            "What you're asking to do is impossible: scan lengths must be static, and vmapped values are non-static by definition.\nWhat you can do instead is replace your scan with a fori_loop or a while_loop, and then the loop boundary does not need to be static. For example, if you implement your function this way and leave the rest of your code unchanged, it should work:\ndef state_predictor(xk, uk, sim_timestep):\n  body_fun = lambda i, x_u: fwd_dynamics(x_u, i)[0]\n  x_next, _ = lax.fori_loop(0, sim_timestep[0], body_fun, (xk, uk))\n  return x_next"
        ],
        "link": "https://stackoverflow.com/questions/78713478/jax-vmap-with-lax-scan-having-different-sequence-length-in-batch-dimension"
    },
    {
        "title": "Colab, Jax, and GPU: why does cell execution take 60 seconds when %%timeit says it only takes 70 ms?",
        "question": "As the basis for a project on fractals, I'm trying to use GPU computation on Google Colab using the Jax library.\nI'm using Mandelbrot on all accelerators as a model, and I'm encountering a problem.\nWhen I use the %%timeit command to measure how long it takes to calculate my GPU function (same as in the model notebook), the times are entirely reasonable, and in line with expected results -- 70 to 80 ms.\nBut actually running %%timeit takes something like a full minute. (By default, it runs the function 7 times in a row and reports the average -- but even that should take less than a second.)\nSimilarly, when I run the function in a cell and output the results (a 6 megapixel image), it takes around 60 seconds for the cell to finish -- to execute a function that supposedly only takes 70-80 ms.\nIt seems like something is producing a massive amount of overhead, that also seems to scale with the amount of computation -- e.g. when the function contains 1,000 iterative calculations %%timeit says it takes 71 ms while in reality it takes 60 seconds, but with just 20 iterations %%timeit says it takes 10 ms while in reality it takes about 10 seconds.\nI am pasting the code below, but here is a link to the Colab notebook itself -- anyone can make a copy, connect to a \"T4 GPU\" instance, and run it themselves to see.\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax\n\nassert len(jax.devices(\"gpu\")) == 1\n\ndef run_jax_kernel(c, fractal):\n    z = c\n    for i in range(1000):\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n    return fractal\n\nrun_jax_gpu_kernel = jax.jit(run_jax_kernel, backend=\"gpu\")\n\ndef run_jax_gpu(height, width):\n\n    mx = -0.69291874321833995150613818345974774914923989808007473759199\n    my = 0.36963080032727980808623018005116209090839988898368679237704\n    zw = 4 / 1e3\n\n    y, x = jax.numpy.ogrid[(my-zw/2):(my+zw/2):height*1j, (mx-zw/2):(mx+zw/2):width*1j]\n    c = x + y*1j\n    fractal = jax.numpy.full(c.shape, 1000, dtype=np.int32)\n    return np.asarray(run_jax_gpu_kernel(c, fractal).block_until_ready())\nTakes about a minute to produce an image:\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nax.imshow(run_jax_gpu(2000, 3000));\nTakes about a minute to report that the function only takes 70-80 ms to execute:\n%%timeit -o\nrun_jax_gpu(2000, 3000)",
        "answers": [
            "The first thing to realize is that %timeit will execute your code multiple times, and then return an average of the times for each run. The number of times it will execute is determined dynamically by the time of the first run.\nThe second thing to realize is that JAX code is just-in-time (JIT) compiled, meaning that on the first execution of any particular function, you will incur a one-time compilation cost. Many things affect compilation cost, but functions that use large for loops (say, 1000 or more repetitions) tend to compile very slowly, because JAX unrolls those loops before passing the operations to XLA for compilation, and XLA compilation scales approximately quadratically with the number of unrolled operations (there is some discussion of this at JAX Sharp Bits: Control Flow).\nPut these together, and you'll see why you're observing the timings that you are: under %timeit, your first run results in a very long compilation, and subsequent runs are very fast. The resulting average time is printed, and is very short compared to the first run, and to the overall time.\nWhen you run your code a single time to plot the results, you are mainly seeing the compilation time. Because it is not amortized away by multiple calls to your function, that compilation time is long.\nThe solution would be to avoid writing Python for loops in your function in order to avoid the long compilation time: one possibility would be to use lax.fori_loop, which allows you to write iterative computations without the huge compilation time penalty, though it will incur a runtime penalty on GPU compared to the for loop solution because the operations are executed sequentially rather than being parallelized by the compiler. In your case it might look like this:\ndef run_jax_kernel(c, fractal):\n    z = c\n    def body_fun(i, carry):\n        z, fractal = carry\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n        return (z, fractal)\n    z, fractal = jax.lax.fori_loop(0, 1000, body_fun, (z, fractal))\n    return fractal"
        ],
        "link": "https://stackoverflow.com/questions/78708817/colab-jax-and-gpu-why-does-cell-execution-take-60-seconds-when-timeit-says"
    },
    {
        "title": "Implementing a vectorized function over LinkedLists using Jax’s vmap function",
        "question": "Trying to implement a vectorized version of an algorithm (from computational geometry) using Jax. I have made the minimum working example using a LinkedList to particularly express my query (I am using a DCEL otherwise).\nThe idea is that this vectorized algorithm will be checking certain criteria over a DCEL. I have substituted this “criteria checking procedure” with a simple summation algorithm for the sake simplicity.\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\n\nclass Node: \n  \n    # Constructor to initialize the node object \n    def __init__(self, data): \n        self.data = data \n        self.next = None\n\nclass LinkedList: \n  \n    def __init__(self): \n        self.head = None\n  \n    def push(self, new_data): \n        new_node = Node(new_data) \n        new_node.next = self.head \n        self.head = new_node \n\n    def printList(self): \n        temp = self.head \n        while(temp): \n            print (temp.data,end=\" \") \n            temp = temp.next\n\ndef summate(list) :\n    prev = None\n    current = list.head\n    sum = 0\n    while(current is not None): \n        sum += current.data\n        next = current.next\n        current = next\n    return sum\n\nlist1 = LinkedList() \nlist1.push(20) \nlist1.push(4) \nlist1.push(15) \nlist1.push(85) \n\nlist2 = LinkedList() \nlist2.push(19)\nlist2.push(13)\nlist2.push(2)\nlist2.push(13)\n\n#list(map(summate, ([list1, list2])))\n\nvmap(summate)(jnp.array([list1, list2]))\nI get the following error.\n TypeError: Value '<__main__.LinkedList object at 0x1193799d0>' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.\nThe objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion. I have implemented what I want in basic Python, but I want to do it in Jax as there is a larger probabilistic function which I will be using this subprocedure for (it’s a Markov Chain).\nIt might be the case that I am completely unable to work over such data structures over Jax as the error suggests that only numeric types are supported. Can I use pytrees in some way to mitigate this constraint?\nIt will be tempting to suggest I use a simple list from jnp, but I am using Linkedlist just as an example of a simple(st) data structure. As mentioned earlier, am actually working over a DCEL.\nPS : the Linkedlist code was taken from GeeksForGeeks, as I wanted to come up with a minimum working example quickly.",
        "answers": [
            "The objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion.\nThis goal is not feasible using JAX. You could register your class as a custom Pytree to make it work with JAX functions (see Extending pytrees), but this won't mean you can vectorize an operation over a list of such objects.\nJAX transformations like vmap and jit work for data stored with a struct-of-arrays pattern (e.g. a single LinkedList object containing arrays that represent multiple batched linked lists) not an array-of-structs pattern (e.g. a list of multiple LinkedList objects).\nFurther, the algorithm you're using, based on a while loop, is not compatible with JAX transformations (See JAX sharp bits: control flow), and the dynamically sized tree of nodes will not fit into the static shape constraints of JAX programs.\nI'd love to point you in the right direction, but I think you either need to give up on using JAX, or give up on using dynamic linked lists. You won't be able to do both."
        ],
        "link": "https://stackoverflow.com/questions/78677115/implementing-a-vectorized-function-over-linkedlists-using-jax-s-vmap-function"
    },
    {
        "title": "Simplest equivalent implementation of numpy.ma.notmasked_edges() for use in JAX",
        "question": "I have a square numpy.ndarray and a numpy boolean mask of the same shape. I want to find the first element in each row of the array that is not masked.\nMy code currently relies on numpy.ma.notmasked_edges(), which does exactly what I need. However, I now need to migrate my code to JAX, which has not implemented numpy.ma within jax.numpy.\nWhat would be the simplest way to find the index of the first unmasked element in each row, calling only numpy functions that have been implemented in JAX (which exclude numpy.ma)?\nThe code I'm trying to reproduce is something like:\nimport numpy as np\nmy_array = np.random.rand(5,5)\nmask = (my_array < 0.5)\nmy_masked_array = np.ma.masked_array(my_array, mask=mask)\nnp.ma.notmasked_edges(my_masked_array, axis=1)[0]\nI'm sure there are many ways to do this, but I'm looking for the least unwieldy way.",
        "answers": [
            "Here's a JAX implementation of nonmasked_edges, which takes a boolean mask and returns the same indices returned by the numpy.ma function:\nimport jax.numpy as jnp\n\ndef notmasked_edges(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  alltrue = mask.all(axis=axis).ravel()\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind)[~alltrue] for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel()[~alltrue])\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel()[~alltrue])\n  \n  return [tuple(first), tuple(last)]\nThis will not be compatible with JIT, because the size of the output arrays depend on the values of the mask (rows which have no unmasked value are left out).\nIf you want a JIT-compatible version, you can remove the [~alltrue] indexing, and the first/last index will be returned for rows that have no unmasked value:\ndef notmasked_edges_v2(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind) for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel())\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel())\n\n  return [tuple(first), tuple(last)]\nHere's an example:\nimport numpy as np\nmask = np.array([[True, False, False, True],\n                 [False, False, True, True],\n                 [True, True, True, True]])\n\narr = np.ma.masked_array(np.ones_like(mask), mask=mask)\nprint(np.ma.notmasked_edges(arr, axis=1))\n# [(array([0, 1]), array([1, 0])), (array([0, 1]), array([2, 1]))]\n\nprint(notmasked_edges(mask, axis=1))\n# [(Array([0, 1], dtype=int32), Array([1, 0], dtype=int32)),\n#  (Array([0, 1], dtype=int32), Array([2, 1], dtype=int32))]\n\nprint(notmasked_edges_v2(mask, axis=1))\n# [(Array([0, 1, 2], dtype=int32), Array([1, 0, 0], dtype=int32)),\n#  (Array([0, 1, 2], dtype=int32), Array([2, 1, 3], dtype=int32))]"
        ],
        "link": "https://stackoverflow.com/questions/78660344/simplest-equivalent-implementation-of-numpy-ma-notmasked-edges-for-use-in-jax"
    },
    {
        "title": "Why is Flax Linear layer not identical to matrix multiplication?",
        "question": "Due to the novelty of Flax, NNX, and JAX, there’s not a lot of resources available. I’m running into the following peculiarity:\nx = jnp.random.normal((1,512), key=KEY)\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = layer.kernel@x.squeeze() + layer.bias\nprint(y1==y2) # returns all False\nMy understanding is that matrix multiplication should be identical to a linear / fully connected layer. The discrepancy demonstrated here hinders the inspection of certain behavior (and the implementation of invertible dense layers using jnp.tensorsolve).\nDoes anyone know what causes this discrepancy?",
        "answers": [
            "The matmul should be transposed; also floating point equality checks should be done via approximate rather than exact comparison, because different ways of computing the same result may lead to different floating point rounding errors:\nimport jax\nfrom flax import nnx\n\nKEY = jax.random.key(0)\nx = jax.random.normal(KEY, (1,512))\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = x @ layer.kernel + layer.bias\nprint(jax.numpy.allclose(y1, y2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78659890/why-is-flax-linear-layer-not-identical-to-matrix-multiplication"
    },
    {
        "title": "Using JAX ndarray.at apply(ufunc) with arguments",
        "question": "Can arguments be passed to a jax.numpy.ufunc within a jax.numpy.ndarray.at call?\nThe following is an attempt to replicate jax.numpy.ndarray.at[...].add(...)\nimport jax.numpy as jnp\n\ndef myadd(a,b=1):\n    return a+b\n\numyadd = jnp.frompyfunc(myadd,2,1,identity=0)\n\nx = jnp.arange(4)\n\n# call jnp.add(x,x)\nx.at[:].add(x)\n# [0 2 4 6]\n\n# call umyadd.at\numyadd.at(x, np.arange(x.size), x, inplace=False)\n# [0 2 4 6]\n\n# Default b=1 (can b be passed here?)\nx.at[:].apply(umyadd)\n# [1 2 3 4]",
        "answers": [
            "arr.at[...].apply() only accepts unary functions that map a scalar to a scalar. So you could pass b via closure, as long as it's a scalar; for example:\nx.at[:].apply(lambda a: umyadd(a, 2))\n# [2, 3, 4, 5]\nBut there is no way to pass b=jnp.arange(4) within apply(), because then the applied function no longer maps a scalar to a scalar."
        ],
        "link": "https://stackoverflow.com/questions/78642505/using-jax-ndarray-at-applyufunc-with-arguments"
    },
    {
        "title": "how to log activation values using jax",
        "question": "I am following a jax tutorial that trains mnist using mlp network. I am trying to add an additional code that saves the activation patterns at every layer except the last. Here is the modified code:\nfrom collections import defaultdict\n# this is my activation pattern logger\nclass ActivationLogger:\n    def __init__(self, epoch):\n       self.reset(epoch)\n\n    def __call__(self, layer, activations):\n        D = activations.shape[0]\n        for i in range(D):\n            self.activations[(layer, i)].append(\n                    jax.lax.stop_gradient(activations[i]))\n\n    def reset(self, epoch):\n        self.epoch = epoch\n        self.activations = defaultdict(list)\n\nactivation_logger = ActivationLogger(epoch=1)\n\n...\n\ndef predict(params, image):\n    # per-example predictions\n    activations = image\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        activation_logger(l+1, activations) # <- this was added\n\n    final_w, final_b = params[-1]\n    logits = jnp.dot(final_w, activations) + final_b\n    return logits - logsumexp(logits)\n\nbatched_predict = jax.vmap(\n        predict, \n        in_axes=(None, 0), \n        out_axes=0)\n\n@jax.jit\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\nWhen I run my training code, I keep getting the following error message:\nTracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[].\nThis BatchTracer with object id 7541955024 was created on line:\n  /var/folders/km/3nj8tmq56s16dsgc9_63530r0000gn/T/ipykernel_73750/3494160804.py:12:20 (ActivationLogger.__call__)\nAny suggestions on how to fix this?",
        "answers": [
            "Python functions in JAX code are executed at trace-time, not runtime, and so as written you're not logging concrete runtime values, but rather their abstract trace-time representations.\nIf you want to log runtime values, the best tool is probably jax.debug.callback; for info on using this, I'd suggest starting with External Callbacks in JAX.\nUsing it in your case would look something like this:\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        jax.debug.callback(activation_logger, l+1, activations)\nFor more background on JAX's execution model, and why your function didn't work as expected when executed directly a trace-time, a good place to start is How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78611094/how-to-log-activation-values-using-jax"
    },
    {
        "title": "jax complaining about static start/stop/step",
        "question": "Here is a very simple computation in jax which errors out with complaints about static indices:\ndef get_slice(ar, k, I):\n  return ar[i:i+k]\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2,3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-32-6c60650ce6b7> in <cell line: 1>()\n----> 1 vec_get_slice(arr, 2, jnp.arange(3))\n\n    [... skipping hidden 3 frame]\n\n4 frames\n<ipython-input-29-9528369725c2> in get_slice(ar, k, i)\n      1 def get_slice(ar, k, i):\n----> 2   return ar[i:i+k]\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/array.py in __getitem__(self, idx)\n    346           return out\n    347 \n--> 348     return lax_numpy._rewriting_take(self, idx)\n    349 \n    350   def __iter__(self):\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _rewriting_take(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4602 \n   4603   treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape)\n-> 4604   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n   4605                  unique_indices, mode, fill_value)\n   4606 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4611             unique_indices, mode, fill_value):\n   4612   idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n-> 4613   indexer = _index_to_gather(shape(arr), idx)  # shared with _scatter_update\n   4614   y = arr\n   4615 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _index_to_gather(x_shape, idx, normalize_indices)\n   4854                \"dynamic_update_slice (JAX does not support dynamically sized \"\n   4855                \"arrays within JIT compiled functions).\")\n-> 4856         raise IndexError(msg)\n   4857 \n   4858       start, step, slice_size = _preprocess_slice(i, x_shape[x_axis])\n\nHorrible error output below. I am obviously missing something simple, but what?\n\n\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0, 1, 2], dtype=int32)\n  batch_dim = 0, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([2, 3, 4], dtype=int32)\n  batch_dim = 0, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).",
        "answers": [
            "Indices passed to slices in JAX must be static. Values that are mapped over in vmap are not static: because you're mapping over the start indices, your indices are not static and you see this error.\nThere is good news though: the size of your subarray is controlled by k, which is unmapped in your code and therefore static; it's only the location of the slice (given by I) that is dynamic. This is exactly the situation that jax.lax.dynamic_slicewas designed for, and so you can rewrite your code like this:\nimport jax\nimport jax.numpy as jnp\n\ndef get_slice(ar, k, I):\n  return jax.lax.dynamic_slice(ar, (I,), (k,))\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2, 3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n# Array([[1, 2],\n#        [2, 3],\n#        [3, 4]], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78588301/jax-complaining-about-static-start-stop-step"
    },
    {
        "title": "TypeError: unhashable type: 'ArrayImpl' when trying to use Equinox module with jax.lax.scan",
        "question": "I'm new to Equinox and JAX but wanted to use them to simulate a dynamical system.\nBut when I pass my system model as an Equinox module to jax.lax.scan I get the unhashable type error in the title. I understand that jax expects the function argument to be a pure function but I thought an Equinox Module would emulate that.\nHere is a test script to reproduce the error\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\n\n\nclass EqxModel(eqx.Module):\n    A: jax.Array\n    B: jax.Array\n    C: jax.Array\n    D: jax.Array\n\n    def __call__(self, states, inputs):\n        x = states.reshape(-1, 1)\n        u = inputs.reshape(-1, 1)\n        x_next = self.A @ x + self.B @ u\n        y = self.C @ x + self.D @ u\n        return x_next.reshape(-1), y.reshape(-1)\n\n\ndef simulate(model, inputs, x0):\n    xk = x0\n    outputs = []\n    for uk in inputs:\n        xk, yk = model(xk, uk)\n        outputs.append(yk)\n    outputs = jnp.stack(outputs)\n    return xk, outputs\n\n\nA = jnp.array([[0.7, 1.0], [0.0, 1.0]])\nB = jnp.array([[0.0], [1.0]])\nC = jnp.array([[0.3, 0.0]])\nD = jnp.array([[0.0]])\nmodel = EqxModel(A, B, C, D)\n\n# Test simulation\ninputs = jnp.array([[0.0], [1.0], [1.0], [1.0]])\nx0 = jnp.zeros(2)\nxk, outputs = simulate(model, inputs, x0)\nassert jnp.allclose(xk, jnp.array([2.7, 3.0]))\nassert jnp.allclose(outputs, jnp.array([[0.0], [0.0], [0.0], [0.3]]))\n\n# This raises TypeError\nxk, outputs = jax.lax.scan(model, x0, inputs)\nWhat is unhashable type: 'ArrayImpl' referring to? Is it the arrays A, B, C, and D? In this model, these matrices are parameters and therefore should be static for the duration of the simulation.\nI just found this issue thread that might be related:\nlax.scan for equinox Modules",
        "answers": [
            "Owen Lockwood (lockwo) has provided an explanation and answer in this issue thread, which I will re-iterate below.\nI believe your issue is happening because jax tries to hash the function you are scanning over, but it can't hash the arrays that are in the module. There are probably a number of things that you could do to solve this, the simplest being to just curry the model, e.g. xk, outputs = jax.lax.scan(lambda carry, y: model(carry, y), x0, inputs) works fine\nOr, re-written in terms of the variable names I am using:\nxk, outputs = jax.lax.scan(lambda xk, uk: model(xk, uk), x0, inputs)"
        ],
        "link": "https://stackoverflow.com/questions/78583009/typeerror-unhashable-type-arrayimpl-when-trying-to-use-equinox-module-with-j"
    },
    {
        "title": "Multiplying chains of matrices in JAX",
        "question": "Suppose I have a vector of parameters p which parameterizes a set of matrices A_1(p), A_2(p),...,A_N(p). I have a computation in which for some list of indices q of length M, I have to compute A_{q_M} * ... * A_{q_2} * A_{q_1} * v for several different q s. Each q has a different length, but crucially doesn't change! What changes, and what I wish to take gradients against is p.\nI'm trying to figure out how to convert this to performant JAX. One way to do it is to have some large matrix Q which contains all the different qs on each row, padded out with identity matrices such that each multiplication chain is the same length, and then scan over a function that switch es between N different functions doing matrix-vector multiplications by A_n(p).\nHowever -- I don't particularly like the idea of this padding. Also, since Q here is fixed, is there potentially a smarter way to do this? The distribution of lengths of q s has a very long tail, so Q will be dominated by padding.\nEDIT: Here's a (edit 2: functional) minimal example\nsigma0 = jnp.eye(2)\nsigmax = jnp.array([[0, 1], [1, 0]])\nsigmay = jnp.array([[0, -1j], [1j, 0]])\nsigmaz = jnp.array([[1, 0], [0, -1]])\nsigma = jnp.array([sigmax, sigmay, sigmaz])\n\ndef gates_func(params):\n    theta = params[\"theta\"]\n    epsilon = params[\"epsilon\"]\n\n    n = jnp.array([jnp.cos(theta), 0, jnp.sin(theta)])\n    omega = jnp.pi / 2 * (1 + epsilon)\n    X90 = expm(-1j * omega * jnp.einsum(\"i,ijk->jk\", n, sigma) / 2)\n\n    return {\n        \"Z90\": expm(-1j * jnp.pi / 2 * sigmaz / 2),\n        \"X90\": X90\n    }\n\ndef multiply_out(params):\n    gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n\n    gates = gates_func(params)\n    out = jnp.zeros(len(gate_lists)) \n    \n    for i, gate_list in enumerate(gate_lists):\n        init = jnp.array([1.0,0.0], dtype=jnp.complex128)\n        for g in gate_list:\n            init = gates[g] @ init\n        out = out.at[i].set(jnp.abs(init[0]))\n\n    return out\n\nparams = dict(theta=-0.0, epsilon=0.001)\nmultiply_out(params)",
        "answers": [
            "The main issue here is that JAX does not support string inputs. But you can use NumPy to manipulate string arrays and turn them into integer categorical arrays that can then be used by jax.jit and jax.vmap. The solution might look something like this:\nimport numpy as np\n\ndef gates_func_int(params, gate_list_vals):\n  g = gates_func(params)\n  identity = jnp.eye(*list(g.values())[0].shape)\n  return jnp.stack([g.get(val, identity) for val in gate_list_vals])\n\n@jax.jit\ndef multiply_out_2(params):\n  # compile-time pre-processing\n  gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n  max_size = max(map(len, gate_lists))\n  gate_array = np.array([gates + [''] * (max_size - len(gates))\n                        for gates in gate_lists])\n  gate_list_vals, gate_list_ints = np.unique(gate_array, return_inverse=True)\n  gate_list_ints = gate_list_ints.reshape(gate_array.shape)\n\n  # runtime computation\n  gates = gates_func_int(params, gate_list_vals)[gate_list_ints]\n  initial = jnp.array([[1.0],[0.0]], dtype=jnp.complex128)\n  return jax.vmap(lambda g: jnp.abs(jnp.linalg.multi_dot([*g, initial]))[0])(gates).ravel()\n\nmultiply_out_2(params)"
        ],
        "link": "https://stackoverflow.com/questions/78562406/multiplying-chains-of-matrices-in-jax"
    },
    {
        "title": "How to set a new learning rate manually in optax optimizer?",
        "question": "I have the following optimizer being create using optax:\ndef create_optimizer(learning_rate=6.25e-2, beta1=0.4, beta2=0.999,\n                     eps=2e-4, centered=False):\n\n  Returns:\n    An optax optimizer.\n  \"\"\"\n \n    return optax.adam(learning_rate, b1=beta1, b2=beta2, eps=eps)\nHow during training update this learning rate manually?\nI couldn't find any documentation about that.",
        "answers": [
            "Disclaimer. Usually, you would use a schedule to adapt the learning rate during training. This answer provides a solution to obtain direct control over the learning rate.\nIn general, you can put any optimizer's hyperparmeters (such as the learning rate) into the optimizer's state and then directly mutate the state. Moving the hyperparameters into the state is necessary as optax optimizers are pure functions. Especially, the only way to dynamically change their behaviour is by changing their input.\nSetup. I am using a stochastic gradient descent optimizer to highlight the effect of the learning rate on the update suggested by the optimizer.\nimport jax.numpy as jnp\nimport optax\n\n# Define example parameters and gradients.\nparams, grads = jnp.array([0.0, 0.0]), jnp.array([1.0, 2.0])\n\n# Ensure the learning rate is part of the optimizer's state.\nopt = optax.inject_hyperparams(optax.sgd)(learning_rate=1e-2)\nopt_state = opt.init(params)\nUpdate computation.\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.01, -0.02], dtype=float32)\nDirectly setting the learning rate.\nopt_state.hyperparams['learning_rate'] = 3e-4\nSame update computation as before (with new learning rate).\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.0003, -0.0006], dtype=float32)\nSee this discussion for more information."
        ],
        "link": "https://stackoverflow.com/questions/78527164/how-to-set-a-new-learning-rate-manually-in-optax-optimizer"
    },
    {
        "title": "Why JAX is considering same list as different data structure depending on appending a new array inside function?",
        "question": "I am very new to JAX. Please excuse me if this something obvious or I am making some stupid mistake. I am trying to implement a function which does the following. All these functions will be called from other JIT-ed function. So, removing JIT may not be possible.\nget_elements function takes a JAX array( call it state (1D)). Looks at each element in it and calls a function get_condition.\nget_condition returns a tuple depending on the element at the given position of state. The tuple may be (1,0),(0,1) or (0,0)\nHere I want to call update_state only if the tuple received from get_conn is (0,1) or (1,0). In that case update_state_vec will get called and add a new vector of same length as state will get appended to the list.\nBut, I couldn't make jax.lax.cond work here. So, I tried to call update_state for each case, but I want the list to remain unchanged if the codition is (0,0).\nIn update_state_vec, no_update_state should return the same array\nthat it receives withourt appending anything\nHere, is the entire code:\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import lax\nimport copy\nfrom copy import deepcopy\n\nimport numpy as np\n\n\ndef get_condition(state, x, y):\n   L = (jnp.sqrt(len(jnp.asarray(state)))).astype(int)\n   state = jnp.reshape(state, (L,L), order=\"F\")\n   s1 = state[x, y]\n\n   branches = [lambda : (0,1), lambda : (1,0), lambda : (0,0)]\n   conditions = jnp.array([s1==2, s1==4, True])\n   result = lax.switch(jnp.argmax(conditions), branches)\n   return tuple(x for x in result)\n\n\n\n\ndef update_state_vec(state, x, y, condition, list_scattered_states):\n   L = (jnp.sqrt(len(state))).astype(int)   \n   def update_state_4(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(4)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n   def update_state_2(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(2)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n\n   def no_update_state (list_scattered_states):\n       #state1 = jnp.ravel(state, order=\"F\")\n       #list_scattered_states.append(jnp.ravel(state, order=\"F\"))\n       #This doesn't work---------------------------------\n       return list_scattered_states\n\n\n\n   conditions = jnp.array([condition == (1, 0), condition == (0, 1), condition == (0, 0)])\n   print(conditions)\n   branches = [update_state_4, update_state_2,no_update_state]\n\n   return(lax.switch(jnp.argmax(conditions), branches, operand=list_scattered_states))\n           \n\n\ndef get_elements(state):\n\n   L = (jnp.sqrt(len(state))).astype(int)\n   list_scattered_states = []\n   for x in range(L):\n       for y in range(L):\n           condition=get_condition(state, x, y)\n           print(condition)\n           list_scattered_states = update_state_vec(state, x, y, condition, list_scattered_states)\n\n\n   return list_scattered_states\nWe can take an example input as follows,\narr=jnp.asarray([2., 1., 3., 4., 1., 2., 3., 4., 4., 1., 2., 3., 4., 2., 1., 3.])\nget_elements(arr)\nI get an error message as below:\n    print(conditions)\n 41 branches = [update_state_4, update_state_2,no_update_state]\n ---> 43 return(lax.switch(jnp.argmax(conditions), branches, \n operand=list_scattered_states))\n TypeError: branch 0 and 2 outputs must have same type structure, got PyTreeDef([*]) \n and PyTreeDef([]).\nSo, the error is coming from the face that no_update_state is returning something that doesn't match with return type of update_state_4 or update_state_2. I am quite clueless at this point. Any help will be much appreciated.",
        "answers": [
            "The root of the issue here is that under transformations like jit, vmap, switch, etc. JAX requires the shape of outputs to be known statically, i.e. at compile time (see JAX sharp bits: dynamic shapes). In your case, the functions you are passing to switch return outputs of different shapes, and since jnp.argmax(conditions) is not known at compile time, there's no way for the compiler to know what memory to allocate for the result of this function.\nSince you're not JIT-compiling or otherwise transforming your code, the easiest way to address this would be to replace the lax.switch statement with this:\n  if condition == (1, 0):\n    list_scattered_states = update_state_4(list_scattered_states)\n  elif condition == (0, 1):\n    list_scattered_states = update_state_2(list_scattered_states)\n  return list_scattered_states\nIf you do want your function to be compatible with jit or other JAX transformations, you'll have to re-write the logic so that the size of list_scattered_states remains constant, e.g. by padding it to the expected size from the beginning."
        ],
        "link": "https://stackoverflow.com/questions/78512663/why-jax-is-considering-same-list-as-different-data-structure-depending-on-append"
    },
    {
        "title": "jax parallel multiplication of pairs of matrix with different shapes",
        "question": "Task: I have two lists of matrices A,B with length N. For each pair of elements A[i], B[i] shapes are such that matrix product is well-defined, however for each i in $0,\\dots, N-1$ shapes can be different. Hence, I can not stack them in array. Shapes are static.\nI would like to do achieve same result as following :\nout = [None] * length(A)\nfor i, a, b in enumerate(zip(A,B)):\n   out[i] = a @ b\nHowever, I would like to do this in parallel with jax. The best option will be vmap, but it is impossible as shapes are different.\nHere I will discuss solutions that I know and why they are not satisfactory.\nWrite for loop and then jit it. This will grow compilation time super linear over length N. This is not good, as I know all shapes of input and output before running computation, so I would expect to constant compilation time (provided say list of shapes).\nUse fori_loop primitive from jax. In documentation, there is following:\nThe semantics of fori_loop are given by this Python implementation:\ndef fori_loop(lower, upper, body_fun, init_val):\n  val = init_val\n  for i in range(lower, upper):\n    val = body_fun(i, val)\n  return val\nHowever, my case is easier: I don't need to care val across iterations. This means that fori is sequential. While my case is parallel. Hence, it should be possible to do better.\nPad with zeros, use vmap, read result. I don't control distribution of shapes, so it can lead to blowing memory if only one shape is big.\nUse lax.map Here (What are the tradeoffs between jax.lax.map and jax.vmap?) I read following:\nThe lax.map solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.\nSo I don't know what to do. Thanks!\nUpd after answer:\nN = 100\nd = 1000\nkey = jrandom.key(0)\nAjnp = jrandom.normal(key, (N, d, d))\nBjnp = jrandom.normal(key, (N, d, d))\n\nAnp = list(np.random.randn(N,d,d))\nBnp = list(np.random.randn(N,d,d))\n\nvmatmul = vmap(jnp.matmul, (0,0))\n\ndef lmatmul(A,B):\n    return [a @ b for a, b in zip(A,B)]\n%timeit vmatmul(Ajnp, Bjnp).block_until_ready()  # jax vmap over arrays\n6.59 ms ± 73.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%timeit block_until_ready(lmatmul(list(Ajnp), list(Bjnp))) # jax loop over lists\n13 ms ± 221 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n%timeit lmatmul(Anp, Bnp) # numpy loop over lists\n1.28 s ± 13.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)",
        "answers": [
            "I think your best approach will be something like your original formulation, though you can avoid pre-allocating the out list:\nout = [a @ b for a, b in zip(A, B)]\nBecause of JAX's Asynchronous dispatch, if you run this on an accelerator like GPU the operations will be executed in parallel to the extent possible.\nAll of your other proposed solutions either won't work due to static shape limitations, will force sequential computation, or will incur overhead that will make them worse in practice than this more straightforward approach."
        ],
        "link": "https://stackoverflow.com/questions/78502704/jax-parallel-multiplication-of-pairs-of-matrix-with-different-shapes"
    },
    {
        "title": "Jax dynamic slicing tracer array",
        "question": "To make this brief: I wrote the following codes:\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef body_func(carry,x):\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nbut got\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nHere's the full situation: I'm trying to develop a model to do phase recognition tasks, and I would like to normalize my logits phase by phase using jax. For example, suppose I have the phase labels and logits:\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\nI would like to normalize the first 4 elements in logits where in the phase labels they all belong to phase 0. Then the next 4 elements, because in the phase labels they all belong to phase 1. So the normalized logits should look like:\nnormalized_logits=[0,0.33,0.66,1.0,0,0.33,0.66,1.0,0,0.33,0.66,1.0]\nHere's what tried:\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef min_max_normalization(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndef body_func(carry,x):\n    jax.debug.print(\"carry is {}\",carry)\n    jax.debug.print(\"x is {}\",x)\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    print(min_max_normalization(jax.lax.dynamic_slice(arr, [start_idx], [jax.lax.tie_in(x, x-start_idx+1)])))\n    print(jax.lax.dynamic_slice(arr, [x+1], [jax.lax.tie_in(x, len(arr)-x-1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nBasically, this is a debug version, the actual return value should concatenate three dynamically sliced array together. But I'm getting the error below:\nTraceback (most recent call last):\n  File \"/path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 250, in scan\n    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n                                                                ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 236, in _create_jaxpr\n    jaxpr, consts, out_tree = _initial_style_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 64, in _initial_style_jaxpr\n    jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 58, in _initial_style_open_jaxpr\n    jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2155, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/slicing.py\", line 110, in dynamic_slice\n    static_sizes = core.canonicalize_shape(slice_sizes)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/core.py\", line 2086, in canonicalize_shape\n    raise _invalid_shape_error(shape, context)\njax._src.traceback_util.UnfilteredStackTrace: TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /Users/wuhaoyang/Documents/Research/Project_Surgical_Robot/Code/SSM_Med/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nThe reason why I'm not simply using a for loop is that I'm later going to wrap this function into another one that uses jit compile, so I want to do this with pure jax API. Any help is appreciated, please tell me if you need more information.",
        "answers": [
            "JAX arrays used in transformations like jit, vmap, and scan must always be statically-shaped (see Sharp bits: Dynamic Shapes for some discussion of this).\ndynamic_slice allows you to slice a static length at a dynamic position, while you're trying to use it to slice a dynamic length at a static position, and thus you're seeing this concretization error.\nTo solve your problem, I would avoid scan and instead use JAX's segment_min and segment_max functions to compute the output in a vectorized rather than iterative manner:\nimport jax\nimport jax.numpy as jnp\n\nlabels = jnp.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits = jnp.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\nl_min = jax.ops.segment_min(logits, labels)[labels]\nl_max = jax.ops.segment_max(logits, labels)[labels]\n\nnormalized_logits = (logits - l_min) / (l_max - l_min)\nprint(normalized_logits)\n# [0.         0.33333334 0.6666667  1.         0.         0.33333334\n#  0.6666667  1.         0.         0.33333334 0.6666667  1.        ]\nIf you want this to be compatible with jit and other transformations, you'll need to pass a static num_segments argument to your segment reductions to specify an upper-bound for the number of segments present:\nl_min = jax.ops.segment_min(logits, labels, num_segments=3)[labels]\nl_max = jax.ops.segment_max(logits, labels, num_segments=3)[labels]"
        ],
        "link": "https://stackoverflow.com/questions/78496911/jax-dynamic-slicing-tracer-array"
    },
    {
        "title": "Finite basis physics-informed neural networks (FBPINNs) JAX problem",
        "question": "I am trying to modify Ben Moseley's code available on github https://github.com/benmoseley/FBPINNs. My intention is to insert a vector of values into the loss fn that is dependent on x y coordinates, and I need the original vector Z to be interpolated as a function of x and y, and then the values at the same coordinates with which the algorithm samples x and y are extracted, so that the values match. The problem I have encountered is that within loss fn I cannot use libraries other than JAX and to my knowledge there are no functions within JAX to interpolate in 2D.\nI'm trying to get around the problem in every way but I'm not succeeding, one of my ideas was to extrapolate the x,y points sampled by the algorithm but I'm not succeeding, the code is really very articulated. Would anyone be able to give me any advice/help on this?\nThere would be the function jax.scipy.ndimage.map_coordinates but it doesn't work properly and the points it extrapolates are meaningless.",
        "answers": [
            "If linear or nearest-neighbor interpolation is sufficient, you may be able to do what you need with jax.scipy.interpolate.RegularGridInterpolator\nIf you need something more sophisticated, like spline interpolation, there is nothing included in jax itself. That said, you may be able to find downstream implementations that work for you. One I came across that might be worth trying is in the jax_cosmo project: https://jax-cosmo.readthedocs.io/en/latest/_modules/jax_cosmo/scipy/interpolate.html."
        ],
        "link": "https://stackoverflow.com/questions/78494686/finite-basis-physics-informed-neural-networks-fbpinns-jax-problem"
    },
    {
        "title": "Why is custom pytree 'aux_data' traced after jax.jit() for jnp.array but not for np.array?",
        "question": "I am trying to understand how pytrees work and registered my own class as a pytree. I noticed that if the aux_data in the pytree is a jax.numpy.ndarray the auxilliary data is subsequently traced and returned as a Traced<ShapedArray(...)>.... However, if the aux_data is a numpy.ndarray (i.e. not JAX array), then it is not traced and returns an array from a jit tranformed function.\nNow, I am aware of the tracing that happens during the jax.jit() transformation, but I do not understand why, on the level of pytrees, this results in the behaviour described above.\nHere is an example to reproduce this behaviour (multiplying both the aux_data and the tree leaves by two, which may be a problem in itself after JIT transformation...?). I have used the custom pytree implementations of accepted libraries (equinox and simple_pytree) for comparison, and they all give the same result, so that I am very sure that this is not a bug but a feature that I am trying to understand.\nimport jax\nfrom jax.tree_util import tree_structure, tree_leaves\nimport numpy as np\n\ndef get_pytree_impl(base):\n    if base == \"equinox\":\n        import equinox as eqx\n        Module = eqx.Module\n        static_field = eqx.static_field\n    elif base == \"simple_pytree\":\n        from simple_pytree import Pytree, static_field\n        Module = Pytree\n    elif base == \"dataclasses\":\n        from dataclasses import dataclass, field\n        @dataclass\n        class Module():\n            pass\n        static_field = field\n    \n    class PytreeImpl(Module):\n        x: jax.numpy.ndarray\n        y: jax.numpy.ndarray = static_field()\n\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n\n    if base == 'dataclasses':\n        from jax.tree_util import register_pytree_node\n        \n        def flatten(ptree):\n            return ((ptree.x,), ptree.y)\n        \n        def unflatten(aux_data, children):\n            return PytreeImpl(*children, aux_data)\n\n        register_pytree_node(PytreeImpl, flatten, unflatten)\n        \n    return PytreeImpl\n\ndef times_two(ptree):\n    return type(ptree)(ptree.x*2, ptree.y*2)\n\ntimes_two_jitted = jax.jit(times_two)\n\nbases = ['dataclasses', 'equinox', 'simple_pytree']\nfor base in bases:\n    print(\"========  \" + base + \"  ========\")\n    for lib_name, array_lib in zip(['jnp', 'np'], [jax.numpy, np]):\n        print(\"====  \" + lib_name)\n        PytreeImpl = get_pytree_impl(base)\n        x = jax.numpy.array([1,2])\n        y = array_lib.array([3,4])\n        input_tree = PytreeImpl(x, y)\n        for tag, pytree in zip([\"input\", \"no_jit\", \"jit\"],[input_tree, times_two(input_tree), times_two_jitted(input_tree)]):\n            print(f' {tag}:')\n            print(f'\\t Structure: {tree_structure(pytree)}')\n            print(f'\\t Leaves: {tree_leaves(pytree)}')\nThis produces the follwing, where dataclasses is my naive custom implementation of a pytree:\n========  dataclasses  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  equinox  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([3, 4], dtype=int32),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([6, 8], dtype=int32),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>,)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([3, 4]),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  simple_pytree  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([3, 4], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([6, 8], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>, '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([3, 4]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\nI ran this example using Python 3.12.1 with equinox 0.11.4 jax 0.4.28 jaxlib 0.4.28 simple-pytree 0.1.5",
        "answers": [
            "From the JAX docs:\nWhen defining unflattening functions, in general children should contain all the dynamic elements of the data structure (arrays, dynamic scalars, and pytrees), while aux_data should contain all the static elements that will be rolled into the treedef structure.\naux_data in a pytree flattening must contain static elements, and static elements must be hashable and immutable. Neither np.ndarray nor jax.Array satisfy this, so they should not be included in aux_data. If you do include such values in aux_data, you'll get unsupported, poorly-defined behavior.\nWith that background: the answer to your question of why you're seeing the results you're seeing is that you are defining your pytrees incorrectly. If you define aux_data to only contain static (i.e. hashable and immutable) attributes, you will no longer see this behavior."
        ],
        "link": "https://stackoverflow.com/questions/78485445/why-is-custom-pytree-aux-data-traced-after-jax-jit-for-jnp-array-but-not-for"
    },
    {
        "title": "What is an efficient method to calculate multiple \"offset-traces\" in JAX?",
        "question": "Given a matrix m with shape (n, n), I need to compute the series of \"offset traces\" [np.trace(m, offset=i) for i in range(q)] in JAX. For my application, n >> q, and q is a static parameter.\nThe obvious JAX approach using vmap does not work, possibly because although the trace has fixed output size, each offset diagonal has a different length?\nI came up with two other approaches using JAX which work but are about 100x slower than NumPy. get_traces_jax_1 is the more efficient of the two. But it does a lot of extra work when I only need a few diagonals, and I don't think that extra work gets compiled away.\nIs there a more efficient way to do this in JAX with similar performance to NumPy? I want to use JAX because:\nI need to vmap this across many matrices;\nIt is part of a larger algorithm, other parts of which are significantly sped up by JAX jit.\nBelow are the methods I explored and timings on my computer.\nimport numpy as np\nfrom numpy import random\nimport jax\njax.config.update(\"jax_enable_x64\", True) # default is float32\nfrom jax import numpy as jnp\nfrom functools import partial\n\nn, q = 1000, 5\n\n# check the methods produce the same result\ndef distance(u, v):\n    return jnp.max(jnp.abs(u - v))\n\n# numpy - this is what I want\ndef get_traces_np(mat, q):\n    return np.array([np.trace(mat, offset=i) for i in range(q)])\n\n# jax\n# !! This does not work\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_broken(mat, q):\n    return jax.vmap(lambda i: jnp.trace(mat, offset=i))(jnp.arange(q)) # !! does not work\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_0(mat, q):\n    return jnp.array([jnp.trace(mat, offset=i) for i in range(q)])\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_1(mat, q):\n    n = mat.shape[0]\n    padded = jnp.pad(mat, ((0, 0), (0, n-1)), 'constant')\n    shifts = jax.vmap(lambda v, i: jnp.roll(v, -i))(padded, jnp.arange(n))[:, :n]\n    return jnp.sum(shifts, axis=0)[:q]\n\nmat = random.uniform(size=(n, n))\n# Check they produce the same result and precompile\nd0 = distance(get_traces_np(mat, q), get_traces_jax_0(mat, q))\nd1 = distance(get_traces_np(mat, q), get_traces_jax_1(mat, q))\nprint(f'Errors: {d0}, {d1}')\n\nmat = jnp.array(mat)\nprint('Numpy:')\n%timeit get_traces_np(mat, q) # 7.43 microseconds\nprint('Jax 0:')\n%timeit get_traces_jax_0(mat, q) # 4.82ms\nprint('Jax 1:')\n%timeit get_traces_jax_1(mat, q) # 1.22ms",
        "answers": [
            "vmapping jnp.trace across offset doesn't work because as currently implemented, the offset parameter of jnp.trace must be static, and vmapped parameters are not static. You could address this by constructing your own version of the trace function that does not require a static parameter; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef dynamic_trace(x, offset):\n  assert x.ndim == 2\n  i = jnp.arange(x.shape[0])[:, None]\n  j = jnp.arange(x.shape[1])\n  return jnp.sum(jnp.where(i + offset == j, x, 0))\n\nx = jnp.arange(12).reshape(3, 4)\n\noffset = jnp.arange(-2, 3)\n\njax.vmap(dynamic_trace, in_axes=(None, 0))(x, offset)\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\n\njnp.array([jnp.trace(x, int(o)) for o in offset])\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\nIn terms of benchmark comparisons, you should first make sure you're measuring what you think you're measuring. For example, your benchmark is written in such a way that it doesn't just measure runtime, but also device transfer and JIT compilation costs. It also ignores potentially confounding effects of asynchronous dispatch. For a discussion of these issues, see JAX FAQ: Benchmarking JAX code.\nEven accounting for this, however, I would not expect JAX to be faster than NumPy here for the reasons discussed at JAX FAQ: Is JAX Faster than NumPy?. Namely, this benchmark is doing a short sequence of relatively small array operations on CPU: this is a regime in which NumPy will always beat JAX due to its lower per-operation dispatch overhead. But not to worry: when this operation is used as part of a larger JIT-compiled function, those dispatch costs are amortized away."
        ],
        "link": "https://stackoverflow.com/questions/78479150/what-is-an-efficient-method-to-calculate-multiple-offset-traces-in-jax"
    },
    {
        "title": "Iterators in jit JAX functions",
        "question": "I'm new to JAX and reading the docs i found that jitted functions should not contain iterators (section on pure functions)\nand they bring this example:\nimport jax.numpy as jnp\nimport jax.lax as lax\nfrom jax import jit\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\ntrying to fiddling with it a little bit in order to see if i can get directly an error instead of undefined behaviour i wrote\n@jit\ndef f(x, arr):\n    for i in range(10):\n        x += arr[i]\n    return x\n\n@jit\ndef f1(x, arr):\n    it = iter(arr)\n    for i in range(10):\n        x += next(it)\n    return x\n\nprint(f(0,array)) # 45 as expected\nprint(f1(0,array)) # still 45 \nIs it a \"chance\" that the jitted function f1() now shows the correct behaviour?",
        "answers": [
            "Your code works because of the way that JAX's tracing model works. When JAX's tracing encounters Python control flow, like for loops, the loop is fully evaluated at trace-time (There's some exploration of this in JAX Sharp Bits: Control Flow).\nBecause of this, your use of an iterator in this context is fine, because every iteration is evaluated at trace-time, and so next(it) is re-evaluated at every iteration.\nIn contrast, when using lax.fori_loop, next(iterator) is only executed a single time and its output is treated as a trace-time constant that will not change during the runtime iterations."
        ],
        "link": "https://stackoverflow.com/questions/78403517/iterators-in-jit-jax-functions"
    },
    {
        "title": "how to vmap over multiple Dense instances in flax model? trying to avoid looping over a list of Dense instances",
        "question": "from jax import random,vmap\nfrom jax import numpy as jnp\nimport pprint\n\ndef f(s,layers,do,dx):\n    x = jnp.zeros((do,dx))\n    for i,layer in enumerate(layers):\n        x=x.at[i].set( layer( s[i] ) )\n    return x\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layers = [ nn.Dense( self.dx, use_bias=False )\n                        for _ in range(self.do) ]\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None,None))(s,self.layers,self.do,self.dx)\n        return x\n\nif __name__ == '__main__':\n    seed = 123\n    key = random.PRNGKey( seed )\n    key,subkey = random.split( key )\n    outer_batches = 4\n    s_observations = 5 # AKA the inner batch\n    x_features = 2\n    s_features = 3\n    s_shape = (outer_batches,s_observations, s_features)\n    s = random.uniform( subkey, s_shape )\n\n    key,subkey = random.split( key )    \n    model = net(x_features,s_observations)\n    p = model.init( subkey, s )\n    x = model.apply( p, s )    \n\n    params = p['params']\n    pkernels = jnp.array([params[key]['kernel'] for key in params.keys()])\n    x_=jnp.zeros((outer_batches,s_observations,x_features))\n    \n    g = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\n    \n    x_=g(s,pkernels)\n    print('s shape:',s.shape)\n    print('p shape:',pkernels.shape)\n    print('x shape:',x.shape)\n    print('x_ shape:',x_.shape)\n    print('sum of difference:',jnp.sum(x-x_))\nHi. I need some \"batch-specific\" parameters in my model. Here, there is an \"inner batch\" of length do such that there is a flax.linen.Dense instance for each element in that batch. The outer batch just passes multiple data instances into those layers. I accomplish this by creating a list of flax.linen.Dense instances in the setup method. Then in the __call__ method, I iterate over those layers to fill up an array. This iteration is encapsulated in a function f, and that function is wrapped in jax.vmap.\nI have also included some equivalent logic written as matrix multiplication (see the function g) to make it explicit what operation I was hoping to capture with this class.\nI would like to replace the for-loop in the __call__ method with a call to jax.vmap. I ofc get an error when I pass a list to vmap, and I ofc get an error when I try to put multiple Dense instances in a jax array. Is there an alternative to using a list to contain my multiple Dense instances? A constraint is that I should be able to create an arbitrary number of Dense instances at the time of model initialization.",
        "answers": [
            "vmap can be used to map a single function over batches of data. You are attempting to use it to map multiple functions over batches of data, which it cannot do.\nUpdated answer based on updated question:\nSince each layer is identical aside from the parameters fit to the input data, it sounds like what you want is to map a single dense layer against a batch of data. It might look something like this:\nkeys = vmap(random.fold_in, in_axes=(None, 0))(subkey, jnp.arange(s_observations))\nmodel = nn.Dense(x_features, use_bias=False)\np = vmap(model.init, in_axes=(0, 1))(keys, s)\nx = vmap(model.apply, in_axes=(0, 1), out_axes=1)(p, s)\n\npkernels = p['params']['kernel']\ng = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\nx_=g(s,pkernels)\n\nprint('sum of difference:',jnp.sum(x-x_))\n# sum of difference: 0.0\nPrevious answer\nIn general, the fix would be to define a single parameterized layer that you can pass to vmap. In the example you gave, every layer is identical, and so to achieve the result you're looking for you could write something like this:\ndef f(s,layer,dx):\n  return layer(s)\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layer = nn.Dense( self.dx, use_bias=False )\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None))(s,self.layer,self.dx)\n        return x\nIf you had different parameterization per layer, then you could achieve this within vmap by passing those parameters to vmap as well."
        ],
        "link": "https://stackoverflow.com/questions/78385261/how-to-vmap-over-multiple-dense-instances-in-flax-model-trying-to-avoid-looping"
    },
    {
        "title": "How to restore a orbax checkpoint with jax/flax?",
        "question": "I saved a orbax checkpoint with the code below:\ncheck_options = ocp.CheckpointManagerOptions(max_to_keep=5, create=True)\ncheck_path = Path(os.getcwd(), out_dir, 'checkpoint')\ncheckpoint_manager = ocp.CheckpointManager(check_path, options=check_options, item_names=('state', 'metadata'))\ncheckpoint_manager.save(\n                    step=iter_num,\n                    args=ocp.args.Composite(\n                        state=ocp.args.StandardSave(state),\n                        metadata=ocp.args.JsonSave((model_args, iter_num, best_val_loss, losses['val'].item(), config))))\nWhen I try to resume from the saved checkpoints, I used the code below to recover the state variable:\nstate, lr_schedule = init_train_state(model, params['params'], learning_rate, weight_decay, beta1, beta2, decay_lr, warmup_iters, \n                     lr_decay_iters, min_lr)  # Here state is the initialied state variable with type Train_state.\nstate = checkpoint_manager.restore(checkpoint_manager.latest_step(), items={'state': state})\nBut when I try to use the recovered state in the training loop, I got this error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:584, in shaped_abstractify(x)\n    583 try:\n--> 584   return _shaped_abstractify_handlers[type(x)](x)\n    585 except KeyError:\n\nKeyError: <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'>\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 37\n     34 if iter_num == 0 and eval_only:\n     35     break\n---> 37 state, loss = train_step(state, get_batch('train'))\n     39 # timing and logging\n     40 t1 = time.time()\n\n    [... skipping hidden 6 frame]\n\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:575, in _shaped_abstractify_slow(x)\n    573   dtype = dtypes.canonicalize_dtype(x.dtype, allow_extended_dtype=True)\n    574 else:\n--> 575   raise TypeError(\n    576       f\"Cannot interpret value of type {type(x)} as an abstract array; it \"\n    577       \"does not have a dtype attribute\")\n    578 return core.ShapedArray(np.shape(x), dtype, weak_type=weak_type,\n    579                         named_shape=named_shape)\n\nTypeError: Cannot interpret value of type <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'> as an abstract array; it does not have a dtype attribute\nSo, how should I correctly recover the state checkpoint and use it in the training loop?\nThanks!",
        "answers": [
            "You're mixing the old and new APIs in a way that is not allowed. Apologies that an error to that effect is not being raised, I can look into that.\nYour saving is correct, but I'd recommend that it look more like the following:\nwith ocp.CheckpointManager(path, options=options, item_names=('state', 'metadata')) as mngr:\n  mngr.save(\n      step, \n      args=ocp.args.Composite(\n          state=ocp.args.StandardSave(state),\n          metadata=ocp.args.JsonSave(...),\n      )\n  )\nWhen restoring, you're currently using items which is part of the old API, and the usage is inconsistent with the CheckpointManager's definition, which is done based on the new API.\nitem_names and args are hallmarks of the new API.\nYou should do:\nwith ocp.CheckpointManager(...) as mngr:\n  mngr.restore(\n      mngr.latest_step(), \n      args=ocp.args.Composite(\n          state=ocp.args.StandardRestore(abstract_state),\n      )\n  )\nLet me know if there's any unexpected issues with that."
        ],
        "link": "https://stackoverflow.com/questions/78376465/how-to-restore-a-orbax-checkpoint-with-jax-flax"
    },
    {
        "title": "acme error - AttributeError: module 'jax' has no attribute 'linear_util'",
        "question": "I am using acme framework to run some experiments, and I installed acme based on documentation. However, I have attribute error that raised likely from JAX, HAIKU, and when I looked into github issue, there was no solution given at this time. Can anyone take a look what package dependecy caused this issue?\nmy venv spec:\nhere is my venv spec\ndm-acme                      0.4.0\ndm-control                   0.0.364896371\ndm-env                       1.6\ndm-haiku                     0.0.10\ndm-launchpad                 0.5.0\ndm-reverb                    0.7.0\ndm-tree                      0.1.8\nacme                         2.10.0\ndm-acme                      0.4.0\njax                          0.4.26\njaxlib                       0.4.26+cuda12.cudnn89\npython -V                    Python 3.9.5\nerror details:\nFile \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in from acme.agents.jax import dqn File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/init.py\", line 18, in from acme.agents.jax.dqn.actor import behavior_policy File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in from acme.agents.jax import actor_core as actor_core_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in from acme.jax import networks as networks_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/init.py\", line 18, in from acme.jax.networks.atari import AtariTorso File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/atari.py\", line 29, in from acme.jax.networks import base File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/base.py\", line 24, in import haiku as hk File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/init.py\", line 20, in from haiku import experimental File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/experimental/init.py\", line 34, in from haiku._src.dot import abstract_to_dot File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/_src/dot.py\", line 163, in @jax.linear_util.transformation File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 54, in getattr raise AttributeError(f\"module {module!r} has no attribute {name!r}\") AttributeError: module 'jax' has no attribute 'linear_util'\nseems it raised from haiku and JAX, how this can be fixed? any quick thoughts?\nupdated attempt\nbased on @jakevdp suggestion, I reinstalled jax, jaxlib, but now I am getting this error again:\nTraceback (most recent call last):\n  File \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in <module>\n    from acme.agents.jax import dqn\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/__init__.py\", line 18, in <module>\n    from acme.agents.jax.dqn.actor import behavior_policy\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in <module>\n    from acme.agents.jax import actor_core as actor_core_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in <module>\n    from acme.jax import networks as networks_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/__init__.py\", line 45, in <module>\n    from acme.jax.networks.multiplexers import CriticMultiplexer\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/multiplexers.py\", line 20, in <module>\n    from acme.jax import utils\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/utils.py\", line 190, in <module>\n    devices: Optional[Sequence[jax.xla.Device]] = None,\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 53, in getattr\n    raise AttributeError(f\"module {module!r} has no attribute {name!r}\")\nAttributeError: module 'jax' has no attribute 'xla'\nhere is my pip freeze list on this public gist: acme pip list\nI looked into this github issue: jax xla attribute issue\n@jakevdp, any updated comment or possible workaround for this jax.xla issue? thanks",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt sounds like you have too new a JAX version for the framework code you are using. I'd try installing an older version; e.g.\npip install --upgrade \"jax[cuda12_pip]<0.4.24\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSee JAX installation for more installation options.\nIf you're hoping to update the framework code for compatibility with more recent JAX versions, you might find replacements for previous functionality in jax.extend.linear_util."
        ],
        "link": "https://stackoverflow.com/questions/78372618/acme-error-attributeerror-module-jax-has-no-attribute-linear-util"
    },
    {
        "title": "How can we cast a `ctypes.POINTER(ctypes.c_float)` to `int`? [duplicate]",
        "question": "This question already has answers here:\nGet the memory address pointed to by a ctypes pointer (2 answers)\nClosed 12 months ago.\nI think this is a simple task, but I could not find a solution on the web to this. I have a external C++ library, which I'm using in my Python code, returning a ctypes.POINTER(ctypes.c_float) to me. I want to pass an array of these pointers to a jax.vmap function. The problem is that jax does not accept the ctypes.POINTER(ctypes.c_float) type. So, can I somehow cast this pointer to an ordinary int. Technically, this is clearly possible. But how do I do this in Python?\nHere is an example:\nlib = ctypes.cdll.LoadLibrary(lib_path)\nlib.foo.argtypes = None\nlib.foo.restype = ctypes.POINTER(ctypes.c_float)\n\nbar = jax.vmap(lambda : dummy lib.foo())(jax.numpy.empty(16))\n\nx = jax.numpy.empty(16, 256, 256, 1)\ny = jax.vmap(lib.bar, in_axes = (0, 1))(x, bar)\nSo, I want to invoke lib.foo 16-times so that I have an array bar containing all the pointers. Then I want to invoke another library function lib.bar which expects bar together with another (batched) parameter x.\nThe problem is that jax claims that ctypes.POINTER(ctypes.c_float) is not a valid jax type. This is why I think the solution is to cast the pointers to ints and store those ints in bar instead.",
        "answers": [
            "Listing:\n[SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer) - a common pitfall when working with CTypes (calling functions)\n[Python.Docs]: ctypes - A foreign function library for Python\nHere's a piece of code exemplifying how to handle pointers and their addresses. The trick is to use ctypes.addressof (documented in the 2nd URL).\ncode00.py:\n#!/usr/bin/env python\n\nimport ctypes as cts\nimport sys\n\n\nCType = cts.c_float\nCTypePtr = cts.POINTER(CType)\n\n\ndef ctype_pointer(seq):  # Helper\n    CTypeArr = (CType * len(seq))\n    ctype_arr = CTypeArr(*seq)\n    return cts.cast(ctype_arr, CTypePtr)\n\n\ndef pointer_elements(addr, count):  # Helper\n    return tuple(CType.from_address(addr + i * cts.sizeof(CType)).value for i in range(count))\n\n\ndef main(*argv):\n    seq = (2.718182, -3.141593, 1.618034, -0.618034, 0)\n    ptr = ctype_pointer(seq)\n    print(f\"Pointer: {ptr}\")\n    print(f\"\\nPointer elements: {tuple(ptr[i] for i in range(len(seq)))}\")  # Check if pointer has correct data\n    ptr_addr = cts.addressof(ptr.contents)  # @TODO - cfati: Straightforward\n    print(f\"\\nAddress: {ptr_addr} (0x{ptr_addr:016X})\\nElements from address: {pointer_elements(ptr_addr, len(seq))}\")\n    ptr_addr0 = cts.cast(ptr, cts.c_void_p).value  # @TODO - cfati: Alternative\n    print(f\"\\nAddresses match: {ptr_addr == ptr_addr0}\")\n\n\nif __name__ == \"__main__\":\n    print(\n        \"Python {:s} {:03d}bit on {:s}\\n\".format(\n            \" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n            64 if sys.maxsize > 0x100000000 else 32,\n            sys.platform,\n        )\n    )\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\nNotes:\nAlthough it adds a bit of complexity, I introduced the CType \"layer\" to show that it should work with any type, not just float (as long as the values in the sequence are of that type)\nThe only truly relevant lines are those marked with @TODO\nOutput:\n(py_pc064_03.08_test0_lancer) [cfati@cfati-5510-0:/mnt/e/Work/Dev/StackExchange/StackOverflow/q078366208]> python ./code00.py \nPython 3.8.19 (default, Apr  6 2024, 17:58:10) [GCC 11.4.0] 064bit on linux\n\nPointer: <__main__.LP_c_float object at 0x7203e97e7d40>\n\nPointer elements: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddress: 125361127594576 (0x00007203E97A9A50)\nElements from address: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddresses match: True\n\nDone."
        ],
        "link": "https://stackoverflow.com/questions/78366208/how-can-we-cast-a-ctypes-pointerctypes-c-float-to-int"
    },
    {
        "title": "Simultaneously going over different kinds of data with Keras training",
        "question": "In a regression task I'm given the following data:\nInput vectors with a known label. MSE loss should be used between the precidtion and the label.\nPairs of input vectors without a label, for which it is known that the model should give similar results. MSE loss should be used between the two predictions.\nWhat's the right way to fit a Keras model with these two kinds of data simultaneously?\nIdeally, I'd like the train loop to iterate the two kinds in an interleaved way - a superivsed (1) batch and then a self-supervised (2) batch, then supervised again etc.\nIf it matters, I'm using the Jax backend. Keras version 3.2.1.",
        "answers": [
            "I eventually found a trick that solved it for my case without too many customizations.\nBut if you do need to pass different kinds of data for training, I don't think there's an easy answer as for today.\nIt should be possible though to write your own training loop, and use any structure that you want for the data and labels. In this case you might also want to use the trainer pattern, implementing a custom version of keras.src.backend.jax.trainer.JAXTrainer."
        ],
        "link": "https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training"
    },
    {
        "title": "Flax neural network with nans in the outputs",
        "question": "I am training a neural network using Flax. My training data has a significant number of nans in the outputs. I want to ignore these and only use the non-nan values for training. To achieve this, I have tried to use jnp.nanmean to compute the losses, i.e.:\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    return jnp.nanmean((pred - targets) ** 2)\n\ndef train_step(state, inputs, targets):\n    loss, grads = jax.value_and_grad(nanloss)(state.params, inputs, targets)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\nHowever, after one training step the loss is nan.\nIs what I am trying to achieve possible? If so, how can I fix this?",
        "answers": [
            "I suspect you are hitting the issue discussed here: JAX FAQ: gradients contain NaN where using where. You've handled the NaNs in the computation itself, but they're still sneaking into the gradient due to how autodiff is implemented.\nIf this is in fact the issue, you can fix this by filtering the values before computing the loss; for example like this:\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    mask = jnp.isnan(pred) | jnp.isnan(targets)\n    pred = jnp.where(mask, 0, pred)\n    targets = jnp.where(mask, 0, targets)\n    return jnp.mean((pred - targets) ** 2, where=~mask)"
        ],
        "link": "https://stackoverflow.com/questions/78332120/flax-neural-network-with-nans-in-the-outputs"
    },
    {
        "title": "Jax ValueError: Incompatible shapes for broadcasting: shapes",
        "question": "I'm trying to write a weighted cross-entropy loss to train my model with Jax. However, I think there are some issues with my input dimension. Here are my codes:\nimport jax.numpy as np\nfrom functools import partial\nimport jax\n\n@partial(np.vectorize, signature=\"(c),(),()->()\")\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[0])\n    return -np.sum(weights* logits*one_hot_label)\n\nlogits=np.array([[1,2,3,4,5,6,7],[2,3,4,5,6,7,8]])\nlabels=np.array([1,2])\nweights=np.array([1,2,3,4,5,6,7])\nprint(weighted_cross_entropy_loss(logits,label,weights))\nHere are my error messages:\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 147, in broadcast_shapes\n    return _broadcast_shapes_cached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 284, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 277, in cached\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 153, in _broadcast_shapes_cached\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/PATH/test.py\", line 15, in <module>\n    print(weighted_cross_entropy_loss(a,label,weights))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 274, in wrapped\n    broadcast_shape, dim_sizes = _parse_input_dimensions(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 123, in _parse_input_dimensions\n    broadcast_shape = lax.broadcast_shapes(*shapes)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 149, in broadcast_shapes\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\nI'm expecting a single number that represents the cross-entropy loss between logits and labels.\nI'm fairly new to this, can somebody tell me what is going on? Any help is appreciated.",
        "answers": [
            "label is length 2, and weights is length 7, which means they cannot be broadcast together.\nIt's not clear to me from your question what your expected outcome was, but you can read more about how broadcasting works in NumPy (and in JAX, which implements NumPy's semantics) at https://numpy.org/doc/stable/user/basics.broadcasting.html.\nEdit: it looks like this is the operation you were aiming for:\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[1])\n    return -np.sum(weights * logits * one_hot_label)\nSince you want a single scalar output, I don't think vectorize is the right mechanism to use here."
        ],
        "link": "https://stackoverflow.com/questions/78323919/jax-valueerror-incompatible-shapes-for-broadcasting-shapes"
    },
    {
        "title": "Stable diffusion: AttributeError: module 'jax.random' has no attribute 'KeyArray'",
        "question": "When I run the stable diffusion on colab https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\nwith no modification, it fails on the line\nfrom diffusers import StableDiffusionPipeline \nThe error log is\nAttributeError: module 'jax.random' has no attribute 'KeyArray'\nHow can I fix this or any clue ?\nThe import should work, the ipynb should run with no error.",
        "answers": [
            "jax.random.KeyArray was deprecated in JAX v0.4.16 and removed in JAX v0.4.24. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier.\nYou can install JAX v0.4.23 with GPU support like this:\npip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nor, if you prefer targeting a local CUDA installation, like this:\npip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nFor more information on GPU installation, see JAX Installation: NVIDIA GPU.\nFrom the colab tutorial, update the second segment into:\n!pip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n!pip install diffusers==0.11.1\n!pip install transformers scipy ftfy accelerate",
            "In the end, we need to downgrade the jax, Try each from the lateset to ealier, and luckily it works for\njax==0.4.23 jaxlib==0.4.23",
            "# Change this\n# !pip install diffusers==0.11.1\n\n# To just\n!pip install diffusers \nIf you've already run pip install in your Colab runtime, you'll need to either disconnect and open a new runtime (my recommendation) or use  --upgrade.\nDiffusers v0.11.1 is now over 18 months old, and the notebook works with current v0.29.0 without any other changes. Instead of using an old version of diffusers, requiring an old version of jax, we can use the latest versions."
        ],
        "link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray"
    },
    {
        "title": "AttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'",
        "question": "I'm trying to run a model written in jax, https://github.com/lindermanlab/S5. However, I ran into some error that says\n   Traceback (most recent call last):\n  File \"/Path/run_train.py\", line 101, in <module>\n    train(parser.parse_args())\n  File \"/Path/train.py\", line 144, in train\n    state = create_train_state(model_cls,\n  File \"/Path/train_helpers.py\", line 135, in create_train_state\n    params = variables[\"params\"].unfreeze()\nAttributeError: 'dict' object has no attribute 'unfreeze'\nI tried to replicate this error by\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\n\nmodel = nn.Dense(features=3)\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1, 2)))\nparams_unfrozen = flax.traverse_util.unfreeze(params)\nAnd the error reads:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'\nI'm using:\nflax 0.7.4\njax 0.4.13\njaxlib 0.4.13+cuda12.cudnn89\nI think this is an issue relating to the version of flax, but does anyone know what exactly is going on? Any help is appreciated. Let me know if you need any further information",
        "answers": [
            "unfreeze is a method of Flax's FrozenDict class: (See FrozenDict.unfreeze). It appears that you have passed a Python dict where a FrozenDict is expected.\nTo fix this, you should ensure that variables['params'] is a FrozenDict, not a dict.\nRegarding the error in your attempted replication: flax.traverse_util does not define an unfreeze function, but this seems unrelated to the original problem."
        ],
        "link": "https://stackoverflow.com/questions/78256559/attributeerror-module-flax-traverse-util-has-no-attribute-unfreeze"
    },
    {
        "title": "jax: How do we solve the error: pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0?",
        "question": "I'm trying to run this simple introduction to score-based generative modeling. The code is using flax.optim, which seems to be moved to optax meanwhile (https://flax.readthedocs.io/en/latest/guides/converting_and_upgrading/optax_update_guide.html).\nI've made a copy of the colab code with the changes I think needed to be made (I'm only unsure how I need to replace optimizer = flax.jax_utils.replicate(optimizer)).\nNow, in the training section, I get the error\npmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nat the line loss, params, opt_state = train_step_fn(step_rng, x, params, opt_state). This obviously comes from the return jax.pmap(step_fn, axis_name='device') in the \"Define the loss function\" section.\nHow can I fix this error? I've googled it, but have no idea what's going wrong here.",
        "answers": [
            "This happens because you are passing a scalar argument to a pmapped function. For example:\nimport jax\nfunc = lambda x: x ** 2\npfunc = jax.pmap(func)\n\npfunc(1.0)\n# ValueError: pmap was requested to map its argument along axis 0, which implies\n# that its rank should be at least 1, but is only 0 (its shape is ())\nIf you want to operate on a scalar, you should use the function without wrapping it in pmap:\nfunc(1.0)\n# 1.0\nAlternatively, if you want to use pmap, you should operate on an array whose leading dimension matches the number of devices:\nnum_devices = len(jax.devices())\nx = jax.numpy.arange(num_devices)\npfunc(x)\n# Array([ 0,  1,  4,  9, 16, 25, 36, 49], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78244620/jax-how-do-we-solve-the-error-pmap-was-requested-to-map-its-argument-along-axi"
    },
    {
        "title": "what are the numbers in the operation names when profiling an application",
        "question": "What are the numbers in \"fusion_2\", \"fusion_4\"? Where do they come from? Thank you!",
        "answers": [
            "These numbers exist to de-duplicate the names of generated HLO operations. The first fusion operation created by the compiler is called fusion, the next is fusion_2, then fusion_3, and so on.\nNote that the order of creation does not necessarily match the order of execution."
        ],
        "link": "https://stackoverflow.com/questions/78236312/what-are-the-numbers-in-the-operation-names-when-profiling-an-application"
    },
    {
        "title": "Cannot import name 'linear_util' from 'jax'",
        "question": "I'm trying to reproduce the experiments of the S5 model, https://github.com/lindermanlab/S5, but I encountered some issues when solving the environment. When I'm running the shell script./run_lra_cifar.sh, I get the following error\nTraceback (most recent call last):\n  File \"/Path/S5/run_train.py\", line 3, in <module>\n    from s5.train import train\n  File \"/Path/S5/s5/train.py\", line 7, in <module>\n    from .train_helpers import create_train_state, reduce_lr_on_plateau,\\\n  File \"/Path/train_helpers.py\", line 6, in <module>\n    from flax.training import train_state\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py\", line 19, in <module>\n    from . import core\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py\", line 15, in <module>\n    from .axes_scan import broadcast\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py\", line 22, in <module>\n    from jax import linear_util as lu\nImportError: cannot import name 'linear_util' from 'jax' (/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py)\nI'm running this on an RTX4090 and my CUDA version is 11.8. My jax version is 0.4.25 and jaxlib version is 0.4.25+cuda11.cudnn86\nI first tried to install the dependencies using the author's\npip install -r requirements_gpu.txt\nHowever, this doesn't seem to work in my case since I can't evenimport jax. So I installed jax according to the instructions on https://jax.readthedocs.io/en/latest/installation.html by typing\npip install --upgrade pip\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo far I've tried:\nUsing a older GPU(3060 and 2070)\nDowngrading python to 3.9\nDoes anyone know what could be wrong? Any help is appreciated",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt appears that flax is the source of the linear_util import, meaning that you are using an older flax version with a newer jax version.\nTo fix your issue, you'll either need to install an older version of JAX which still has jax.linear_util, or update to a newer version of flax which is compatible with more recent JAX versions."
        ],
        "link": "https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax"
    },
    {
        "title": "Numpyro AR(1) mean switching model sampling incongrouencies",
        "question": "I'm trying to estimate an AR(1) process y with a switching mean according to a latent state S =0,1 that evolves as a markov process with fixed transition probabilities (as in here). In short, it takes the form:\ny_t - mu_{0/1} = phi * (y_{t-1} - mu_{0/1})+ epsilon_t\nwhere mu_0 would be used if state_t = 0 and mu_1 if state_t =1. I'm using jax/numpyro with DiscreteHMCGibbs (although normal NUTS with latent state enumeration yields the same result) but I can't seem to have the sampler work properly. From all diagnostics I run, it seems that all hyperparameters are stuck at initialization value, and summary returns accordingly with all std==0. Here below I have a MWE that reproduces my problem. Is there an obvious mistake I am making in the implementation?\nMWE:\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import MCMC, NUTS,DiscreteHMCGibbs\nfrom jax import random, pure_callback\nimport jax\nimport numpy as np\n\ndef generate_synthetic_data(T=100, mu=[0, 5], phi=0.5, sigma=1.0, p=np.array([[0.95, 0.05], [0.1, 0.9]])):\n    states = np.zeros(T, dtype=np.int32)\n    y = np.zeros(T)\n    current_state = np.random.choice([0, 1], p=[0.5, 0.5])\n    states[0] = current_state\n    y[0] = np.random.normal(mu[current_state], sigma)\n\n    for t in range(1, T):\n        current_state = np.random.choice([0, 1], p=p[current_state,:])\n        states[t] = current_state\n        y[t] = np.random.normal(mu[current_state] + phi * (y[t-1] - mu[current_state]), sigma)\n\n    return y, states\n\n\ndef mean_switching_AR1_model(y):\n    T = len(y)\n    phi = numpyro.sample('phi', dist.Normal(0, 1))\n    sigma = numpyro.sample('sigma', dist.Exponential(1))\n    \n    \n    with numpyro.plate('state_plate', 2):\n        mu = numpyro.sample('mu', dist.Normal(0, 5))\n        p = numpyro.sample('p', dist.Dirichlet(jnp.ones(2)))\n\n    probs_init = numpyro.sample('probs_init', dist.Dirichlet(jnp.ones(2)))\n    s_0 = numpyro.sample('s_0', dist.Categorical(probs_init))\n\n    def transition_fn(carry, y_t):\n        prev_state = carry\n        state_probs = p[prev_state]\n        state = numpyro.sample('state', dist.Categorical(state_probs))\n\n        mu_state = mu[state]\n        y_mean = mu_state + phi * (y_t - mu_state)\n        y_next = numpyro.sample('y_next', dist.Normal(y_mean, sigma), obs=y_t)\n        return state, (state, y_next)\n\n    _ , (signal, y)=scan(transition_fn, s_0, y[:-1], length=T-1)\n    return (signal, y)\n\n# Synthetic data generation\nT = 1000\nmu_true = [0, 3]\nphi_true = 0.5\nsigma_true = 0.25\ntransition_matrix_true = np.array([[0.95, 0.05], [0.1, 0.9]])\ny, states_true = generate_synthetic_data(T, mu=mu_true, phi=phi_true, sigma=sigma_true, p=transition_matrix_true)\n\n\nrng_key = random.PRNGKey(0)\nnuts_kernel = NUTS(mean_switching_AR1_model)\ngibbs_kernel = DiscreteHMCGibbs(nuts_kernel, modified=True)\n\n# Run MCMC\nmcmc = MCMC(gibbs_kernel, num_samples=1000, num_warmup=1000)\nmcmc.run(rng_key, y=y)\nmcmc.print_summary()",
        "answers": [
            "So it turns out there was indeed a pretty obvious mistake in the sense that I was not correctly carrying down y_{t-1} as part of the state variables. The following corrected transition functions yield the intended result without problems.\ndef transition_fn(carry, y_curr):\n    prev_state, y_prev = carry\n    state_probs = p[prev_state]\n    state = numpyro.sample('state', dist.Categorical(state_probs))\n\n    mu_state = mu[state]\n    y_mean = mu_state + phi * (y_prev - mu_state)\n    y_curr = numpyro.sample('y_curr', dist.Normal(y_mean, sigma), obs=y_curr)\n    return (state, y_curr), (state, y_curr)\n\n_, (signal, y) = scan(transition_fn, (s_0, y[0]), y[1:], length=T-1)"
        ],
        "link": "https://stackoverflow.com/questions/78209454/numpyro-ar1-mean-switching-model-sampling-incongrouencies"
    },
    {
        "title": "Slow JAX Optimization with ScipyBoundedMinimize and Optax - Seeking Speedup Strategies",
        "question": "I'm working on optimizing a model in jax that involves fitting a large observational dataset (4800 data points) with a complex model containing interpolation. The current optimization process using jaxopt.ScipyBoundedMinimize takes around 30 seconds for 100 iterations, with most of the time spent seemingly during or before the first iteration starts. You can find the relevant code snippet below. you can find the necessary data for the relevant code at the following link.\nnecessary data (idc, sg and cpcs)\nimport jax.numpy as jnp\nimport time as ela_time\nfrom jaxopt import ScipyBoundedMinimize\nimport optax\nimport jax\nimport pickle\n\n\nfile1 = open('idc.pkl', 'rb')\nidc = pickle.load(file1)\nfile1.close()\n\nfile2 = open('sg.pkl', 'rb')\nsg = pickle.load(file2)\nfile2.close()\n\nfile3 = open('cpcs.pkl', 'rb')\ncpcs = pickle.load(file3)\nfile3.close()\n\n\ndef model(fssc, fssh, time, rv, amp):\n\n    fssp = 1.0 - (fssc + fssh)\n\n    ivis = cpcs['common'][time]['ivis']\n    areas = cpcs['common'][time]['areas']\n    mus = cpcs['common'][time]['mus']\n\n    vels = idc['vels'].copy()\n\n    ldfs_phot = cpcs['line'][time]['ldfs_phot']\n    ldfs_cool = cpcs['line'][time]['ldfs_cool']\n    ldfs_hot = cpcs['line'][time]['ldfs_hot']\n\n    lps_phot = cpcs['line'][time]['lps_phot']\n    lps_cool = cpcs['line'][time]['lps_cool']\n    lps_hot = cpcs['line'][time]['lps_hot']\n\n    lis_phot = cpcs['line'][time]['lis_phot']\n    lis_cool = cpcs['line'][time]['lis_cool']\n    lis_hot = cpcs['line'][time]['lis_hot']\n\n    coeffs_phot = lis_phot * ldfs_phot * areas * mus\n    wgt_phot = coeffs_phot * fssp[ivis]\n    wgtn_phot = jnp.sum(wgt_phot)\n\n    coeffs_cool = lis_cool * ldfs_cool * areas * mus\n    wgt_cool = coeffs_cool * fssc[ivis]\n    wgtn_cool = jnp.sum(wgt_cool)\n\n    coeffs_hot = lis_hot * ldfs_hot * areas * mus\n    wgt_hot = coeffs_hot * fssh[ivis]\n    wgtn_hot = jnp.sum(wgt_hot)\n\n    prf = jnp.sum(wgt_phot[:, None] * lps_phot + wgt_cool[:, None] * lps_cool + wgt_hot[:, None] * lps_hot, axis=0)\n    prf /= wgtn_phot + wgtn_cool + wgtn_hot\n\n    prf = jnp.interp(vels, vels + rv, prf)\n\n    prf = prf + amp\n\n    avg = jnp.mean(prf)\n\n    prf = prf / avg\n\n    return prf\n\n\ndef loss(x0s, lmbd):\n\n    noes = sg['noes']\n\n    noo = len(idc['times'])\n\n    fssc = x0s[:noes]\n    fssh = x0s[noes: 2 * noes]\n    fssp = 1.0 - (fssc + fssh)\n    rv = x0s[2 * noes: 2 * noes + noo]\n    amp = x0s[2 * noes + noo: 2 * noes + 2 * noo]\n\n    chisq = 0\n    for i, itime in enumerate(idc['times']):\n        oprf = idc['data'][itime]['prf']\n        oprf_errs = idc['data'][itime]['errs']\n\n        nop = len(oprf)\n\n        sprf = model(fssc=fssc, fssh=fssh, time=itime, rv=rv[i], amp=amp[i])\n\n        chisq += jnp.sum(((oprf - sprf) / oprf_errs) ** 2) / (noo * nop)\n\n    wp = sg['grid_areas'] / jnp.max(sg['grid_areas'])\n\n    mem = jnp.sum(wp * (fssc * jnp.log(fssc / 1e-5) + fssh * jnp.log(fssh / 1e-5) +\n                    (1.0 - fssp) * jnp.log((1.0 - fssp) / (1.0 - 1e-5)))) / sg['noes']\n\n    ftot = chisq + lmbd * mem\n\n    return ftot\n\n\nif __name__ == '__main__':\n\n    # idc: a dictionary containing observational data (150 x 32)\n    # sg and cpcs: dictionaries with related coefficients\n\n    noes = sg['noes']\n    lmbd = 1.0\n    maxiter = 1000\n    tol = 1e-5\n\n    fss = jnp.ones(2 * noes) * 1e-5\n    x0s = jnp.hstack((fss, jnp.zeros(len(idc['times']) * 2)))\n\n    minx0s = [1e-5] * (2 * noes) + [-jnp.inf] * len(idc['times']) * 2\n    maxx0s = [1.0 - 1e-5] * (2 * noes) + [jnp.inf] * len(idc['times']) * 2\n\n    bounds = (minx0s, maxx0s)\n\n    start = ela_time.time()\n\n    optimizer = ScipyBoundedMinimize(fun=loss, maxiter=maxiter, tol=tol, method='L-BFGS-B',\n                                 options={'disp': True})\n    x0s, info = optimizer.run(x0s, bounds,  lmbd)\n\n    # optimizer = optax.adam(learning_rate=0.1)\n    # optimizer_state = optimizer.init(x0s)\n    #\n    # for i in range(1, maxiter + 1):\n    #\n    #     print('ITERATION -->', i)\n    #\n    #     gradients = jax.grad(loss)(x0s, lmbd)\n    #     updates, optimizer_state = optimizer.update(gradients, optimizer_state, x0s)\n    #     x0s = optax.apply_updates(x0s, updates)\n    #     x0s = jnp.clip(x0s, jnp.array(minx0s), jnp.array(maxx0s))\n    #     print('Objective function: {:.3E}'.format(loss(x0s, lmbd)))\n\n    end = ela_time.time()\n\n    print(end - start)   # total elapsed time: ~30 seconds\nHere's a breakdown of the relevant aspects:\nNumber of free parameters (x0s): 5263\nData: Observational data stored in idc dictionary (4800 data points)\nModel: Defined in model function, also utilizes interpolation\nOptimization methods tried:\njaxopt.ScipyBoundedMinimize with L-BFGS-B method (slow ~30 seconds, with most of the time spent during or just before the first iteration)\noptax.adam (too slow ~200 seconds)\nAttempted parallelization: I attempted to parallelize optax.adam, yet due to the inherent nature of the modeling, I couldn't succeed as the x0s couldn't be divided. (assuming I understood parallelization correctly)\nQuestions:\nWhat are potential reasons for the slowness before or during the first iteration in ScipyBoundedMinimize ?\nAre there alternative optimization algorithms in jax that might be faster for my scenario (large number of free parameters and data points, complex model with interpolation)?\nDid I misunderstand parallelization with optax.adam? Are there any strategies for potential parallelization in this case?\nAre there any code optimizations within the provided snippet that could improve performance (e.g., vectorization)?\nAdditional Information:\nHardware: Intel® Core™ i7-9750H CPU @ 2.60GHz × 12, 16 GiB RAM (laptop)\nSoftware: OS Ubuntu 22.04, Python 3.10.12, JAX 0.4.25, optax 0.2.1\nI'd appreciate any insights or suggestions to improve the optimization performance.",
        "answers": [
            "JAX code is Just-in-time (JIT) compiled, meaning that the long duration of the first step is likely related to compilation costs. The longer your code is, the more time it will take to compile.\nOne common issue leading to long compile times is the use of Python control flow such as for loops. JAX's tracing machinery essentially flattens out these loops (see JAX Sharp Bits: Control Flow). In your case, you loop over 4800 entries in your data structure, and thus are creating a very long and inefficient program.\nThe typical solution in a case like this is to rewrite your program using jax.vmap. Like most JAX constructs, this works best with a struct-of-arrays pattern rather than the array-of-structs pattern used in your data. So the first step to using vmap is to restructure your data in a way that JAX can use; it might look something like this:\nitimes = jnp.arange(len(idc['times']))\nprf = jnp.array([idc['data'][i]['prf'] for i in itimes])\nerrs = jnp.array([idc['data'][i]['errs'] for i in itimes])\n\nsprf = jax.vmap(model, in_axes=[None, None, 0, 0, 0])(fssc, fssh, itimes, rv, amp)\nchi2 = jnp.sum((oprf - sprf) / oprf_errs) ** 2) / len(times) / sprf.shape[1]\nThis will not work directly: you'll also have to restructure the data used by your model function into the struct-of-arrays style, but hopefully this gives you the general idea.\nNote also that this assumes that every entry of idc['data'][i]['prf'] and idc['data'][i]['errs'] has the same shape. If that's not the case, then I'm afraid your problem is not particularly well-suited to JAX's SPMD programming model, and there's not an easy way to work around the need for long compilations."
        ],
        "link": "https://stackoverflow.com/questions/78174997/slow-jax-optimization-with-scipyboundedminimize-and-optax-seeking-speedup-stra"
    },
    {
        "title": "Equivalent of `jax.lax.cond` for multiple boolean conditions",
        "question": "Currently jax.lax.cond works for one boolean condition. Is there a way to extend it to multiple boolean conditions?\nAs an example, below is an untraceable function:\ndef func(x):\n    if x < 0: return x\n    elif (x >= 0) & (x < 1): return 2*x\n    else: return 3*x\nHow to write this function in JAX in a traceable way?",
        "answers": [
            "One compact way to write something like this is using jnp.select:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef func(x):\n  return jnp.select([x < 0, x < 1], [x, 2 * x], default=3 * x)\n\nx = jnp.array([-0.5, 0.5, 1.5])\nprint(func(x))\n# [-0.5  1.   4.5]"
        ],
        "link": "https://stackoverflow.com/questions/78122820/equivalent-of-jax-lax-cond-for-multiple-boolean-conditions"
    },
    {
        "title": "\"The truth value of an array with more than one element is ambiguous\" when trying to train a new JAX+Equinox model a second time",
        "question": "TL;DR: I create a new instance of my equinox.Module model and fit it using Optax. Everything works fine. When I create a new instance of the same model and try to fit it from scratch, using the same code, same initial values, same everything, I get:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n...somewhere deep in Optax code. My code doesn't compare any arrays. The error message doesn't show where exactly the comparison happens. What's wrong?\nCode\n# 1. Import dependencies.\nimport jax; jax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as np, jax.random as rnd, equinox as eqx\nimport optax\n\n# 2. Define loss function. I'm fairly confident this is correct.\ndef npdf(x, var):\n    return np.exp(-0.5 * x**2 / var) / np.sqrt(2 * np.pi * var)\n\ndef mixpdf(x, ps, vars):\n    return ps.dot(npdf(x, vars))\n\ndef loss(model, series):\n    weights, condvars = model(series)\n    return -jax.vmap(\n        lambda x, vars: np.log(mixpdf(x, weights, vars))\n    )(series[1:], condvars[:-1]).mean()\n\n# 3. Define recurrent neural network.\nclass RNNCell(eqx.Module):\n    bias: np.ndarray\n    Wx: np.ndarray\n    Wh: np.ndarray\n    def __init__(self, ncomp: int, n_in: int=1, *, key: np.ndarray):\n        k1, k2, k3 = rnd.split(key, 3)\n        self.bias = rnd.uniform(k1, (ncomp, ))\n        self.Wx = rnd.uniform(k2, (ncomp, n_in))\n        self.Wh = 0.9 * rnd.uniform(k3, (ncomp, ))\n\n    def __call__(self, vars_prev, obs):\n        vars_new = self.bias + self.Wx @ obs + self.Wh * vars_prev\n        return vars_new, vars_new\n\nclass RNN(eqx.Module):\n    cell: RNNCell\n    logits: np.ndarray\n    vars0: np.ndarray = eqx.field(static=True)\n\n    def __init__(self, vars0: np.ndarray, n_in=1, *, key: np.ndarray):\n        self.vars0 = np.array(vars0)\n        K = len(self.vars0)\n        self.cell = RNNCell(K, n_in, key=key)\n        self.logits = np.zeros(K)\n\n    def __call__(self, series: np.ndarray):\n        _, hist = jax.lax.scan(self.cell.__call__, self.vars0, series**2)\n        return jax.nn.softmax(self.logits), abs(hist)\n\n    def condvar(self, series):\n        weights, variances = self(series)\n        return variances @ weights\n\n    def predict(self, series: np.ndarray):\n        return self.condvar(series).flatten()[-1]\n\n# 4. Training/fitting code.\ndef fit(model, logret, nepochs: int, optimizer, loss):\n    loss_and_grad = eqx.filter_value_and_grad(loss)\n    \n    @eqx.filter_jit\n    def make_step(model, opt_state):\n        loss_val, grads = loss_and_grad(model, logret)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss_val, model, opt_state\n\n    opt_state = optimizer.init(model)\n    for epoch in range(nepochs):\n        loss_val, model, opt_state = make_step(model, opt_state)\n    print(\"Works!\")\n    return model\n\ndef experiment():\n    series = rnd.normal(rnd.PRNGKey(8), (100, 1))\n    model = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\n    return fit(model, series, 100, optax.adam(0.01), loss)\n\n# 5. Run the exact same code twice.\nexperiment() # 1st call, works\nexperiment() # 2nd call, error\nError message\n> python my_RNN.py\nWorks!\nTraceback (most recent call last):\n  File \"/Users/forcebru/test/my_RNN.py\", line 75, in <module>\n    experiment() # 2nd call, error\n    ^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 72, in experiment\n    return fit(model, series, 100, optax.adam(0.01), loss)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 65, in fit\n    loss_val, model, opt_state = make_step(model, opt_state)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 206, in __call__\n    return self._call(False, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_module.py\", line 935, in __call__\n    return self.__func__(self.__self__, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 200, in _call\n    out = self._cached(dynamic_donate, dynamic_nodonate, static)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 495, in common_infer_params\n    jaxpr, consts, out_shardings, out_layouts_flat, attrs_tracked = _pjit_jaxpr(\n                                                                    ^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1150, in _pjit_jaxpr\n    jaxpr, final_consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n                                                   ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 350, in memoized_fun\n    ans = call(fun, *args)\n          ^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1089, in _create_pjit_jaxpr\n    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/profiler.py\", line 336, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2314, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2336, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 49, in fun_wrapped\n    out = fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 59, in make_step\n    updates, opt_state = optimizer.update(grads, opt_state)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/combine.py\", line 59, in update_fn\n    updates, new_s = fn(updates, s, params, **extra_args)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/base.py\", line 337, in update\n    return tx.update(updates, state, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/transform.py\", line 369, in update_fn\n    mu_hat = bias_correction(mu, b1, count_inc)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 491, in common_infer_params\n    canonicalized_in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 4, in __eq__\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 745, in __bool__\n    check_bool_conversion(self)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 662, in check_bool_conversion\n    raise ValueError(\"The truth value of an array with more than one element is \"\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nProblem\nThe error message says File \"<string>\", line 4, in __eq__, which doesn't help.\nIt refers to the line mu_hat = bias_correction(mu, b1, count_inc) in Optax code, but as far as I understand, it doesn't compare any arrays.\nIt also refers to JAX code that's supposedly responsible for JIT compilation, but this seems outside my control.\nIs there a bug in my model definition (RNNCell or RNN)? Did I implement the training loop wrong? I basically copied it straight from Equinox docs, so it should be fine. Why does it work when I call experiment() the first time, but not the second?",
        "answers": [
            "It appears this is a bug in equinox. The function _process_in_axis_resources is decorated in functools.lru_cache, meaning that all inputs are checked for equality with arguments from the previous call. On the second run, this triggers a call to equinox.Module.__eq__, which raises the error. You can see this problem by doing the equality check directly:\nmodel = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel2 = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel == model2\n# ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nI would suggest reporting this bug at https://github.com/patrick-kidger/equinox/issues\nYou could probably work around this issue by not storing a numpy array (vars0) as a static attribute. I suspect that equinox assumes that all static attributes are hashable, and numpy arrays are not.\nEdit: I just checked, and changing this:\nvars0: np.ndarray = eqx.field(static=True)\nto this:\nvars0: np.ndarray\nresolves the issue.\nEdit 2: Indeed it looks like static fields in equinox must be hashable, so this is not an equinox bug but rather a usage error (see the discussion at https://github.com/patrick-kidger/equinox/issues/154#issuecomment-1561735995). You might try storing vars0 as a tuple (which is hashable) rather than an array (which isn't)."
        ],
        "link": "https://stackoverflow.com/questions/78074623/the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous-when-tryin"
    },
    {
        "title": "Jax scan with dynamic number of iterations",
        "question": "I wanted to perform a scan with a dynamic number of iterations. To accomplish that, I want to recompile the function each time when iters_to_do changes.\nTo avoid a huge slowdown, I'll be using a recompilation_cache but that's beside the point.\nHowever, when I mark the argument in @partial(jax.jit) I'm still obtaining a concretization error:\n@partial(jax.jit, static_argnums=(3))\ndef iterate_for_steps(self,\n                        interim_thought: Array, \n                        mask: Array,\n                        iters_to_do: int, \n                        input_arr: Array, \n                        key: PRNGKeyArray) -> Array:\n\n    # These are constants\n    input_arr = input_arr.astype(jnp.bfloat16)\n    interim_thought = interim_thought.astype(jnp.bfloat16)\n    \n    def body_fun(i: int, thought: Array) -> Array:\n        latent = jnp.concatenate([thought, input_arr], axis=-1).astype(jnp.bfloat16)\n        latent = self.main_block(latent, input_arr, mask, key).astype(jnp.bfloat16)\n        latent = jax.vmap(self.post_ln)(latent).astype(jnp.bfloat16)  # LN to keep scales tidy\n\n        return latent\n    \n    iters_to_do = iters_to_do.astype(int).item()\n    final_val = jax.lax.scan(body_fun, interim_thought, xs=None, length=iters_to_do)\n    \n    return final_val\nFull traceback is here.\nI've tried marking multiple arguments with @partial but to no avail.\nI'm not sure how to approach debugging this - with a python debugger, I'm getting no help apart from the fact that its definitely a tracer.\nMRE\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\ninit = jnp.ones((5,))\niterations = jnp.array([1, 2, 3])\n\n@partial(jax.jit, static_argnums=(0,))\ndef iterate_for_steps(iters: int):\n    def body_fun(carry):\n        return carry * 2\n    \n    iters = iters.astype(int)\n    output = jax.lax.scan(body_fun, init, xs=None, length=iters)\n    \n    return output\n\nprint(jax.vmap(iterate_for_steps)(iterations))",
        "answers": [
            "First of all, the number of iterations in a scan must be static. If you want something similar to scan that allows a dynamic number of iterations, you can take a look at while_loop.\nRegarding your code: in isolation, your fix of marking iters_to_do as static using static_argnums is probably roughly the right idea, so long as you are passing a static int in this position when you call the function.\nBut the fact that you are calling the astype array method in your function (in iters_to_do.astype(int).item()) and getting a ConcretizationError rather than an AttributeError makes me think that the error you linked to is not coming from the code as pasted in your question.\nTo help address this discrepancy, I'd suggest trying to construct a minimal reproducible example of the problem you're having. Without that, any answer to your question is going to require too much guesswork regarding what code you're actually executing.",
            "One can use equinox's (internal as of right now) while_loop implementation which would also be able to handle a dynamic amount of iterations with checkpointing to reduce memory usage.\nNote that this can be used as a drop-in replacement to jax's native while_loop. One can also use equinox's eqx.internal.scan if they wish to leverage similar checkpointing with scan."
        ],
        "link": "https://stackoverflow.com/questions/78070050/jax-scan-with-dynamic-number-of-iterations"
    },
    {
        "title": "How to train a model using gradient descent with multioutput (vector-valued) loss function in JAX?",
        "question": "I am trying to train a model that has two outputs with gradient descent. My cost function therefore returns two errors. What is the typical way to deal with this problem?\nI've seen mentions here and there of this problem, but I haven't come up with a satisfactory solution.\nThis is a toy example that reproduces my problem:\nfrom jax import jit, random, grad\nimport optax\n\n\n@jit\ndef my_model(forz, params):\n    a, b = params\n\n    a_vect = a + forz**b\n    b_vect = b + forz**a\n\n    return a_vect, b_vect*50.\n\n\n@jit\ndef rmse(predictions, targets):\n\n    rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n    return rmse\n\n\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a, loss_b\n\n\ngrad_myloss = jit(grad(my_loss, argnums=1))\n\n# synthetic true data\nkey = random.PRNGKey(758493)\nforz = random.uniform(key, shape=(1000,))\n\ntrue_params = [8.9, 6.6]\ntrue_a, true_b = my_model(forz, true_params)\n\n# Train\nmodel_params = random.uniform(key, shape=(2,))\noptimizer = optax.adabelief(1e-1)\nopt_state = optimizer.init(model_params)\n\nfor i in range(1000):\n\n    grads = grad_myloss(forz, model_params, true_a, true_b)  # this fails\n    updates, opt_state = optimizer.update(grads, opt_state)\n    model_params = optax.apply_updates(model_params, updates)\nI understand that either the two errors has to be somehow aggregated to a single one implementing some kind of normalization to the losses (my output vectors have non-comparable units),\n@jit\ndef normalized_rmse(predictions, targets):\n   std_dev_targets = jnp.std(targets)\n   rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n   return rmse/std_dev_targets\n\n\n@jit\ndef my_loss_single(forz, params, true_a, true_b):\n\n   sim_a, sim_b = my_model(forz, params)\n\n   loss_a = normalized_rmse(sim_a, true_a)\n   loss_b = normalized_rmse(sim_b, true_b)\n\n   return jnp.sqrt((loss_a ** 2) + (loss_b * 2)) \nor I should use the Jacobian matrix (jacrev) somehow?",
        "answers": [
            "optax, like most optimization frameworks, is only able to optimize a single-valued loss function. You should decide what single-valued loss makes sense for your particular problem. A good option given the RMS form of your individual losses might be the square sum:\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a ** 2 + loss_b ** 2\nWith this change, your code executes without an error."
        ],
        "link": "https://stackoverflow.com/questions/78044014/how-to-train-a-model-using-gradient-descent-with-multioutput-vector-valued-los"
    },
    {
        "title": "Using Orbax to checkpoint flax `TrainState` with new `CheckpointManager` API",
        "question": "Context\nThe Flax docs describe how to checkpoint a flax.training.train_state.TrainState with orbax. In a nutshell, you set up a orbax.checkpoint.CheckpointManager which keeps track of checkpoints. Next, you use the CheckpointManager to save the state to disk. Summarising the code snippets from the Flax docs:\nimport orbax\n\n# <-- Code building an empty and a full chkpt. -->.\nabstract_chkpt = ...\nchkpt = ...\n\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\nsave_args = orbax_utils.save_args_from_target(ckpt)\n\noptions = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_manager = orbax.checkpoint.CheckpointManager(\n    '/tmp/flax_ckpt/orbax/managed', orbax_checkpointer, options)\n\n# Save and restore a checkpoint.\ncheckpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args})\ncheckpoint_manager.restore(1, items=abstract_ckpt)\nThe notebook provided by the Flax docs does what I want: periodically track TrainState, which can then be restored. However, when executing the code provided by the Flax docs warn that this orbax checkpoint API is deprecated:\nWARNING:absl:Configured CheckpointManager using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\nThe link indicated by the error message gives some pointers how to use the new orbax.checkpoint.CheckpointManager.\nQuestion\nHow do I save and restore a Flax TrainState with the new orbax.checkpoint.CheckpointManager API?\nHere is my failed attempt (based on the Orbax migration instructions) at saving and restoring a trivial flax.training.train_state.TrainState:\nimport orbax.checkpoint as obc\nfrom flax.training.train_state import TrainState\n\nabstract_ckpt = TrainState(step=0, apply_fn=lambda _: None, params={}, tx={}, opt_state={})\nckpt = abstract_ckpt.replace(step=1)\n\n# Set up the checkpointer.\noptions = obc.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_dir = obc.test_utils.create_empty('/tmp/checkpoint_manager')\ncheckpoint_manager = obc.CheckpointManager(checkpoint_dir, options=options)\nsave_args = obc.args.StandardSave(abstract_ckpt)\n\n# Do actual checkpointing.\ncheckpoint_manager.save(1, ckpt, args=save_args)\n\n# Restore checkpoint.\nrestore_args = obc.args.StandardRestore(abstract_ckpt)\nrestored_ckpt = checkpoint_manager.restore(1, args=restore_args)\n\n# Verify if it is correctly restored.\nassert ckpt.step == restored_ckpt.step  # AssertionError\nMy guess would be that the problem relates to save_args, but I haven't managed to pinpoint the problem and figure out a fix. Any suggestions how to correctly restore the checkpoint using the new CheckpointManager API?",
        "answers": [
            "You created save_args = ocp.args.StandardSave(abstract_ckpt) instead of save_args = ocp.args.StandardSave(ckpt), so you're just saving the wrong thing.\nAlso note that checkpoint_dir = ocp.test_utils.create_empty('/tmp/checkpoint_manager') is a bit unnecessary - it's just a test utility for deleting a directory if it already exists - makes running our colabs a bit easier. Probably you shouldn't need to use it in real life, as the create option in CheckpointManager will create the directory for you."
        ],
        "link": "https://stackoverflow.com/questions/78033458/using-orbax-to-checkpoint-flax-trainstate-with-new-checkpointmanager-api"
    },
    {
        "title": "JAX jax.grad on simple function that takes an array: `ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected`",
        "question": "I'm trying to implement this function and use JAX to automatically build the gradient function:\n$f(x) = \\sum\\limits_{k=1}^{n-1} [100 (x_{k+1} - x_k^2)^2 + (1 - x_k)^2]$\n(sorry, I don't know how to format math on stackoverflow. Some sister sites allow TeX, but apparently this site does not?)\nimport jax\nimport jax.numpy as jnp\n\n# x is an array, which does not handle type hints well.\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].item()\n        xip1 = x[i].item()\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\n\n\n# with n=2.\ndef rosenbrock2(x: any) -> float:\n    return rosenbrock(2, x)\n\n\ngrad_rosenbrock2 = jax.grad(rosenbrock2)\n\nx = jnp.array([-1.2, 1], dtype=jnp.float32).reshape(2,1)\n\n# this line fails with the error given below\ngrad_rosenbrock2(x)\nThis last line results in:\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[1].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nI'm trying to follow the docs, and I'm confused. This is my first time using JAX or Autograd, can someone help me resolve this? Thanks!",
        "answers": [
            "The problem is that the .item() method attempts to convert an array to a static Python scalar, and since you have traced arrays within your grad transformation, conversion to a static value is not possible.\nWhat you need here is to convert a size-1 array to a scalar array, which you can do using .reshape(()):\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].reshape(())\n        xip1 = x[i].reshape(())\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\nFor more background on jax transformations and traced arrays, I'd recommend How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78030853/jax-jax-grad-on-simple-function-that-takes-an-array-concretizationtypeerror-a"
    },
    {
        "title": "How to return last index with jnp.where in jit function",
        "question": "Say I have two arrays:\nz = jnp.array([[5.55751118],\n              [5.18212974],\n              [4.35981727],\n              [3.4559711 ],\n              [3.35750248],\n              [2.65199945],\n              [2.02298999],\n              [1.59444971],\n              [0.80865185],\n              [0.77579791]])\n\nz1 = jnp.array([[ 1.58559484],\n               [ 3.79094097],\n               [-0.52712522],\n               [-1.0178286 ],\n               [-3.51076985],\n               [ 1.30108161],\n               [-1.29824303],\n               [-0.19209007],\n               [ 0.37451138],\n               [-2.33619987]])\nI would like to start at the first row in array z and find where in the second matrix a second value is within a threshold of this value.\nexample without @jit: I would like to return the last index of array z1. Value should be -3.51x\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nWhen using JIT (as needed in a larger scale model)\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nthis error is produced:\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThe error occurred while tracing the function test at /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:9 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:bool[10,1] = ge b c\n    from line /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:11:10 (test)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
        "answers": [
            "The problem is that jnp.where returns a dynamically-sized array, and JAX transformations like jit are not compatible with dynamically-sized arrays (See JAX Sharp Bits: Dynamic Shapes). You can pass a size argument to jnp.where to make the result statically sized. Since we don't know how many elements will be returned, we can choose the maximum possible number of returned elements, which is idx.shape[0]. Since the result will be padded with zeros, the maximum index will give what you're looking for:\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx, size=idx.shape[0])[0].max()]\n    return val\ntest()"
        ],
        "link": "https://stackoverflow.com/questions/78030406/how-to-return-last-index-with-jnp-where-in-jit-function"
    },
    {
        "title": "Can jax.vmap() do a hstack()?",
        "question": "As the title says, I currently manually hstack() the first axis of a 3D array returned by jax.vmap(). In my code, the copy operation in hstack() is a currently a speed bottleneck. Can I avoid this by instructing jax.vmap() to do this directly?\nHere is a simplified example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(a, b, c):\n  return jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n\ndef arr(m, n):\n  return jnp.arange(m*n).reshape((m, n))\n\nm = 3\n\na = arr(m, 2)\nb = arr(m, 5)\nc = arr(m, 7)\n\nfv = jax.vmap(f)\n\nvmap_output = fv(a, b, c)\ndesired_output = jnp.hstack(fv(a, b, c))\n\nprint(vmap_output)\nprint(desired_output)\nThis yields:\n# vmap() output\n[[[  1.  10.]\n  [ 21.   0.]]\n\n [[  5.  35.]\n  [ 70.   0.]]\n\n [[  9.  60.]\n  [119.   0.]]]\n# Desired output\n[[  1.  10.   5.  35.   9.  60.]\n [ 21.   0.  70.   0. 119.   0.]]\nIf this is not possible, I would resort to pre-allocating an array and simply writing to the columns manually, but I hope to avoid this. Thanks for any clue!\nUpdate from @jakevdp's answer\nAlright, it isn't possible. So I resort to writing to the columns, but this fails as well:\ndef g(output, idx, a, b, c):\n  block = jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n  jax.lax.dynamic_update_slice_in_dim(output, block, idx*2, axis=1)\n\n# Defined above: jax, jnp, m, a, b, c\n\ng_output = jnp.zeros((2, 2*m))\nidxs = jnp.arange(m)\n\ngv = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n\ngv(g_output, idxs, a, b, c)\n\nprint(g_output)\nThis yields:\n[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\nSo writing to g_output in the function g is not retained. Is there a way around this?",
        "answers": [
            "No, vmap does not have any built-in capability to stack outputs differently than the batching semantics would imply. But if you're interested in fusing the hstack operation with the vmap operation to the extent possible, you could do so by wrapping it in jit. For example:\n@jax.jit\ndef do_the_thing(a, b, c):\n  return jnp.hstack(fv(a, b, c))\n\nprint(do_the_thing(a, b, c))\nEdit: responding to your edited question: the reason the result is all zeros is because your function doesn't do anything: it returns None, so there's no way for it to affect the input array called g_output. JAX requires pure functions so side-effecting code like what you wrote above is not compatible. If you wanted to replace the hstack with an indexed update, you could do something like this:\ni = jnp.arange(2).reshape(1, 2, 1)\nj = jnp.arange(6).reshape(3, 1, 2)\ng_output = jnp.zeros((2, 2*m)).at[i, j].set(fv(a, b, c))\nbut a nontrivial scatter operation like this will not typically be faster than a simple reshape, especially if you're running on an accelerator like GPU.\nIf your arrays are large enough that reshapes are costly, you might find that a more direct implementation is better; for example:\n@jax.jit\ndef g(a, b, c):\n  output = jnp.zeros((2, 6))\n  output = output.at[0, 0::2].set(a.sum(1))\n  output = output.at[0, 1::2].set(b.sum(1))\n  output = output.at[1, 0::2].set(c.sum(1))\n  return output\n\ng_output = g(a, b, c)"
        ],
        "link": "https://stackoverflow.com/questions/78027629/can-jax-vmap-do-a-hstack"
    },
    {
        "title": "How can I use PyTorch 2.2 with Google Colab TPUs?",
        "question": "I'm having trouble getting PyTorch 2.2 running with TPUs on Google Colab. I'm getting an error about a JAX bug, but I'm confused about this because I'm not doing anything with JAX.\nMy setup process is very simple:\n!pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html\nAnd then\nimport torch\nimport torch_xla.core.xla_model as xm\nwhich gives the error\n/usr/local/lib/python3.10/dist-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')\n This a JAX bug; please report an issue at https://github.com/google/jax/issues\n  _warn(f\"cloud_tpu_init failed: {repr(exc)}\\n This a JAX bug; please report \"\n/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nThen trying\nt1 = torch.tensor(100, device=xm.xla_device())\nt2 = torch.tensor(200, device=xm.xla_device())\nprint(t1 + t2)\ngives the error\n2 frames\n/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py in xla_device(n, devkind)\n    121 \n    122   if n is None:\n--> 123     return torch.device(torch_xla._XLAC._xla_get_default_device())\n    124 \n    125   devices = xm.get_xla_supported_devices(devkind=devkind)\n\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: No ba16c7433 device found.",
        "answers": [
            "Colab currently only provides an older generation of TPUs which is not compatible with recent JAX or PyTorch releases. It’s possible that may change in the future, but I don’t know of any official timeline of when that might happen. In the meantime, you can access recent-generation TPUs via Kaggle or Google Cloud."
        ],
        "link": "https://stackoverflow.com/questions/78014487/how-can-i-use-pytorch-2-2-with-google-colab-tpus"
    },
    {
        "title": "Does jax save the jaxpr of jit compiled functions?",
        "question": "Consider the following example:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n    \nprint(test(jnp.ones(8,)))\nprint(test(jnp.ones(3,)))\nThe output is\n1\n-1\nHowever, I thought that on the first call jax compiles a function to use in subsequent calls. Shouldn't this then give the output 1 and 1, because jax traces through an if and does not use a conditional here? In the jaxpr of the first call is no conditional:\n{ lambda ; a:f32[8]. let\n    b:i32[] = pjit[name=test jaxpr={ lambda ; c:f32[8]. let  in (1,) }] a\n  in (b,) }\nSo how exactly does this work under the hood. Is the jaxpr unique for every call. Does jax only reuse jaxprs if the shape matches? Does jax recompile functions if the shape is different?",
        "answers": [
            "JAX does cache the jaxpr and compiled artifact for each compatible call of the function. This compatibility is determined via the cache key, which contains the shape and dtype of array arguments, as well as the hash of any static arguments and some additional information such as global flags that may affect the computation. Any time something in the cache key changes, it results in a new tracing & compilation of the function. You can see this by printing the _cache_size() of the compiled function. For example:\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n\nx8 = jnp.ones(8)\nx3 = jnp.ones(3)\n\nprint(test._cache_size())  # no calls yet, so no cache\n# 0\n\ntest(x8)\nprint(test._cache_size())  # first call caches the jaxpr\n# 1\n\ntest(x8)\nprint(test._cache_size())  # repeated call, so size doesn't change\n# 1\n\ntest(x3)\nprint(test._cache_size())  # new call, so size increases\n# 2\n\ntest(x8)\nprint(test._cache_size())  # repeated call -> size doesn't change\n# 2\nBy keeping track of these static attributes, jit-compiled functions can change their output based on static attributes, but still avoid recompilation for compatible inputs."
        ],
        "link": "https://stackoverflow.com/questions/78011718/does-jax-save-the-jaxpr-of-jit-compiled-functions"
    },
    {
        "title": "multivariate derivatives in jax - efficiency question",
        "question": "I have the following code which computes derivatives of the function:\nimport jax\nimport jax.numpy as jnp\n\n\ndef f(x):\n    return jnp.prod(x)\n\n\ndf1 = jax.grad(f)\ndf2 = jax.jacobian(df1)\ndf3 = jax.jacobian(df2)\nWith this, all the partial derivatives are available, for example (with vmap additionally):\nx = jnp.array([[ 1.,  2.,  3.,  4.,  5.],\n               [ 6.,  7.,  8.,  9., 10.],\n               [11., 12., 13., 14., 15.],\n               [16., 17., 18., 19., 20.],\n               [21., 22., 23., 24., 25.],\n               [26., 27., 28., 29., 30.]])\ndf3_x0_x2_x4 = jax.vmap(df3)(x)[:, 0, 2, 4]\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nThe question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?",
        "answers": [
            "The question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?\nEssentially, you're asking for a way to compute sparse Hessians and Jacobians; JAX does not have general support for this (see previous issue threads; e.g https://github.com/google/jax/issues/1032).\nEdit\nIn this particular case, though, since you're effectively computing the gradient/jaacobian with respect to a single element per derivative pass, you can do better by just applying the JVP to a single one-hot vector in each transformation. For example:\ndef deriv(f, x, v):\n  return jax.jvp(f, [x], [v])[1]\n\ndef one_hot(i):\n  return jnp.zeros(x.shape[1]).at[i].set(1)\n\ndf_x0 = lambda x: deriv(f, x, one_hot(0))\ndf2_x0_x2 = lambda x: deriv(df_x0, x, one_hot(2))\ndf3_x0_x2_x4 = lambda x: deriv(df2_x0_x2, x, one_hot(4))\nprint(jax.vmap(df3_x0_x2_x4)(x))\n# [  8.  63. 168. 323. 528. 783.]\nPrevious answer\nIf you're willing to relax your \"leaving f with a single argument\" criterion, you could do something like this:\ndef f(*x):\n  return jnp.prod(jnp.asarray(x))\n\ndf1 = jax.grad(f, argnums=4)\ndf2 = jax.jacobian(df1, argnums=2)\ndf3 = jax.jacobian(df2, argnums=0)\n\ndf3_x0_x2_x4 = jax.vmap(df3)(*(x.T))\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nHere rather than computing all gradients and slicing out the result, you are only computing the gradients with respect to the specific three elements you are interested in."
        ],
        "link": "https://stackoverflow.com/questions/78001753/multivariate-derivatives-in-jax-efficiency-question"
    },
    {
        "title": "JAX `custom_vjp` for functions with multiple outputs",
        "question": "In the JAX documentation, custom derivatives for functions with a single output are covered. I'm wondering how to implement custom derivatives for functions with multiple outputs such as this one?\n# want to define custom derivative of out_2 with respect to *args\ndef test_func(*args, **kwargs):\n    ...\n    return out_1, out_2",
        "answers": [
            "You can define custom derivatives for functions with any number of inputs and outputs: just add the appropriate number of elements to the primals and tangents tuples in the custom_jvp rule. For example:\nimport jax\nimport jax.numpy as jnp\n\n@jax.custom_jvp\ndef f(x, y):\n  return x * y, x / y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primals_out = f(x, y)\n  tangents_out = (x_dot * y + y_dot * x, \n                  x_dot / y - y_dot * x / y ** 2)\n  return primals_out, tangents_out\n\nx = jnp.float32(0.5)\ny = jnp.float32(2.0)\n\njax.jacobian(f, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))\nComparing this with the result computed using the standard non-custom derivative rule for the same function shows that the results are equivalent:\ndef f2(x, y):\n  return x * y, x / y\n\njax.jacobian(f2, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))"
        ],
        "link": "https://stackoverflow.com/questions/77952302/jax-custom-vjp-for-functions-with-multiple-outputs"
    },
    {
        "title": "Can't calculate matrix exponential in python",
        "question": "I want to calculate:\nfrom jax.scipy.linalg import expm\nimport jax.numpy as jnp\nfrom functools import lru_cache, reduce\n\nnum_qubits=2\ntheta = jnp.asarray(np.pi*np.random.random((15,2,2,2,2,2,2,2,2)))\n\ndef pauli_matrix(num_qubits):\n    _pauli_matrices = jnp.array(\n    [[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]]\n    )\n    return reduce(jnp.kron, (_pauli_matrices for _ in range(num_qubits)))[1:]\n\ndef SpecialUnitary(num_qubits,theta):\n    assert theta.shape[0] == 15\n    A = jnp.tensordot(theta, pauli_matrix(num_qubits), axes=[[0], [0]])\n    print(f'{A.shape= }{pauli_matrix(num_qubits).shape=}{theta.shape=}')\n    return expm(1j*A/2)\n\nSpecialUnitary(num_qubits,theta)\nShapes: A.shape= (2, 2, 2, 2, 2, 2, 2, 2, 4, 4)pauli_matrix(num_qubits).shape=(15, 4, 4)theta.shape=(15, 2, 2, 2, 2, 2, 2, 2, 2) Error: ValueError: expected A to be a square matrix\nI'm stuck because the documentation says that the expm is calculated on the last two axes, which must be square, which is done.",
        "answers": [
            "Batched expm is supported in recent JAX versions, and you should find that this works fine in JAX v0.4.7 or newer:\nimport jax.numpy as jnp\nimport jax.scipy.linalg\n\nX = jnp.arange(128.0).reshape(2, 2, 2, 4, 4)\n\nresult = jax.scipy.linalg.expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)\nIf for some reason you must use an older JAX version, you can work around this by using jax.numpy.vectorize. For example:\nexpm = jnp.vectorize(jax.scipy.linalg.expm, signature='(n,n)->(n,n)')\n\nresult = expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)"
        ],
        "link": "https://stackoverflow.com/questions/77946266/cant-calculate-matrix-exponential-in-python"
    },
    {
        "title": "Jax Implementation of function similar to Torch's 'Scatter'",
        "question": "For graph learning purposes, I am trying to implement a global sum batching function, that takes as inputs batched graph representations 'x' of size (n x d) and a corresponding vector of batches (n x 1). I then want to compute the sum over all graph representations for each batch. Here is a graphical representation: torch's scatter function\nThis is my current attempt:\ndef global_sum_pool(x, batch):\n    graph_reps = []\n    i = 0\n    n = jnp.max(batch)\n    while True:\n        ind = jnp.where(batch == i, True, False).reshape(-1, 1)\n        ind = jnp.tile(ind, x.shape[1])\n        x_ind = jnp.where(ind == True, x, 0.0)\n        graph_reps.append(jnp.sum(x_ind, axis=0))\n        if i == n:\n            break\n        i += 1\n    return jnp.array(graph_reps)\nI get the following exception on the line if i == n:\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function make_step at /venvs/jax_env/lib/python3.11/site-packages/equinox/_jit.py:37 for jit. \nI understand this is due to the fact that at compile time, Jax does not a priori know the max value of the 'batch' array and hence cannot allocate memory. Does anyone know a workaround or different implementation?",
        "answers": [
            "Rather than implementing this via a for loop, you should use JAX's built-in scatter operator. The most convenient interface for this is the Array.at syntax. If I understand your goal correctly, it might look something like this:\nimport jax.numpy as jnp\nimport numpy as np\n\n# Generate some data\nnum_batches = 4\nn = 10\nd = 3\nx = np.random.randn(n, d)\nind = np.random.randint(low=0, high=num_batches, size=(n,))\n\n#Compute the result with jax.lax.scatter\nresult = jnp.zeros((num_batches, d)).at[ind].add(x)\nprint(result.shape)\n# (4, 3)"
        ],
        "link": "https://stackoverflow.com/questions/77932146/jax-implementation-of-function-similar-to-torchs-scatter"
    },
    {
        "title": "JAX `vjp` fails for vmapped function with `custom_vjp`",
        "question": "Below is an example where a function with a custom-defined vector-Jacobian product (custom_vjp) is vmapped. For a simple function like this, invoking vjp fails:\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef test_func(f: Callable[..., float],\n              R: Array\n              ) -> float:\n\n    return f(jnp.dot(R, R))\n\n\ndef test_func_fwd(f, primal):\n\n    primal_out = test_func(f, primal)\n    residual = 2. * primal * primal_out\n    return primal_out, residual\n\n\ndef test_func_bwd(f, residual, cotangent):\n\n    cotangent_out = residual * cotangent\n    return (cotangent_out, )\n\n\ntest_func.defvjp(test_func_fwd, test_func_bwd)\n\ntest_func = vmap(test_func, in_axes=(None, 0))\n\n\nif __name__ == \"__main__\":\n\n    def f(x):\n        return x\n\n    # vjp\n    primal, f_vjp = vjp(partial(test_func, f),\n                        jnp.ones((10, 3))\n                        )\n\n    cotangent = jnp.ones(10)\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[0].shape)\nThe error message says:\nValueError: Shape of cotangent input to vjp pullback function (10,) must be the same as the shape of corresponding primal input (10, 3).\nHere, I think the error message is misleading, because the cotangent input should have the same shape as the primal output, which should be (10, ) in this case. Still, it's not clear to me why this error occurs.",
        "answers": [
            "The problem is that in test_func_fwd, you recursively call test_func, but you've overwritten test_func in the global namespace with its vmapped version. If you leave the original test_func unchanged in the global namespace, your code will work as expected:\n...\n\ntest_func_mapped = vmap(test_func, in_axes=(None, 0))\n\n... \n\nprimal, f_vjp = vjp(partial(test_func_mapped, f),\n                    jnp.ones((10, 3))\n                    )"
        ],
        "link": "https://stackoverflow.com/questions/77930920/jax-vjp-fails-for-vmapped-function-with-custom-vjp"
    },
    {
        "title": "JAX `vjp` does not recognize cotangent argument with `custom_vjp`",
        "question": "I have a JAX function cart_deriv() which takes another function f and returns the Cartesian derivative of f, implemented as follows:\n@partial(custom_vjp, nondiff_argnums=0)\ndef cart_deriv(f: Callable[..., float],\n               l: int,\n               R: Array\n               ) -> Array:\n\n    df = lambda R: f(l, jnp.dot(R, R))\n\n    for i in range(l):\n        df = jacrev(df)\n\n    return df(R)\n\n\ndef cart_deriv_fwd(f, l, primal):\n\n    primal_out = cart_deriv(f, l, primal)\n    residual = cart_deriv(f, l+1, primal)  ## just a test\n\n    return primal_out, residual\n\n\ndef cart_deriv_bwd(f, residual, cotangent):\n\n    cotangent_out = jnp.ones(3)  ## just a test\n\n    return (None, cotangent_out)\n\n\ncart_deriv.defvjp(cart_deriv_fwd, cart_deriv_bwd)\n\n\n\nif __name__ == \"__main__\":\n\n    def test_func(l, r2):\n        return l + r2\n\n    primal_out, f_vjp = vjp(cart_deriv, \n                            jax.tree_util.Partial(test_func),\n                            2,\n                            jnp.array([1., 2., 3.])\n                            )\n\n    cotangent = jnp.ones((3, 3))\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[1].shape)\nHowever this code produces the error:\nTypeError: cart_deriv_bwd() missing 1 required positional argument: 'cotangent'\nI have checked that the syntax agrees with that in the documentation. I'm wondering why the argument cotangent is not recognized by vjp, and how to fix this error?",
        "answers": [
            "The issue is that nondiff_argnums is expected to be a sequence:\n@partial(custom_vjp, nondiff_argnums=(0,))\nWith this properly defined, it's better to avoid wrapping the function in Partial, and just pass it as a static argument by closing over it in the vjp call:\nprimal_out, f_vjp = vjp(partial(cart_deriv, test_func),\n                        2,\n                        jnp.array([1., 2., 3.])\n                        )\ncotangent_out = f_vjp(jnp.ones((3, 3)))\n\nprint(*cotangent_out)\n# (b'',) [1. 1. 1.]"
        ],
        "link": "https://stackoverflow.com/questions/77924142/jax-vjp-does-not-recognize-cotangent-argument-with-custom-vjp"
    },
    {
        "title": "How to implement the next function (the use of Dynamic Shapes) in JAX?",
        "question": "I have a simple function that takes an jax Array as input, searches for the first occurrence of 1, and replaces it with another jax Array (specified as a second input):\nrules_int = [\n    jnp.array([0,0]),\n    jnp.array([1,1,1]),\n]\n# Even with the same size of inputs, the sizes of outputs can be different\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1)[0]\n    if index.size == 0:\n        return arr\n    index = index[0]\n    new_arr = jnp.concatenate([arr[:index], rules_int[action], arr[index+1:]])\n    return new_arr\n\nreplace_first_one(jnp.array([1]), 0)\n# result is Array([0, 0], dtype=int32)\nBut when I use vmap a get an exception:\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])  # Corresponding actions for each array\n\n# Vectorize the function\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nresult = vectorized_replace_first_one(batch_arr, batch_actions)\nindex = jnp.where(arr == 1)[0] The size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations. This BatchTracer with object id 140260750414512 was created on line:\nI read on JAX docs:\nJAX code used within transforms like jax.jit, jax.vmap, jax.grad, etc. requires all output arrays and intermediate arrays to have static shape: that is, the shape cannot depend on values within other arrays.\nPlease suggest how to make it work?\nIdeally, these rules should be applied recursively until there are no rules to apply. (string rewriting system)",
        "answers": [
            "As written, it is impossible to do this with vmap because the output of your function has a shape that depends on the value of action, and so the output would have to be a ragged array, which JAX does not support (see JAX Sharp Bits: Dynamic Shapes).\nTo make the function compatible with vmap, you'll have to adjust it so that it has static shape semantics: in particular, every entry of rules_int must have the same length, and you cannot return arr alone in cases where arr doesn't have any 1 entries. Making these changes and adjusting the logic to avoid dynamically-shaped intermediates, you could write something like this:\nimport jax\n\nrules_int = jnp.array([\n    [0,0],\n    [1,1],\n])\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1, size=1)[0][0]\n    arr_to_insert = rules_int[action]\n    output_size = len(arr) - 1 + len(arr_to_insert)\n    new_arr = jnp.where(jnp.arange(output_size) < index,\n                        jnp.concatenate([arr[:-1], arr_to_insert]),\n                        jnp.concatenate([arr_to_insert, arr[1:]]))\n    return jax.lax.dynamic_update_slice(new_arr, arr_to_insert, (index,))\n\nreplace_first_one(jnp.array([1]), 0)\n# Array([0, 0], dtype=int32)\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])\n\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nvectorized_replace_first_one(batch_arr, batch_actions)\n# Array([[0, 0, 4, 5, 1],\n#        [6, 1, 1, 8, 1]], dtype=int32)\nIf adjusting the semantics of your function in this way to avoid dynamic shapes is not viable given your use-case, then your use-case is unfortunately not compatible with vmap or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77915540/how-to-implement-the-next-function-the-use-of-dynamic-shapes-in-jax"
    },
    {
        "title": "Vectorizing power of `jax.grad`",
        "question": "I'm trying to vectorize the following \"power-of-grad\" function so that it accepts multiple orders: (see here)\ndef grad_pow(f, order, argnum):\n\n    for i in jnp.arange(order):\n        f = grad(f, argnums=argnum)\n\n    return f\nThis function produces the following error after applying vmap on the argument order:\njax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nIt arose in the jnp.arange argument 'stop'\nI have tried writing a static version of grad_pow using jax.lax.cond and jax.lax.scan, following the logic here:\ndef static_grad_pow(f, order, argnum):\n\n    order_max = 3  ## maximum order\n\n    def grad_pow(f, i):\n        return cond(i <= order, grad(f, argnum), f), None\n\n    return scan(grad_pow, f, jnp.arange(order_max+1))[0]\n\n\nif __name__ == \"__main__\":\n\n    test_func = lambda x: jnp.exp(-2*x)\n    test_func_grad_pow = static_grad_pow(jax.tree_util.Partial(test_func), 1, 0)\n    print(test_func_grad_pow(1.))\nNevertheless, this solution still produces an error:\n    return cond(i <= order, grad(f, argnum), f), None\nTypeError: differentiating with respect to argnums=0 requires at least 1 positional arguments to be passed by the caller, but got only 0 positional arguments.\nJust wondering how this issue can be resolved?",
        "answers": [
            "The fundamental issue with your question is that a vmapped function cannot return a function, it can only return arrays. All other details aside, that precludes any possibility of writing a valid function that does what you intend.\nThere are alternatives: for example, rather than attempting to create a function that will return a function, you could instead create a function that accepts arguments and applies that function to those arguments.\nIn that case, you'll run into another issue: if n is traced, there is no way to apply grad n times. JAX transformations like grad are evaluated at trace-time, and traced values like n are not available until runtime. One way to work around this is to pre-define all the functions you're interested in, and to use lax.switch to choose between them at runtime. The result would look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=[0], static_argnames=['argnum', 'max_order'])\ndef apply_multi_grad(f, order, *args, argnum=0, max_order=10):\n  funcs = [f]\n  for i in range(max_order):\n    funcs.append(jax.grad(funcs[-1], argnum))\n  return jax.lax.switch(order, funcs, *args)\n\n\norder = jnp.arange(3)\nx = jnp.ones(3)\nf = jnp.sin\n\nprint(jax.vmap(apply_multi_grad, in_axes=(None, 0, 0))(f, order, x))\n# [ 0.84147096  0.5403023  -0.84147096]\n\n# Compare by doing it manually:\nprint(jnp.array([f(x[0]), jax.grad(f)(x[1]), jax.grad(jax.grad(f))(x[2])]))\n# [ 0.84147096  0.5403023  -0.84147096]"
        ],
        "link": "https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad"
    },
    {
        "title": "jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed",
        "question": "I am trying to run multiple sbx programs (that use JAX) concurrently using joblib. Here is my program -\n'''\nFor installation please do -\npip install gym\npip install sbx-rl\npip install mujoco\npip install shimmy\n'''\nfrom joblib import Parallel, delayed\n\nimport gym\nfrom sbx import SAC\n\n# from stable_baselines3 import SAC\ndef train():\n\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env, verbose=1)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\ndef train_model():\n\n    train()\n\n\n\nif __name__ == '__main__':\n    Parallel(n_jobs=10)(delayed(train)() for i in range(3))\nThis is the error that I am getting -\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n2024-01-30 11:19:12.354168: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:\nINTERNAL: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory\n2024-01-30 11:19:12.354264: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\njoblib.externals.loky.process_executor._RemoteTraceback: \n\"\"\"\njax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/work/LAS/usr/tbd/5_test.py\", line 23, in my_func\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 109, in __init__\n    self._setup_model()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 126, in _setup_model\n    self.key = self.policy.build(self.key, self.lr_schedule, self.qf_learning_rate)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/policies.py\", line 143, in build\n    key, actor_key, qf_key, dropout_key = jax.random.split(key, 4)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 303, in split\n    return _return_prng_keys(wrapped, _split(typed_key, num))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 289, in _split\n    return prng.random_split(key, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 769, in random_split\n    return random_split_p.bind(keys, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 444, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 447, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 935, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 781, in random_split_impl\n    base_arr = random_split_impl_base(\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 787, in random_split_impl_base\n    return split(base_arr)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 786, in <lambda>\n    split = iterated_vmap_unary(keys_ndim, lambda k: impl.split(k, shape))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 1291, in threefry_split\n    return _threefry_split(key, shape)\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/work/LAS/usr/tbd/5_test.py\", line 27, in <module>\n    Parallel(n_jobs=3)(delayed(my_func)() for i in range(3))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1952, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\n    yield from self._retrieve()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1699, in _retrieve\n    self._raise_error_fast()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1734, in _raise_error_fast\n    error_job.get_result(self.timeout)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 736, in get_result\n    return self._return_or_raise()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 754, in _return_or_raise\n    raise self._result\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\nI am using a 40 GB GPU (a100-pcie). Therefore I doubt that my GPU is running out of memory. Please let me know if any clarification is needed.\nEdit 1: This is how I call my program - export XLA_PYTHON_CLIENT_PREALLOCATE=false && python 5_test.py (The name of my program is 5_test.py)",
        "answers": [
            "It appears you are using multiple processes targeting the same GPU. In each process, JAX will attempt to reserve 75% of the available GPU memory (see GPU memory allocation), so attempting this with two or more processes will exhaust the available memory.\nYou could fix this by turning off pre-allocation as mentioned in that doc, by setting the environment variables XLA_PYTHON_CLIENT_PREALLOCATE=false or XLA_PYTHON_CLIENT_MEM_FRACTION=.XX (with .XX set to .08 or something suitable), but I suspect the end result will be less efficient than if you had just run your full program from a single JAX process: multiple host processes targeting a single GPU device concurrently will just compete with each other for resources and lead to suboptimal results."
        ],
        "link": "https://stackoverflow.com/questions/77908236/jaxlib-xla-extension-xlaruntimeerror-internal-failed-to-execute-xla-runtime-ex"
    },
    {
        "title": "JAX `grad` error for function with `jax.lax.switch` and compound boolean conditions",
        "question": "I have encountered a scenario where applying jax.grad to a function with jax.lax.switch and compound boolean conditions yields jax.errors.TracerBoolConversionError. A minimal program to reproduce this behavior is the following:\nfrom jax.lax import switch\nimport jax.numpy as jnp\nfrom jax import grad\n\nfunc_0 = lambda x: jnp.where(0. < x < 1., x, 0.)\nfunc_1 = lambda x: jnp.where(0. < x < 1., x, 1.)\n\nfunc_list = [func_0, func_1]\n\nfunc = lambda index, x: switch(index, func_list, x)\n\ndf = grad(func, argnums=1)(1, 2.)\nprint(df)\nThe error is the following:\nTraceback (most recent call last):\n  File \"***/grad_test.py\", line 12, in <module>\n    df = grad(func, argnums=1)(1, 0.5)\n  File \"***/grad_test.py\", line 10, in <lambda>\n    func = lambda index, x: switch(index, func_list, x)\n  File \"***/grad_test.py\", line 5, in <lambda>\n    func_0 = lambda x: jnp.where(0 < x < 1., x, 0.)\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function <lambda> at ***/grad_test.py:5 for switch. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\nHowever, if the boolean condition is changed to a single condition (for example, x < 1), then no error occurs. I'm wondering if this could be a bug, or otherwise, how the original program should be changed.",
        "answers": [
            "You cannot use chained inequalities with JAX or NumPy arrays. Instead of 0 < x < 1, you should write (0 < x) & (x < 1) (note that due to operator precedence, the parentheses are not optional here)."
        ],
        "link": "https://stackoverflow.com/questions/77900299/jax-grad-error-for-function-with-jax-lax-switch-and-compound-boolean-conditi"
    },
    {
        "title": "Getting derivatives of NNs according to its inputs by batches in JAX",
        "question": "There is a neural network that takes as an input a two variables: net(x, t), where x is usually d-dim, and t is a scalar. The NN outputs a vector of length d. x and t might be batches, so x is of shape (b, d), and t is (b, 1), and the output is (b,d). I need to find\nderivative d out/dt of the NN output. It should be d dim vector (or (batch, d));\nderivative d out/dx of the NN\ngradient of divergence of the NN output according to x, it still should be (batch, d) vector\nSince the NN doesn’t output a scalar, I don’t think Jax grad would help here. I know how to do what I described in torch, but I’m totally new to JAX. I’d really appreciate your help with this question!\nThere is an example:\nimport jaxlib\nimport jax\nfrom jax import numpy as jnp\nimport flax.linen as nn\nfrom flax.training import train_state\n\n\n\nclass NN(nn.Module):\n    hid_dim : int # Number of hidden neurons\n    output_dim : int # Number of output neurons\n\n    @nn.compact  \n    def __call__(self, x, t):\n        out = jnp.hstack((x, t))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.Dense(features=self.output_dim)(out)\n        return out\n\nd = 3\nbatch_size = 10\nnet = NN(hid_dim=100, output_dim=d)\n\nrng_nn, rng_inp1, rng_inp2 = jax.random.split(jax.random.PRNGKey(100), 3)\ninp_x = jax.random.normal(rng_inp1, (1, d)) # batch, d\ninp_t = jax.random.normal(rng_inp2, (1, 1))\nparams_net = net.init(rng_nn, inp_x, inp_t)\n\nx = jax.random.normal(rng_inp2, (batch_size, d)) # batch, d\nt = jax.random.normal(rng_inp1, (batxh_size, 1))\n\nout_net = net.apply(params_net, x, t)\n\noptimizer = optax.adam(1e-3)\n\nmodel_state = train_state.TrainState.create(apply_fn=net.apply,\n                                            params= params_net,\n                                            tx=optimizer)\nI'd like to calculate an $L_2$ loss based on some derivatives of the NN's outputs according to its inputs. For example, I'd like to have d f/dx or d f/dt where f is the NN. ALso the gradient of the divergence by x. I assume it'd be something like\ndef find_derivatives(net, params, X, t):\n    d_dt = lambda net, params, X, t: jax.jvp(lambda time: net(params, X, t), (t, ), (jnp.ones_like(t), ))\n    d_dx = lambda net, params, X, t: jax.jvp(lambda X: net(params, X, t), (Xs_all, ), (jnp.ones_like(X), ))\n    out_f, df_dt = d_dt(net.apply, params, X, t)\n\n    d_ddx = lambda net, params, X, t: d_dx(lambda params, X, t: d_dx(net, params, X, t)[1], params, X, t)\n    df_dx, df_ddx = d_ddx(net.apply, params, X, t)\n    \n    return out_f, df_dt, df_dx, df_ddx\n\n\nout_f, df_dt, df_dx, df_ddx = find_derivatives(net, params_net, x, t)",
        "answers": [
            "I would avoid using jax.jvp here, because this is meant as a lower-level API. You can use jax.jacobian to compute the Jacobian (since your function has multiple outputs), and vmap for batching. For example:\ndf_dx = jax.vmap(\n    jax.jacobian(net.apply, argnums=1),\n    in_axes=(None, 0, 0)\n  )(params_net, x, t)\nprint(df_dx.shape)  # (10, 3, 3)\n\ndf_dt = jax.vmap(\n    jax.jacobian(net.apply, argnums=2),\n    in_axes=(None,0, 0)\n  )(params_net, x, t).reshape(10, 3)\nprint(df_dt.shape)  # (10, 3)\nHere df_dx is the batch-wise Jacobian of the 3-dimensional output vector with respect to the 3-dimensional x input vector, and df_dt is the batch-wise gradient of the 3-dimensional output vector with respect to the input t."
        ],
        "link": "https://stackoverflow.com/questions/77897419/getting-derivatives-of-nns-according-to-its-inputs-by-batches-in-jax"
    },
    {
        "title": "`jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].`",
        "question": "I am trying to perform multiprocessing to parallelize my program (that uses JAX) using pmap. I am a newbie with JAX and realize that maybe pmap isn't the right approach. The documentation here, said that pmap can express SPMD programs (which is the case here) and therefore I decided to use it.\nHere's my program. I am basically trying to run a reinforcement learning program (that uses JAX too) twice, using parallel execution -\n'''\nFor installation please do -\npip install gym\npip install sbx\npip install mujoco\npip install shimmy\n'''\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\nimport gym\nfrom sbx import SAC\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda _: my_func())(jnp.arange(2))\nI get the following error -\n(tbd) thoma@thoma-Lenovo-Legion-5-15IMH05H:~/PycharmProjects/tbd$ python new.py\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 678, in parallel_callable\n    pmap_computation = lower_parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 825, in lower_parallel_callable\n    jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 748, in stage_parallel_callable\n    jaxpr, out_sharded_avals, consts = pe.trace_to_jaxpr_final(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2233, in trace_to_jaxpr_final\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/core.py\", line 605, in __array__\n    raise TracerArrayConversionError(self)\njax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n   0% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/700,000  [ 0:00:00 < -:--:-- , ? it/s ]Exception ignored in: <function tqdm.__del__ at 0x7fa875eb8af0>\nTraceback (most recent call last):\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/std.py\", line 1149, in __del__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/rich.py\", line 120, in close\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1177, in __exit__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1163, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/live.py\", line 155, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1137, in line\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1674, in print\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1535, in _collect_renderables\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/protocol.py\", line 28, in rich_cast\nImportError: sys.meta_path is None, Python is likely shutting down\nBasically I am trying to replace the parallelization performed by joblib with JAX. Here's my original program that I am changing -\nfrom joblib import Parallel, delayed\nimport gym\nimport os\nfrom sbx import SAC\nimport multiprocessing\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\n\nParallel(n_jobs=2)(delayed(my_func)() for i in range(2))",
        "answers": [
            "The problem is here:\n File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\nThe sbx package is calling np.array on the inputs – this tells me that sbx is built on NumPy, not on JAX. JAX transformations like pmap are not compatible with NumPy functions, they require functions written with JAX operations. Unless sbx is substantially re-designed, you won't be able to use it with pmap, vmap, jit, grad, or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77892458/jax-errors-tracerarrayconversionerror-the-numpy-ndarray-conversion-method-ar"
    },
    {
        "title": "How to use JAX pmap with CPU cores",
        "question": "I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible - Here's my code -\nimport jax.numpy as jnp\nimport os\nfrom jax import pmap\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\nHere's the error -\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 682, in parallel_callable\n    pmap_executable = pmap_computation.compile()\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 923, in compile\n    executable = UnloadedPmapExecutable.from_hlo(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 993, in from_hlo\n    raise ValueError(msg.format(shards.num_global_shards,\njax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\nValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\nBased on this and this discussion, I did this os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8', but it doesn't seem to work.\nEdit 1:\nI tried this but it still doesn't work -\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\n\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)",
        "answers": [
            "XLA flags are read when JAX is imported, so you need to set them before importing JAX if you want the flags to have an effect.\nYou should also make sure you're in a clean runtime (i.e. not using a Jupyter kernel where you have previously imported jax).\nAdditionally, keep in mind that --xla_force_host_platform_device_count=8 only affects the host (CPU) device count, so the code as written above won't work if you're using GPU-enabled JAX with a single GPU device. If this is the case, you can force pmap to run on the non-default CPU devices using the devices argument:\nout = pmap(lambda x: x ** 2, devices=jax.devices('cpu')(jnp.arange(8))"
        ],
        "link": "https://stackoverflow.com/questions/77889712/how-to-use-jax-pmap-with-cpu-cores"
    },
    {
        "title": "Unexpected behavior of JAX `vmap` for multiple arguments",
        "question": "I have found that vmap in JAX does not behave as expected when applied to multiple arguments. For example, consider the function below:\ndef f1(x, y, z):\n    f = x[:, None, None] * z[None, None, :] + y[None, :, None]\n    return f\nFor x = jnp.arange(7), y = jnp.arange(5), z = jnp.arange(3), the output of this function has shape (7, 5, 3). However, for the vmap version below:\n@partial(vmap, in_axes=(None, 0, 0), out_axes=(1, 2))\ndef f2(x, y, z):\n    f = x*z + y\n    return f\nIt outputs this error:\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 5: axis 0 of argument y of type int32[5];\n  * one axis had size 3: axis 0 of argument z of type int32[3]\nCould someone kindly explain what's behind this error?",
        "answers": [
            "The semantics of vmap are that it does a single batching operation along one or more arrays. When you specify in_axes=(None, 0, 0), the meaning is \"map simultaneously along the leading dimension of y and z\": the error you're seeing is telling you that the leading dimensions of y and z have different sizes, and so they are not compatible for batching.\nYour function f1 essentially uses broadcasting to encode three batching operations, so to replicate that logic with vmap you'll need three applications of vmap. You can express that as follows:\n@partial(vmap, in_axes=(0, None, None))\n@partial(vmap, in_axes=(None, 0, None))\n@partial(vmap, in_axes=(None, None, 0))\ndef f2(x, y, z):\n    f = x*z + y\n    return f"
        ],
        "link": "https://stackoverflow.com/questions/77886057/unexpected-behavior-of-jax-vmap-for-multiple-arguments"
    },
    {
        "title": "How is it possible that jax vmap returns not iterable?",
        "question": "import jax\nimport pgx\nfrom jax import vmap, jit\nimport jax.numpy as jnp\n\nenv = pgx.make(\"tic_tac_toe\")\nkey = jax.random.PRNGKey(42)\n\nstates = jax.jit(vmap(env.init))(jax.random.split(key, 4))\ntype(states)\nstates has a type pgx.tic_tac_toe.State. I was expecting an Iterable object with a size 4. Somehow iterable results are inside pgx.tic_tac_toe.State.\nCan you please explain how is it possible that jax vmap returns not iterable?\nHow to force vmap to return the next result:\nstates = [env.init(key) for key in jax.random.split(key, 4)]\nNote, this code works as expected:\ndef square(x):\n    return x ** 2\ninputs = jnp.array([1, 2, 3, 4])\nresult = jax.vmap(square)(inputs)\nprint(result) # list object",
        "answers": [
            "Can you please explain how is it possible that jax vmap returns not iterable?\nWhen passed a non-array object, vmap will map the leading axes of each array in its flattened pytree representation. You can see the shapes in the flattened object here:\nprint([arr.shape for arr in jax.tree_util.tree_flatten(states)[0]])\n# [(4,), (4, 3, 3, 2), (4, 2), (4,), (4,), (4, 9), (4,), (4,), (4, 9)]\nThis is an example of the struct-of-arrays pattern used by vmap, where it sounds like you were expececting an array-of-structs pattern.\nHow to force vmap to return the next result\nIf you wanted to convert this output into the list of state objects you were expecting, you could do so using utilities in jax.tree_util:\nleaves, treedef = jax.tree_util.tree_flatten(states)\nstates_list = [treedef.unflatten(leaf) for leaf in zip(*leaves)]\nprint(len(states_list))\n# 4\nThat said, it appears that pgx is built to work natively with the original struct-of-arrays pattern, so you may find that you won't actually need this unstacked version in practice."
        ],
        "link": "https://stackoverflow.com/questions/77881821/how-is-it-possible-that-jax-vmap-returns-not-iterable"
    },
    {
        "title": "Why do I get different values from jnp.round and np.round?",
        "question": "I'm writing tests for some jax code and using np.testing.assert_array...-type functions and came across this difference in values that I didn't expect:\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\na = jnp.array([-0.78073686, -0.7908204 ,  2.174842])\nb = np.array(a, dtype='float32')\nassert_array_equal(a, b)\n\nprint(a.round(2), a.dtype)\nprint(b.round(2), b.dtype)\nOutput:\n[-0.78       -0.78999996  2.1699998 ] float32\n[-0.78 -0.79  2.17] float32\nTest:\nassert_array_equal(a.round(2), b.round(2))\nOutput:\nAssertionError: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 2.3841858e-07\nMax relative difference: 1.0987031e-07\n x: array([-0.78, -0.79,  2.17], dtype=float32)\n y: array([-0.78, -0.79,  2.17], dtype=float32)\nFootnote:\nI get exactly the same results if I define b as follows, so it's not a problem with the conversion of the array from jax to numpy:\nb = np.array([-0.78073686, -0.7908204 ,  2.174842], dtype='float32')",
        "answers": [
            "This is an example of a general property of floating point computations: two different ways of expressing the same computation will not always produce bitwise-equivalent outputs (see e.g. Is floating point math broken?).\nJAX and NumPy use identical implementations for x.round(2); essentially it is round_to_int(x * 100) / 100 (compare the JAX implementation and the NumPy implementation).\nThe difference is that JAX jit-compiles jnp.round by default. When you disable compilation and perform these operations in sequence, the results are identical:\nimport jax\nwith jax.disable_jit():\n  assert_array_equal(a.round(2), b.round(2))  # passes!\nBut JAX's JIT optimizes the implementation by fusing some operations – this leads to faster computation but in general you should not expect the result to be bitwise-equivalent to the unoptimized version.\nTo address this, whenever you are comparing floating point values, you should avoid exact equality checks in favor of checks that take this floating point roundoff error into account. For example:\nnp.testing.assert_allclose(a.round(2), b.round(2), rtol=1E-6)  # passes!"
        ],
        "link": "https://stackoverflow.com/questions/77868226/why-do-i-get-different-values-from-jnp-round-and-np-round"
    },
    {
        "title": "finding the maximum of a function using jax",
        "question": "I have a function which I would like to find its maximum by optimizing two of its variables using Jax.\nThe current code that I have currently, which does not work, reads\nimport jax.numpy as jnp\nimport jax \nimport scipy\nimport numpy as np\n\ndef temp_func(x,y,z):\n    tmp = x + jnp.dot( jnp.power(y,3), jnp.tanh(z) )\n    return -tmp\ndef obj_func(xy, z):\n    x,y = xy[:2], xy[2:].reshape(2,2)\n    return jnp.sum(temp_func(jnp.array(x),jnp.array(y),z))\n\ngrad_tmp = jax.grad(obj_func, argnums=0) # x,y\n\nxy = jnp.concatenate([np.random.rand(2), np.random.rand(2*2) ])\nz= jnp.array( np.random.rand(2,2) )\nprint(obj_func(xy,z))\n\nresult = scipy.optimize.minimize(obj_func,\n                                 xy,\n                                 args=(z,),\n                                 method='L-BFGS-B',\n                                 jac=grad_tmp\n                                )\nWith this code, I get the error ValueError: failed in converting 7th argument g' of _lbfgsb.setulb to C/Fortran array` Do you have any suggestions to resolve the issue?",
        "answers": [
            "You might think about using the jax version of scipy.optimize.minimize, which will automatically compute and use the derivative:\nimport jax.scipy.optimize\nresult = jax.scipy.optimize.minimize(obj_func, xy, args=(z,), method='BFGS')\nThat said, the results in either case are not going to be very meaningful, because your objective function is linearly decreasing in x and y, so it will be minimized when x, y → ∞"
        ],
        "link": "https://stackoverflow.com/questions/77860052/finding-the-maximum-of-a-function-using-jax"
    },
    {
        "title": "Custom JVP and VJP for higher order functions in JAX",
        "question": "I find custom automatic differentiation capabilities (JVP, VJP) very useful in JAX, but am having a hard time applying it to higher order functions. A minimal example of this sort is as follows: given a higher order function:\ndef parent_func(x):\n    def child_func(y):\n        return x**2 * y\n    return child_func\nI would like to define custom gradients of child_func with respect to x and y. What would be the correct syntax to achieve this?",
        "answers": [
            "Gradients in JAX are defined with respect to a function’s explicit inputs. Your child_func does not take x as an explicit input, so you cannot directly differentiate child_func with respect to x. However, you could do so indirectly by calling it from another function that takes x. For example:\ndef func_to_differentiate(x, y):\n  child_func = parent_func(x)\n  return child_func(y)\n\njax.grad(func_to_differentiate, argnums=0)(1.0, 1.0)  # 2.0\nThen if you wish, you could define standard custom derivative rules for func_to_differentiate."
        ],
        "link": "https://stackoverflow.com/questions/77859418/custom-jvp-and-vjp-for-higher-order-functions-in-jax"
    },
    {
        "title": "JAX python C callbacks",
        "question": "Numba allows to create C-callbacks directly in python with the @cfunc-decorator ( https://numba.pydata.org/numba-doc/0.42.0/user/cfunc.html ):\n@cfunc(\"float64(float64)\") \ndef square(x):\n    return x**2\nTo clarify, the resulting function is a pure C-function, which can then be called directly from C-code.\nIs there an equivalent functionality available in JAX ( https://jax.readthedocs.io/en/latest/# )?\nI have been searching for a while but couldn't find anything. I would appreciate any tips.",
        "answers": [
            "No, JAX doesn't provide any API similar to Numba's cfunc."
        ],
        "link": "https://stackoverflow.com/questions/77855169/jax-python-c-callbacks"
    },
    {
        "title": "How to loop a random number of times in jax with jit compilation?",
        "question": "I am using jax in python, and I want to loop over some code for a random number of times. This is part of a function which is jit compiled later. I have a small example below which should explain what I want to do.\nnum_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\narr = []\nfor i in range(num_iters):\n  arr += [i*i]\nThis works without any error and gives arr=[0,1,4] at the end of the loop (with the fixed seed of 0 that we're using in PRNGKey).\nHowever, if this is part of a jit-compiled function:\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nI get a TracerIntegerConversionError on num_iters. The function works fine without the jit decorator. How to get this to work with jit? I basically just want to construct the list arr whose length depends on a random number. Alternatively, I can also use a list with the maximum possible size, but then I'd have to loop over it a random number of times.\nFurther context\nIt's possible to make it not throw an error using a numpy random number generator instead:\n@jax.jit\ndef do_stuff(start):\n  np_rng = np.random.default_rng()\n  num_iters = np_rng.integers(1, 10)\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nHowever, this is not what I want. There is a jax rng which is passed to my function which I wish to use to generate num_iters. Otherwise, arr always has the same length since the numpy seed is fixed to what was available at jit-compile time, and I always get the same result without any randomness. However, if I use that rng key as seed for numpy (like np.random.default_rng(seed=rng[0])) it again gives the following error:\nTypeError: SeedSequence expects int or sequence of ints for entropy not Traced<ShapedArray(uint32[])>with<DynamicJaxprTrace(level=1/0)>",
        "answers": [
            "You could use jax.lax.fori_loop for this:\nimport jax\n\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  return jax.lax.fori_loop(0, num_iters, lambda i, val: val + i * i, start)\n\nprint(do_stuff(10))\n# 15",
            "Jax complains in this case, because you try to use a traced value as a static integer. See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError for more information.\nAs one possible solution you could pass the num_iters as an argument to do_stuff, declare it as static and create the keys outside, along the lines of:\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(1,))\ndef do_stuff(start, num_iters):    \n  arr = []\n  \n  for i in range(num_iters):\n    arr += [i*i]\n  \n  for value in arr:\n    start += value\n  \n  return start\n\nkey = jax.random.PRNGKey(238)\n\nfor _ in range(4):\n  key, _ = jax.random.split(key)\n  num_iters = int(jax.random.randint(key, (1,), 1, 10))\n  print(do_stuff(0, num_iters))\nWhich prints:\n5\n0\n140\n30\nOther alternative solutions are proposed in the link I listed above.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/77844256/how-to-loop-a-random-number-of-times-in-jax-with-jit-compilation"
    },
    {
        "title": "Update JAX array based on values in another array",
        "question": "I have a Jax array X like this:\n[[[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]]\nHow do I set the values of this array to 1, whose indices are given by array Y:\n[[[1 2]\n [1 2]]\n\n [[0 2]\n [0 1]]\n\n [[1 0]\n [1 0]]]\nDesired output:\n([[[0., 1., 1.],\n        [0., 1., 1.]],\n\n       [[1., 0., 1.],\n        [1., 1., 0.]],\n\n       [[1., 1., 0.],\n        [1., 1., 0.]]]",
        "answers": [
            "There are a couple ways to approach this. First let's define the arrays:\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.zeros((3, 2, 3))\nindices = jnp.array([[[1, 2],\n                      [1, 2]],\n                     [[0, 2],\n                      [0, 1]],\n                     [[1, 0],\n                      [1, 0]]])\nOne way to do this is to use typical numpy-style broadcasting of indices. It might look like this:\ni = jnp.arange(3).reshape(3, 1, 1)\nj = jnp.arange(2).reshape(2, 1)\nx = x.at[i, j, indices].set(1)\nprint(x)\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]\nAnother option is to use a double-vmap transformation to compute the batched indices:\nf = jax.vmap(jax.vmap(lambda x, i: x.at[i].set(1)))\nprint(f(x, indices))\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]"
        ],
        "link": "https://stackoverflow.com/questions/77799930/update-jax-array-based-on-values-in-another-array"
    },
    {
        "title": "Occurence of NaN in softmax & JIT issues",
        "question": "I'm trying to implement the Transformer architecture from scratch using Jax. I find three issues while training:\njax.disable_jit() does not remove implicit jit compilations.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nclass SelfAttention(eqx.Module):\n    def __call__(self, query, key, value, mask):\n        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n        scaled_dot_prod = mask + scaled_dot_prod\n        return (jax.nn.softmax(scaled_dot_prod) @ value)\n\ndef create_mask(arr):\n    return jnp.where(arr == 0, np.NINF, 0)\n\ndef loss(model, X, y, X_mask, y_mask, labels):\n    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n    y_pred = jnp.where(labels==0, 0, jnp.take(y_pred, labels, axis=-1))\n    count = jnp.count_nonzero(y_pred)\n    return -jnp.sum(y_pred)/count\n\nwith jax.disable_jit():\n    for e in range(EPOCHS):\n        total_loss = 0\n        num_batches = 0\n        total_tokens = 0\n        for i, (Xbt, ybt, labelbt) in enumerate(dataloader(Xtr, ytr, SEQ_LEN)):\n            total_tokens += len([token for seq in labelbt for token in list(filter(lambda x: x!=0, seq))])\n            Xbt, ybt, labelbt = [jnp.array(x) for x in (Xbt, ybt, labelbt)]\n            Xmask, ymask = [create_mask(x) for x in (Xbt, ybt)]\n\n            model, opt_state, batch_loss = step(model, opt_state, Xbt, ybt, Xmask, ymask, labelbt)\n            total_loss += batch_loss\n            num_batches += 1\n\n            if num_batches % 20 == 0:\n                print(f\"Batches trained: {num_batches} | Avg. Batch loss: {total_loss/num_batches}\")\n\n        epoch_loss = total_loss / num_batches\n        print(f\"Epoch {e} | loss: {epoch_loss}\")\nError:\ndef _softmax_deprecated(\n    478     x: ArrayLike,\n    479     axis: Optional[Union[int, tuple[int, ...]]] = -1,\n    480     where: Optional[ArrayLike] = None,\n    481     initial: Optional[ArrayLike] = None) -> Array:\n    482   x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n--> 483   unnormalized = jnp.exp(x - lax.stop_gradient(x_max))\n    484   result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)\n    485   if where is not None:\n\nFloatingPointError: invalid value (nan) encountered in jit(sub)\nThe above problem is encountered only after crossing 200 batches of training. I haven't checked that skipping the specific batch where the error occurs. Maybe I sohuld inspect if some specific inputs are the resason to this error.\nBut I can't find the answer to the above 3 questions :(",
        "answers": [
            "To answer your questions:\njax.disable_jit() does not remove implicit jit compilations.\nIf this is true, it is a bug and you should report it on the JAX issue tracker. It's unclear from your question what makes you believe this is the case.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nBecause _softmax_deprecated is the old default algorithm, that will someday be deprecated but the deprecation has not happened yet. See https://github.com/google/jax/pull/15677 for details. To use the newer algorithm, you can set the jax_softmax_custom_jvp=True configuration.\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nYou didn't include enough code to reproduce your issue (next time, try to add a minimal reproducible example to allow others to answer your question without guesswork). But it would be worth setting jax_softmax_custom_jvp=True to see if that addresses your issue. The pull request linked above has details."
        ],
        "link": "https://stackoverflow.com/questions/77677455/occurence-of-nan-in-softmax-jit-issues"
    },
    {
        "title": "Why does installing JAX with Docker create such a large image?",
        "question": "I am trying to pip install JAX using Docker and I found that using it just blows up the size of Docker image. The size of image currently is 4.82 GB.\nI made sure to bypass caching while installing packages by doing --no-cache-dir. While that did reduce the size, the size is still unreasonable huge.\nHere is my Dockerfile -\nFROM ubuntu:22.04\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    libosmesa6-dev \\\n    sudo \\\n    wget \\\n    curl \\\n    unzip \\\n    gcc \\\n    g++\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\nRUN mkdir -p ~/miniconda3\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nRUN bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nRUN rm -rf ~/miniconda3/miniconda.sh\nRUN ~/miniconda3/bin/conda init bash\nRUN conda init\n\nRUN pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nThis is how I built it -\ndocker build -t tbd_jax .\nWhen I do docker images, I get this -\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\ntbd_jax      latest    812292e2264e   7 minutes ago    4.82GB\nAfter doing docker history --no-trunc tbd_jax:latest -\nSIZE      COMMENT\nsha256:812292e2264e4340b7715956824055d7409f9546f8dfa54ccad1da056febf300   8 minutes ago    RUN |1 PATH=/root/miniconda3/bin:/root/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin /bin/sh -c pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # buildkit   3.54GB    buildkit.dockerfile.v0\nIs there something I can do to reduce the size? I am a bit of a Docker and Linux newbie so pardon my slowness.",
        "answers": [
            "Note that jax[cuda12_pip] installs all the cuda drivers listed here:\n'cuda12_pip': [\n  ...\n  \"nvidia-cublas-cu12>=12.2.5.6\",\n  \"nvidia-cuda-cupti-cu12>=12.2.142\",\n  \"nvidia-cuda-nvcc-cu12>=12.2.140\",\n  \"nvidia-cuda-runtime-cu12>=12.2.140\",\n  \"nvidia-cudnn-cu12>=8.9\",\n  \"nvidia-cufft-cu12>=11.0.8.103\",\n  \"nvidia-cusolver-cu12>=11.5.2\",\n  \"nvidia-cusparse-cu12>=12.1.2.141\",\n  \"nvidia-nccl-cu12>=2.18.3\",\nThese nvidia driver packages are quite large: for example the nvidia_cublas_cu12 wheel is over 400MB, and nvidia-cudnn-cu12 is over 700MB. You may be able to do better by setting up your docker image with system-native CUDA & CUDNN drivers, installed via apt. You can find a description of the requirements here. You can also use NVIDIA's pre-defined GPU containers, as mentioned here."
        ],
        "link": "https://stackoverflow.com/questions/77669055/why-does-installing-jax-with-docker-create-such-a-large-image"
    },
    {
        "title": "How to understand and debug memory usage with JAX?",
        "question": "I am new to JAX and trying to learn use it for running some code on a GPU. In my example I want to search for regular grids in a point cloud (for indexing X-ray diffraction data).\nWith test_mats[4_000_000,3,3] the memory usage seems to be 15 MB. But with test_mats[5_000_000,3,3] I get an error about it wanting to allocate 19 GB.\nI can't tell whether this is a glitch in JAX, or because I am doing something wrong. My example code and output are below. I guess the problem is that it wants to create a temporary array of (N, 3, gvec.shape[1]) before doing the reduction, but I don't know how to see the memory profile for what happens inside the jitted/vmapped function.\nimport sys\nimport os\nimport jax\nimport jax.random\nimport jax.profiler\n\nprint('jax.version.__version__',jax.version.__version__)\n\nimport scipy.spatial.transform\nimport numpy as np\n\n# (3,N) integer grid spot positions\nhkls = np.mgrid[-3:4, -3:4, -3:4].reshape(3,-1)\n\nUmat = scipy.spatial.transform.Rotation.random( 10, random_state=42 ).as_matrix()\na0 = 10.13\ngvec = np.swapaxes( Umat.dot(hkls)/a0, 0, 1 ).reshape(3,-1)\n\ndef count_indexed_peaks_hkl( ubi, gve, tol ):\n    \"\"\" See how many gve this ubi can account for \"\"\"\n    hkl_real = ubi.dot( gve )\n    hkl_int = jax.numpy.round( hkl_real )\n    drlv2 = ((hkl_real - hkl_int)**2).sum(axis=0)\n    npks = jax.numpy.where( drlv2 < tol*tol, 1, 0 ).sum()\n    return npks\n\ndef testsize( N ):\n    print(\"Testing size\",N)\n    jfunc = jax.vmap( jax.jit(count_indexed_peaks_hkl), in_axes=(0,None,None))\n    key = jax.random.PRNGKey(0)\n    test_mats = jax.random.orthogonal(key, 3, (N,) )*a0\n    dev_gvec = jax.device_put( gvec )\n    scores = jfunc( test_mats, gvec, 0.01 )\n    jax.profiler.save_device_memory_profile(f\"memory_{N}.prof\")\n    os.system(f\"~/go/bin/pprof -top {sys.executable} memory_{N}.prof\")\n\ntestsize(400000)\ntestsize(500000)\nOutput is:\ngpu4-03:~/Notebooks/JAXFits % python mem.py \njax.version.__version__ 0.4.16\nTesting size 400000\nFile: python\nType: space\nShowing nodes accounting for 15.26MB, 99.44% of 15.35MB total\nDropped 25 nodes (cum <= 0.08MB)\n      flat  flat%   sum%        cum   cum%\n   15.26MB 99.44% 99.44%    15.26MB 99.44%  __call__\n         0     0% 99.44%    15.35MB   100%  [python]\n         0     0% 99.44%     1.53MB 10.00%  _pjit_batcher\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl_python\n         0     0% 99.44%    15.30MB 99.70%  _python_pjit_helper\n         0     0% 99.44%    15.35MB   100%  bind\n         0     0% 99.44%    15.35MB   100%  bind_with_trace\n         0     0% 99.44%    15.30MB 99.70%  cache_miss\n         0     0% 99.44%    15.30MB 99.70%  call_impl_cache_miss\n         0     0% 99.44%     1.53MB 10.00%  call_wrapped\n         0     0% 99.44%    13.74MB 89.51%  deferring_binary_op\n         0     0% 99.44%    15.35MB   100%  process_primitive\n         0     0% 99.44%    15.30MB 99.70%  reraise_with_filtered_traceback\n         0     0% 99.44%    15.35MB   100%  testsize\n         0     0% 99.44%     1.53MB 10.00%  vmap_f\n         0     0% 99.44%    15.31MB 99.74%  wrapper\nTesting size 500000\n2023-12-14 10:26:23.630474: W external/tsl/tsl/framework/bfc_allocator.cc:296] Allocator\n(GPU_0_bfc) ran out of memory trying to allocate 19.18GiB with freed_by_count=0. The caller\nindicates that this is not a failure, but this may mean that there could be performance \ngains if more memory were available.\nTraceback (most recent call last):\n  File \"~/Notebooks/JAXFits/mem.py\", line 38, in <module>\n    testsize(500000)\n  File \"~/Notebooks/JAXFits/mem.py\", line 33, in testsize\n    scores = jfunc( test_mats, gvec, 0.01 )\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to \nallocate 20596777216 bytes.\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following \nexception. Set JAX_TRACEBACK_FILTERING=off to include these.```",
        "answers": [
            "The vmapped function is attempting to create an intermediate array of shape [N, 3, 3430]. For N=400_000, with float32 this amounts to 15GB, and for N=500_000 this amounts to 19GB.\nYour best option in this situation is probably to split your computation into sequentially-executed batches using lax.map or similar. Unfortunately there's not currently any automatic way to do that kind of chunked vmao, but there is a relevant feature request at https://github.com/google/jax/issues/11319, and there are some useful suggestions in that thread."
        ],
        "link": "https://stackoverflow.com/questions/77659069/how-to-understand-and-debug-memory-usage-with-jax"
    },
    {
        "title": "JAX dynamic slice inside of control flow function",
        "question": "I would like to do dynamic slicing inside of lax.while_loop() using a variable carried over, getting an error as below. I know in the case of a simple function, I can pass the variable as a static value, using partial , but how can I handle the case in which the variable (in my case length) is carried over?\nnew_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got (0, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThis is how I coded. This code is just to illustrate the problem. What I would like to do is to extract a part of u and do some operations. Thank you.\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import lax\nfrom functools import partial\nimport jax\n\nu = jnp.array([[1,2,3,4,5],[0,0,0,0,0]])\n\ndef body_fun(carry):\n    length, sum_u = carry\n    new_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n    #new_u = lax.dynamic_slice(u,(0,0),(2,4))\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n\ndef cond_fun(carry):\n    length, sum_u = carry\n    keep_condition = sum_u < 5\n    return keep_condition\n\ninit_carry = (5,10)\nout = lax.while_loop(cond_fun, body_fun, init_carry)\nprint(out)",
        "answers": [
            "The problem is that you are attempting to construct a dynamically-shaped array, and JAX does not support dynamically-shaped arrays (length is a dynamic variable in your loop). See JAX Sharp Bits: Dynamic Shapes for more.\nA typical strategy in these cases is to use a statically-sized array while masking out a dynamic range of values; in your case, you could use a value of 0 for the masked values so that they don't contribute to the sum. It might look like this:\ndef body_fun(carry):\n    length, sum_u = carry\n    idx = jnp.arange(u.shape[1])\n    new_u = jnp.where(idx < length, u, 0)\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n(Side-note: it seems like you were using dynamic_slice in hopes that you could generate dynamic array shapes, but the dynamic in dynamic_slice refers to the dynamic offset, not a dynamic size)."
        ],
        "link": "https://stackoverflow.com/questions/77603954/jax-dynamic-slice-inside-of-control-flow-function"
    },
    {
        "title": "Efficient copying of an ensemble in JAX",
        "question": "I have an ensemble of models and want to assign the same parameters to each of the models. Both the models' parameters as well as the new parameters have the same underlying structure. Currently I use the following approach that uses a for-loop.\nimport jax\nimport jax.numpy as jnp\n\nmodel1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nmodel2 = [\n    [jnp.asarray([2]), jnp.asarray([3, 4])],\n    [jnp.asarray([5]), jnp.asarray([6, 7])],\n]\n\nmodels = [model1, model2]\n\nparams = [\n    [jnp.asarray([3]), jnp.asarray([4, 5])],\n    [jnp.asarray([6]), jnp.asarray([7, 8])],\n]\n\nmodels = [jax.tree_map(jnp.copy, params) for _ in range(len(models))]\nIs there a more efficient way in JAX to assign the parameters from params to each model in models?",
        "answers": [
            "Since JAX arrays are immutable, there's no need to copy the parameter arrays, and you could achieve the same result like this:\nmodels = len(models) * [params]"
        ],
        "link": "https://stackoverflow.com/questions/77595758/efficient-copying-of-an-ensemble-in-jax"
    },
    {
        "title": "Parallelize inference of ensemble",
        "question": "I used the this tutorial from JAX to create an ensemble of networks. Currently I compute the loss of each network in a for-loop which I would like to avoid:\nfor params in ensemble_params:\n    loss = mse_loss(params, inputs=x, targets=y)\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\nHere ensemble_params is a list of pytrees (lists of tuples holding JAX parameter arrays). The parameter structure of each network is the same.\nI tried to get rid of the for-loop by applying jax.vmap:\nensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\nHowever, I keep getting the following error message which I do not understand.\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (8 of them) had size 3, e.g. axis 0 of argument params[0][0][0] of type float32[3,2];\n  * some axes (8 of them) had size 4, e.g. axis 0 of argument params[0][1][0] of type float32[4,3]\nHere is a minimal reproducible example:\nimport jax\nfrom jax import Array\nfrom jax import random\nimport jax.numpy as jnp\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits\n\nbatched_predict = jax.vmap(predict, in_axes=(None, 0))\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\n\nif __name__ == \"__main__\":\n\n    num_models = 4\n    dim_in = 2\n    dim_out = 4\n    layer_dims = [dim_in, 3, dim_out]\n    batch_size = 2\n\n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble_params = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    key_x, key_y = random.split(key)\n    x = random.normal(key=key_x, shape=(batch_size, dim_in))\n    y = random.normal(key=key_y, shape=(batch_size, dim_out))\n\n    for params in ensemble_params:\n        loss = mse_loss(params, inputs=x, targets=y)\n        print(f\"{loss = }\")\n\n    ensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\n    losses = ensemble_loss(ensemble_params, x, y)\n    print(f\"{losses = }\")  # Same losses expected as above.",
        "answers": [
            "The main issue here is that vmap maps over arrays, not over lists.\nYou are passing a list of parameter objects, expecting vmap to map over the elements of that list. However, the semantics of vmap are that it maps over the first axis of each tree leaf in the argument, and the leaves in your argument differ in their leading axis.\nTo fix this, instead of passing a list of parameter objects containing unbatched arrays, you need to pass a single parameter object containing batched arrays; in other words you need a struct-of-arrays pattern rather than a list-of-structs pattern.\nIn your case, you can create your batched ensemble parameters this way:\nensemble_params = jax.tree_map(lambda *args: jnp.stack(args), *ensemble_params)\nIf you pass this to the ensemble_loss function, you get the expected output:\nlosses = Array([3.762451 , 4.39846  , 4.1425314, 6.045669 ], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77581033/parallelize-inference-of-ensemble"
    },
    {
        "title": "Weighted sum of pytrees in JAX",
        "question": "I have a pytree represented by a list of lists holding parameter tuples. The sub-lists all have the same structure (see example).\nNow I would like to create a weighted sum so that the resulting pytree has the same structure as one of the sub-lists. The weights for each sub-list are stored in a separate array / list.\nSo far I have the following code that seems to works but requires several steps and for-loop that I would like avoid for performance reasons.\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n]\n\nlist_2 = [\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n]\n\nlist_3 = [\n    [jnp.asarray([[7, 1], [4, 4]]), jnp.asarray([6, 2])],\n    [jnp.asarray([[6, 4], [3, 7]]), jnp.asarray([7, 3])],\n]\n\nweights = [1, 2, 3] \npytree = [list_1, list_2, list_3]\n\nweighted_pytree = [jax.tree_map(lambda tree: weight * tree, tree) for weight, tree in zip(weights, pytree)]\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *weighted_pytree)",
        "answers": [
            "I think this will do what you have in mind:\ndef wsum(*args, weights=weights):\n  return jnp.asarray(weights) @ jnp.asarray(args)\n\nreduced = jax.tree_util.tree_map(wsum, *pytree)\nFor the edited question, where tree elements have more general shapes, you can define wsum like this instead:\ndef wsum(*args, weights=weights):\n  return sum(weight * arg for weight, arg in zip(weights, args))"
        ],
        "link": "https://stackoverflow.com/questions/77550969/weighted-sum-of-pytrees-in-jax"
    },
    {
        "title": "Reduce list of lists in JAX",
        "question": "I have a list holding many lists of the same structure (Usually, there are much more than two sub-lists inside the list, the example shows two lists for the sake of simplicity). I would like to create the sum or product over all sub-lists so that the resulting list has the same structure as one of the sub-lists. So far I tried the following using the tree_reduce method but I get errors that I don't understand.\nI could need some guidance on how to use tree_reduce() in such a case.\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nlist_2 = [\n    [jnp.asarray([7]), jnp.asarray([8, 9])],\n    [jnp.asarray([10]), jnp.asarray([11, 12])],\n]\n    \nlist_of_lists = [list_1, list_2]\n   \nreduced = jax.tree_util.tree_reduce(lambda x, y: x + y, list_of_lists, 0, is_leaf=True)\n    \n# Expected\n# reduced = [\n#     [jnp.asarray([8]), jnp.asarray([10, 12])],\n#     [jnp.asarray([14]), jnp.asarray([16, 18])],\n# ]",
        "answers": [
            "You can do this with tree_map of a sum over the splatted list:\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *list_of_lists)\nprint(reduced)\n[[Array([8], dtype=int32), Array([10, 12], dtype=int32)],\n [Array([14], dtype=int32), Array([16, 18], dtype=int32)]]"
        ],
        "link": "https://stackoverflow.com/questions/77548225/reduce-list-of-lists-in-jax"
    },
    {
        "title": "Add noise to parameters of ensemble in JAX",
        "question": "I use the following code to create parameters for an ensemble of models stored as list of lists holding tuples of weights and biases. How do I efficiently add random noise to all parameters of the ensemble with JAX? I tried to use tree_map() but run into many errors probably caused by the nested structure.\nCould you please provide guidance on how to use tree_map() in this case or point to other methods that JAX provides for such a case?\nfrom jax import Array\nfrom jax import random\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = 0 * random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = 0 * random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\nif __name__ == \"__main__\":\n    num_models = 2\n    layer_dims = [2, 3, 4]\n    \n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    # Add noise to ensemble.",
        "answers": [
            "Here's one way you could do this:\nfrom jax import tree_util\nleaves, tree = tree_util.tree_flatten(ensemble)\nkey, *subkeys = random.split(key, len(leaves) + 1)\nsubkeys = tree_util.tree_unflatten(tree, subkeys)\n\ndef add_noise(val, key, eps=0.1):\n  return val + eps * random.normal(key, val.shape)\n\nensemble_with_noise = tree_util.tree_map(add_noise, ensemble, subkeys)\nEssentially, you create a tree of subkeys with the same structure as the tree of parameters, then use tree_map to apply the noise function to the tree.",
            "Another possiblity is to use ravel_pytree to first flatten the parameters into a single vector, then add noise, then unravel the noised vector back to the original tree structure:\nimport jax\nimport jax.flatten_util\n\nvector, unravel = jax.flatten_util.ravel_pytree(ensemble)\nnoisy_vector = vector + jax.random.normal(key, vector.shape)\nnoisy_ensemble = unravel(vector)"
        ],
        "link": "https://stackoverflow.com/questions/77539206/add-noise-to-parameters-of-ensemble-in-jax"
    },
    {
        "title": "Jax vmap limit memory",
        "question": "I'm wondering if there is a good way to limit the memory usage for Jax's VMAP function? Equivalently, to vmap in batches at a time if that makes sense?\nIn my specific use case, I have a set of images and I'd like to calculate the affinity between each pair of images; so ~order((num_imgs)^2 * (img shape)) bytes of memory used all at once if I'm understanding vmap correctly (which gets huge since in my real example I have 10,000 100x100 images).\nA basic example is:\ndef affininty_matrix_ex(n_arrays=10, img_size=5, key=jax.random.PRNGKey(0), gamma=jnp.array([0.5])):\n    arr_of_imgs = jax.random.normal(jax.random.PRNGKey(0), (n_arrays, img_size, img_size))\n    arr_of_indices = jnp.arange(n_arrays)\n    inds_1, inds_2 = zip(*combinations(arr_of_indices, 2))\n    v_cPA = jax.vmap(calcPairAffinity2, (0, 0, None, None), 0)\n    affinities = v_cPA(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma)\n    print()\n    print(jax.make_jaxpr(v_cPA)(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma))\n    \n    affinities = affinities.reshape(-1)\n    \n    arr = jnp.zeros((n_arrays, n_arrays), dtype=jnp.float16)\n    arr = arr.at[jnp.triu_indices(arr.shape[0], k=1)].set(affinities)\n    arr = arr + arr.T\n    arr = arr + jnp.identity(n_arrays, dtype=jnp.float16)\n    \n    return arr\n\n\ndef calcPairAffinity2(ind1, ind2, imgs, gamma):\n    #Returns a jnp array of 1 float, jnp.sum adds all elements together\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\nI suppose I could just say something like \"only feed into vmap X pairs at a time, and loop through n_chunks = n_arrays/X, appending each groups results to a list\" but that doesn't seem to be ideal. My understanding is vmap does not like generators, not sure if that would be an alternative way around the issue.",
        "answers": [
            "Edit, Aug 13 2024\nAs of JAX version 0.4.31, what you're asking for is possible using the batch_size argument of lax.map. For an iterable of size N, this will perform a scan with N // batch_size steps, and within each step will vmap the function over the batch. lax.map has less flexible semantics than jax.vmap, but for the simplest cases they look relatively similar. Here's an example using your calcPairAffinity function:\nFor example\nimport jax\nimport jax.numpy as jnp\n\ndef calcPairAffinity(ind1, ind2, imgs, gamma=0.5):\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\n\nimgs = jax.random.normal(jax.random.key(0), (100, 5, 5))\ninds = jnp.arange(imgs.shape[0])\ninds1, inds2 = map(jnp.ravel, jnp.meshgrid(inds, inds))\n\ndef f(inds):\n  return calcPairAffinity(*inds, imgs, 0.5)\n\n\nresult_vmap = jax.vmap(f)((inds1, inds2))\nresult_batched = jax.lax.map(f, (inds1, inds2), batch_size=1000)\nassert jnp.allclose(result_vmap, result_batched)\nOriginal answer\nThis is a frequent request, but unfortunately there's not yet (as of JAX version 0.4.20) any built-in utility to do chunked/batched vmap (xmap does have some functionality along these lines, but is experimental/incomplete and I wouldn't recommend relying on it).\nAdding chunking to vmap is tracked in https://github.com/google/jax/issues/11319, and there's some code there that does a limited version of what you have in mind. Hopefully something like what you describe will be possible with JAX's built-in vmap soon. In the meantime, you might think about applying vmap to chunks manually in the way you describe in your question."
        ],
        "link": "https://stackoverflow.com/questions/77527847/jax-vmap-limit-memory"
    },
    {
        "title": "JAX grad: derivate with respect an specific variable in a matrix",
        "question": "I am using Jax to do the grad of a matrix. For example I have a function f(A) where A is a matrix like A = \\[\\[a,b\\], \\[c,d\\]\\]. I want to just do the grad of f(A) for a,c and d (more specific for the lower-triangular part). How can I do that? also for a general NxN matrix not just the 2x2.\nI tried to convert the regular grad in a lower-triangular, but I am not sure if that is the same of if the output is correct.",
        "answers": [
            "JAX does not offer any way to take the gradient with respect to individual matrix elements. There are two ways you could proceed; first, you could take the gradient with respect to the entire array and extract the elements you're interested in; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(A):\n  return (A ** 2).sum()\n\nA = jnp.array([[1.0, 2.0], [3.0, 4.0]])\ndf_dA = jax.grad(f)(A)\nprint(df_dA[0, 0], df_dA[0, 1], df_dA[1, 2])\n2.0 4.0 8.0\nAlternatively, you could split the entries of the array into individual function arguments, and then use argnums to take the gradient with respect to just the ones you're interested in:\ndef f(a, b, c, d):\n  A = jnp.array([[a, b], [c, d]])\n  return (A ** 2).sum()\n\ndf_da, df_db, df_dc = jax.grad(f, argnums=(0, 1, 2))(1.0, 2.0, 3.0, 4.0)\nprint(df_da, df_db, df_dc)\n2.0 4.0 8.0\nIn general you'll probably find the first approach to be both easier to use in practice, and also more efficient. It does have some wasted computation, but sticking with vectorized computations will generally be a net win, especially if you're running on accelerators like GPU or TPU."
        ],
        "link": "https://stackoverflow.com/questions/77517357/jax-grad-derivate-with-respect-an-specific-variable-in-a-matrix"
    },
    {
        "title": "How to implement nested for loops with branches efficiently in JAX",
        "question": "I am wanting to reimplement a function in jax that loops over a 2d array and modifies the output array at an index that is not necessarily the same as the current iterating index based on conditions. Currently I am implementing this via repeated use of jnp.where for the conditions separately, but the function is ~4x slower than the numba implementation on cpu, on gpu it is ~10x faster - which I suspect is due to the fact that I am iterating over the whole array again for every condition.\nThe numba implementation is as follows:\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numba as nb\n\nrng = np.random.default_rng()\n\n\n@nb.njit\ndef raytrace_np(ir, dx, dy):\n    assert ir.ndim == 2\n    n, m = ir.shape\n    assert ir.shape == dx.shape == dy.shape\n    output = np.zeros_like(ir)\n\n    for i in range(ir.shape[0]):\n        for j in range(ir.shape[1]):\n            dx_ij = dx[i, j]\n            dy_ij = dy[i, j]\n            \n            dxf_ij = np.floor(dx_ij)\n            dyf_ij = np.floor(dy_ij)\n\n            ir_ij = ir[i, j]\n            index0 = i + int(dyf_ij)\n            index1 = j + int(dxf_ij)\n\n            if 0 <= index0 <= n - 1 and 0 <= index1 <= m - 1:\n                output[index0, index1] += (\n                    ir_ij * (1 - (dx_ij - dxf_ij)) * (1 - (dy_ij - dyf_ij))\n                )\n            if 0 <= index0 <= n - 1 and 0 <= index1 + 1 <= m - 1:\n                output[index0, index1 + 1] += (\n                    ir_ij * (dx_ij - dxf_ij) * (1 - (dy_ij - dyf_ij))\n                )\n            if 0 <= index0 + 1 <= n - 1 and 0 <= index1 <= m - 1:\n                output[index0 + 1, index1] += (\n                    ir_ij * (1 - (dx_ij - dxf_ij)) * (dy_ij - dyf_ij)\n                )\n            if 0 <= index0 + 1 <= n - 1 and 0 <= index1 + 1 <= m - 1:\n                output[index0 + 1, index1 + 1] += (\n                    ir_ij * (dx_ij - dxf_ij) * (dy_ij - dyf_ij)\n                )\n    return output\nand my current jax reimplementation is:\n@jax.jit\ndef raytrace_jax(ir, dx, dy):\n    assert ir.ndim == 2\n    n, m = ir.shape\n    assert ir.shape == dx.shape == dy.shape\n\n    output = jnp.zeros_like(ir)\n\n    dxfloor = jnp.floor(dx)\n    dyfloor = jnp.floor(dy)\n    \n    dxfloor_int = dxfloor.astype(jnp.int64)\n    dyfloor_int = dyfloor.astype(jnp.int64)\n    \n    meshyfloor = dyfloor_int + jnp.arange(n)[:, None]\n    meshxfloor = dxfloor_int + jnp.arange(m)[None]\n\n    validx = (meshxfloor >= 0) & (meshxfloor <= m - 1)\n    validy = (meshyfloor >= 0) & (meshyfloor <= n - 1)\n    validx2 = (meshxfloor + 1 >= 0) & (meshxfloor + 1 <= m - 1)\n    validy2 = (meshyfloor + 1 >= 0) & (meshyfloor + 1 <= n - 1)\n\n    validxy = validx & validy\n    validx2y = validx2 & validy\n    validxy2 = validx & validy2\n    validx2y2 = validx2 & validy2\n    \n    dx_dxfloor = dx - dxfloor\n    dy_dyfloor = dy - dyfloor\n\n    output = output.at[\n        jnp.where(validxy, meshyfloor, 0), jnp.where(validxy, meshxfloor, 0)\n    ].add(\n        jnp.where(validxy, ir * (1 - dx_dxfloor) * (1 - dy_dyfloor), 0)\n    )\n    output = output.at[\n        jnp.where(validx2y, meshyfloor, 0),\n        jnp.where(validx2y, meshxfloor + 1, 0),\n    ].add(jnp.where(validx2y, ir * dx_dxfloor * (1 - dy_dyfloor), 0))\n    output = output.at[\n        jnp.where(validxy2, meshyfloor + 1, 0),\n        jnp.where(validxy2, meshxfloor, 0),\n    ].add(jnp.where(validxy2, ir * (1 - dx_dxfloor) * dy_dyfloor, 0))\n    output = output.at[\n        jnp.where(validx2y2, meshyfloor + 1, 0),\n        jnp.where(validx2y2, meshxfloor + 1, 0),\n    ].add(jnp.where(validx2y2, ir * dx_dxfloor * dy_dyfloor, 0))\n    return output\nTest and timings:\nshape = 2000, 2000\nir = rng.random(shape)\ndx = (rng.random(shape) - 0.5) * 5\ndy = (rng.random(shape) - 0.5) * 5\n\n_raytrace_np = raytrace_np(ir, dx, dy)\n_raytrace_jax = raytrace_jax(ir, dx, dy).block_until_ready()\n\nassert np.allclose(_raytrace_np, _raytrace_jax)\n\n%timeit raytrace_np(ir, dx, dy)\n%timeit raytrace_jax(ir, dx, dy).block_until_ready()\nOutput:\n14.3 ms ± 84.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n62.9 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nSo is there a way to implement this algorithm in jax with performance more comparable to the numba implementation?",
        "answers": [
            "The way you implemented it in JAX is pretty close to what I'd recommend. Yes, it's 3x slower than a custom Numba implementation on CPU, but I think for an operation like this, that is to be expected.\nThe operation you defined applies specific logic to each individual entry of the array – that is precisely the computational regime that Numba is designed for, and precisely the kind of computation that CPUs were designed for: it's not surprising that with Numba on CPU your computation is very fast.\nI suspect the reason you used Numba rather than NumPy here is that NumPy is not designed for this sort of algorithm: it is an array-oriented language, not an array-element-oriented language. JAX/XLA is more similar to NumPy than to Numba: it is an array-oriented language; it encodes operations across whole arrays at once, rather than choosing a different computation per-element.\nThe benefit of this array-oriented computing model becomes really apparent when you move away from CPU and run the code on an accelerator like a GPU or TPU: this hardware is specifically designed for vectorized array operations, which is why you found that the same, array-oriented code was 10x faster on GPU."
        ],
        "link": "https://stackoverflow.com/questions/77459386/how-to-implement-nested-for-loops-with-branches-efficiently-in-jax"
    },
    {
        "title": "JAX @jit for nested class method",
        "question": "I am trying to use @jit with nested function, having a problem. I have a class One that take in another class Plant with a method func. I would like to call this method jitted func from One. I think that I followed the FAQ of JAX, \"How to use jit with methods?\" section. https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods However, I encountered an error saying that TypeError: One.__init__() got multiple values for argument 'plant'. Would anyone tell me how to solve this?\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np\nfrom functools import partial\nfrom jax import tree_util\n\nclass One:\n    def __init__(self, plant,x):\n        self.plant = plant\n        self.x = x\n    \n    @jit\n    def call_plant_func(self,y):\n        out = self.plant.func(y) + self.x\n        return out\n    \n    def _tree_flatten(self):\n        children = (self.x,)  # arrays / dynamic values\n        aux_data = {'plant':self.plant}  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        import pdb; pdb.set_trace();\n        return cls(*children, **aux_data)\n        \ntree_util.register_pytree_node(One,\n                               One._tree_flatten,\n                               One._tree_unflatten)    \n    \nclass Plant:\n    def __init__(self, z,kk):\n        self.z =z\n    \n    @jit\n    def func(self,y):\n        y = y + self.z\n        return y\n    \n    def _tree_flatten(self):\n        children = (self.z,)  # arrays / dynamic values\n        aux_data = None # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, children):\n        return cls(*children)\n   \ntree_util.register_pytree_node(Plant,\n                               Plant._tree_flatten,\n                               Plant._tree_unflatten)\n\nplant = Plant(5,2)\none = One(plant,2)\nprint(one.call_plant_func(10))\nThe last line gives me an error described above.",
        "answers": [
            "You have issues in the tree_flatten and tree_unflatten code in both classes.\nOne._tree_flatten treats plant as static data, but it is not: it is a pytree that has non-static elements.\nOne._tree_unflatten instantiates One with arguments in the wrong order, leading to the error you're seeing\nPlant.__init__ does nothing with the kk argument.\nPlant._tree_unflatten is missing the aux_data argument, and fails to pass the kk argument to Plant.__init__\nWith these issues fixed, your code executes without error:\nclass One:\n    def __init__(self, plant,x):\n        self.plant = plant\n        self.x = x\n    \n    @jit\n    def call_plant_func(self,y):\n        out = self.plant.func(y) + self.x\n        return out\n    \n    def _tree_flatten(self):\n        children = (self.plant, self.x)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n        \ntree_util.register_pytree_node(One,\n                               One._tree_flatten,\n                               One._tree_unflatten)    \n    \nclass Plant:\n    def __init__(self, z, kk):\n        self.kk = kk\n        self.z =z\n    \n    @jit\n    def func(self, y):\n        y = y + self.z\n        return y\n    \n    def _tree_flatten(self):\n        children = (self.z, self.kk)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n   \ntree_util.register_pytree_node(Plant,\n                               Plant._tree_flatten,\n                               Plant._tree_unflatten)\n\nplant = Plant(5,2)\none = One(plant,2)\nprint(one.call_plant_func(10))"
        ],
        "link": "https://stackoverflow.com/questions/77439217/jax-jit-for-nested-class-method"
    },
    {
        "title": "JAX vmap JIT behind the scenes?",
        "question": "I'm trying to vmap a function. My understanding of vmap is essentially anywhere I would write a ~for loop/list comprehension I should instead consider vmapping. I have a few points of confusion:\nDoes vmap need fixed sizes for everything through the function(s) being vmapped?\nDoes vmap try to JIT my function behind the scenes? (Wondering bc. 1 is a behavior I expect from JIT, I didn't expect it from vmap but I don't really know vmap).\nIf vmap is jit-ing something, how would one use something like a static-arguments with vmap?\nWhat is the best practice for dealing with ~extraneous information (eg if some outputs are sized a and some sized b, do you just make an array sized max(a,b) then ~ignore the extra values?)\nThe reason I'm asking is that it seems like vmap, like JIT, runs into all sorts of ConcretizationTypeError and seems (not 100% clear yet) to need constant sized items for everything. I associate this behavior with any function I'm trying to Jit, but not necessarily any function I write in Jax.",
        "answers": [
            "Does vmap need fixed sizes for everything through the function(s) being vmapped?\nyes – vmap, like all JAX transformations, requires any arrays defined in the function to have static shapes.\nDoes vmap try to JIT my function behind the scenes? (Wondering bc. 1 is a behavior I expect from JIT, I didn't expect it from vmap but I don't really know vmap).\nNo, vmap does not jit-compile a function by default, although you can always compose both if you wish (e.g. jit(vmap(f)))\nIf vmap is jit-ing something, how would one use something like a static-arguments with vmap?\nAs mentioned, vmap is unrelated to jit, but an analogy of jit static_argnums is passing None to in_axes, which will keep the argument unmapped and therefore static within the transformation.\nWhat is the best practice for dealing with ~extraneous information (eg if some outputs are sized a and some sized b, do you just make an array sized max(a,b) then ~ignore the extra values?)",
            "A section of my code now looks like:\nvmaped_f = jax.vmap(my_func, parallel_axes, 0)\nn_batches = int(num_items / batch_size)\nif num_items % batch_size != 0:\n    n_batches += 1 #Round up\n    \nall_vals = []\nfor i in range(n_batches):\n    top = min([num_items, (i+1)*batch_size])\n    batch_inds = jnp.arange(i*batch_size, top)\n    batch_inds_1, batch_inds_2 = jnp.array(inds_1)[batch_inds], \\\n                                 jnp.array(inds_2)[batch_inds]\n    f_vals = vmaped_f(batch_inds_1, batch_inds2, other_relevant_inputs)\n    all_vals.extend(f_vals.tolist())\nThe vmap'd function basically takes in all of my data, and the indices of that data to use (which will be constant sized except for potentially the last batch, so only need to jit compile 2x if you'd want to jit it)."
        ],
        "link": "https://stackoverflow.com/questions/77427904/jax-vmap-jit-behind-the-scenes"
    },
    {
        "title": "Idiomatic ways to handle errors in JAX jitted functions",
        "question": "As the title states, I'd like to know what idiomatic methods are available to raise exceptions or handle errors in JAX jitted functions. The functional nature of JAX makes it unclear how to accomplish this.\nThe closest official documentation I could find is the jax.experimental.checkify module, but this wasn't very clear and seemed incomplete.\nThis Github comment claims that Python exceptions can be raised by using jax.debug.callback() and jax.lax.cond() functions. I attempted to do this, but an error is thrown during compilation. A minimum working example is below:\nimport jax\nfrom jax import jit\n\ndef _raise(ex):\n    raise ex\n\n\n@jit\ndef error_if_positive(x):\n    jax.lax.cond(\n        x > 0,\n        lambda : jax.debug.callback(_raise, ValueError(\"x is positive\")),\n        lambda : None,\n    )\n\nif __name__ == \"__main__\":\n\n    error_if_positive(-1)\nThe abbreviated error statement:\nTypeError: Value ValueError('x is positive') with type <class 'ValueError'> is not a valid JAX type",
        "answers": [
            "You can use callbacks to raise errors, for example:\nimport jax\nfrom jax import jit\n\ndef _raise_if_positive(x):\n  if x > 0:\n    raise ValueError(\"x is positive\")\n\n@jit\ndef error_if_positive(x):\n  jax.debug.callback(_raise_if_positive, x)\n\nif __name__ == \"__main__\":\n  error_if_positive(-1)  # no error\n  error_if_positive(1)\n  # XlaRuntimeError: INTERNAL: Generated function failed: CpuCallback error: ValueError: x is positive\nThe reason your approach didn't work is becuase your error is raised at trace-time rather than at runtime, and both branches of the cond will always be traced."
        ],
        "link": "https://stackoverflow.com/questions/77381356/idiomatic-ways-to-handle-errors-in-jax-jitted-functions"
    },
    {
        "title": "Convert for loop to jax.lax.scan",
        "question": "How does one convert the following (to accelerate compiling)? The for loop version works with jax.jit,\nimport functools\nimport jax\nimport jax.numpy as jnp\n\n@functools.partial(jax.jit, static_argnums=0)\ndef func(n):\n\n    p = 1\n    x = jnp.arange(8)\n    y = jnp.zeros((n,))\n\n    for idx in range(n):\n        y = y.at[idx].set(jnp.sum(x[::p]))\n        p = 2*p\n\n    return y\n\nfunc(2)\n# >> Array([28., 12.], dtype=float32)\nbut will return static start/stop/step errors when using scan\nimport numpy as np\n\ndef body(p, xi):\n\n    y = jnp.sum(x[::p])\n\n    p = 2*p\n\n    return p, y\n\nx = jnp.arange(8)\n\njax.lax.scan(body, 1, np.arange(2))\n# >> IndexError: Array slice indices must have static start/stop/step ...",
        "answers": [
            "The issue here is that within scan, the p variable represents a dynamic value, meaning that x[::p] is a dynamically-sized array, so the operation is not allowed in JAX transformations (see JAX sharp bits: dynamic shapes).\nOften in such cases it's possible to replace approaches using dynamically-shaped intermediates with other approaches that compute the same thing using only use static arrays; in this case one thing you might do is replace this problematic line:\njnp.sum(x[::p])\nwith this, which does the same sum using only statically-sized arrays:\njnp.sum(x, where=jnp.arange(len(x)) % p == 0)\nUsing this idea, here's a version of your original function that uses scan:\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnums=0)\ndef func_scan(n):\n    p = 1\n    x = jnp.arange(8)\n    y = jnp.zeros((n,))\n\n    def body(carry, _):\n      idx, y, p = carry\n      y = y.at[idx].set(jnp.sum(x, where=jnp.arange(len(x)) % p == 0))\n      return (idx + 1, y, 2 * p), None\n\n    (i, y, p), _ = jax.lax.scan(body, (0, y, p), xs=None, length=n)\n    return y\n\nfunc_scan(2)\n# Array([28., 12.], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77364000/convert-for-loop-to-jax-lax-scan"
    },
    {
        "title": "Why matrix multiplication results with JAX are different if the data is sharded differently on the GPU",
        "question": "I am running a tutorial on muatrix multiplication with JAX with data sharded in different ways across multiple GPUs. I found not only the computation time is different for different way of sharding, the results are also slightly different.\nHere are my observations:\nResults of method 1 is exactly the same as method 0; but different from method\nComputation speed is the fastest in method 1, then method 2, then method 0.\nCan anyone help me understand these two observations? One additional question is: if the way of sharding is so important, will mainstream machine learning algorithms have ways to deal with it automatically so that different way of sharding won't give different models?\nMethod 0: Perform the matrix multiplication on the same GPU device (just use 1 device).\nMethod 1:\nx = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\ny = jax.device_put(x, sharding.reshape(4, 2).replicate(1))\nz = jax.device_put(x, sharding.reshape(4, 2).replicate(0))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)\nMethod 2:\nx = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\ny = jax.device_put(x, sharding.reshape(4, 2))\nz = jax.device_put(x, sharding.reshape(4, 2))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)",
        "answers": [
            "Regarding your observation of differing results: this is to be expected with floating point operations. Every time you do a floating point operation, it accumulates a small amount of error, and when you express the \"same\" floating point computation in different ways, the errors accumulate differently.\nHere's an example of this using NumPy:\nimport numpy as np\nx = np.random.rand(10000).astype('float32')\nx_reversed = x[::-1]\nnp.dot(x, x) == np.dot(x_reversed, x_reversed)\n# False\nIf we were dealing with real numbers, we'd expect these two to be identical. But because we're representing our computation with floating point values, the two approaches return slightly different results. This is similar to the situation in your question: different sharding layouts lead to different ordering of the dot product accumulations, which leads to slightly different results.\nRegarding your observation about computation speed: the results seem reasonable. Method 0 is the slowest because it only uses a single device, and method 1 is faster than method 2 because pre-replicating the data means that less data movement is required during the actual computation."
        ],
        "link": "https://stackoverflow.com/questions/77362635/why-matrix-multiplication-results-with-jax-are-different-if-the-data-is-sharded"
    },
    {
        "title": "vectorized minimization and root finding in jax",
        "question": "I have a family of functions parameterized by args\nf(x, args)\nand want to determine the minimum of f over x for N = 1000 values of args. I have access to both the function and its derivative. My first attempt was to loop through the different values of args and use a scipy.optimizer at each iteration, but it takes too long. I believe the operations can be sped up with vectorization. My next attempt was to use jax.vmap inside a jax.scipy.optimize.minimize or jaxopt.ScipyMinimize, but I can't seem to pass more than one value for args.\nAlternatively, I can code my own vectorized optimization method, e.g. bisection, where by vectorized I mean doing operations on arrays for a fixed number of iterations and not stopping early if one of the optimization problems has reached a certain error tolerance level early. I was hoping to use some optimized off-shelf algorithm.\nI was hoping to use some already optimized, off-the-shelf algorithm if an implementation is available in jax.this thread is related, but the args are not changing.",
        "answers": [
            "You can define a function to find the minimum given particular args, and then wrap it in jax.vmap to automatically vectorize it. For example:\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy import optimize\n\ndef f(x, args):\n  a, b = args\n  return jnp.sum(a + (x - b) ** 2)\n\ndef find_min(a, b):\n  x0 = jnp.array([1.0])\n  args = (a, b)\n  return optimize.minimize(f, x0, (args,), method=\"BFGS\")\n\na_grid, b_grid = jnp.meshgrid(jnp.arange(5.0), jnp.arange(5.0))\n\nresults = jax.vmap(find_min)(a_grid.ravel(), b_grid.ravel())\n\nprint(results.success)\n# [ True  True  True  True  True  True  True  True  True  True  True  True\n#   True  True  True  True  True  True  True  True  True  True  True  True\n#   True]\n\nprint(results.x.T)\n# [[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n#   3. 3. 3. 3. 3. 4. 4. 4. 4. 4.]]"
        ],
        "link": "https://stackoverflow.com/questions/77359908/vectorized-minimization-and-root-finding-in-jax"
    },
    {
        "title": "Cross dimensional segmented operation",
        "question": "Say you have the following a array\n>>> a = np.arange(27).reshape((3,3,3))\n>>> a\narray([[[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8]],\n\n       [[ 9, 10, 11],\n        [12, 13, 14],\n        [15, 16, 17]],\n\n       [[18, 19, 20],\n        [21, 22, 23],\n        [24, 25, 26]]], dtype=int64)\nAnd m, an array that specifies segment ids\n>>> m = np.linspace(start=0, stop=6, num=27).astype(int).reshape(a.shape)\n>>> m\narray([[[0, 0, 0],\n        [0, 0, 1],\n        [1, 1, 1]],\n\n       [[2, 2, 2],\n        [2, 3, 3],\n        [3, 3, 3]],\n\n       [[4, 4, 4],\n        [4, 5, 5],\n        [5, 5, 6]]])\nWhen using JAX and wishing to perform, say, a sum over the scalars in a that share the same id in m, we can rely on jax.ops.segment_sum.\n>>> jax.ops.segment_sum(data=a.ravel(), segment_ids=m.ravel())\nArray([10, 26, 42, 75, 78, 94, 26], dtype=int64)\nNote that I had to resort to numpy.ndarray.ravel since ~.segment_sum assumes m to indicate the segments of data along its leading axis.\nQ1 : Can you confirm there is no better approach, either with or without JAX ?\nQ2 : How would one then build n, an array that results from the replacement of the ids with the just-performed sums ? Note that I am not interested in non-vectorized approaches such as numpy.where.\n>>> n\narray([[[10, 10, 10],\n        [10, 10, 26],\n        [26, 26, 26]],\n\n       [[42, 42, 42],\n        [42, 75, 75],\n        [75, 75, 75]],\n\n       [[78, 78, 78],\n        [78, 94, 94],\n        [94, 94, 26]]], dtype=int64)",
        "answers": [
            "Use np.bincount with a as the weights parameter:\ns = np.bincount(m.ravel(), weights = a.ravel())\ns\nOut[]: array([10., 26., 42., 75., 78., 94., 26.])\nAnd to put the values back in the array:\nn = s[m]\nn\nOut[]: \narray([[[10., 10., 10.],\n        [10., 10., 26.],\n        [26., 26., 26.]],\n\n       [[42., 42., 42.],\n        [42., 75., 75.],\n        [75., 75., 75.]],\n\n       [[78., 78., 78.],\n        [78., 94., 94.],\n        [94., 94., 26.]]])",
            "The segment_sum operation is somewhat more specialized than what you're asking about. In the case you describe, I would use ndarray.at directly:\nsums = jnp.zeros(m.max() + 1).at[m].add(a)\nprint(sums[m])\n[[[10. 10. 10.]\n  [10. 10. 26.]\n  [26. 26. 26.]]\n\n [[42. 42. 42.]\n  [42. 75. 75.]\n  [75. 75. 75.]]\n\n [[78. 78. 78.]\n  [78. 94. 94.]\n  [94. 94. 26.]]]\nThis will also work when the segments are non-adjacent."
        ],
        "link": "https://stackoverflow.com/questions/77344277/cross-dimensional-segmented-operation"
    },
    {
        "title": "Why does jnp.einsum produce a different result from manual looping?",
        "question": "Let's say I want to compute an inner product along the last dimension of two matrices\na = jax.random.normal(jax.random.PRNGKey(0), shape=(64,16), dtype=jnp.float32)\nb = jax.random.normal(jax.random.PRNGKey(1), shape=(64,16), dtype=jnp.float32)\nI can do it with jnp.einsum:\ninner_prod1 = jnp.einsum('i d, j d -> i j', a, b)\nor manually call jnp.dot in a loop:\ninner_prod2 = jnp.zeros((64,64))\nfor i1 in range(64):\n  for i2 in range(64):\n    inner_prod2 = inner_prod2.at[i1, i2].set(jnp.dot(a[i1], b[i2]))\nprint(jnp.amax(inner_prod1 - inner_prod2)) # 0.03830552\nThis is quite a large difference between the two, even if they are mathematically equivalent. What gives?",
        "answers": [
            "All operations in floating point accumulate rounding errors, so in general when you express the same operation in two different ways, you should expect the results to not be bitwise-equivalent.\nThe magnitude of the difference you're seeing is larger than is typical for float32 precision; it makes me think you're probably running your code on TPU, where matrix multiplication is done at lower-precision by default. You can adjust this using the default_matmul_precision configuration; for example like this:\nwith jax.default_matmul_precision('float32'):\n  inner_prod1 = jnp.einsum('i d, j d -> i j', a, b)\n  inner_prod2 = jnp.zeros((64,64))\n  for i1 in range(64):\n    for i2 in range(64):\n      inner_prod2 = inner_prod2.at[i1, i2].set(jnp.dot(a[i1], b[i2]))\nIf you do the computation this way, I suspect you'll probably see a smaller difference more typical of float32 computations, on order 1E-6 or so."
        ],
        "link": "https://stackoverflow.com/questions/77334401/why-does-jnp-einsum-produce-a-different-result-from-manual-looping"
    },
    {
        "title": "how to use jit with control flow and arrays without slowing down",
        "question": "import jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n\narray1 = np.random.normal(size=(1000,1000))\ndef first():\n  for i in range(1000):\n    for j in range(1000):\n      if array1[i,j] >= 0:\n        array1[i,j] = 1\n      else:\n        array1[i,j] = -1\n# %timeit first()\n\nfrom functools import partial\nkey = jax.random.PRNGKey(seed=0)\narray2 = jax.random.normal(key, shape=(1000,1000))\n\n@partial(jit, static_argnums=(0,1,2))\ndef f( i,j):\n  r = jax.lax.cond(array2[i,j] >= 0, lambda x: 1, lambda x: -1, None)\n  # if array2[i,j] >= 0:\n  # # if i == j:\n  #   array2.at[i,j].set(1)\n  # else: array2.at[i,j].set(-1)\n  array2.at[i,j].set(r)\n\n# f_jit = jit(f, static_argnums=(0,1))\ndef second():\n  for i in range(1000):\n    for j in range(1000):\n      # jax.lax.cond(array2[i,j]>=0, lambda x: True, lambda x: False, None)\n      f(i,j)\n%timeit second()\nI have two functions: first and second. I want second to run as fast as (or faster) first does. first function is a function using numpy. second function uses jax. What is the best way to implement first function using jax in this case? jax.lax.cond significantly slows down the process I think.\nI left the comments on purpose to show what I've tried.",
        "answers": [
            "The reason first runs relatively quickly is because it does 1,000,000 numpy array operations, and numpy has been optimized for fast per-operation dispatch.\nThe reason second runs relatively slowly is because it does 1,000,000 JAX array operations, and JAX has not been optimized for fast per-operation dispatch.\nFor some general background on this, see JAX FAQ: Is JAX faster than NumPy?.\nBut if you're asking about the fastest way to accomplish what you're doing, in both NumPy and JAX the answer would be to avoid writing loops. Here is the equivalent, making the computation pure rather than in-place for ease of comparison (your original second function actually does nothing, because array.at[i].set() does not operate in-place):\ndef first_fast(array):\n  return np.where(array >= 0, 1, 0)\n\ndef second_fast(array):\n  return jnp.where(array >= 0, 1, 0)\nIn general, if you find yourself writing loops over array values in NumPy or in JAX, you can expect that the resulting code will be slow. In both NumPy and JAX, there's almost always a better way to compute the result using built-in vectorized operations.\nIf you're interested in further benchmarks between JAX and NumPy, be sure to read FAQ: Benchmarking JAX Code to ensure that you're comparing the right things."
        ],
        "link": "https://stackoverflow.com/questions/77333949/how-to-use-jit-with-control-flow-and-arrays-without-slowing-down"
    },
    {
        "title": "How to get get the index position of a value with jit?",
        "question": "What would be a workaround of this code in jitted function?\nj = indices.index(list(neighbor)) where neighbor is, for example, (2,3), indices = [[1,2], [4,5], ...]\nOther alternatives like partial didn't work. One issue when using partial is that indices is not hashable so can't use partial function.",
        "answers": [
            "list.index is a Python function that will not work within JIT if the contents of the list are traced values. I would recommend converting your lists to arrays, and do something like this:\nimport jax\nimport jax.numpy as jnp\n\nindices = jnp.array([[1, 2], [4, 5], [3, 6], [2, 3], [5, 7]])\nneighbor = jnp.array([2, 3])\n\n@jax.jit\ndef get_index(indices, neighbor):\n  return jnp.where((indices == neighbor).all(-1), size=1)[0]\n\nidx = get_index(indices, neighbor)\nprint(idx)\n# [3]"
        ],
        "link": "https://stackoverflow.com/questions/77328302/how-to-get-get-the-index-position-of-a-value-with-jit"
    },
    {
        "title": "What is the difference between xmap vs pmap as the parallel processing model?",
        "question": "Can anyone help me understand the similarities and differences between the xmap and pmap in JAX? Read through the documentation multiple times, but still cannot understand the new concepts in the tutorials.\nI am specifically interested in if there are any examples for how to convert pmap train setup to xmap train setup.\nIt seems to me that: mesh+xmap can do similar things as pmap. Am I understanding correctly?",
        "answers": [
            "pmap is a simple parallelizing transform that only supports distribution of data over a single leading axis. It is deprecated, and will likely be removed in a future version of JAX.\nxmap is a generalization of pmap that allows for more flexible parallelization over multiple named axes. It has always been experimental, and will likely be removed in a future version of JAX.\nThe best way to do parallel computation in JAX going forward is either implicitly via sharded inputs into jit, or explicitly via shard_map. Unfortunately, neither of these approaches is very well documented at the moment; there is some information at Distributed Arrays and Automatic Parallelization and shard_map for simple per-device code but both are written more for developers than for end-users. That said, more comprehensive docs for the newer parallelism models in JAX are currently in progress, and should be on the website soon."
        ],
        "link": "https://stackoverflow.com/questions/77275923/what-is-the-difference-between-xmap-vs-pmap-as-the-parallel-processing-model"
    },
    {
        "title": "aggregate calculation vmap jax",
        "question": "I'm trying to implement fast routine to calculate array of energies and find the smallest calculated value and its index. Here is my code that is working fine:\n@jit\ndef findMinEnergy(x):\n  def calcEnergy(a):\n    return a*a  # very simplified body, it is actually 15 lines of code\n  energies = vmap(calcEnergy, in_axes=(0))(x)\n  idx = energies.argmin(axis=0)\n  minenrgy = energies[idx]\n  return idx, minenrgy\nI wonder if it is possible to not use the (separate) argmin call, but return the min calculated energy value and it's index from the vmap (similar like other aggregate functions work, e.g. jax.sum)? I hope that it could be more efficient.",
        "answers": [
            "If you JIT-compile your current approach, you should find that it's as efficient as doing something more sophisticated.\nLooking at the implementation of argmin, you'll see that it computes both the value and the index before returning only the index: https://github.com/google/jax/blob/jax-v0.4.18/jax/_src/lax/lax.py#L3892-L3914\nIf you want, you could follow this implementation and define a function using lax.reduce that returns both these values in a single pass:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef min_and_argmin_onepass(x):\n  # This only works for 1D float arrays, but you could generalize it.\n  assert x.ndim == 1\n  assert jnp.issubdtype(x.dtype, jnp.floating)\n  def reducer(op_val_index, acc_val_index):\n    op_val, op_index = op_val_index\n    acc_val, acc_index = acc_val_index\n    pick_op_val = (op_val < acc_val) | jnp.isnan(op_val)\n    pick_op_index = pick_op_val | ((op_val == acc_val) & (op_index < acc_index))\n    return (jnp.where(pick_op_val, op_val, acc_val),\n            jnp.where(pick_op_index, op_index, acc_index))\n  indices = jnp.arange(len(x))\n  return jax.lax.reduce((x, indices), (jnp.inf, 0), reducer, (0,))\nTesting this, we see it matches the output of the less sophisticated approach:\n@jax.jit\ndef min_and_argmin(x):\n  i = jnp.argmin(x)\n  return x[i], i\n\nx = jax.random.uniform(jax.random.key(0), (1000000,))\nprint(min_and_argmin_onepass(x))\n# (Array(9.536743e-07, dtype=float32), Array(24430, dtype=int32))\nprint(min_and_argmin(x))\n# (Array(9.536743e-07, dtype=float32), Array(24430, dtype=int32))\nIf you compare the runtime of the two, you'll see comparable runtimes:\n%timeit jax.block_until_ready(min_and_argmin_onepass(x))\n# 2.17 ms ± 68.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%timeit jax.block_until_ready(min_and_argmin(x))\n# 2.07 ms ± 66.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nThe jax.jit decorator here means that the compiler optimizes the sequence of operations in the less sophisticated approach, and the result is that you don't gain much advantage from trying to express things more cleverly. Given this, I think your best option is to stick with your original code rather than trying to out-optimize the XLA compiler.",
            "Assuming that by efficient you mean not having to keep a large array (energies) in memory, you can just stack the individual values of idx and minenergy into a single array within calcEnergy and return a (2,) array to vmap instead of an (N,) array. It's not pretty as you'll (presumably) have to cast both values to the same dtype, but it should work fine."
        ],
        "link": "https://stackoverflow.com/questions/77254706/aggregate-calculation-vmap-jax"
    },
    {
        "title": "How to use FLAX LSTM in 2023",
        "question": "I am wondering if anyone here knows how to get FLAX LSTM layers to work in 2023. I have tried some of the code snippets on the actual Flax documentation, such as:\nhttps://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.scan.html\nand, the first example provided there,\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\n\nclass LSTM(nn.Module):\n  features: int\n\n  @nn.compact\n  def __call__(self, x):\n    ScanLSTM = nn.scan(\n      nn.LSTMCell, variable_broadcast=\"params\",\n      split_rngs={\"params\": False}, in_axes=1, out_axes=1)\n\n    lstm = ScanLSTM(self.features)\n    input_shape =  x[:, 0].shape\n    carry = lstm.initialize_carry(jax.random.key(0), input_shape)\n    carry, x = lstm(carry, x)\n    return x\n\nx = jnp.ones((4, 12, 7))\nmodule = LSTM(features=32)\ny, variables = module.init_with_output(jax.random.key(0), x)\nthrows an error. I have looked for other examples but it seems they have changed their API at some point in 2023, so what I could find online wasn't working anymore.\nIn short, what I am looking for is a simple example on how to pass a time series into an LSTM in FLAX.\nThank you for your help.",
        "answers": [
            "The snippet you provided runs correctly with the most recent version of flax (version 0.7.4). If you're using an older version of flax, you should change jax.random.key to jax.random.PRNGKey. For some information about this JAX PRNG key change, see JEP 9263: Typed Keys and Pluggable PRNGs."
        ],
        "link": "https://stackoverflow.com/questions/77222395/how-to-use-flax-lstm-in-2023"
    },
    {
        "title": "jax PRNG key error with tfp normal distribution",
        "question": "I am trying to run the following piece of Pyhon code from the Ubuntu WSL from Windows\nimport tensorflow_probability as tfp; tfp = tfp.substrates.jax\ntfd = tfp.distributions\ndist = tfd.Normal(loc=0., scale=3.)\ndist.cdf(1.)\ndist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\ndist.prob([0, 1.5])\ndist.sample([3])\nI am getting the following error\n> AttributeError                            Traceback (most recent call\n> last) Cell In[45], line 4\n>       2 tfd = tfp.distributions\n>       3 dist = tfd.Normal(loc=0., scale=3.)\n> ----> 4 dist.cdf(1.)\n>       5 dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\n>       6 dist.prob([0, 1.5])\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1429,\n> in Distribution.cdf(self, value, name, **kwargs)    1411 def cdf(self,\n> value, name='cdf', **kwargs):    1412   \"\"\"Cumulative distribution\n> function.    1413     1414   Given random variable `X`, the cumulative\n> distribution function `cdf` is:    (...)    1427       values of type\n> `self.dtype`.    1428   \"\"\"\n> -> 1429   return self._call_cdf(value, name, **kwargs)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1405,\n> in Distribution._call_cdf(self, value, name, **kwargs)    1403 with\n> self._name_and_control_scope(name, value, kwargs):    1404   if\n> hasattr(self, '_cdf'):\n> -> 1405     return self._cdf(value, **kwargs)    1406   if hasattr(self, '_log_cdf'):    1407     return\n> tf.exp(self._log_cdf(value, **kwargs))\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/normal.py:195,\n> in Normal._cdf(self, x)\n>     194 def _cdf(self, x):\n> --> 195   return special_math.ndtr(self._z(x))\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/internal/special_math.py:136,\n> in ndtr(x, name)\n>     132 if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n>     133   raise TypeError(\n>     134       \"x.dtype=%s is not handled, see docstring for supported types.\"\n>     135       % x.dtype)\n> --> 136 return _ndtr(x)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/internal/special_math.py:141,\n> in _ndtr(x)\n>     139 def _ndtr(x):\n>     140   \"\"\"Implements ndtr core logic.\"\"\"\n> --> 141   half_sqrt_2 = tf.constant(\n>     142       0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n>     143   half = tf.constant(0.5, x.dtype)\n>     144   one = tf.constant(1., x.dtype)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:117,\n> in _constant(value, dtype, shape, name)\n>     116 def _constant(value, dtype=None, shape=None, name='Const'):  # pylint: disable=unused-argument\n> --> 117   x = convert_to_tensor(value, dtype=dtype)\n>     118   if shape is None:\n>     119     return x\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:167,\n> in _convert_to_tensor(value, dtype, dtype_hint, name)\n>     164     pass\n>     166 if ret is None:\n> --> 167   ret = conversion_func(value, dtype=dtype)\n>     168 return ret\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:222,\n> in _default_convert_to_tensor(value, dtype)\n>     218 \"\"\"Default tensor conversion function for array, bool, int, float, and complex.\"\"\"\n>     219 if JAX_MODE:\n>     220   # TODO(b/223267515): We shouldn't need to specialize here.\n>     221   if hasattr(value, 'dtype') and jax.dtypes.issubdtype(\n> --> 222       value.dtype, jax.dtypes.prng_key\n>     223   ):\n>     224     return value\n>     225   if isinstance(value, (list, tuple)) and value:\n> \n> AttributeError: module 'jax.dtypes' has no attribute 'prng_key'\nThe installed packages are\n> Package                  Version             \n------------------------ --------------------\nabsl-py                  2.0.0               \nAmbit-Stochastics        1.0.6               \nanyio                    3.6.2               \nargon2-cffi              21.3.0              \nargon2-cffi-bindings     21.2.0              \nasttokens                2.2.1               \nattrs                    19.3.0              \nAutomat                  0.8.0               \nBabel                    2.11.0              \nbackcall                 0.2.0               \nbeautifulsoup4           4.11.2              \nbleach                   6.0.0               \nblinker                  1.4                 \ncertifi                  2019.11.28          \ncffi                     1.15.1              \nchardet                  3.0.4               \ncharset-normalizer       3.0.1               \nClick                    7.0                 \ncloud-init               23.2.1              \ncloudpickle              2.2.1               \ncolorama                 0.4.3               \ncomm                     0.1.2               \ncommand-not-found        0.3                 \nconfigobj                5.0.6               \nconstantly               15.1.0              \ncryptography             2.8                 \ncycler                   0.11.0              \ndbus-python              1.2.16              \ndebugpy                  1.6.6               \ndecorator                5.1.1               \ndefusedxml               0.7.1               \ndistlib                  0.3.6               \ndistro                   1.4.0               \ndistro-info              0.23ubuntu1         \ndm-tree                  0.1.8               \nentrypoints              0.3                 \nenv                      0.1.0               \nexecuting                1.2.0               \nfastjsonschema           2.16.2              \nfilelock                 3.9.0               \ngast                     0.5.4               \nhttplib2                 0.14.0              \nhyperlink                19.0.0              \nidna                     2.8                 \nimportlib-metadata       6.0.0               \nimportlib-resources      5.10.2              \nincremental              16.10.1             \nipykernel                6.20.2              \nipython                  8.9.0               \nipython-genutils         0.2.0               \nipywidgets               8.1.1               \njax                      0.4.13              \njaxlib                   0.4.13              \njedi                     0.18.2              \nJinja2                   3.1.2               \njson5                    0.9.11              \njsonpatch                1.22                \njsonpointer              2.0                 \njsonschema               4.17.3              \njupyter                  1.0.0               \njupyter-client           8.0.2               \njupyter-console          6.6.3               \njupyter-core             5.2.0               \njupyter-events           0.6.3               \njupyter-server           2.2.0               \njupyter-server-terminals 0.4.4               \njupyterlab               3.5.3               \njupyterlab-pygments      0.2.2               \njupyterlab-server        2.19.0              \njupyterlab-widgets       3.0.9               \nkeyring                  18.0.1              \nkiwisolver               1.3.2               \nlanguage-selector        0.1                 \nlaunchpadlib             1.10.13             \nlazr.restfulclient       0.14.2              \nlazr.uri                 1.0.3               \nMarkupSafe               2.1.2               \nmatplotlib               3.4.3               \nmatplotlib-inline        0.1.6               \nmistune                  2.0.4               \nml-dtypes                0.2.0               \nmore-itertools           4.2.0               \nnbclassic                0.5.1               \nnbclient                 0.7.2               \nnbconvert                7.2.9               \nnbformat                 5.7.3               \nnest-asyncio             1.5.6               \nnetifaces                0.10.4              \nnotebook                 6.5.2               \nnotebook-shim            0.2.2               \nnumpy                    1.21.3              \noauthlib                 3.1.0               \nopt-einsum               3.3.0               \npackaging                23.0                \npandas                   1.3.4               \npandocfilters            1.5.0               \nparso                    0.8.3               \npexpect                  4.6.0               \npickleshare              0.7.5               \nPillow                   8.4.0               \npip                      20.0.2              \npkgutil-resolve-name     1.3.10              \nplatformdirs             2.6.2               \nprometheus-client        0.16.0              \nprompt-toolkit           3.0.36              \npsutil                   5.9.4               \nptyprocess               0.7.0               \npure-eval                0.2.2               \npyasn1                   0.4.2               \npyasn1-modules           0.2.1               \npycparser                2.21                \nPygments                 2.14.0              \nPyGObject                3.36.0              \nPyHamcrest               1.9.0               \nPyJWT                    1.7.1               \npymacaroons              0.13.0              \nPyNaCl                   1.3.0               \npyOpenSSL                19.0.0              \npyparsing                3.0.4               \npyrsistent               0.15.5              \npyserial                 3.4                 \npython-apt               2.0.1+ubuntu0.20.4.1\npython-dateutil          2.8.2               \npython-debian            0.1.36+ubuntu1.1    \npython-json-logger       2.0.4               \npytz                     2021.3              \nPyYAML                   5.3.1               \npyzmq                    25.0.0              \nqtconsole                5.4.4               \nQtPy                     2.4.0               \nrequests                 2.28.2              \nrequests-unixsocket      0.2.0               \nrfc3339-validator        0.1.4               \nrfc3986-validator        0.1.1               \nscipy                    1.10.1              \nSecretStorage            2.3.1               \nSend2Trash               1.8.0               \nservice-identity         18.1.0              \nsetuptools               45.2.0              \nsimplejson               3.16.0              \nsix                      1.14.0              \nsniffio                  1.3.0               \nsos                      4.4                 \nsoupsieve                2.3.2.post1         \nssh-import-id            5.10                \nstack-data               0.6.2               \nsystemd-python           234                 \nterminado                0.17.1              \ntfp-nightly              0.22.0.dev20231002  \ntinycss2                 1.2.1               \ntomli                    2.0.1               \ntornado                  6.2                 \ntraitlets                5.9.0               \nTwisted                  18.9.0              \ntyping-extensions        4.5.0               \nubuntu-advantage-tools   8001                \nufw                      0.36                \nunattended-upgrades      0.1                 \nurllib3                  1.25.8              \nvirtualenv               20.17.1             \nwadllib                  1.3.3               \nwcwidth                  0.2.6               \nwebencodings             0.5.1               \nwebsocket-client         1.5.0               \nwheel                    0.34.2              \nwidgetsnbextension       4.0.9               \nzipp                     1.0.0               \nzope.interface           4.7.1     \nIf I change the Normal distribution to the Gamma distribution, I no longer get an error. Any ideas why this might be? The code snippet is taken directly from the tensorflow website. thanks!",
        "answers": [
            "jax.dtypes.prng_key was added in JAX version 0.4.14. You should update JAX to a newer version (0.4.14 or later), or if that is not possible, downgrade tensorflow_probability to an older version (0.21.0 or older should be sufficient for this issue)."
        ],
        "link": "https://stackoverflow.com/questions/77221932/jax-prng-key-error-with-tfp-normal-distribution"
    },
    {
        "title": "Should models be trained using fori_loop?",
        "question": "When optimizing weights and biases of a model, does it make sense to replace:\nfor _ in range(epochs):\n    w, b = step(w, b)\nWith:\nw, b = lax.fori_loop(0, epochs, lambda wb: step(wb[0], wb[1]), (w, b))\nIf I understand correctly, this means that the entire training process can then be a single compiled JAX function (takes in training data, outputs optimized weights and biases).\nIs this a standard approach? What are the tradeoffs to consider?",
        "answers": [
            "It's fine to train your model with fori_loop, particularly for simple models. It may be slightly faster, but in general XLA won't fuse operations across different loop steps. It's also not possible to return early within a fori_loop when you reach a certain loss threshold (though you could do that with while_loop if you wish).\nFor more complicated models, you often will want to do some sort of I/O at every step (e.g. loading new training data, logging fit parameters, etc.) While this is possible to do within fori_loop via jax.experimental.io_callback, it is somewhat less convenient than doing it directly from the host within a Python for loop, so in general users tend to use for loops for their training iterations."
        ],
        "link": "https://stackoverflow.com/questions/77217387/should-models-be-trained-using-fori-loop"
    },
    {
        "title": "Does equinox (jax) do no batch dim broadcasting and expects you to use vmap instead?",
        "question": "https://docs.kidger.site/equinox/api/nn/mlp/#equinox.nn.MLP\nThe only way I was able to use MLP is like this\nimport jax\nimport equinox as eqx\nimport numpy as np\n\n\njax.vmap(eqx.nn.MLP(in_size=12, out_size=4, width_size=6, depth=5, key=key))(np.random.randn(5, 12)\nIs this the intended usage? It differs a bit from other frameworks then. But maybe safer.",
        "answers": [
            "Yup, this is intended!\nEvery layer in eqx.nn acts on a single batch element, and you can apply them to batches by calling jax.vmap, exactly as you're doing.\nSee also this FAQ entry: https://docs.kidger.site/equinox/faq/#how-do-i-input-higher-order-tensors-eg-with-batch-dimensions-into-my-model\nI hope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/77090293/does-equinox-jax-do-no-batch-dim-broadcasting-and-expects-you-to-use-vmap-inst"
    },
    {
        "title": "How to select between different function based on a value of a parameter in flax?",
        "question": "I am iterating through each head and applying either f1 or f2 function depending on the value of the parameter self.alpha.\nI only want to evaluate either function f1 or f2 not both and then select output of one based on conditional.\n        def f1 (x):\n            print('f1')\n            return x/x.shape[2]\n        def f2 (x):\n            print('f2')\n            temp = nn.relu(x)\n            return temp/(jnp.sum(temp,axis=-1,keepdims=True) + 1e-5)\n        \n        def choose_attention(alpha, x):\n            return jax.lax.cond(alpha[0, 0, 0,0],f2,f1,operand=x)\n        \n        results = []\n        func = [f1,f2]\n        for i in range(self.alpha.shape[1]):\n            print(i)\n            alpha_i = self.alpha[:, i:i+1, :, :]\n            x_i = attn_weights[:, i:i+1, :, :]\n            result_i = jax.lax.switch(self.alpha[0,0,0,0].astype(int),func,x_i)\n            results.append(result_i)\n\n        final_result = jnp.concatenate(results, axis=1)\nMy print statements read like 0 f1 f2 1 2 3 4 5 6 7 8 9 10 11",
        "answers": [
            "jax.lax.switch does what you want: it chooses between two different functions based on a runtime value. Your use of print statements is misleading you: Python print runs at trace-time rather than runtime, and all code will be traced even if it is not eventually executed.\nFor some background on how to think about the execution model of JAX programs, I would suggest How to think in JAX.\nSide note: for better performance, I would also suggest avoiding using Python for loops to loop through array values, and instead express your algorithm using either Numpy-style explicit vectorization, or using jax.vmap to automatically vectorize your code."
        ],
        "link": "https://stackoverflow.com/questions/77083595/how-to-select-between-different-function-based-on-a-value-of-a-parameter-in-flax"
    },
    {
        "title": "jax.errors.UnexpectedTracerError only when using jax.debug.breakpoint()",
        "question": "My jax code runs fine but when I try to insert a breakpoint with jax.debug.breakpoint I get the error: jax.errors.UnexpectedTracerError.\nI would expect this error to show up also without setting a breakpoint.\nIs this intended behavior or is something weird happening? When using jax_checking_leaks none of the reported tracers seem to actually be leaked.",
        "answers": [
            "There is currently a bug in jax.debug.breakpoint that can lead to spurious tracer leaks in some situations: see https://github.com/google/jax/issues/16732.\nThere's not any easy workaround at the moment, unfortunately, but hopefully the issue will be addressed soon."
        ],
        "link": "https://stackoverflow.com/questions/77067644/jax-errors-unexpectedtracererror-only-when-using-jax-debug-breakpoint"
    },
    {
        "title": "JAX produces memory error for simple program on GPU",
        "question": "I installed JAX (pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html) and even for a simple code like\na = jnp.array([1,2,3])\na.dot(a)\nI get the following error:\n2023-09-08 10:12:55.791658: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:445] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n2023-09-08 10:12:55.791696: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:449] Memory usage: 8058437632 bytes free, 8513978368 bytes total.\nIt looks like a memory issue. I tried the tips mentioned here\nhttps://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\nbut to no success.\nThis is the result of nvidia-smi on my system:\nNVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   64C    P0    37W /  N/A |    364MiB /  8119MiB |      2%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\nIt seems strange to me that there seems to be a out of memory issue when it is just a single array.\nAny ideas?",
        "answers": [
            "I don't believe this is a memory issue; rather it looks like you have a mismatch between your CUDA and CUDNN versions.\nOne way to ensure your CUDA versions are compatible is to use the pip-based installation (see JAX pip installation: GPU (CUDA, installed via pip, easier)). This should ensure that you install mutually-compatible CUDA, CUDNN, and jaxlib versions on your system. Installing JAX using pip-installed CUDA looks something like this:\n$ pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nIt looks like this may be the approach you used; if so, you should check that your system path (e.g. LD_LIBRARY_PATH) is not pre-empting the pip-installed CUDA with a local version. There is some relevant discussion at https://github.com/google/jax/issues/17497.\nIf you want to use a local CUDA installation, you can follow JAX pip installation: GPU (CUDA, installed locally, harder), but then it is up to you to ensure that your CUDA, CUDNN, and jaxlib versions are mutually compatible. Installing JAX using local CUDA looks something like this:\n$ pip install --upgrade \"jax[cuda12_local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nbut if you use this approach, be sure to read the details at the above link."
        ],
        "link": "https://stackoverflow.com/questions/77065313/jax-produces-memory-error-for-simple-program-on-gpu"
    },
    {
        "title": "Reason to return updated stack along with top element from pop() method",
        "question": "This:\n  def pop(self) -> tuple[Any, Stack]:\n    \"\"\"Pops from the stack, returning an (elem, updated stack) pair.\"\"\"\nWhat is the reason behind returning updated stack along with top element from pop() method?",
        "answers": [
            "From the docstring of the Stack class: \"A bounded functional stack implementation.\"\nA functional implementation generally means that pure functions are used. The pop function implements two pure function properties:\nThe output does not change when called multiple times with the same input.\nThe function does not mutate the stack, ie it does not introduce side effects.\nThis is why it must return a (mutated) copy of the input Stack instead of modifying it."
        ],
        "link": "https://stackoverflow.com/questions/77057836/reason-to-return-updated-stack-along-with-top-element-from-pop-method"
    },
    {
        "title": "Parallelize with JAX over all GPU cores",
        "question": "In order to minimize the function x^2+y^2, I tried to implement the Adam optimizer from scratch with JAX:\n@jax.jit\ndef fit(X, batches, params=[0.001, 0.9, 0.99, 1e-8]):\n    global fun\n\n    # batches is array containing n batches\n\n    @jax.jit\n    def adam_update(X, t, grads, m, v, alpha, b1, b2, epsilon):\n        m = b1*m + (1-b1)*grads\n        v = b2*v + (1-b2)*grads**2\n        m_hat = m / (1-b1**t)\n        v_hat = v / (1-b2**t)\n        X = X - alpha*(1-b2**t)**(1/2)*m_hat/(1-b1**t)/((v_hat)**(1/2)+epsilon)\n        return [X, m, v]\n\n    dim=jnp.shape(X)[0]\n\n    params = jnp.array(params)\n    alpha = params[0]\n    b1=params[1]\n    b2=params[2]\n    epsilon=params[3]\n    \n    adam_update = jax.jit(partial(adam_update, alpha=alpha, b1=b1, b2=b2, epsilon=epsilon))\n\n    m=jnp.zeros(dim)\n    v=jnp.zeros(dim)\n\n    for t, batch in enumerate(batches):\n        fun_ = jax.jit(partial(fun, batch=batch))\n        grads = jax.grad(fun_)(X)\n        X, m, v = adam_update(X, t+1, grads, m, v)\n    return X\nWith JAX I could parallelize this simply with jax.pmap, however it would only be parallelized over the 8 GPUs, instead over all GPU cores. Is there a way too parallelize this code over all cores?\nCan it be that all cores of one GPU are miraculously used upon using @jax.jit. Also, why does it need 200 seconds for compiling for 1000 iterations, while the optax-Adam optimizer does not take so long too compile?",
        "answers": [
            "Can it be that all cores of one GPU are miraculously used upon using @jax.jit\nIn general, yes. For computations on a single device, the XLA GPU compiler will use all available cores of the GPU to complete a computation.\nAlso, why does it need 200 seconds for compiling for 1000 iterations, while the optax-Adam optimizer does not take so long too compile?\nThis is because you are JIT-compiling a Python for-loop. Python loops within JIT are unrolled by JAX into a linear program (see JAX Sharp Bits: Control Flow), and compilation time grows with the size of the program.\nBy contrast, the optax quick-start recommends JIT-compiling the step function, but does not JIT-compile the fitting loop. This would lead to much faster compilation times than the pattern used in your code, where the full for-loop is within a JIT-compiled function."
        ],
        "link": "https://stackoverflow.com/questions/77036442/parallelize-with-jax-over-all-gpu-cores"
    },
    {
        "title": "JAX update breaks working code of linear algebra solver",
        "question": "I am dealing with an issue related to an update of jax. I have a library which is supposed to solve a system of linear equations using the bicgstab algorithm.\nThe solver is implemented as follows:\ndef bicgstabsolver(A, b, eps):\n  '''Returns the loop initialization and iteration functions.'''\n\n  def init(z, b, x0):\n    '''Forms the args that will be used to update stuff.'''\n\n    x = x0\n    r = b - A(x, z)\n    rstrich = r\n    v = vecfield.zeros(b.shape)\n    p = vecfield.zeros(b.shape)\n    alpha = 1\n    rho = 1\n    omega = 1\n\n    term_err = eps * vecfield.norm(b)\n\n    return x, r, rstrich, v, p, alpha, rho, omega, term_err\n    \n  @jax.jit\n  def iter(x, r, rstrich, v, p, alpha, rho, omega, z):\n    '''Run the iteration loop `n` times.'''\n    rhoold = rho\n    rho = vecfield.dot(vecfield.conj(rstrich),r)\n    beta = (rho / rhoold) * (alpha / omega)\n    p = r + beta * (p - omega * v)\n    v = A(p,z)\n    alpha = rho / vecfield.dot(vecfield.conj(rstrich),v)\n    h = x + alpha * p\n    s = r - alpha * v\n    t = A(s,z)\n    omega = vecfield.dot(vecfield.conj(t),s) / vecfield.dot(vecfield.conj(t),t)\n    x = h + omega * s\n    r = s - omega * t\n    err = vecfield.norm(r)\n    return x, r, rstrich, v, p, alpha, rho, omega, err\n\n  return init, iter\nThe implementation of the VecField class:\nimport jax.numpy as np\nfrom typing import Any, NamedTuple\n\n\nclass VecField(NamedTuple):\n  '''Represents a 3-tuple of arrays.'''\n  x: Any\n  y: Any\n  z: Any\n\n  @property\n  def shape(self):\n    assert self.x.shape == self.y.shape == self.z.shape\n    return self.x.shape\n\n  @property\n  def dtype(self):\n    assert self.x.dtype == self.y.dtype == self.z.dtype\n    return self.x.dtype\n\n  def as_array(self):\n    return VecField(*(np.array(a) for a in self))\n\n  def __add__(x, y):\n    return VecField(*(a + b for a, b in zip(x, y)))\n\n  def __sub__(x, y):\n    return VecField(*(a - b for a, b in zip(x, y)))\n\n  def __mul__(x, y):\n    return VecField(*(a * b for a, b in zip(x, y)))\n\n  def __rmul__(y, x):\n    return VecField(*(x * b for b in y))\n\n\ndef zeros(shape):\n  return VecField(*(np.zeros(shape, np.complex128) for _ in range(3)))\n\ndef ones(shape):\n  return VecField(*(np.ones(shape, np.complex128) for _ in range(3)))\n\n# TODO: Check if this hack is still necessary to obtain good performance.\ndef dot(x, y):\n  z = VecField(*(a * b for a, b in zip(x, y)))\n  return sum(np.sum(np.real(c)) + 1j * np.sum(np.imag(c)) for c in z)\n\n\ndef norm(x):\n  return np.sqrt(sum(np.square(np.linalg.norm(a)) for a in x))\n\n\ndef conj(x):\n  return VecField(*(np.conj(a) for a in x))\n\n\ndef real(x):\n  return VecField(*(np.real(a) for a in x))\n\n\ndef from_tuple(x):\n  return VecField(*(np.reshape(a, (1, 1) + a.shape) for a in x))\n\n\ndef to_tuple(x):\n  return tuple(np.reshape(a, a.shape[2:]) for a in x)\nThe code is running perfectly fine using jax and jaxlib version 0.3.10. However, if I update jax to 0.4.13 it stops working with a cryptic error:\nFile \"***\", line 66, in iter\n    p = r + beta * (p - omega * v)\n  File \"***/python3.8/site-packages/jax/_src/numpy/array_methods.py\", line 791, in op\n    return getattr(self.aval, f\"_{name}\")(self, *args)\n  File \"***/python3.8/site-packages/jax/_src/numpy/array_methods.py\", line 260, in deferring_binary_op\n    raise TypeError(f\"unsupported operand type(s) for {opchar}: \"\njax._src.traceback_util.UnfilteredStackTrace: TypeError: unsupported operand type(s) for *: 'DynamicJaxprTracer' and 'VecField'\nI have no clue so far how to migrate this code to be compatible with the newer version of jax. Probably I'm missing something very obvious. Any help would be greatly appreciated!",
        "answers": [
            "It looks like JAX's array __mul__ methods are raising a TypeError on unsupported input rather than returning NotImplemented, which means that omega * v is not correctly dispatching to v.__rmul__().\nThis is a bug in JAX: I would suggest reporting this in a new issue at http://github.com/google/jax/issues/\nIn the meantime, you should be able to work around this by making sure that every time you operate between a VecField by a JAX array, the VecField appears on the left of the operand; e.g. change this:\np = r + beta * (p - omega * v)\nto this:\np = (p - v * omega) * beta + r\nEdit: it looks like the bug was introduced in https://github.com/google/jax/pull/11234 (meaning it's present in all JAX versions 0.3.14 and newer) and only affects subtypes of builtin collections (which includes NamedTuple).\nEdit 2: this has been fixed in https://github.com/google/jax/pull/17406, which should be part of a future JAX 0.4.16 release."
        ],
        "link": "https://stackoverflow.com/questions/77024323/jax-update-breaks-working-code-of-linear-algebra-solver"
    },
    {
        "title": "How to extract chunks of a 2D numpy array that has been flattened",
        "question": "I would like to know the best way of extacting chunks of elements from a 2D numpy array that has been flattened. See example python code below which hopefully explains what I want to do a little better.\nimport numpy as np\n\nnx = 5\nnz = 7\nnumGPs = nx*nz\n\nGPs_matrix = np.arange(numGPs).reshape((nx,nz), order='F')\nav = np.zeros_like(GPs_matrix)\nav[1:-1,1:-1] = (GPs_matrix[1:-1,2:] + GPs_matrix[1:-1,:-2] + GPs_matrix[2:,1:-1] + GPs_matrix[:-2,1:-1])/4\n# How to do the above if GPs is flattened as per below?\nGPs_flat = GPs_matrix.reshape(-1, order='F')\n# One (very clunky) way is to do the following\ncor = GPs_matrix[1:-1,1:-1].reshape(-1, order='F')\nbtm = GPs_matrix[1:-1,:-2].reshape(-1, order='F')\ntop = GPs_matrix[1:-1,2:].reshape(-1, order='F')\nlft = GPs_matrix[:-2,1:-1].reshape(-1, order='F')\nrgt = GPs_matrix[2:,1:-1].reshape(-1, order='F')\nav_flat = np.zeros_like(GPs_flat)\nav_flat[cor] = (GPs_flat[top] + GPs_flat[btm] + GPs_flat[rgt] + GPs_flat[lft])/4\n# Check\nprint(av.reshape(-1, order='F'))\nprint(av_flat)\n# Is there a better way?\n# I see np.r_() may be useful but I don't know how to best use it. I assume the below attempt can be improved upon\nsl_cor = np.r_[nx+1:2*nx-1,2*nx+1:3*nx-1,3*nx+1:4*nx-1,4*nx+1:5*nx-1,5*nx+1:6*nx-1] # Should match cor above\n# Check\nprint(sl_cor)\nprint(cor)\n# The below also works but I would like to avoid using loops if possible\nprint(np.r_[*[np.arange(1,nx-1)+nx*j for j in range(1,nz-1)]])\nEssentially, I am trying to solve the possion equation in two spatial dimensions. It is convenient to set up the problem in the form of a 2D array as the location of the elements in the array corresponds to the position of the grid points in the cartesian mesh. Ultimately I will be using an iterate solver in jax to solve the linear system (e.g. bicgstab) which requires a linear operator function as input. Therefore, the function needs to return a vector and loops are not efficient.",
        "answers": [
            "First off, it looks like what you're attempting to compute is a convolution. In general I'd avoid doing a convolution by hand, and instead use something like scipy.signal.convolve2d. Here's the equivalent for your case:\nfrom scipy.signal import convolve2d\nkernel = np.array(([[0, 1, 0],\n                    [1, 0, 1],\n                    [0, 1, 0]])) / 4\nav = np.zeros_like(GPs_matrix)\nav[1:-1, 1:-1] = convolve2d(GPs_matrix, kernel, mode='valid').astype(av.dtype)\nIn the flattened case, your best approach is probably going to be to reshape the 1D input, perform the 2D convolution, and then flatten the output. For example:\ndef eval_1D(vec):\n  mat = vec.reshape(nx, nz, order='F')\n  kernel = np.array(([[0, 1, 0],\n                      [1, 0, 1],\n                      [0, 1, 0]])) / 4\n  av = np.zeros_like(mat)\n  av[1:-1, 1:-1] = convolve2d(mat, kernel, mode='valid').astype(av.dtype)\n  return av.ravel(order='F')\n\nprint(eval_1D(GPs_matrix.ravel(order='F')))\n# [ 0  0  0  0  0  0  6  7  8  0  0 11 12 13  0  0 16 17\n#  18  0  0 21 22 23  0  0 26 27 28  0  0  0  0  0  0]"
        ],
        "link": "https://stackoverflow.com/questions/76952780/how-to-extract-chunks-of-a-2d-numpy-array-that-has-been-flattened"
    },
    {
        "title": "Migration from haiku: Alternative to Haiku's PRNGSequence?",
        "question": "I am writing a Markov chain Monte Carlo simulation in JAX which involves a large series of sampling steps. I currently rely on haiku's PRNGSequence to do the pseudo random number generator key bookkeeping:\nimport haiku as hk\n\ndef step(key, context):\n  key_seq = hk.PRNGSequence(key)\n  x1 = sampler(next(key_seq), context_1)\n  ...\n  xn = other_sampler(next(key_seq), context_n)\nQuestion:\nSince Haiku has been discontinued, I am looking for an alternative to PRNGSequence.\nI find the standard JAX approach:\ndef step(key, context):\n  key, subkey = jax.random.split(key)\n  x1 = sampler(subkey, context_1)\n  ...\n  key, subkey = jax.random.split(key)\n  xn = other_sampler(subkey, context_n)\nunsatisfactory on two accounts:\nVery error prone: It is easy to slip up and re-use a key. This is especially problematic in MCMC simulations, which are sensitive to these biases and very difficult to debug.\nIt is quite bulky: I need to roughly double the size of my code to split keys.\nAny suggestions how to mitigate these problems?\nThanks!\nHylke",
        "answers": [
            "If all you need is a simple class that locally handles splitting keys for you, why not define it yourself? You could create a suitable one in a few lines – for example:\nimport jax\n\nclass PRNGSequence:\n  def __init__(self, key):\n    self._key = key\n  def __next__(self):\n    self._key, key = jax.random.split(self._key)\n    return key\n\ndef step(key):\n  key_seq = PRNGSequence(key)\n  print(jax.random.uniform(next(key_seq)))\n  print(jax.random.uniform(next(key_seq)))\n\nstep(jax.random.PRNGKey(0))\n# 0.10536897\n# 0.2787192\nAs always, though, you have to be careful about this kind of hidden state when you're using JAX transformations like jit: see JAX Sharp Bits: Pure Functions for information on this.",
            "I recently came across this nice one-liner alternative:\nimport itertools\n\nkey_seq = map(jax.random.key, itertools.count())\nkey = next(key_seq)\nwhich uses the infinite iterator count."
        ],
        "link": "https://stackoverflow.com/questions/76912173/migration-from-haiku-alternative-to-haikus-prngsequence"
    },
    {
        "title": "how does the bind() function work in JAX when making a new primitive?",
        "question": "While I am checking and studying the jax code, I see lots of bind() usage. I wanted to know how it works and what's its functionality. Here is one of the example.\ndef test_unimplemented_interpreter_rules(self):\n    foo_p = core.Primitive('foo')\n    def foo(x):\n      return foo_p.bind(x)",
        "answers": [
            "You can think of bind() as essentially equivalent to calling the function represented by the primitive. But because JAX primitives are designed to work with its abstract evaluation and transformation framework, binding a primitive is a bit more complicated than just calling a function, because the meaning of binding a primitive changes depending on the context. For example:\nin normal, non-transformed code, it will result in a call to the primitive's impl rule\nduring general abstract evaluation like jax.make_jaxpr or jax.eval_shape, it will call the primitive's abstract evaluation rule\ninside jax.vmap, it will result in a call to the primitive's batching rule\ninside jax.grad, it will result in a call to the primitive's autodiff-related rules (jvp and/or transpose)\nIf you want to understand more about the details of JAX primitives and how they interact with JAX tracing, abstract evaluation, and transformations, a good resource is Autodidax: JAX core from scratch, which walks through creating a simplified version of JAX from the ground up."
        ],
        "link": "https://stackoverflow.com/questions/76908334/how-does-the-bind-function-work-in-jax-when-making-a-new-primitive"
    },
    {
        "title": "Installing jaxlib for cuda 11.8",
        "question": "I'm trying to install jax and jaxlib on my Ubuntu 18 with python 3.8 for snerg (https://github.com/google-research/google-research/tree/master/snerg). Unfortunately when I try to install jax and jaxlib for Cuda 11.8 with the following command :\npip install --upgrade jax jaxlib==0.1.69+cuda118 -f https://storage.googleapis.com/jax-releases/jax_releases.html \nI get the following error:\nERROR: Ignored the following versions that require a different python version: 0.4.14 Requires-Python >=3.9\nERROR: Could not find a version that satisfies the requirement jaxlib==0.1.69+cuda118 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15, 0.3.18, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13)\nERROR: No matching distribution found for jaxlib==0.1.69+cuda118\nWould appreciate any help. Thanks",
        "answers": [
            "Follow the following instructions which are primarily obtained from the source:\nUninstall previous versions (if any):\n$ pip uninstall jax jaxlib jaxtyping -y\nUpgrade your pip:\n$ pip install --upgrade pip\nFind out which CUDA is already installed on your machine:\n$ nvidia-smi\n\nThu Jan  4 11:24:58 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A1000 6GB Lap...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   58C    P0              12W /  35W |      8MiB /  6144MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      3219      G   /usr/lib/xorg/Xorg                            4MiB |\n+---------------------------------------------------------------------------------------+\nDepending on the CUDA version of your machine( wheels only available on linux ), run EITHER of the following:\n# CUDA 12.X installation\n$ pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n#### OR ####\n\n# CUDA 11.X installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nTo double check if you have have successfully configured the gpu:\n$ python -c \"import jax; print(f'Jax backend: {jax.default_backend()}')\"\nJax backend: gpu ",
            "jaxlib version 0.1.69 is quite old (it was released in July 2021) CUDA 11.8 was released over a year later, in September 2022. Thus I would not expect there to be pre-built binaries for jaxlib version 0.1.69 targeting CUDA 11.8.\nIf possible, your best bet would be to install a newer version of jaxlib, one which has builds targeting CUDA 11.8. The current jaxlib+CUDA GPU installation instructions can be found here.\nIf for some reason you absolutely need this very old jaxlib version, you'll probably first have to install an older CUDA version on your system. The CUDA jaxlib installation instructions from jaxlib 0.1.69 can be found here: it looks like it was built to target CUDA 10.1-10.2, 11.0, or 11.1-11.3."
        ],
        "link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8"
    },
    {
        "title": "Is it possible to obtain values from jax traced arrays with dynamicjaxprtrace level larger than 1 using any of the callback functions?",
        "question": "So I have a program that have multiple functions with its own jax calls and here is the main function:\n@partial(jax.jit, static_argnames=(\"numberOfVoxels\",))    \ndef process_valid_voxels(numberOfVoxels, voxelPositions, voxelLikelihoods, ps, t, M, tmp):\n   \n    func = lambda tmp_val: process_voxel(tmp_val, voxelPositions, voxelLikelihoods, ps, t, M, tmp)\n    ys, likelihoods = jax.vmap(func)(jnp.arange(numberOfVoxels))\n   \n    return ys, likelihoods\nThis is the output of ys and likelihoods:\n(Pdb) ys\nTraced<ShapedArray(int32[3700,3,1])>with<DynamicJaxprTrace(level=3/0)>`\nlikelihoods\nTraced<ShapedArray(float32[3700,7,1])>with<DynamicJaxprTrace(level=3/0)>\nI want to get values from traced arrays ys, likelihoods so that I can modify them. I have tried using the io_callback function:\ndef callback1(x):\n    return jax.experimental.io_callback(process_voxel, x , x)\na = callback1(jnp.arange(numberOfVoxels))\nbut the output is the same except for the shape of the array:\nTraced<ShapedArray(int32[3700])>with<DynamicJaxprTrace(level=3/0)>",
        "answers": [
            "This is similar to one of JAX's FAQs: How can I convert a tracer to a numpy array?. That answer mentions callbacks, which you use above, but I think you have the wrong mental model of what io_callback does.\nWhen you run transformed JAX code, there are essentially two stages of execution:\nTracing happens within the Python runtime, using abstract representations of the arrays (tracers) to extract the sequence of operations implied by your code. During tracing in most cases, array values are not available by design.\nExecution happens within the XLA runtime once tracing has encoded the sequence of operations to be run. Array values are available during XLA execution, but this is not a Python runtime, and so Python debugging, printing, etc. is not available.\nYour question amounts to \"How can I access array values from stage 2 by inserting breakpoints into the runtime during stage 1\" The answer is: you can't!\nBut you can use callbacks and jax debugging tools to encode an instruction to tell XLA to pause execution and pass its values to some callback function during stage 2 execution and let you interact with array values from within Python.\nOne way to do so might look like this:\n@partial(jax.jit, static_argnames=(\"numberOfVoxels\",))    \ndef process_valid_voxels(numberOfVoxels, voxelPositions, voxelLikelihoods, ps, t, M, tmp):\n    func = lambda tmp_val: process_voxel(tmp_val, voxelPositions, voxelLikelihoods, ps, t, M, tmp)\n    ys, likelihoods = jax.vmap(func)(jnp.arange(numberOfVoxels))\n    jax.debug.breakpoint()\n    return ys, likelihoods\nThis will tell XLA during stage 2 to pause execution, call back to a Python-side debugging tool, and let you interact with the values there.\nFor more intuition on the mental model of JAX program execution, you may find How to think in JAX useful."
        ],
        "link": "https://stackoverflow.com/questions/76827852/is-it-possible-to-obtain-values-from-jax-traced-arrays-with-dynamicjaxprtrace-le"
    },
    {
        "title": "Computing the gradient of a batched function using JAX",
        "question": "I would need to compute the gradient of a batched function using JAX. The following is a minimal example of what I would like to do:\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.expand_dims(jnp.linspace(-1, 1, 20), axis=1)\n\nu = lambda x: jnp.sin(jnp.pi * x)\nux = jax.vmap(jax.grad(u))\n\nplt.plot(x, u(x))\nplt.plot(x, ux(x))  # Use vx instead of ux\nplt.show()\nI have tried a variety of ways of making this work using vmap, but I don't seem to be able to get the code to run without removing the batch dimension in the input x. I have seen some workarounds using the Jacobian but this doesn't seem natural as the given is a scalar function of a single variable.\nIn the end u will be a neural network (implemented in Flax) that I need to differentiate with respect to the input (not the parameters of the network), so I cannot remove the batch dimension.",
        "answers": [
            "To ensure the kernel (u) returns a scalar value, so that jax.grad makes sense, the batched dimension also needs to be mapped over.\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.expand_dims(jnp.linspace(-1, 1, 20), axis=1)\n\nu = lambda x: jnp.sin(jnp.pi * x)\nux = jax.vmap(jax.vmap(jax.grad(u)))\n# ux = lambda x : jax.lax.map(jax.vmap(jax.grad(u)), x) # sequential version\n# ux = lambda x : jax.vmap(jax.grad(u))(x.reshape(-1)).reshape(x.shape) # flattened map version\n\nplt.plot(x, u(x))\nplt.plot(x, ux(x))  # Use vx instead of ux\nplt.show()\nWhich composition of maps to use depends on what's happening in the batched dimension."
        ],
        "link": "https://stackoverflow.com/questions/76791201/computing-the-gradient-of-a-batched-function-using-jax"
    },
    {
        "title": "Does Haiku cache parameters when combined with jax.vmap?",
        "question": "I have a haiku Module with a call function as follows\nclass MyModule(hk.Module):\n    __call__(self, x):\n        A = hk.get_parameter(\"A\", shape=[self.Ashape], init=A_init)\n        B = hk.get_parameter(\"B\", shape=[self.Bshape], init=B_init)\n        C = self.demanding_computation(A, B)\n\n        res = easy_computation(C, x)\n        return res\nI use this module via\ndef _forward(x):\n    module = MyModule()\n    return module(x)\n\n\nforward = hk.without_apply_rng(hk.transform(_forward))\nx_test = jnp.ones(1)\nparams = forward.init(jax.random.PRNGKey(42), x_test)\nf = jax.vmap(forward.apply, in_axes=(None, 0))\nThen I apply f with the same params to many different x. Is the demanding_computation (that is not depending on x) then cached within the jax.vmap call? If not, what is the correct pattern to separate these computations and get demanding_computation cached?\nI have tried to test this by adding a print statement from jax.experimental.host_callback:\n    def demanding_computation(self, A, B):\n        C = compute(A, B)\n        id_print(C)\n        return C\nand it indeed only printed once. Is that sufficient evidence that this computation is actually cached or is only the printing omitted in subsequent iterations?",
        "answers": [
            "demanding_computation will only be called once, but not because of caching.\nvmap doesn't loop over the batched axes, it replaces the operations with vectorized versions (e.g. scalar additions become vector additions). Since demanding_computation doesn't involve inputs with batch axes it won't be modified by this use of vmap. (Even if it did, it would still only be run once, it would just be a vectorized version)."
        ],
        "link": "https://stackoverflow.com/questions/76788819/does-haiku-cache-parameters-when-combined-with-jax-vmap"
    },
    {
        "title": "How to compute the number of gradient evaluations in Jax.Scipy.minimize.optimize?",
        "question": "I wish to obtain the total number of gradient evaluations during optimization using jax.scipy.optimize.minimize. How can I do so?",
        "answers": [
            "You can find this in the njev (number of Jacobian evaluations) attribute of the output of minimize. For example:\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\n\ndef f(x):\n  return jnp.sum(x ** 2)\n\nout = minimize(f, jnp.array([1., 2.]), method=\"BFGS\")\nprint(out.njev)\n3\nYou can find a list of the information available in the optimization output in the documentation for OptimizeResults."
        ],
        "link": "https://stackoverflow.com/questions/76721987/how-to-compute-the-number-of-gradient-evaluations-in-jax-scipy-minimize-optimize"
    },
    {
        "title": "pytorch and jax networks give different accuracy with same settings",
        "question": "I have pytorch code which performs with more than 95% accuracy. The code essentially implements a feedforward neural network using PyTorch to classify the digits dataset. It trains the model using the Adam optimizer and computes the cross-entropy loss, and then evaluates the model's performance on the test set by calculating the accuracy.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the digits dataset\ndigits = load_digits()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, test_size=0.2, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert the data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Define the FFN model\nclass FFN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, output_size):\n        super(FFN, self).__init__()\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_sizes)):\n            if i == 0:\n                self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[i]))\n            else:\n                self.hidden_layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n            self.hidden_layers.append(nn.ReLU())\n        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n\n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.output_layer(x)\n        return x\n\n# Define the training parameters\ninput_size = X_train.shape[1]\nhidden_sizes = [64, 32]  # Modify the hidden layer sizes as per your requirement\noutput_size = len(torch.unique(y_train_tensor))\nlearning_rate = 0.001\nnum_epochs = 200\nbatch_size = len(X_train)  # Set batch size to the size of the training dataset\n\n# Create the FFN model\nmodel = FFN(input_size, hidden_sizes, output_size)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n\n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\n# Evaluate the model on the test set\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs.data, 1)\n    for j in range(len(predicted)):\n        print(predicted[j], y_test_tensor[j])\n    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0) * 100\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\nAlso I have the equivalent jax code, with performs with less than 10% of accuracy\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, random, value_and_grad\nfrom jax.scipy.special import logsumexp\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom jax.example_libraries.optimizers import adam, momentum, sgd, nesterov, adagrad, rmsprop\nfrom jax import nn as jnn\n\n\n# Load the digits dataset\ndigits = load_digits()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n\n# Reshape the target variables\ny_train_reshaped = jnp.reshape(y_train, (-1, 1))\ny_test_reshaped = jnp.reshape(y_test, (-1, 1))\n\nX_train_reshaped = jnp.reshape(X_train, (-1, 1))\nX_test_reshaped = jnp.reshape(X_test, (-1, 1))\n#print(np.shape(X_train),np.shape(y_train_reshaped),np.shape(y_train))\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_reshaped)\ny_test_scaled = scaler.transform(y_test_reshaped)\n\n# Convert the data to JAX arrays\nX_train_array = jnp.array(X_train, dtype=jnp.float32)\ny_train_array = jnp.array(y_train_reshaped, dtype=jnp.int32)\nX_test_array = jnp.array(X_test, dtype=jnp.float32)\ny_test_array = jnp.array(y_test_reshaped, dtype=jnp.int32)\n\n# Define the FFN model\ndef init_params(rng_key):\n    sizes = [X_train_array.shape[1]] + hidden_sizes + [output_size]\n    keys = random.split(rng_key, len(sizes))\n    params = []\n    for i in range(1, len(sizes)):\n        params.append((random.normal(keys[i], (sizes[i-1], sizes[i])), \n                       random.normal(keys[i], (sizes[i],))))\n    return params\n\ndef forward(params, x):\n    for w, b in params[:-1]:\n        x = jnp.dot(x, w) + b\n        x = jax.nn.relu(x)\n    w, b = params[-1]\n    x = jnp.dot(x, w) + b\n    return x\n\ndef softmax(logits):\n    logsumexp_logits = logsumexp(logits, axis=1, keepdims=True)\n    return jnp.exp(logits - logsumexp_logits)\n\ndef cross_entropy_loss(logits, labels):\n    log_probs = logits - logsumexp(logits, axis=1, keepdims=True)\n    return -jnp.mean(jnp.sum(log_probs * labels, axis=1))\n\n# Define the training parameters\ninput_size = X_train_array.shape[1]\nhidden_sizes = [64, 32]  # Modify the hidden layer sizes as per your requirement\noutput_size = len(jnp.unique(y_train_array))\nlearning_rate = 0.001\nnum_epochs = 200\nbatch_size = len(X_train_array)  # Set batch size to the size of the training dataset\n# Create the FFN model\nrng_key = random.PRNGKey(0)\nparams = init_params(rng_key)\n\n# Define the loss function\ndef loss_fn(params, x, y):\n    logits = forward(params, x)\n    probs = softmax(logits)\n    labels = jax.nn.one_hot(y, output_size)\n    return cross_entropy_loss(logits, labels)\n\n# Create the optimizer\nopt_init, opt_update, get_params = adam(learning_rate)\nopt_state = opt_init(params)\n\n# Define the update step\n@jit\ndef update(params, x, y, opt_state):\n    grads = grad(loss_fn)(params, x, y)\n    return opt_update(0, grads, opt_state)\n\n# Train the model\nfor epoch in range(num_epochs):\n    perm = random.permutation(rng_key, len(X_train_array))\n    for i in range(0, len(X_train_array), batch_size):\n        batch_idx = perm[i:i+batch_size]\n        X_batch = X_train_array[batch_idx]\n        y_batch = y_train_array[batch_idx]\n        params = get_params(opt_state)\n        opt_state = update(params, X_batch, y_batch, opt_state)\n\n    if (epoch + 1) % 10 == 0:\n        params = get_params(opt_state)\n        loss = loss_fn(params, X_train_array, y_train_array)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n\n# Evaluate the model on the test set\nparams = get_params(opt_state)\nlogits = forward(params, X_test_array)\npredicted = jnp.argmax(logits, axis=1)\n\nfor j in range(len(predicted)):\n    print(predicted[j], y_test_array[j])\n\naccuracy = jnp.mean(predicted == y_test_array) * 100\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\nI dont understand why the jax code performs poorly. Could you please help me in underding the bug in the jax code.",
        "answers": [
            "There are 2 probles in your jax code that are, actually, in data processing:\nYour data are not scaled. If you look at your X_train_array definition, it is the jax version of X_train, that is the raw data. Please consider using:\n# Scale the features\nscaler = StandardScaler().fit(X_train)  # No need to flat it!\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert the data to JAX arrays\nX_train_array = jnp.array(X_train, dtype=jnp.float32)\ny_train_array = jnp.array(y_train_reshaped, dtype=jnp.int32)\nX_test_array = jnp.array(X_test, dtype=jnp.float32)\ny_test_array = jnp.array(y_test_reshaped, dtype=jnp.int32)\nYour labels are of shape (N, 1) before one-hot encoding. After one-hot encoding it is (N, 1, n_out) while your predictions are of shape (N, n_out) so when you make your loss computation the two arrays are cast in (N, n_out, n_out) with repetitions and your loss is wrong. You can solve it very simply by remove the 1 in the reshape:\n# Reshape the target variables\ny_train_reshaped = jnp.reshape(y_train, (-1,))\ny_test_reshaped = jnp.reshape(y_test, (-1,))\nI tested your code with 300 epochs and lr=0.01 and I got an accuracy of 90% in test (and the loss decreased to 0.0001)"
        ],
        "link": "https://stackoverflow.com/questions/76703681/pytorch-and-jax-networks-give-different-accuracy-with-same-settings"
    },
    {
        "title": "How to apply constraints to optimisation in jax.scipy.optimize.minimize",
        "question": "I am trying to minimize the function so that the value of x that minimizes it remains between 0 and 1. One possible walk around is using sigmoid transformation. Is there any other solution associated with jax in such cases? Here is a minimal example:\nimport jax\nfrom jax.scipy.optimize import minimize\nimport jax.numpy as jnp\nrng = jax.random.PRNGKey(0)\n\ndef gen_num(shape):\n  return jax.random.uniform(rng,shape)\n\nshape = (10,) \nmu = gen_num(shape)*2+3\nlog_sigma = gen_num(shape)*2\nc = gen_num(shape)\nx1 = gen_num((10,10))*4 +5\n\ndef kl_loss(x1, mu, log_sigma, c):\n    return -(log_sigma* c[:, jnp.newaxis]).mean() + jnp.log(jnp.std(x1))\n\nkl_value = lambda x: kl_loss(x1, mu, log_sigma, x)\nres = minimize(kl_value, c, method='BFGS', tol=1e-5)\nans = res.x",
        "answers": [
            "jax.scipy.optimize.minimize is quite limited. The JAX team recommends jaxopt instead; it has information on constrained optimization here.\nIf you can edit your question here to expand your example snippet into a full minimal reproducible example, I could add an example of how to adapt your code for use with jaxopt.",
            "The only supported method in Jax is \"BFGS\". As you might see it doens't support hard constraint. However passing the constraint in the objective function can do the trick. For instance you can artificially increase the gradient outside [0, 1]:\ndef objective_function(x: jnp.array, mu: float, log_sigma: float, x1: jnp.array):\n  \"\"\"Objective function.\"\"\"\n  kl_value = kl_loss(x1, mu, log_sigma, x)\n  constraint = 10 * (1 - jnp.where(0 <= x <= 1)) * (x - 0.5)**2\n  return k1_value + constraint\n\nres = minimize(kl_value, c, args=(mu, log_sigma, x1), method='BFGS', tol=1e-5, )\nIn this case the gradient outside the domain is proportional to 10*(x-0.5) which may be sufficient to maintain the value in the domain while having no impact inside the domain.\nFinally you can try to fit the factor \"10\" to have a stable training and an efficient constraint.\nThis trick worked for me when I tried to implement a SVM in jax with jax.scipy.optimize.minimize."
        ],
        "link": "https://stackoverflow.com/questions/76695661/how-to-apply-constraints-to-optimisation-in-jax-scipy-optimize-minimize"
    },
    {
        "title": "Jaxlib 0.4.7 can't be installed or built in my OSX 10.13, Python 3.9.13",
        "question": "I've been trying to upgrade my jaxlib but it is impossible. Jax is fine. None of the wheels in the google repository work for me. I suppose it is because I'm using OSX 10.13?\nRuntimeError: jaxlib is version 0.3.10, but this version of jax requires version >= 0.4.7.\nWhen I attempt to install Jaxlib through pip, it throws this:\n$ pip install --upgrade jaxlib==0.4.7 -f https://storage.googleapis.com/jax-releases/jax_releases.html\nLooking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\nERROR: Could not find a version that satisfies the requirement jaxlib==0.4.7 (from versions: 0.1.60, 0.1.63, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10)\nERROR: No matching distribution found for jaxlib==0.4.7\nI am using the latest version of pip, but also tried downgrading pip. The problem still occurs. Please help me solve this issue.\nI tried installing through pip, building my own jaxlib as the site's instructions (https://jax.readthedocs.io/en/latest/developer.html) and installing through the google repositories. I also downgraded pip, with no luck.\nMy objective is to have Jaxlib 0.4.7 running in my computer.",
        "answers": [
            "Recent jaxlib releases require OSX version 10.14 or newer (see https://github.com/google/jax/blob/main/CHANGELOG.md#jaxlib-0314-june-27-2022).\nYour options are either to update your OSX to a more recent version, to build jaxlib yourself (this can be tricky; see building from source), or to use an older jaxlib release where your OS version is supported."
        ],
        "link": "https://stackoverflow.com/questions/76665537/jaxlib-0-4-7-cant-be-installed-or-built-in-my-osx-10-13-python-3-9-13"
    },
    {
        "title": "How to make a function a valid jax type?",
        "question": "When I pass an object created using the following function function into a jax.lax.scan function:\ndef logdensity_create(model, centeredness = None, varname = None):\n    if centeredness is not None:\n        model = reparam(model, config={varname: LocScaleReparam(centered= centeredness)})\n          \n    init_params, potential_fn_gen, *_ = initialize_model(jax.random.PRNGKey(0),model,dynamic_args=True)\n    logdensity = lambda position: -potential_fn_gen()(position)\n    initial_position = init_params.z\n    return (logdensity, initial_position)\nI get the following error (on passing the logdensity to an iterative function created using jax.lax.scan):\nTypeError: Value .logdensity_create.. at 0x13fca7d80> with type  is not a valid JAX type\nHow can I resolve this error?",
        "answers": [
            "I would probably do this via jax.tree_util.Partial, which wraps callables in a PyTree for compatibility with jit and other transformations:\nlogdensity = jax.tree_util.Partial(lambda position: -potential_fn_gen()(position))"
        ],
        "link": "https://stackoverflow.com/questions/76655153/how-to-make-a-function-a-valid-jax-type"
    },
    {
        "title": "How to slice jax arrays using jax tracer?",
        "question": "I am trying to modify a code base to create a subarray using an existing array and indices in the form of Jax tracer. When I try to pass these Jax tracers directly for indices. I get the following error:\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\nWhat is a possible workaround/ solution for this?",
        "answers": [
            "There are two main workarounds here that may be applicable depending on your problem: using static indices, or using dynamic_slice.\nQuick background: one constraint of arrays used in JAX transformations like jit, vmap, etc. is that they must be statically shaped (see JAX Sharp Bits: Dynamic Shapes for some discussion of this).\nWith that in mind, a function like f below will always fail, because i and j are non-static variables and so the shape of the returned array cannot be known at compile time:\n@jit\ndef f(x, i, j):\n  return x[i:j]\nOne workaround for this is to make i and j static arguments in jit, so that the shape of the returned array will be static:\n@partial(jit, static_argnames=['i', 'j'])\ndef f(x, i, j):\n  return x[i:j]\nThat's the only possible workaround to use jit in such a situation, because of the static shape constraint.\nAnother flavor of slicing problem that can lead to the same error might look like this:\n@jit\ndef f(x, i):\n  return x[i:i + 5]\nThis will also result in a non-static index error. It could be fixed as above by marking i as static, but there is more information here: assuming that 0 <= i < len(x) - 5 holds, we know that the shape of the output array is (5,). This is a case where jax.lax.dynamic_slice is applicable (when you have a fixed slice size at a dynamic location):\n@jit\ndef f(x, i):\n  return jax.lax.dynamic_slice(x, (i,), (5,))\nNote that this will have different semantics than x[i:i + 5] in cases where the slice overruns the bounds of the array, but in most cases of interest it is equivalent.\nThere are other examples where neither of these two workarounds are applicable, for example when your program logic is predicated on creating dynamic-length arrays. In these cases, there is no easy work-around, and your best bet is to either (1) re-write your algorithm in terms of static array shapes, perhaps using padded array representations, or (2) not use JAX."
        ],
        "link": "https://stackoverflow.com/questions/76626143/how-to-slice-jax-arrays-using-jax-tracer"
    },
    {
        "title": "Selecting all elements of subsets if at least one element is selected (JAX)",
        "question": "I have the following problem, for which I cannot manage to write a solution in JAX that is jittable and efficient.\nI have a set of elements. Some of these elements are included (on the basis of a condition, which is not important now). The included elements are denoted by 1, the not-included elements by 0. For example, the array arr = jnp.array([1, 0, 0, 0, 0, 0]) indicates that I have 6 elements, the first of them included based on my condition.\nThese elements are grouped into subsets. I have a second array that indicates where each subset starts in the first array arr. For example, the array subsets = jnp.array([0, 2]) indicates that the first subset starts at position 0 and the second subset starts at position 2.\nNow, if one element is included based on arr, I would like to include all the elements in the same subset. In this example, the output should then be [1, 1, 0, 0, 0, 0].\nI have tried with a jax.lax.fori_loop, but it is slow.\n@jax.jit                                                                        \ndef select_subsets(arr, subsets):                 \n    new_arr = arr.copy()            \n    n_resid = subsets.shape[0]    \n    indices = jnp.arange(arr.shape[0])       \n                                                                                    \n    def func(i, new_arr):    \n        start = subsets[i]    \n        stop = subsets[i+1]    \n        arr_sliced = jnp.where((indices >= start) & (indices < stop), arr, 0.0)    \n        sum_ = jnp.sum(arr_sliced)    \n        new_arr = jnp.where(sum_ > 0.5, jnp.where((indices >= start) & (indices < stop), 1, new_arr), new_arr)    \n        return new_arr                          \n                                                 \n    new_arr = jax.lax.fori_loop(0, n_resid-1, func, new_arr)    \n    return new_arr\nthis function works if I use a subsets with the last element equal to the number of elements in arr, subsets = jnp.array([0, 2, 6]).\nI then thought about writing a vectorized version (using jax.numpy operations), but I cannot manage to do it.\nIs there a JAX guru that can help me with this?\nThanks a lot!",
        "answers": [
            "Here's a vectorized version. It instantiates a mask with shape len(subsets) x len(arr), which might be undesirable depending on how big those values are.\n           \n@jax.jit                                                                             \ndef vectorized_select_subsets(arr, subsets):                         \n    l, = arr.shape               \n                               \n    indices = jnp.arange(l)[None, :]\n                \n    # Broadcast to mask of shape (n_subsets, input_length)\n    subset_masks = (\n        (indices >= subsets[:-1, None])\n        & (indices < subsets[1:, None])        \n    )                                                           \n                                                    \n    # Shape (n_subsets,) array indicating whether each subset is included\n    include_subset = jnp.any(subset_masks & arr[None, :], axis=1)          \n                                                          \n    # Reduce down columns \n    result = jnp.any(subset_masks & include_subset[:, None], axis=0).astype(jnp.int32)   \n    return result\nI timed this against the loop-based version on an array with length 512 and 32 subsets:\nLoop: 6254.647 it/s\nVectorized: 37940.335 it/s"
        ],
        "link": "https://stackoverflow.com/questions/76617821/selecting-all-elements-of-subsets-if-at-least-one-element-is-selected-jax"
    },
    {
        "title": "What are the tradeoffs between jax.lax.map and jax.vmap?",
        "question": "This Github issue hints that there are tradeoffs in performance / memory / compilation time when choosing between jax.lax.map and jax.vmap. What are the specific details of these tradeoffs with respect to both GPUs and CPUs?",
        "answers": [
            "The main difference is that jax.vmap is a vectorizing transformation, while lax.map is an iterative transformation. Let's look at an example.\nExample function: vector_dot\nSuppose you have implemented a simple function that takes 1D vectors as inputs. For simplicity let's make it a simple dot product, but one that asserts the inputs are one-dimensional:\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef vector_dot(x, y):\n  assert x.ndim == y.ndim == 1, \"vector inputs required\"\n  return jnp.dot(x, y)\nWe can create some random 1D vectors to test this:\nrng = np.random.default_rng(8675309)\nx = rng.uniform(size=50)\ny = rng.uniform(size=50)\n\nprint(vector_dot(x, y))\n# 14.919376\nTo see what JAX is doing with this function under the hood, we can print the jaxpr, which is JAX's intermediate-level representation of a function:\nprint(jax.make_jaxpr(vector_dot)(x, y))\n# { lambda ; a:f32[50] b:f32[50]. let\n#     c:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] a b\n#   in (c,) }\nThis shows that JAX lowers this code to a single call to dot_general, the primitive for generalized dot products in JAX and XLA.\nIterating over vector_dot\nNow, suppose you have a 2D input, and you'd like to apply this function to each row. There are several ways you could imagine doing this: three examples are using a Python for loop, using jax.vmap, or using jax.lax.map:\ndef batched_dot_for_loop(x_batched, y):\n  return jnp.array([vector_dot(x, y) for x in x_batched])\n\ndef batched_dot_lax_map(x_batched, y):\n  return jax.lax.map(lambda x: vector_dot(x, y), x_batched)\n\nbatched_dot_vmap = jax.vmap(vector_dot, in_axes=(0, None))\nApplying these three functions to a batched input yields the same results, to within floating point precision:\nx_batched = rng.uniform(size=(4, 50))\n\nprint(batched_dot_for_loop(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_lax_map(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_vmap(x_batched, y))\n# [11.964927  12.485697  13.683528  12.9286175]\nBut if we look at the jaxpr for each, we can see that the three approaches lead to very different computational characteristics.\nThe for loop solution looks like this:\nprint(jax.make_jaxpr(batched_dot_for_loop)(x_batched, y))\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[1,50] = slice[\n      limit_indices=(1, 50)\n      start_indices=(0, 0)\n      strides=(1, 1)\n    ] a\n    d:f32[50] = squeeze[dimensions=(0,)] c\n    e:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d b\n    f:f32[1,50] = slice[\n      limit_indices=(2, 50)\n      start_indices=(1, 0)\n      strides=(1, 1)\n    ] a\n    g:f32[50] = squeeze[dimensions=(0,)] f\n    h:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] g b\n    i:f32[1,50] = slice[\n      limit_indices=(3, 50)\n      start_indices=(2, 0)\n      strides=(1, 1)\n    ] a\n    j:f32[50] = squeeze[dimensions=(0,)] i\n    k:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] j b\n    l:f32[1,50] = slice[\n      limit_indices=(4, 50)\n      start_indices=(3, 0)\n      strides=(1, 1)\n    ] a\n    m:f32[50] = squeeze[dimensions=(0,)] l\n    n:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] m b\n    o:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e\n    p:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] h\n    q:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] k\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] n\n    s:f32[4] = concatenate[dimension=0] o p q r\n  in (s,) }\nThe key feature is that the iterations in the for loop are unrolled into a single long program.\nThe lax.map version looks like this:\nprint(jax.make_jaxpr(batched_dot_lax_map)(x_batched, y))\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = scan[\n      jaxpr={ lambda ; d:f32[50] e:f32[50]. let\n          f:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] e d\n        in (f,) }\n      length=4\n      linear=(False, False)\n      num_carry=0\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] b a\n  in (c,) }\nThe key feature is that it is loaded into a scan primitive, which is XLA's native static loop operation.\nThe vmap version looks like this:\nprint(jax.make_jaxpr(batched_dot_vmap)(x_batched, y))\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n  in (c,) }\nThe key feature here is that the vmap transformation is able to recognize that a batched 1D dot product is equivalent to a 2D dot product, so the result is a single extremely efficient native operation.\nPerformance considerations\nThese three approaches can have very different performance characteristics. The details will depend on the specifics of the original function (here vector_dot) but in broad strokes, we can consider three aspects:\nCompilation Cost\nIf you JIT-compile your program, you'll find:\nThe for-loop based solution will have compilation times that grow super-linearly with the number of iterations. This is due to the unrolling seen in the jaxpr above.\nThe lax.map and jax.vmap solutions will have fast compilation time, which under normal circumstances will not grow with the size of the batch dimension.\nRuntime\nIn terms of runtime:\nThe for loop solution can be very fast, because XLA can often fuse operations between the unrolled iterations. This is the flip side of the long compilation times.\nThe lax.map solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.\nThe jax.vmap solution will generally be the fastest, especially on accelerators like GPU or TPU, because it can make use of native batching parallelism on the device.\nMemory Cost\nThe for loop and lax.map solutions generally have good memory performance, because they execute sequentially and don't require storage of large intermediate results.\nThe main downside of the jax.vmap solution is that it can cause memory to blow up because the entire problem must fit into memory at once. This is not an issue with the simple vector_dot function used here, but can be for more complicated functions.\nBenchmarks\nYou can see these general principles at play when benchmarking the above functions. The following timings are on a Colab T4 GPU:\ny = rng.uniform(size=1000)\nx_batched = rng.uniform(size=(200, 1000))\n%time jax.jit(batched_dot_for_loop).lower(x_batched, y).compile()\n# CPU times: user 4.96 s, sys: 55 ms, total: 5.01 s\n# Wall time: 7.24 s\n%timeit jax.jit(batched_dot_for_loop)(x_batched, y).block_until_ready()\n# 1.09 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n%time jax.jit(batched_dot_lax_map).lower(x_batched, y).compile()\n# CPU times: user 117 ms, sys: 2.71 ms, total: 120 ms\n# Wall time: 172 ms\n%timeit jax.jit(batched_dot_lax_map)(x_batched, y).block_until_ready()\n# 2.67 ms ± 56.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%time jax.jit(batched_dot_vmap).lower(x_batched, y).compile()\n# CPU times: user 51 ms, sys: 941 µs, total: 52 ms\n# Wall time: 103 ms\n%timeit jax.jit(batched_dot_vmap)(x_batched, y).block_until_ready()\n# 719 µs ± 129 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
        ],
        "link": "https://stackoverflow.com/questions/76615802/what-are-the-tradeoffs-between-jax-lax-map-and-jax-vmap"
    },
    {
        "title": "Creating a jax array using existing jax arrays of different lengths throws error",
        "question": "I am using the following code to set a particular row of a jax 2D array to a particular value using jax arrays:\nzeros_array = jnp.zeros((3, 8))\nvalue = jnp.array([1,2,3,4])\nvalue_2 = jnp.array([1])\nvalue_3 = jnp.array([1,2])\nvalues = jnp.array([value,value_2,value_3])\nzeros_array = zeros_array.at[0].set(values)\nBut, I am receiving the following error:\nValueError: All input arrays must have the same shape.\nUpon modifying the jnp to np (numpy) the error disappears. Is there any way to resolve this error? I know one walk around this would be to set each of the separate arrays in the 2D array using at[0,1].set(), at[0,2:n].set().",
        "answers": [
            "What you have in mind is a \"ragged array\", and no, there is not currently any way to do this in JAX. In older versions of NumPy, this will work by returning an array of dtype object, but in newer versions of NumPy this results in an error because object arrays are generally inconvenient and inefficient to work with (for example, there's no way to efficiently do the equivalent of the index update operation in your last line if the updates are stored in an object array).\nDepending on your use-case, there are several workarounds for this you might use in both JAX and NumPy, including storing the rows of your array as a list, or using a padded 2D array representation.\nI'll note also that the JAX team is exploring native support for ragged arrays (see e.g. https://github.com/google/jax/pull/16541) but it's still fairly far from being generally useful."
        ],
        "link": "https://stackoverflow.com/questions/76600803/creating-a-jax-array-using-existing-jax-arrays-of-different-lengths-throws-error"
    },
    {
        "title": "How to create a jax array and store dictionaries inside it later?",
        "question": "I wish to create a Jax-based array and use this array later to store dictionaries inside it. Is it possible to do so?",
        "answers": [
            "JAX arrays cannot store dictionaries as items. They can only store items of simple numerical types, including:\nintegers: int8, int16, int32, int64\nunsigned integers: uint8, uint16, uint32, uint64\nfloating point: bfloat16, float16, float32, float64\ncomplex; complex64, complex128\nAdditionally, several experimental narrow-width float and integer types from the ml_dtypes library have support on some hardware.\nDepending on your use-case, you may be able to use a struct-of-arrays pattern rather than an array-of-structs pattern, but it's hard to say whether this is applicable without more information about what you're trying to do."
        ],
        "link": "https://stackoverflow.com/questions/76594409/how-to-create-a-jax-array-and-store-dictionaries-inside-it-later"
    },
    {
        "title": "How to vectorize cho_solve?",
        "question": "This question solved my problem of using vmap on cho_solve, is it possible to vectorize cho_solve, or does the definition of cho_solve preclude it from being vectorized? vectorize seems to need the arguments to all be arrays, whereas cho_solve takes a tuple as the first argument?\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\n\nk_y = jax.random.normal(subkey, (3, 5, 10, 10))\ny = jnp.broadcast_to(jnp.eye(10), k_y.shape)\n\nmatmul = jnp.vectorize(jnp.matmul, signature='(a,b),(b,c)->(a,c)')\ncholesky = jnp.vectorize(jsp.linalg.cholesky, excluded={1}, signature='(d,d)->(d,d)')\ncho_solve = jnp.vectorize(jsp.linalg.cho_solve, signature='(d,d),(d,d)->(d,d)')  # what to put here?\n\nk_y = matmul(k_y, jnp.moveaxis(k_y, -1, -2))\nchol = cholesky(k_y, True)\nresult = cho_solve((chol, True), y)\nValueError: All input arrays must have the same shape.\nMy use case is that I have an unspecified amount of \"batch\" dimensions that I want to vmap over, and vectorize handles the auto broadcasting beautifully. I can once again write my own cho_solve using solve_triangular but this seems like a waste. Is it possible for vectorize to have a similar interface to vmap, which can take nested signatures?",
        "answers": [
            "I don't believe you can use vectorize directly with cho_solve. The vectorize API requires that the function take arrays as an argument, while cho_solve takes a tuple as the first argument. The only way you could use vectorize with this function is to wrap it in one with a different API. For example:\ncho_solve = jnp.vectorize(\n    lambda chol, flag, y: jsp.linalg.cho_solve((chol, flag), y),\n    excluded={1}, signature='(d,d),(d,d)->(d,d)')\nresult = cho_solve(chol, True, y)",
            "Here's the solve_triangular solution I managed:\nsolve_tri = jnp.vectorize(\n  jsp.linalg.solve_triangular, excluded={2, 3}, signature='(d,d),(d,d)->(d,d)')\nchol = cholesky(k_y, True)\nresult2 = solve_tri(chol, solve_tri(chol, y, 0, True), 1, True)\nresult3 = jnp.array([\n  jsp.linalg.cho_solve((chol[a, b], True), y[a, b])\n  for a in range(k_y.shape[0])\n  for b in range(k_y.shape[1])\n]).reshape(k_y.shape)\nprint(jnp.allclose(result2, result3))\nTrue"
        ],
        "link": "https://stackoverflow.com/questions/76588276/how-to-vectorize-cho-solve"
    },
    {
        "title": "Passing the returned stacked output to jax.lax.scan",
        "question": "I wish to pass on the returned stacked values from the jax.lax.scan back to one of its arguments. Is it possible to do so? For example:\nfrom jax import lax\n\n\ndef cumsum(res, el):\n    \"\"\"\n    - `res`: The result from the previous loop.\n    - `el`: The current array element.\n    \"\"\"\n    v, u = res\n    print(u)\n    v = v + el\n    return (v,u),v  # (\"carryover\", \"accumulated\")\n\n\nresult_init = 0\nresult = []\n\n(final,use), result = lax.scan(cumsum, (result_init,result), a)\nIn the above code, I want to extract the cumulated res values during the runtime and pass it back. Thus, I have passed the result as an argument in the lax function, but it always prints an empty list.",
        "answers": [
            "There's no built-in way to access the \"current state\" of the accumulated values in the course of a scan operation: in particular, the current state will be a dynamically-shaped array (it will have size 0 in the first iteration, size 1 in the second, etc.) and scan, like other JAX transformations and higher-order functions, requires static shapes.\nBut you could do something similar by passing along an array that you manually update. It might look something like this:\nfrom jax import lax\nimport jax.numpy as jnp\nfrom jax import debug\n\ndef cumsum(carry, el):\n    i, v, running_result = carry\n    v = v + el\n    running_result = running_result.at[i].set(v)\n    debug.print(\"iteration {}: running_result={}\", i, running_result)\n    return (i + 1, v, running_result), v\n\na = jnp.arange(5)\nrunning_result = jnp.zeros_like(a)\n(i, v, running_result), result = lax.scan(cumsum, (0, 0, running_result), a)\n\nprint(\"\\nfinal running result:\", running_result)\nprint(\"final result:\", result)\niteration 0: running_result=[0 0 0 0 0]\niteration 1: running_result=[0 1 0 0 0]\niteration 2: running_result=[0 1 3 0 0]\niteration 3: running_result=[0 1 3 6 0]\niteration 4: running_result=[ 0  1  3  6 10]\n\nfinal running result: [ 0  1  3  6 10]\nfinal result: [ 0  1  3  6 10]\nNotice that I used jax.debug.print to print the intermediate results, because this function is traced and compiled."
        ],
        "link": "https://stackoverflow.com/questions/76587199/passing-the-returned-stacked-output-to-jax-lax-scan"
    },
    {
        "title": "Error using JAX, Array slice indices must have static start/stop/step",
        "question": "I'll be happy to help you with your code. If I understand correctly, you want to create a 2D Gaussian patch for each value in the darkField array. The size of the patch should ideally be calculated as 2 * np.ceil(3 * sigma) + 1, where sigma is the corresponding value from the darkField array. You have fixed the size value to 10 in your example to avoid errors.\nOnce the Gaussian patch is normalized to 1, you want to multiply it by the corresponding value from the intensityRefracted2DF array to obtain the generated blur. Finally, you want to add this blur patch to the intensityRefracted3 array.\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom jax.scipy.signal import convolve2d\nfrom functools import partial\n\n@partial(jax.jit,static_argnums=(1,))\ndef gaussian_shape(sigma, size):\n    \"\"\"\n    Generate a Gaussian shape.\n\n    Args:\n        sigma (float or 2D numpy array): Standard deviation(s) of the Gaussian shape.\n        size (int): Size of the Gaussian shape.\n\n    Returns:\n        exponent (2D numpy array): Gaussian shape.\n    \"\"\"\n\n    x = jnp.arange(0, size) - jnp.floor(size / 2)\n    exponent = jnp.exp(-(x ** 2) / (2 * sigma ** 2))\n    exponent = jnp.outer(exponent, exponent)\n    exponent /= jnp.sum(exponent)\n    return exponent\n\n@partial(jax.jit)\ndef apply_dark_field(i, j, intensityRefracted2DF, intensityRefracted3, darkField):\n    currDF_ij=darkField[i,j]\n    patch = gaussian_shape(currDF_ij,10)\n    size2 = patch.shape[0] // 2\n    patch = patch * intensityRefracted2DF[i, j]\n\n\n    intensityRefracted3 = intensityRefracted3.at[i - size2:i + size2 + 1, j - size2:j + size2 + 1].add(patch * intensityRefracted2DF[i, j])\n    # intensityRefracted3 = jax.ops.index_add(intensityRefracted3, (i, j), intensityRefracted2DF[i, j] * (darkField[i, j] == 0))\n    return intensityRefracted3\n\n@jax.jit\ndef darkFieldLoop(intensityRefracted2DF, intensityRefracted3, darkField):\n    currDF = jnp.zeros_like(intensityRefracted3)\n    currDF = jnp.where(intensityRefracted2DF!=0,darkField,0)\n\n    i = jnp.nonzero(currDF,size=currDF.shape[0])\n    indices_i=i[0]\n    indices_j=i[1]\n    intensityRefracted3 = jnp.zeros_like(intensityRefracted3)\n\n    intensityRefracted3 = jax.vmap(apply_dark_field, in_axes=(0, 0, None, None, None))(indices_i, indices_j, intensityRefracted2DF, intensityRefracted3, darkField)\n\n    return intensityRefracted3\n\nintensityRefracted2DF = np.random.rand(10,10)\nintensityRefracted3 = np.zeros((10, 10))\ndarkField = np.random.rand(10, 10)\n\na=darkFieldLoop(intensityRefracted2DF,intensityRefracted3,darkField)\n\nfor i in range(a.shape[0]):\n    plt.imshow(a[i])\n    plt.show()\nAnd there is the error message :\nIndexError: Array slice indices must have static start/stop/step to   be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=3/0)>, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\nI've also try to put i,j into static_argnums using partial\n@partial(jax.jit, static_argnums=(0,1))\ndef apply_dark_field(i, j, intensityRefracted2DF, intensityRefracted3, darkField):\nand there is the error:\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 0) of type <class 'jax._src.interpreters.batching.BatchTracer'> for function apply_dark_field is non-hashable.",
        "answers": [
            "The issue comes from the fact that JAX arrays cannot have a dynamic shape, and so dynamic start & end indices cannot be used in indexing expressions.\nYour solution of marking i and j as static would work, except that you are vmapping across these values, so by definition they cannot be static.\nThe best solution here is probably to use lax.dynamic_slice and lax.dynamic_update_slice, which are operations designed exactly for the case that you have (where indices are dynamic, but slice sizes are static).\nYou can replace this line:\nintensityRefracted3 = intensityRefracted3.at[i - size2:i + size2 + 1, j - size2:j + size2 + 1].add(patch * intensityRefracted2DF[i, j])\nwith this:\nstart_indices = (i - size2, j - size2)\nupdate = jax.lax.dynamic_slice(intensityRefracted3, start_indices, patch.shape)\nupdate += patch * intensityRefracted2DF[i, j]\nintensityRefracted3 = jax.lax.dynamic_update_slice(\n    intensityRefracted3, update,  start_indices)\nand it should work correctly with dynamic i and j. Though you should be careful, because if any of the specified indices are out-of-bounds, dynamic_slice and dynamic_update_slice will clip them into the valid range."
        ],
        "link": "https://stackoverflow.com/questions/76578461/error-using-jax-array-slice-indices-must-have-static-start-stop-step"
    },
    {
        "title": "is there a more efficient equivalent of np.sum(np.cumprod(1 / (1 + y*x)))?",
        "question": "I have a 1D NumPy array, such as x:\nx = np.array([0.05, 0.06, 0.06, 0.04])\nI'm showing a small array, but in reality, x can be very large. To x, I want to perform the following calculation:\ny = 1./12.\nnp.sum(np.cumprod(1 / (1 + y * x)))\nBecause x is very large, I want to do this more efficiently. I tried to use np.exp(np.cumsum(np.log(1 / (1 + y * x)))).sum() but this makes it slower. Is there a more efficient NumPy/JAX function?",
        "answers": [
            "You pretty much vectorized as much as possible.\nWhat you could spare here, if your were writing in C, is the numerous (but implicit, that is done thanks to vectorization inside numpy's code, that is in C) for loops involved. Since what your code means is that you\nFirstly iterate all elements of x to multiply them by y. In pure python that would be compound list [y*a for a in x]\nSecondly iterate again all elements of former result, to add 1 to all of them. So, [1+z for z in [y*a for a in x]]\nThirdly, iterate again all elements of the former result to invert all of them. So [1/u for u in [1+z for z in [y*a for a in x]]]\nFourthly, iterate again to compute cumulative product. So p=1; [p:=v*p for v in [1/u for u in [1+z for z in [y*a for a in x]]]]\nFifthly, iterate again to compute sum of former\nSo, sure, all those for loops are in C, so very fast. But there are many (non-nested) of them. And each of them doesn't do much. So time spend in the iteration itself the for(int i=0; i<arr_len; i++) that occurs somewhere in numpy's C code, is not that negligible before the content of that iteration (the result[i] = y*x[i] that is repeated by this loop in numpy's C code).\nIf you were writing this in pure python\ndef cumsumprod(x,y):\n    z=[y*a for a in x]\n    u=[1+a for a in z]\n    v=[1/a for a in u]\n    p=1; w=[p:=p*a for a in v]\n    s=0\n        for a in w: s+=a\n    return s\nThat would be way less efficient than this other pure python implementation\ndef ff(x,y):\n    s=0\n    p=1\n    for a in x:\n        p/=(1+y*a)\n        s+=p\n    return s\nSame computation. But one for loop instead of 5.\nTo be quantitative in what I say, on your example, in microseconds, your code takes 9.8 μs on my machine. My 1st python code 3.6 μs. And my 2nd, 1.9 μs.\nAnd yes, with that small data, pure python codes are both faster than numpy. If array is size 1000 instead, those timings become 17.7, 464 and 288. But point is, my second code is faster than my first, unsurprisingly. And your numpy code is the equivalent of my first code, but in C.\nAnd that is even an understatement, since I just use the example of for loops. It is not the only redundant thing that numpy does. For example, it also allocates a new array for each intermediary operation.\nNot that you did anything wrong. You did exactly what you are supposed to do with numpy. Just that is what numpy does: if provides many vectorized operation that we can sequence, each of them being a for loop on all our data. And we pay the price of having several unnecessary for loops, in exchange of the reward of having them in C, when pure python would be way slower. That is pretty much the best you can have from numpy.\nIf you want to have more, a way is numba. Numba allows you to write, otherwise naive, code, and yet have it fast, in C.\nJust add @jit before my previous pure python's code\nfrom numba import jit\n\n@jit(nopython=True)\ndef ff(x,y):\n    s=0\n    p=1\n    for a in x:\n        p/=(1+y*a)\n        s+=p\n    return s\nAnd you get something that is both in C, and doesn't contain the unnecessary bu yet unavoidable operations that sequencing many numpy's vectorized operation does.\nTimings for this function is 0.25 μs for your list. So way better than 1.9 μs of the same in pure python, thanks to compilation.\nAnd for a size 1000 list, where numpy's beat pure python, timing is 3.4 μs. So not only way better than python 276 μs, but also better than numpy's 14.9 μs, thanks to the simplicity of the algorithm.\nSo long story short: numba allows to write plain, simple, naive algorithm on numpy array, that are compiled."
        ],
        "link": "https://stackoverflow.com/questions/76563019/is-there-a-more-efficient-equivalent-of-np-sumnp-cumprod1-1-yx"
    },
    {
        "title": "Oryx: How to `inverse` a JAX function with additional parameters",
        "question": "I would like to use the package Oryx to invert an affine transformation written in JAX. The transformation maps x->y and depends on a set of adjustable parameters (which I call params). Specifically, the affine transformation is defined as:\nimport jax.numpy as jnp\n\ndef affine(params, x):\n  return x * params['scale'] + params['shift']\n\nparams = dict(scale=1.5, shift=-1.)\nx_in = jnp.array(3.)\ny_out = affine(params, x_in)\nI would like to invert affine wrt to input x as a function of params. Oryx has a function oryx.core.inverse to invert JAX functions. However, inverting a function with parameters, like this:\nimport oryx\n\noryx.core.inverse(affine)(params, y_out)\ndoesn't work (AssertionError: length mismatch: [1, 3]), presumably because inverse doesn't know that I want to invert y_out but not params. What is the most elegant way to solve this problem for all possible values (i.e., as a function) of params using oryx.core.inverse? I find the inverse docs not very illuminating.\nUpdate: Jakevdp gave an excellent suggestion for a given set of params. I've clarified the question to indicate that I am wondering how to define the inverse as a function of params.",
        "answers": [
            "You can do this by closing over the static parameters, for example using partial:\nfrom functools import partial\nx = oryx.core.inverse(partial(affine, params))(y_out)\n\nprint(x)\n# 3.0\nEdit: if you want a single inverted function to work for multiple values of params, you will have to return params in the output (otherwise, there's no way from a single output value to infer all three inputs). It might look something like this:\ndef affine(params, x):\n  return params, x * params['scale'] + params['shift']\n\nparams = dict(scale=1.5, shift=-1.)\nx_in = jnp.array(3.)\n_, y_out = affine(params, x_in)\n\n_, x = oryx.core.inverse(affine)(params, y_out)\nprint(x)\n# 3.0"
        ],
        "link": "https://stackoverflow.com/questions/76538976/oryx-how-to-inverse-a-jax-function-with-additional-parameters"
    },
    {
        "title": "JAX vmap vs pmap vs Python multiprocessing",
        "question": "I am rewriting some code from pure Python to JAX. I have gotten to the point where in my old code, I was using Python's multiprocessing module to parallelize the evaluation of a function over all of the CPU cores in a single node as follows:\n# start pool process \npool = multiprocessing.Pool(processes=10) # if node has 10 CPU cores, start 10 processes\n\n# use pool.map to evaluate function(input) for each input in parallel\n# suppose len(inputs) is very large and 10 inputs are processed in parallel at a time\n# store the results in a list called out\nout = pool.map(function,inputs)\n\n# close pool processes to free memory\npool.close()\npool.join()\nI know that JAX has vmap and pmap, but I don't understand if either of those are a drop-in replacement for how I'm using multiprocessing.pool.map above.\nIs vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).",
        "answers": [
            "Is vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nNo, vmap has nothing to do with parallelization. It is a vectorizing transformation, not a parallelizing transformation. In the course of normal operation, JAX may use multiple cores via XLA, so vmapped operations may also do this. But there's no explicit parallelization in vmap.\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\npmap parallelizes over multiple XLA devices. vmap does not parallelize, but rather vectorizes on a single device. multiprocessing parallelizes over multiple Python processes.\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nYes, it could be described as SPMD across multiple python processes.\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\npmap parallelizes over multiple XLA devices, and you have configured only a single XLA device, so the requested operation is not possible.\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).\nYes, I believe that pmap can be used to compute on multiple CPU cores. Whether it's nested with vmap is irrelevant. See JAX pmap with multi-core CPU.\nNote also that jax.pmap is deprecated in favor of the newer jax.shard_map, which is a much more flexible transformation for multi-device/multi-host computation. There's some info here: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html and https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html"
        ],
        "link": "https://stackoverflow.com/questions/76536601/jax-vmap-vs-pmap-vs-python-multiprocessing"
    },
    {
        "title": "How can I test if a jitted Jax function creates new tensor or a view?",
        "question": "I have a basic code like this:\n@jit\ndef concat_permute(indices, in1, in2):\n    tensor = jnp.concatenate([jnp.atleast_1d(in1), jnp.atleast_1d(in2)])\n    return tensor[indices]\nHere is my test tensors:\nkey = jax.random.PRNGKey(758493)\nin1 = tens = jax.random.uniform(key, shape=(15,5,3))\nin2 = tens = jax.random.uniform(key, shape=(10,5,3))\nindices = jax.random.choice(key, 25, (25,), replace=False)\nAnd here is the Jaxpr of the function:\n{ lambda ; a:i32[25] b:f32[15,5,3] c:f32[10,5,3]. let\n    d:f32[25,5,3] = xla_call[\n      call_jaxpr={ lambda ; e:i32[25] f:f32[15,5,3] g:f32[10,5,3]. let\n          h:f32[15,5,3] = xla_call[\n            call_jaxpr={ lambda ; i:f32[15,5,3]. let  in (i,) }\n            name=atleast_1d\n          ] f\n          j:f32[10,5,3] = xla_call[\n            call_jaxpr={ lambda ; k:f32[10,5,3]. let  in (k,) }\n            name=atleast_1d\n          ] g\n          l:f32[25,5,3] = concatenate[dimension=0] h j\n          m:bool[25] = lt e 0\n          n:i32[25] = add e 25\n          o:i32[25] = select_n m e n\n          p:i32[25,1] = broadcast_in_dim[\n            broadcast_dimensions=(0,)\n            shape=(25, 1)\n          ] o\n          q:f32[25,5,3] = gather[\n            dimension_numbers=GatherDimensionNumbers(offset_dims=(1, 2), collapsed_slice_dims=(0,), start_index_map=(0,))\n            fill_value=None\n            indices_are_sorted=False\n            mode=GatherScatterMode.PROMISE_IN_BOUNDS\n            slice_sizes=(1, 5, 3)\n            unique_indices=False\n          ] l p\n        in (q,) }\n      name=concat_permute\n    ] a b c\n  in (d,) }\nIt seems it creates a new tensor using my permutation array but I'm not sure. Is there a more clear way to see if this opeeration is made by creating new tensor or not?\nI tried \"jax.make_jaxpr\" and see the results but not sure about the problem.",
        "answers": [
            "The short answer is, no the output of your function will not share memory with the array allocated for tensor.\nIn XLA, an array is represented by a uniformly-strided buffer, and when you select random values from an array, the result cannot in general be constructed via uniform-striding over a view of the input buffer."
        ],
        "link": "https://stackoverflow.com/questions/76479539/how-can-i-test-if-a-jitted-jax-function-creates-new-tensor-or-a-view"
    },
    {
        "title": "JAX's slow performance on simple loops",
        "question": "I'm learning JAX and trying to do a simple test on the performance of JAX. The run time of the following code using JAX is weirdly slower than numpy or even simple addition of list members using python lists. I mean the following code has no purpose other than testing the speed.\nI'm wondering what might be the reason for this? I see that there is a functionality of JAX called \"fori_loop\" that may help in this examples or I can vectorize my actual code, but I want to know why this simple loop is so slow and do I need to avoid writing code like this and completely understand things in the JAX way?\nHere is the code in JAX:\nimport jax.numpy as jnp\nfrom jax import random\nimport time\n\nkey = random.PRNGKey(0)\nx = random.uniform(key, shape=(100,3))\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\nwhich takes 11 seconds\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n11.254772663116455\nThe same code using Numpy:\nimport numpy as np\nimport time\n\nx = np.random.rand(100,3)\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\ntakes around a milli second:\n0.005955934524536133\nThank you!",
        "answers": [
            "If you're interested in comparing the speed of JAX vs. NumPy speed, the place to start is here: JAX FAQ: Is JAX Faster Than NumPy? Quoting from there:\nin summary: if you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead. If you’re running your code on GPU or TPU, or are benchmarking more complicated JIT-compiled sequences of operations on CPU, you can generally expect JAX to outperform NumPy.\nYour benchmark does many individual array operations, each of which is very inexpensive, so effectively all you're measuring is the single-operation dispatch time. This is precisely in the regime where we'd expect NumPy to be fast, and JAX to be slow.\nFor real use-cases involving repeated operations, there are several ways you might optimize the code (including wrapping the whole function in jit, using vmap for efficient batching, or using fori_loop for sequential operations) but for the example function you give it's hard to say what's best. Taking your example at face value, I'd optimize it this way:\ndef func(x):\n  return 0\nAs written, there's no need to do any operations on x at all (but I suspect that's not particularly helpful for your real use-case)."
        ],
        "link": "https://stackoverflow.com/questions/76474532/jaxs-slow-performance-on-simple-loops"
    },
    {
        "title": "Understanding how JAX's tracer vs static work",
        "question": "I'm new to JAX and trying to write a simple code using JAX where at some point I need to use a scipy method. Then I want to take its derivative.\nThe code doesn't run and gives me error. The following is the code and the error. I read a the documentation of JAX a couple of times but couldn't Figure out what to do to write the code correctly\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np\nimport scipy\nkey = random.PRNGKey(1)\n\nsize = 3\nx = random.uniform(key, (size, size), dtype=jnp.float32)\n\ndef error_func(x):\n    dists = scipy.spatial.distance.cdist(x, x, metric='euclidean')\n    error = jnp.sum(jnp.array(dists))\n    return error\n\nerror_diff = grad(error_func)\n\nprint(error_func(x))\nprint(error_diff(x))\nAnd I get the followig error:\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n3.2158318\nTraceback (most recent call last):\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 26, in <module>\n    print(error_diff(x))\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/api.py\", line 646, in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/api.py\", line 722, in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/api.py\", line 2179, in _vjp\n    out_primal, out_vjp = ad.vjp(\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/interpreters/ad.py\", line 139, in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/interpreters/ad.py\", line 128, in linearize\n    jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/interpreters/partial_eval.py\", line 777, in trace_to_jaxpr_nounits\n    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 15, in error_func\n    dists = scipy.spatial.distance.cdist(x, x, metric='euclidean')\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/scipy/spatial/distance.py\", line 2909, in cdist\n    XA = np.asarray(XA)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/core.py\", line 598, in __array__\n    raise TracerArrayConversionError(self)\njax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[0.7551559  0.3129729  0.12388372]\n [0.548188   0.8851279  0.30576992]\n [0.82008433 0.95633745 0.3566252 ]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[0.7551559 , 0.3129729 , 0.12388372],\n       [0.548188  , 0.8851279 , 0.30576992],\n       [0.82008433, 0.95633745, 0.3566252 ]], dtype=float32)\n  tangent = Traced<ShapedArray(float32[3,3])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[3,3]), None)\n    recipe = LambdaBinding()\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 26, in <module>\n    print(error_diff(x))\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 15, in error_func\n    dists = scipy.spatial.distance.cdist(x, x, metric='euclidean')\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/scipy/spatial/distance.py\", line 2909, in cdist\n    XA = np.asarray(XA)\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[0.7551559  0.3129729  0.12388372]\n [0.548188   0.8851279  0.30576992]\n [0.82008433 0.95633745 0.3566252 ]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[0.7551559 , 0.3129729 , 0.12388372],\n       [0.548188  , 0.8851279 , 0.30576992],\n       [0.82008433, 0.95633745, 0.3566252 ]], dtype=float32)\n  tangent = Traced<ShapedArray(float32[3,3])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[3,3]), None)\n    recipe = LambdaBinding()\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
        "answers": [
            "JAX transformations only work on JAX functions, not numpy or scipy functions (this is discussed briefly at the link shown in the error message above) If you want to use grad and other JAX transformations, you need to write your logic using JAX operations, not operations from numpy, scipy, or other non-JAX-compatible libraries.\nJAX does not currently include any wrappers of scipy.spatial.distance (though there are some in progress, see #16147), so the best option would be to write the code yourself. Fortunately, cdist is pretty straightforward:\ndef cdist(x, y, metric='euclidean'):\n  assert x.ndim == y.ndim == 2\n  if metric != 'euclidean':\n    raise NotImplementedError(f\"{metric=}\")\n  return jnp.sqrt(jnp.sum((x[:, None, :] - y[None, :, :]) ** 2, axis=-1))\n\ndef error_func(x):\n    dists = cdist(x, x, metric='euclidean')\n    error = jnp.sum(dists)\n    return error\n\nerror_diff = grad(error_func)\n\nprint(error_func(x))\n# 3.2158318\n\nprint(error_diff(x))\n# [[nan nan nan]\n#  [nan nan nan]\n#  [nan nan nan]]\nYou'll notice that the gradient is everywhere nan. This is the expected result due to the fact that grad(jnp.sqrt)(0.0) diverges (returns infinity), and 0.0 * inf by definition is nan."
        ],
        "link": "https://stackoverflow.com/questions/76460770/understanding-how-jaxs-tracer-vs-static-work"
    },
    {
        "title": "How to vmap over cho_solve and cho_factor?",
        "question": "The following error appears because of the last line of code below:\njax.errors.ConcretizationTypeError Abstract tracer value encountered where concrete value is expected...\nThe problem arose with the bool function.\nIt looks like it is due to the lower return value from cho_factor, which _cho_solve (note underscore) requires as static.\nI'm new to jax, so I was hoping that vmap-ing cho_factor into cho_solve would just work. What have I done wrong here?\nimport jax\n\nkey = jax.random.PRNGKey(0)\nk_y = jax.random.normal(key, (100, 10, 10))\ny = jax.random.normal(key, (100, 10, 1))\n\nmatmul = jax.vmap(jax.numpy.matmul)\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor)\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve)\n\nk_y = matmul(k_y, jax.numpy.transpose(k_y, (0, 2, 1)))\nchol, lower = cho_factor(k_y)\nresult = cho_solve((chol, lower), y)",
        "answers": [
            "The issue is that in each case, lower is a static scalar that should not be mapped over. So if you specify in_axes and out_axes so that lower is mapped over axis None, the vmap should work:\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor, out_axes=(0, None))\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve, in_axes=((0, None), 0))",
            "So I didn't manage to get cho_factor and cho_solve working, but worked around it using cholesky and solve_triangular:\n  cholesky = jax.vmap(jax.scipy.linalg.cholesky, in_axes=(0, None))\n  solve_tri = jax.vmap(jax.scipy.linalg.solve_triangular, in_axes=(0, 0, None, None))\n\n  L = cholesky(k_y, True)\n  result2 = solve_tri(L, solve_tri(L, y, 0, True), 1, True)"
        ],
        "link": "https://stackoverflow.com/questions/76458629/how-to-vmap-over-cho-solve-and-cho-factor"
    },
    {
        "title": "Use JAX to convert TIFF images to JPG?",
        "question": "I need to convert 3 million images of size 4 MB from TIFF to JPG. Could I use JAX?\nThe reason I ask is that I have an NVIDIA DGX Cloud environment with JAX set up. I'd like to do this conversion task in this environment because I'm hoping it could accelerate the process vs using a CPU-based approach. I am also new to JAX and DGX, so I wonder if this could be a learning opportunity.\nResearch Done\nI looked at the jax.image module and pix. These offer functions related to matrix manipulation (e.g. sizing, deforming, etc.).\nI also see from the documentation that\nJAX is a language for expressing and composing transformations of numerical programs. JAX is also able to compile numerical programs for CPU or accelerators (GPU/TPU). JAX works great for many numerical and scientific programs, but only if they are written with certain constraints....\nIt would therefore not surprise me that JAX does not (easily) support image type conversion. I wanted to ask the question in case there was something I'm missing.",
        "answers": [
            "No, it is not possible to perform JPEG compression in JAX."
        ],
        "link": "https://stackoverflow.com/questions/76446550/use-jax-to-convert-tiff-images-to-jpg"
    },
    {
        "title": "How to rewrite this JAX snippet to prevent TypeError: unhashable type: 'DynamicJaxprTracer'?",
        "question": "I am rewriting some Python code in JAX with the goal of speeding it up using jax.jit. There is a part of my old code where I have a dict that maps some integer to a list of functions. Every integer has a unique list of functions assigned to it. The functions are all JAX compatible, but the problem is my use case. I have a master function whose arguments are the number to be fed into these functions and the integer that tells me which list of functions to grab from the dict.\nHere is a simple example to demonstrate what I mean:\nfrom jax import jit\n\n# set up a dict that maps integers to a list of functions\ntest_dict = {1122997037:[lambda x: x**2, lambda x: 2*x],\n             1124279607:[lambda x: x**3, lambda x: 3*x]}\n             \n@jit\ndef evaluate_functions(xval,index):\n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n    \n    # given the input index, pull the relevant list of functions from test_dict\n    func1, func2 = test_dict[index]\n    \n    # evaluate both functions at the input xval and return values\n    return func1(xval), func2(xval)\n\n\nprint(evaluate_functions(2,1122997037))\nprint(evaluate_functions(2,1124279607))\nIf you remove the @jit decorator, it works as expected and prints (4,4) and (8,6). But with the @jit decorator, it raises the following error:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[60], line 16\n     11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n---> 16 print(evaluate_functions(2,1))\n     17 print(evaluate_functions(2,2))\n\n    [... skipping hidden 12 frame]\n\nCell In[60], line 11, in evaluate_functions(xval, index)\n      7 @jit\n      8 def evaluate_functions(xval,index):\n      9     \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n---> 11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n\nTypeError: unhashable type: 'DynamicJaxprTracer'\nIs there any way to rewrite this to make it work with @jit? For example using jax.lax.switch?",
        "answers": [
            "The problem with this is that you can't do conditional code execution in JAX based on input data (Edit: I was mistaken about the semantics of lax.cond). You could mark the index argument as static, which means that JAX will recompile the function each time a new value of it is used:\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef evaluate_functions(xval,index):\n    ...\nThis is the sort of compilation behavior you'll see:\nevaluate_functions(0, 0) # JIT runs, may be slow\nevaluate_functions(1, 0) # No compilation, fast \nevaluate_functions(1, 3) # JIT runs again\nevaluate_functions(7, 3) # No compilation\n# etc\nThis is a good solution if you'll be using each of the functions many times. If there's a large number of functions and you'll only be using each one once this won't help you.\nEdit: Here's a solution using jax.lax.switch, I'm not sure how the performance will be:\nkeys, branches = zip(*test_dict.items())    \nbranches1, branches2 = zip(*branches)    \n    \n@jit    \ndef evaluate_functions(xval,index):    \n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\"·    \n    ind_to_key = jnp.asarray(keys)    \n    \n    ind = jnp.where(ind_to_key == index, size=1)[0][0]    \n    out1 = jax.lax.switch(ind, branches1, xval)                                                                               \n    out2 = jax.lax.switch(ind, branches2, xval)                                                                               \n    return out1, out2 "
        ],
        "link": "https://stackoverflow.com/questions/76418151/how-to-rewrite-this-jax-snippet-to-prevent-typeerror-unhashable-type-dynamicj"
    },
    {
        "title": "data_format in JAX/FLAX",
        "question": "I did not find any settings for data_format=channels_first or data_format=channels_last in FLAX modules ( which are based on JAX ).\nOn the contrary, TensorFlow does have that designation. Does the choice of data_format is irrelevant to JAX performance ?\nUnfortunately, I did not find any kind of information on this subject.",
        "answers": [
            "JAX/Flax has no equivalent to the concept of data_format as used in Tensorflow/Keras."
        ],
        "link": "https://stackoverflow.com/questions/76413480/data-format-in-jax-flax"
    },
    {
        "title": "JAX code for minimizing Lennard-Jones potential for 2 points in Python gives unexpected results",
        "question": "I am trying to practice using JAX fo optimization problem and I am trying to do a simple problem, which is to minimize Lennard-Jones potential for just 2 points and I set both epsilon and sigma in Lennard-Jones potential equal 1, so the potential is just: F = 4(1/r^12-1/r^6) and r is the distance between the two points. And the result should be r = 2^(1/6), which is approximately 1.12.\nUsing JAX, I wrote following code, which is pretty simple and short, my initial guess values for two points are [0,1], which I think it is reasonable(because for Lennard-Jones potential it could be a problem because it approach infinite if r guess is too small). As I mentioned, I am expecting a value of r around 1.12 after the minimization, however, the result I get is [-0.71276042 1.71276042], so the distance is 2.4, which is clearly too big and I am wondering how can I fix it. I original doubt it might be the precision so I change the data type to float64, but the results are still the same. Any help will be greatly appreciated! Here is my code\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\nfrom jax import vmap\nimport matplotlib.pyplot as plt\n\nN = 2\njax.config.update(\"jax_enable_x64\", True)\nx_init = jnp.arange(N, dtype=jnp.float64)\nepsilon = 1\nsigma = 1\n\ndef potential(r):\n    r = jnp.where(r == 0, jnp.finfo(jnp.float64).eps, r)\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\ndef F(x):\n    # Compute all pairwise distances\n    r = jnp.abs(x[:, None] - x[None, :])\n    # Compute all pairwise potentials\n    pot = vmap(vmap(potential))(r)\n    # Exclude the diagonal (distance = 0) and avoid double-counting by taking upper triangular part\n    pot = jnp.triu(pot, 1)\n    # Sum up all the potentials\n    total = jnp.sum(pot)\n    return total\n\n# Minimize the function\nprint(F)\nresult = minimize(F, x_init, method='BFGS')\n\n# Extract the optimized positions of the points\nx_solutions = result.x\nprint(x_solutions)",
        "answers": [
            "This function is one that would be very difficult for any unconstrained gradient-based optimizer to correctly optimize. Holding one point at zero and varying the other point on the range (0, 10], we see the potential looks like this:\nr = jnp.linspace(0.1, 5.0, 1000)\nplt.plot(r, jax.vmap(lambda ri: F(jnp.array([0, ri])))(r))\nplt.ylim(-2, 10)\nTo the left of the minimum, the gradient quickly diverges to negative infinity, meaning for nearly any reasonable step size, the optimizer will likely overshoot the minimum. Then on the right side, if the optimizer goes even a few units too far, the gradient tends to zero, meaning for nearly any reasonable step size, the optimizer will get stuck in a regime where the potential has almost no variation.\nAdd to this the fact that you've set up the model with two degrees of freedom in a degenerate potential, and it's not surprising that gradient-based optimization methods are failing.\nYou can make some progress here by minimizing the log of the shifted potential, which has the effect of smoothing the steep gradients, and lets the BFGS minimizer find an expected minimum:\nresult = minimize(lambda x: jnp.log(2 + F(x)), x_init, method='BFGS')\nprint(result.x)\n# [-0.06123102  1.06123102]\nBut in general my suggestion would probably be to opt for a constrained optimization approach instead, perhaps one of the JAXOpt constrained optimization methods, where you can rule-out problematic regions of the parameter space."
        ],
        "link": "https://stackoverflow.com/questions/76353392/jax-code-for-minimizing-lennard-jones-potential-for-2-points-in-python-gives-une"
    },
    {
        "title": "How can I implement a vmappable sum over a dynamic range in Jax?",
        "question": "I want to implement something like the following Python function in Jax, and wrap it with a call to vmap. I want it to be fully reverse-mode differentiable (with respect to x) using grad(), even after the vmap.\ndef f(x,kmax):\n  return sum ([x**k for k in range(1,kmax+1)])\n(This is a deliberately simplified version of the function; I realize in this case I could use the closed-form expression for the geometric series; sadly the actual function I'm trying to implement does not have a closed-form sum that I'm aware of.)\nIs there any way to do this? It seems like there has to be; but fori_loop is not reverse-mode differentiable if kmax is dynamic, jax.lax.scan needs a statically-shaped array or it will throw ConcretizationTypeErrors, and similarly Python primitives like range (as used above) throw TracerIntegerConversionError if wrapped in vmap.\nI think I understand the restrictions on needing arrays to be fixed-shape, but every autodiff framework I've ever used allows you to construct arbitrarily-sized expressions dynamically somehow. A sum over a varying integer range is a pretty basic mathematical tool. How does one implement this in Jax?\nEDITED to refocus the problem definition (the issue is more vmap than grad) and provide the following examples.\nThis, specifically, is what I'd like to be able to do\nimport jax\n\ndef f(x,kmax):\n  return sum ([x**k for k in range(1,kmax+1)])\n\nfmap = jax.vmap(f,in_axes=(None,-1))\n\nx = 3.\nkmaxes = jax.numpy.array([1,2,3])\n\nprint(fmap(x,kmaxes))\n\nfmap_sum = lambda k,kmaxes:jax.numpy.sum(fmap(k,kmaxes))\n\nprint(fmap_sum(x,kmaxes))\nprint(jax.grad(fmap_sum)(x,kmaxes))\nThis throws a TracerIntegerConversionError at range(1,kmax+1). What I would like it to be doing is something like this:\nimport jax\n\ndef f(x,kmax):\n  return sum ([x**k for k in range(1,kmax+1)])\n\ndef fmap(x,kmaxes):\n  return [f(x,kmax) for kmax in kmaxes]\n\nx = 3.\nkmaxes = jax.numpy.array([1,2,3])\n\nprint(fmap(x,kmaxes))\n\ndef fmap_sum(x,kmaxes):\n  return sum(fmap(x,kmaxes))\n\nprint(fmap_sum(x,kmaxes))\nprint(jax.grad(fmap_sum)(x,kmaxes))\nwhich gives the correct result (but loses the parallelization and acceleration of vmap).",
        "answers": [
            "First, to make your function compatible with vmap, you'll need to replace the Python control flow with jax.lax control flow operations. In this case, lax.fori_loop seems applicable:\ndef f1(x, k):\n  def body_fun(i, val):\n    return val + x ** i\n  return jax.lax.fori_loop(1, k + 1, body_fun, jnp.zeros_like(x))\n\nf1map = jax.vmap(f1, (None, 0))\nprint(f1map(x, kmaxes))\n# [ 3. 12. 39.]\nBut because the size of the loop is dynamic, this is not compatible with reverse-mode autodiff:\njax.jacrev(f1map)(x, kmaxes)\n# ValueError: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.\nTo get around this, you can modify your function such that it uses a static loop size. Here's one way you might do that:\ndef f2(x, k, kmax):  # kmax should be static\n  def body_fun(i, val):\n    return val + jnp.where(i <= k, x ** i, 0)\n  return jax.lax.fori_loop(1, kmax + 1, body_fun, jnp.zeros_like(x))\n\nf2map = jax.vmap(f2, (None, 0, None))\n\nprint(f2map(x, kmaxes, kmaxes.max()))  # compatible with vmap\n# [ 3. 12. 39.]\n\nprint(jax.jacrev(f2map)(x, kmaxes, kmaxes.max()))  # and with reverse-mode autodiff\n# [ 1.  7. 34.]"
        ],
        "link": "https://stackoverflow.com/questions/76334231/how-can-i-implement-a-vmappable-sum-over-a-dynamic-range-in-jax"
    },
    {
        "title": "How Jax use LAX-backend implementation of functions",
        "question": "I need to compute the kron procuts of two arrays and I want to test if doing it using Jax is faster than doing it using Numpy.\nNow, in numpy my code there is res = numpy.kron(x1,x2), in Jax there is jax.numpy.kron(x1,x2) but how can I use it properly? My doubs are:\nis it sufficient to replace numpy with jax.numpy as follows: res = jax.numpy.kron(x1,x2)?\nshould I first sent x1 and x2 to the device using x1_dev = jax.device_put(x1) and after that run res = jax.numpy.kron(x1_dev,x2_dev)?\nshould I add jax.block_until_ready() to the jax.numpy.kron() call?",
        "answers": [
            "This is covered in JAX's FAQ under Benchmarking JAX Code.\nIn particular, if you're interested in the speed of JAX vs NumPy I would read the second section of this, Is JAX Faster Than Numpy? which gives a broad overview of when you should expect JAX to be faster or slower than equivalent NumPy code.\nAs for benchmarking kron: following the advice there, I would benchmark them like this (using IPython's %timeit for convenience). I ran the following on a Colab T4 GPU runtime:\nimport numpy as np\nx1 = np.random.rand(1000)\nx2 = np.random.rand(1000)\n%timeit np.kron(x1, x2)\n6.52 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nimport jax.numpy as jnp\nx1_jax = jnp.array(x1)\nx2_jax = jnp.array(x2)\n%timeit jnp.kron(x1_jax, x2_jax).block_until_ready()\n1.39 ms ± 2.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nIf you want to see the effect of just-in-time compilation with jax.jit, you can do something like this:\nimport jax\njit_kron = jax.jit(jnp.kron)\n_ = jit_kron(x1_jax, x2_jax)  # trigger compilation before timing\n%timeit jit_kron(x1_jax, x2_jax).block_until_ready()\n116 µs ± 33.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
        ],
        "link": "https://stackoverflow.com/questions/76322383/how-jax-use-lax-backend-implementation-of-functions"
    },
    {
        "title": "jax.numpy.delete assume_unique_indices unexpected keyword argument",
        "question": "I can not seem to get the assume_unique_indices from jax.numpy working. According to the documentation here, the jnp.delete has a keyword argument \"assume_unique_indices\" that is supposed to make this function jit compatible when we are sure that the index array is an integer array and is guaranteed to contain unique entries.\nHere is an minimum reproducible example\nimport jax\n\narr = jnp.array([1, 2, 3, 4, 5])\nidx = jnp.array([0, 2, 4])\n\nprint(jax.__version__)\n\n# Delete elements at indices idx\nout = jax.numpy.delete(arr, idx, assume_unique_indices=True)\n\nprint(out) # [2 4]\nThe error message\n0.4.8\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-bf0277118922> in <cell line: 9>()\n      7 \n      8 # Delete elements at indices idx\n----> 9 out = jax.numpy.delete(arr, idx, assume_unique_indices=False)\n     10 \n     11 print(out) # [2 4]\n\nTypeError: delete() got an unexpected keyword argument 'assume_unique_indices'\nDeleting the assume_unique_indices made it work as expected.",
        "answers": [
            "assume_unique_indices was added in https://github.com/google/jax/pull/15671, after JAX version 0.4.8 was released. If you update to version 0.4.9 or newer, your code should work.",
            "Ok, as it turns out, the 'assume_unique_indices' is only added rather recently, updating to jax version 0.4.10 did the trick"
        ],
        "link": "https://stackoverflow.com/questions/76244047/jax-numpy-delete-assume-unique-indices-unexpected-keyword-argument"
    },
    {
        "title": "Building a Neural Network using JAX",
        "question": "I tried to build a Neural network from scratch using JAX numpy moldule. In the training phase it seems that the accuracy of the model doesn't improve at all. Here is the code.\nimport jax\nimport jax.numpy as jnp\n\ndef init_params():\n    key = jax.random.PRNGKey(0)\n    W1 = jax.random.uniform(key, (10, 784), minval=-0.5, maxval=0.5)\n    b1 = jax.random.uniform(key, (10, 1), minval=-0.5, maxval=0.5)\n    W2 = jax.random.uniform(key, (10, 10), minval=-0.5, maxval=0.5)\n    b2 = jax.random.uniform(key, (10, 1), minval=-0.5, maxval=0.5)\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return jnp.maximum(Z, 0)\n\ndef softmax(Z):\n    A = jnp.exp(Z) / jnp.sum(jnp.exp(Z))\n    return A\n\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = jnp.dot(W1, X) + b1\n    A1 = ReLU(Z1)\n    Z2 = jnp.dot(W2, A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef ReLU_deriv(Z):\n    return Z > 0\n\ndef one_hot(Y):\n    one_hot_Y = jnp.zeros((Y.size, Y.max() + 1))\n    one_hot_Y = one_hot_Y.at[jnp.arange(Y.size), Y].set(1)\n    return one_hot_Y.T\n\n\ndef backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * jnp.dot(dZ2, A1.T)\n    db2 = 1 / m * jnp.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = jnp.dot(W2.T, dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * jnp.dot(dZ1, X.T)\n    db1 = 1 / m * jnp.sum(dZ1, axis=1, keepdims=True)\n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2\n\n\ndef get_predictions(A2):\n    return jnp.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return jnp.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2\n\n\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 1e-3, 500)\nI'm a complete newbie to JAX and I cannot solve this. What is the issue and how to solve this ?\nI tried to change the weight updating function as it seems to have some problem with updating the model weights",
        "answers": [
            "In your softmax function, you need to specify the axis parameter in the jnp.sum function to sum along the correct axis. Here's how:\ndef softmax(Z):\n    A = jnp.exp(Z) / jnp.sum(jnp.exp(Z), axis=0)\n    return A\nBy specifying axis=0, you ensure that the exponential values are summed across the correct axis."
        ],
        "link": "https://stackoverflow.com/questions/76236539/building-a-neural-network-using-jax"
    },
    {
        "title": "Using JAX and Vectorizing a Function",
        "question": "Here is my code that returns a value if you give it a probability between 0 and 1 (it is an inverseCDF function).\nimport jax.numpy as jnp\nfrom jax import jit, vmap, lax\nfrom jaxopt import Bisection\n\ndef find_y(M, a1, a2, a3):\n    \"\"\"\n    Finds the value of y that corresponds to a given value of M(y), using the bisection method implemented with JAX.\n\n    Parameters:\n    M (float): The desired value of M(y).\n    a1 (float): The value of coefficient a1.\n    a2 (float): The value of coefficient a2.\n    a3 (float): The value of coefficient a3.\n\n    Returns:\n    float: The value of y that corresponds to the given value of M(y).\n    \"\"\"\n    # Define a function that returns the value of M(y) for a given y\n    @jit\n    def M_fn(y):\n        eps = 1e-8  # A small epsilon to avoid taking the log of a negative number\n        return a1 + a2 * jnp.log(y / (1 - y + eps)) + a3 * (y - 0.5) * jnp.log(y / (1 - y + eps))\n\n    # Define a function that returns the difference between M(y) and M\n    @jit\n    def f(y):\n        return M_fn(y) - M\n\n    # Set the bracketing interval for the root-finding function\n    interval = (1e-7, 1 - 1e-7)\n\n    # Use the bisection function to find the root\n    y = Bisection(f, *interval).run().params\n\n    # Return the value of y\n    return y\n\n## test the algorithm\na1 = 16.\na2 = 3.396\na3 = 0.0\n\ny = find_y(16, a1, a2, a3)\nprint(y)\nI would like to pass an array for argument M instead of a scalar, but no matter what I try, I get an error (usually about some Boolean trace). Any ideas? Thanks!!",
        "answers": [
            "You can do this with jax.vmap, as long as you set check_bracket=False in Bisection (see here):\ny = Bisection(f, *interval, check_bracket=False).run().params\nWith that change to your function, you can pass a vector of values for M like this:\nimport jax\nM = jnp.array([4, 8, 16, 32])\nresult = jax.vmap(find_y, in_axes=(0, None, None, None))(M, a1, a2, a3)\nprint(result)\n[0.02837202 0.08661279 0.5        0.99108815]"
        ],
        "link": "https://stackoverflow.com/questions/76231302/using-jax-and-vectorizing-a-function"
    },
    {
        "title": "CuDNN error when running JAX on GPU with apptainer",
        "question": "I have an application written in Python 3.10+ with JAX that I would like to run on GPU. I can run containers on my local computer cluster using apptainer (but not Docker) which has an NVIDIA A40 GPU. Based on the proposed Dockerfile for JAX I made an Ubuntu-based image from the following Dockerfile:\nFROM nvidia/cuda:11.8.0-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI then convert the Docker image to an apptainer image using apptainer pull docker://my-image and then run the container using apptainer run --nv docker://my-image as described in the apptainer GPU docs.\nError\nWhen I run the following code\nimport jax\njax.numpy.array(1.)\nJAX immediately crashes with the following error message:\n2023-05-11 14:41:50.580441: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1993, in array\n    out_array: Array = lax_internal._convert_element_type(out, dtype, weak_type=weak_type)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\", line 537, in _convert_element_type\n    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 360, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 363, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 817, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 117, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 253, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 246, in cached\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 208, in xla_primitive_callable\n    compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 254, in _xla_callable_uncached\n    return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 2816, in compile\n    self._executable = UnloadedMeshExecutable.from_hlo(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 3028, in from_hlo\n    xla_executable = dispatch.compile_or_get_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 526, in compile_or_get_cached\n    return backend_compile(backend, serialized_computation, compile_options,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 471, in backend_compile\n    return backend.compile(built_c, compile_options=options)\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I've tried\nBased on a github thread with a similar error message (https://github.com/google/jax/issues/4920), I have tried to add some CUDA paths:\nexport PATH=/usr/local/cuda-11/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-11/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\nHowever, this did not resolve my problem.\nLocal docker container works without gpu\nWhen I test the image built from the Dockerfile on my local machine without GPU, everything works fine:\n$ docker run -ti my-image python3 -c 'import jax; jax.numpy.array(1.)'\n$\nApptainer container detects GPU's\nI can confirm that the GPU's are detected in the apptainer container. I get the following output when I run nvidia-smi:\n$ apptainer run --nv docker://my-image nvidia-smi\nINFO:    Using cached SIF image\n\n==========\n== CUDA ==\n==========\n\nCUDA Version 11.8.0\n\nContainer image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nThu May 11 14:34:18 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A40          On   | 00000000:00:08.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A40          On   | 00000000:00:09.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A40          On   | 00000000:00:0A.0 Off |                    0 |\n|  0%   30C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A40          On   | 00000000:00:0B.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   4  NVIDIA A40          On   | 00000000:00:0C.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   5  NVIDIA A40          On   | 00000000:00:0D.0 Off |                    0 |\n|  0%   32C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   6  NVIDIA A40          On   | 00000000:00:0E.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   7  NVIDIA A40          On   | 00000000:00:0F.0 Off |                    0 |\n|  0%   30C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nEdit\nCuda 11 image with CuDNN 8.7 gives different error\nWhen I use a different base image with cudnn 8.7.0:\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI get a different error:\n$ apptainer run --nv docker://my-image python3 -c 'import jax; jax.numpy.array(1.)'\n\nCould not load library libcudnn_ops_infer.so.8. Error: libnvrtc.so: cannot open shared object file: No such file or directory\nAborted (core dumped)",
        "answers": [
            "Based on the pointers of jakevdp I managed to find a solution. What was needed was:\nThe CUDA 11 image with CuDNN 8.7.\nThe devel instead of the runtime image.\nTogether, I could succesfully run JAX on apptainer with the following Dockerfile:\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/76229252/cudnn-error-when-running-jax-on-gpu-with-apptainer"
    },
    {
        "title": "The kernel dies with jax.random.PGRNKey",
        "question": "Kernel dies\nThe kernel dies with jax.random.PGRNKey. python version is 3.10.10. jax version is 0.4.9. jaxlib version is 0.4.9. It was run on M2 MacBook using Jupyter lab. What should I do to make it work?\nI upgraded the version of packages, and it didn't work. I have no idea on why it keep crashing.",
        "answers": [
            "It looks like the jaxlib 0.4.9 release is broken on Mac ARM (see JAX Issue #15951). For now I'd recommend installing jax/jaxlib version 0.4.8 to fix the issue.\nUpdate: jax & jaxlib 0.4.10 have been released, and should fix this issue."
        ],
        "link": "https://stackoverflow.com/questions/76224371/the-kernel-dies-with-jax-random-pgrnkey"
    },
    {
        "title": "Local Jax variable not updating in `jit`ted function, but updating in standard?",
        "question": "So, I've got some code and I could really use help deciphering the behavior and how to get it to do what I want.\nSee my code as follows:\nfrom typing import Callable, List\n\nimport chex\nimport jax.numpy as jnp\nimport jax\n\nWeights = List[jnp.ndarray]\n\n\n@chex.dataclass(frozen=True)\nclass Model:\n    mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    jitted_mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    weight_updater: Callable[\n        [jnp.ndarray], None\n    ]\n\n\ndef create_weight():\n    return jnp.ones((2, 5))\n\n\ndef wrapper():\n    weights = create_weight()\n\n    def mult(input_var):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var):\n        return weights.dot(input_var)\n\n    def update_locally_created(new_weights):\n        nonlocal weights\n        weights = new_weights\n        return weights\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=update_locally_created\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult)))\n        print(jnp.sum(tester.jitted_mult(to_mult)))\n\n        if i % 2 == 0:\n            tester.weight_updater(jnp.zeros((2, 5)))\n        else:\n            tester.weight_updater(jnp.ones((2, 5)))\n\n        print(\"*\" * 10)\nTL;DR I'm defining some \"weights\" within a function closure, and I'm trying to modify the weights via a nonlocal. The problem seems to be that the jit-ted version (jitted_mult of the function doesn't recognize the \"updated\" weights, whereas the non-jit function (mult) does.\nWhat can I do to make it recognize the update? I think that I might be able to do what Build your own Haiku does, but that seems like a lot of work for an experiment",
        "answers": [
            "This is working as expected: the reason it's not respecting the update is because your function is not pure (see JAX Sharp Bits: Pure Functions). In your case, the function is not pure because the output depends on an input that is not explicitly passed to the function. This violates the assumptions made by jit and other JAX transformations, which leads to unexpected behavior.\nTo fix it I would make this implicit input explicit, so that your function is pure. It might look something like this:\ndef wrapper():\n    def mult(input_var, weights):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var, weights):\n        return weights.dot(input_var)\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=None\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    weights = create_weight()\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult, weights)))\n        print(jnp.sum(tester.jitted_mult(to_mult, weights)))\n\n        if i % 2 == 0:\n            weights = jnp.zeros((2, 5))\n        else:\n            weights = jnp.ones((2, 5))\n\n        print(\"*\" * 10)"
        ],
        "link": "https://stackoverflow.com/questions/76205477/local-jax-variable-not-updating-in-jitted-function-but-updating-in-standard"
    },
    {
        "title": "How to unroll the training loop so that Jax can train multiple steps in GPU/TPU",
        "question": "When using powerful hardware, especially TPU, it is often preferable to train multiple steps. For example, in TensorFlow, this is possible.\nwith strategy.scope():\n  model = create_model()\n  optimizer_inner = AdamW(weight_decay=1e-6)\n  optimizer_middle = SWA(optimizer_inner)\n  optimizer = Lookahead(optimizer_middle)\n  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n  training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      'training_accuracy', dtype=tf.float32)\n\n# Calculate per replica batch size, and distribute the `tf.data.Dataset`s\n# on each TPU worker.\nactual_batch_size = 128\ngradient_accumulation_step = 1\nbatch_size = actual_batch_size * gradient_accumulation_step\nsteps_per_epoch = 60000 // batch_size\nvalidation_steps = 10000 // batch_size\n\ntrain_dataset = get_dataset(batch_size, is_training=True)\nper_replica_batch_size = batch_size // strategy.num_replicas_in_sync\n\ntrain_dataset = strategy.experimental_distribute_datasets_from_function(\n    lambda _: get_dataset(per_replica_batch_size, is_training=True))\n\n@tf.function(jit_compile=True)\ndef train_multiple_steps(iterator, steps):\n  \"\"\"The step function for one training step.\"\"\"\n\n  def step_fn(inputs):\n    \"\"\"The computation to run on each TPU device.\"\"\"\n    images, labels = inputs\n    with tf.GradientTape() as tape:\n      logits = model(images, training=True)\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          labels, logits, from_logits=True)\n      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\n    grads = tape.gradient(loss, model.trainable_variables)\n\n    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n    training_loss.update_state(loss * strategy.num_replicas_in_sync)\n    training_accuracy.update_state(labels, logits)\n\n  for _ in tf.range(steps):\n    strategy.run(step_fn, args=(next(iterator),))\n\ntrain_iterator = iter(train_dataset)\n# Convert `steps_per_epoch` to `tf.Tensor` so the `tf.function` won't get\n# retraced if the value changes.\n\nfor epoch in range(10):\n  print('Epoch: {}/10'.format(epoch))\n\n\n  train_multiple_steps(train_iterator, tf.convert_to_tensor(steps_per_epoch))\nIn Jax or Flax, however, I haven't seen a complete working example of doing so. I guess it would be something like\n@jax.jit\ndef train_for_n_steps(train_state, batches):\n    for batch in batches:\n        train_state = train_step_fn(train_state, batch)\n    return train_state\nHowever, in my case when I am trying to test the complete example, I am not sure how one can create multiple batches. Here is a working example using GPU without training multiple steps. The relevant code should probably be here:\nfor step,batch in enumerate(train_ds.as_numpy_iterator()):\n\n  # Run optimization steps over training batches and compute batch metrics\n  state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n  state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n\n  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n    for metric,value in state.metrics.compute().items(): # compute metrics\n      metrics_history[f'train_{metric}'].append(value) # record metrics\n    state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n    # Compute metrics on the test set after each training epoch\n    test_state = state\n    for test_batch in test_ds.as_numpy_iterator():\n      test_state = compute_metrics(state=test_state, batch=test_batch)\n\n    for metric,value in test_state.metrics.compute().items():\n      metrics_history[f'test_{metric}'].append(value)\n\n    print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n          f\"loss: {metrics_history['train_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n    print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")\nMy goal is to unroll 5 loops when training.\nAny suggestions are welcomed.",
        "answers": [
            "You could use more_itertools.chunked to get something like this:\nfor step, five_batches in chunked(train_ds.as_numpy_iterator()):\n    state = five_steps(state, five_batches):\nThen do the unrolling\n@jax.jit\ndef five_steps(state, batches):\n    for batch in batches:\n        state = train_step(state, batch)\n    return state\nThe reason this works is that batches has a length that isn't data dependent, so the loop will just get executed 5 times during tracing.\nThis will likely make jitting take much longer than you want, so the perferred but more difficult way is to pack the batches into [N x batch_size x ...] tensors, then use scan to loop your update function over the inputs."
        ],
        "link": "https://stackoverflow.com/questions/76191996/how-to-unroll-the-training-loop-so-that-jax-can-train-multiple-steps-in-gpu-tpu"
    },
    {
        "title": "No module named 'jax.experimental.global_device_array' when running the official Flax Example on Colab with V100",
        "question": "I have been trying to understand this official flax example, based on a Coalb pro+ account with V100. When I execute the command python main.py --workdir=./imagenet --config=configs/v100_x8.py , the returned error is\nFile \"/content/FlaxImageNet/main.py\", line 29, in <module>\nimport train\nFile \"/content/FlaxImageNet/train.py\", line 30, in <module>\nfrom flax.training import checkpoints\nFile \"/usr/local/lib/python3.10/dist-packages/flax/training/checkpoints.py\", line 34, \nin <module>\nfrom jax.experimental.global_device_array import GlobalDeviceArray\nModuleNotFoundError: No module named 'jax.experimental.global_device_array'\nI am not sure whether global_device_array has been moved from jax.experimental package or it is no longer needed or replaced by other equivalent methods.",
        "answers": [
            "GlobalDeviceArray was deprecated in JAX version 0.4.1 and removed in JAX version 0.4.7.\nWith that in mind, it seems the code in question requires JAX version 0.4.6 or older. You might consider reporting this incompatibility to the flax project: http://github.com/google/flax/."
        ],
        "link": "https://stackoverflow.com/questions/76191911/no-module-named-jax-experimental-global-device-array-when-running-the-official"
    },
    {
        "title": "Jit a JAX function that select a function from a dictionary",
        "question": "I have a JAX function that, given the order and the index, selects a polynomial from a pre-defined dictionary, as follows:\npoly_dict = {\n    (0, 0): lambda x, y, z: 1.,\n    (1, 0): lambda x, y, z: x,\n    (1, 1): lambda x, y, z: y,\n    (1, 2): lambda x, y, z: z,\n    (2, 0): lambda x, y, z: x*x,\n    (2, 1): lambda x, y, z: y*y,\n    (2, 2): lambda x, y, z: z*z,\n    (2, 3): lambda x, y, z: x*y,\n    (2, 4): lambda x, y, z: y*z,\n    (2, 5): lambda x, y, z: z*x\n}\n\ndef poly_func(order: int, index: int):\n    \n    try:\n        return poly_dict[(order, index)]\n    \n    except KeyError:\n        print(\"(order, index) must be a key in poly_dict!\")\n        return\nNow I want to jit poly_func(), but it gives an error TypeError: unhashable type: 'DynamicJaxprTracer' Moreover, if I just do\ndef poly_func(order: int, index: int):\n    \n    return poly_dict[(order, index)]\nit still gives the same error. Is there a way to resolve this issue?",
        "answers": [
            "There are two issues with your approach: first, traced values cannot be used to index into Python collections like dicts or lists. Second, JIT-compiled functions can only return array values, not functions.\nWith that in mind, it is impossible to make the function you propose work with JAX transforms like JIT. But you could modify your approach to use a list rather than a dict of functions, and then use lax.switch to dynamically select which function you call. Here's how it might look:\ndef get_index(order, index):\n  return order * 5 + index\n\npoly_list = 16 * [lambda x, y, z: 0.0]  # place-holder function\n\nfor key, val in poly_dict.items():\n  poly_list[get_index(*key)] = poly_dict[key]\n\n\ndef eval_poly_func(order: int, index: int, args):\n  ind = get_index(order, index)\n  return jax.lax.switch(ind, poly_list, *args)\n\nresult = jax.jit(eval_poly_func)(2, 0, (5.0, 6.0, 7.0))\nprint(result)\n# 25.0"
        ],
        "link": "https://stackoverflow.com/questions/76191616/jit-a-jax-function-that-select-a-function-from-a-dictionary"
    },
    {
        "title": "Fail to understand the usage of partial argument in Flax Resnet Official Example",
        "question": "I have been trying to understand this official example. However, I am very confused about the use of partial in two places.\nFor example, in line 94, we have the following:\nconv = partial(self.conv, use_bias=False, dtype=self.dtype)\nI am not sure why it is possible to apply a partial to a class, and where later in the code we fill in the missing argument (if we need to).\nComing to the final definition, I am even more confused. For example,\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n               block_cls=ResNetBlock)\nWhere do we apply the argument such as stage_size=[2,2,2,2]?\nThank you",
        "answers": [
            "functools.partial will partially evaluate a function, binding arguments to it for when it is called later. here's an example of it being used with a function:\nfrom functools import partial\n\ndef f(x, y, z):\n  print(f\"{x=} {y=} {z=}\")\n\ng = partial(f, 1, z=3)\ng(2)\n# x=1 y=2 z=3\nand here is an example of it being used on a class constructor:\nfrom typing import NamedTuple\n\nclass MyClass(NamedTuple):\n  a: int\n  b: int\n  c: int\n\nmake_class = partial(MyClass, 1, c=3)\nprint(make_class(b=2))\n# MyClass(a=1, b=2, c=3)\nThe use in the flax example is conceptually the same: partial(f) returns a function that when called, applies the bound arguments to the original callable, whether it is a function, a method, or a class constructor.\nFor example, the ResNet18 function created here:\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n                   block_cls=ResNetBlock)\nis a partially-evaluated ResNet constructor, and the function is called in a test here:\n  @parameterized.product(\n      model=(models.ResNet18, models.ResNet18Local)\n  )\n  def test_resnet_18_v1_model(self, model):\n    \"\"\"Tests ResNet18 V1 model definition and output (variables).\"\"\"\n    rng = jax.random.PRNGKey(0)\n    model_def = model(num_classes=2, dtype=jnp.float32)\n    variables = model_def.init(\n        rng, jnp.ones((1, 64, 64, 3), jnp.float32))\n\n    self.assertLen(variables, 2)\n    self.assertLen(variables['params'], 11)\nmodel here is the partially evaluated function ResNet18, and when it is called it returns the fully-instantiated ResNet object with the parameters specified in the ResNet18 partial definition."
        ],
        "link": "https://stackoverflow.com/questions/76178123/fail-to-understand-the-usage-of-partial-argument-in-flax-resnet-official-example"
    },
    {
        "title": "Failing to implement logistic regression using 'equinox' and 'optax' library",
        "question": "I am trying to implement logistic regression using equinox and optax libraries, with the support of JAX. While training the model, the loss is not decreasing over time,and model is not learning. Herewith attaching a reproducible code with toy dataset for reference:\nimport jax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jrandom\nimport equinox as eqx\nimport optax\n\ndata_key,model_key = jax.random.split(jax.random.PRNGKey(0),2)\n\n### Generating toy-data\n\nX_train = jax.random.normal(data_key, (1000,2))\ny_train = X_train[:,0]+X_train[:,1]\ny_train = jnp.where(y_train>0.5,1,0)\n\n### Using equinox and optax\nprint(\"Training using equinox and optax\")\n\nepochs = 10000             \nlearning_rate = 0.1\nn_inputs = X_train.shape[1]\n\nclass Logistic_Regression(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n    def __init__(self, in_size, out_size, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (out_size, in_size))\n        self.bias = jax.random.normal(bkey, (out_size,))\n        #self.weight = jnp.zeros((out_size, in_size))\n        #self.bias = jnp.zeros((out_size,))\n    def __call__(self, x):\n        return jax.nn.sigmoid(self.weight @ x + self.bias)\n\n@eqx.filter_value_and_grad\ndef loss_fn(model, x, y):\n    pred_y = jax.vmap(model)(x) \n    return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n\n@eqx.filter_jit\ndef make_step(model, x, y, opt_state):\n    loss, grads = loss_fn(model, x, y)\n    updates, opt_state = optim.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    return loss, model, opt_state\n\nin_size, out_size = n_inputs, 1\nmodel = Logistic_Regression(in_size, out_size, key=model_key)\noptim = optax.sgd(learning_rate)\nopt_state = optim.init(model)\nfor epoch in range(epochs):\n    loss, model, opt_state = make_step(model,X_train,y_train, opt_state)\n    loss = loss.item()\n    if (epoch+1)%1000 ==0:\n        print(f\"loss at epoch {epoch+1}:{loss}\")\n\n# The following code is implementation of Logistic regression using scikit-learn and pytorch, and it is working well. It is added just for reference\n\n\n### Using scikit-learn\nprint(\"Training using scikit-learn\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_train)\nprint(\"Train accuracy:\",accuracy_score(y_train,y_pred))\n\n## Using pytorch\nprint(\"Training using pytorch\")\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\nfrom torch.nn import Sequential\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\",device)\ntorch_LR= Sequential(nn.Linear(n_inputs, 1),\n                nn.Sigmoid())\ntorch_LR.to(device)\ncriterion = nn.BCELoss() # define the optimization\noptimizer = SGD(torch_LR.parameters(), lr=learning_rate)\n\ntrain_loss = []\nfor epoch in range(epochs):\n    inputs, targets = torch.tensor(X_train).to(device), torch.tensor(y_train).to(device) # move the data to GPU if available\n    optimizer.zero_grad() # clear the gradients\n    yhat = torch_LR(inputs.float()) # compute the model output\n    loss = criterion(yhat, targets.unsqueeze(1).float()) # calculate loss\n    #train_loss_batch.append(loss.cpu().detach().numpy()) # store the loss\n    loss.backward() # update model weights\n    optimizer.step()\n    if (epoch+1)%1000 ==0:\n        print(f\"loss at epoch {epoch+1}:{loss.cpu().detach().numpy()}\")\nI tried SGD and adam optmizers with different learning rates, but the result is same. Also, I tried zero weight initialisation and ranodom weight initialisation. For the same data, I tried pytorch and LogisticRegression module from scikit-learn library (I understood in sklearn SGD is not used, but just as a reference to observe performance). Scikit-learn and pytorch modeling is added in the code block for reference. I have tried this with multiple classification datasets but still facing this problem.",
        "answers": [
            "The first time you print your loss is after 1000 epochs. If you change it to print the loss of the first 10 epochs, you see that the optimizer is rapidly converging:\n    # ...\n    if epoch < 10 or (epoch + 1)%1000 ==0:\n        print(f\"loss at epoch {epoch+1}:{loss}\")\nHere is the result:\nTraining using equinox and optax\nloss at epoch 1:1.237254023551941\nloss at epoch 2:1.216030478477478\nloss at epoch 3:1.1952687501907349\nloss at epoch 4:1.174972414970398\nloss at epoch 5:1.1551438570022583\nloss at epoch 6:1.1357849836349487\nloss at epoch 7:1.1168975830078125\nloss at epoch 8:1.098482370376587\nloss at epoch 9:1.0805412530899048\nloss at epoch 10:1.0630732774734497\nloss at epoch 1000:0.6320337057113647\nloss at epoch 2000:0.6320337057113647\nloss at epoch 3000:0.6320337057113647\nBy epoch 1000, the loss has converged to a minimum value from which it does not move.\nGiven this, it looks like your optimizer is functioning correctly.\nEdit: I did some debugging and found that y_pred = jax.vmap(model)(X_train) returns an array of shape (1000, 1), so (y - y_pred) is not a length-1000 array of differences, but rather a shape (1000, 1000) array of pairwise differences between all outputs. The log-loss over these pairwise differences is not a standard logistic regression model."
        ],
        "link": "https://stackoverflow.com/questions/76146548/failing-to-implement-logistic-regression-using-equinox-and-optax-library"
    },
    {
        "title": "High Memory Consumption in JAX with Nested vmap",
        "question": "I'm working on a problem that involves computing the value of many interpolants on a three dimensional grid using jax. Following standard jax practice, I wrote everything for \"single-batch\" inputs and then vmap over all interpolants and evaluation grid points in the end. The code below is a reduced, non-sensical version of this.\nfrom collections import namedtuple\nfrom functools import partial\n\nimport jax.numpy as jnp\nfrom jax import vmap\nfrom jax.lax import dynamic_slice, stop_gradient\n\ninterpolation_params = namedtuple(\"interpolation_params\", [\"a\", \"dx\", \"f\", \"lb\", \"ub\"])\n\n\n@partial(vmap, in_axes=(None, None, 0))\ndef init_1d_interpolation_params(a, dx, f):\n    f = jnp.pad(f, 1)\n    lb, ub = a, a + (f.shape[0] - 1) * dx\n    return interpolation_params(a=a, dx=dx, f=f, lb=lb, ub=ub)\n\n\n@partial(vmap, in_axes=(None, 0))\ndef eval_interp1d(x, interpolation_params):\n    A = jnp.array([-1.0 / 16, 9.0 / 16, 9.0 / 16, -1.0 / 16])\n    B = jnp.array([1.0 / 24, -9.0 / 8, 9.0 / 8, -1.0 / 24])\n    C = jnp.array([1.0 / 4, -1.0 / 4, -1.0 / 4, 1.0 / 4])\n    D = jnp.array([-1.0 / 6, 1.0 / 2, -1.0 / 2, 1.0 / 6])\n\n    x = (\n        jnp.minimum(jnp.maximum(x, interpolation_params.lb), interpolation_params.ub)\n        - interpolation_params.a\n    )\n    ix = jnp.atleast_1d(jnp.array(x // interpolation_params.dx, int))\n    ratx = x / interpolation_params.dx - (ix + 0.5)\n    asx = A + ratx * (B + ratx * (C + ratx * D))\n    return jnp.dot(dynamic_slice(interpolation_params.f, ix, (4,)), asx)\n\n\n# Init 300 interpolants on a uniform grid with 4096 points\nx = jnp.linspace(0, 1, 4096)\nf = x**2\nff = jnp.repeat(f.reshape(1, -1), 300, axis=0)\n\nparams = init_1d_interpolation_params(x[0], x[1] - x[0], ff)\n\n\n@partial(vmap, in_axes=(0, None))\ndef foo(x, interpolation_params):\n    g_x = (eval_interp1d(x, interpolation_params)) ** 2\n    return jnp.sum(g_x)\n\n\nlarge_x_array = stop_gradient(jnp.repeat(jnp.array([0.0]), 100**3))\nfoo(large_x_array, params)\nNow, if I run the code below, I end up with very large memory foot prints (14GB) which is a little puzzling to me. Initially, I thought the issue is the computation graph tracing of jax's autodiff backend, the size of which should be naively comparable to the \"cartesian product\" of params and large_x_array. However, using stop_gradient to turn off the graph tracing didn't help, so I'm not exactly sure what's going on and how to fix this elegantly. Any thoughts on this?",
        "answers": [
            "Your code is essentially a doubly-nested vmap over axes of shape 300 and 1000000 respectively. This means that the effective memory footprint of a fully-vmapped float32 scalar in your inner function is 300 * 1000000 * 4 bytes, or just over 1GB. Your inner function constructs fully-mapped arrays of length 4, which take up about 4GB – with that in mind, its not surprising that your full function would require allocating a few times that amount of memory.\nFor what it's worth, if you want to see the array sizes implied by your outer function, one way to do that is to construct the jaxpr representing your end-to-end operation:\nimport jax\nprint(jax.make_jaxpr(foo)(large_x_array, params))\nThe output is long, so I won't paste it in full here, but in the jaxpr you see direct evidence of what I said above, for example:\n...\n    cf:f32[1000000,300,4] = add ce cc\n    cg:f32[1000000,300,4] = mul bv cf\n...\nThese are the intermediate arrays of size 1000000x300x4x4 bytes (or just over 4 GB) which are allocated in the course of executing your code.\nIf you want to reduce memory consumption, you could do so by serializing some of the computations using scan or fori_loop in place of vmap, in order to avoid allocating the full 4GB intermediate arrays."
        ],
        "link": "https://stackoverflow.com/questions/76109349/high-memory-consumption-in-jax-with-nested-vmap"
    },
    {
        "title": "Calling an initialized function from a list inside a jitted JAX function",
        "question": "Given is a jitted function, which is calling another function that maps over a batch, which again calls a function, i.e. inner_function, to compute a certain property. Also given is a list of initialized functions intialized_functions_dic, from which we want to call the proper initialized function based on some information passed as argument, e.g. info_1. Is there a way to make this work? Thanks in advance.\ninitialized_functions_dic = {1:init_function1, 2:init_function_2, 3:init_function_3}\n\n\ndef inner_function(info_1, info_2, info_3):\n    return 5 + outside_dic[info_1]\nCalling outside_dic[info_1] will throw an error due to trying to access a dictionary with a traced value.\nTrying to pass info_1 as static_argnums also fails due to info_1 being an unhashable type 'ArrayImpl'.",
        "answers": [
            "It sounds like you're looking for jax.lax.switch, which will switch between entries in a list of functions given an index:\ninitialized_functions = [init_function_1, init_function_2, init_function_3]\n\ndef inner_function(info_1, info_2, info_3):\n    idx = info_1 - 1  # lists are zero-indexed\n    args = (info_2, info_3) # tuple of arguments to pass to the function\n    return 5 + lax.switch(idx, initialized_functions, *args)"
        ],
        "link": "https://stackoverflow.com/questions/76094143/calling-an-initialized-function-from-a-list-inside-a-jitted-jax-function"
    },
    {
        "title": "How to write tensorboard events files without installing / importing TF or PyTorch?",
        "question": "Obviously events-file logging is included with TensorFlow and apparently there's an implementation included with PyTorch, but is there an officially supported standalone implementation of something like SummaryWriter for use outside of these two frameworks (e.g. if one is using JAX and doesn't want to also install/import TensorFlow or PyTorch)?\nThe flax library apparently has flax.metrics.tensorboard, but it just imports TensorFlow and implements a SummaryWriter using tf.summary.",
        "answers": [
            "TensorboardX might be helpful: https://tensorboardx.readthedocs.io/en/latest/tensorboard.html#\nIt does not require installing Tensorflow/Pytorch, but does require installation of the above, a much lighter-weight, library."
        ],
        "link": "https://stackoverflow.com/questions/76049789/how-to-write-tensorboard-events-files-without-installing-importing-tf-or-pytor"
    },
    {
        "title": "Is there a way to accept a function while taking the gradient using jax.grad?",
        "question": "I am trying to make a neural network-based differential equation solver for the differential equation y' + 2xy = 0.\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\n\ndef softplus(x):\n    return jnp.log(1 + jnp.exp(x))\n\ndef init_params():\n    params = jax.random.normal(key, shape=(241,))\n    return params\n\ndef linear_model(params, x):\n    w0 = params[:80]\n    b0 = params[80:160]\n    w1 = params[160:240]\n    b1 = params[240]\n    h = softplus(x*w0 + b0)\n    o = jnp.sum(h*w1) + b1\n    return o\n\ndef loss(derivative, initial_condition, params, model, x):\n    dfdx = jax.grad(model, 1)\n    dfdx_vect = jax.vmap(dfdx, (None, 0))\n    model_vect = jax.vmap(model, (None, 0))\n    eq_difference = dfdx_vect(params, x) - derivative(x, model(params, x))\n    condition_difference = model(params, 0) - initial_condition\n    return jnp.mean(eq_difference ** 2 - condition_difference ** 2)\n\ndef dfdx(x, y):\n    return -2. * x * y\n\nkey = jax.random.PRNGKey(0)\ninputs = np.linspace(0, 1, num=401)\nparams = init_params()\n\nepochs = 2000\nlearning_rate = 0.0005\n\n# Training Neural Network\n\nfor epoch in tqdm(range(epochs)):\n    grad_loss = jax.grad(loss)\n    gradient = grad_loss(dfdx, 1., params, linear_model, inputs)\n    params -= learning_rate*gradient\n\nmodel_vect = jax.vmap(linear_model, (None, 0))\npreds = model_vect(params, inputs)\n\nplt.plot(inputs, jnp.exp(inputs**2), label='exact')\nplt.plot(inputs, model_vect(params, inputs), label='approx')\nplt.legend()\nplt.show()\nThe issue is that Jax doesn't like taking the gradient of a function that receives another function as an argument:\nTypeError: Argument '<function dfdx at 0x7fce88340af0>' of type <class 'function'> is not a valid JAX type.\nIs there any workaround for this?",
        "answers": [
            "You just orderd arguments wrong. Jax differentiates wrt. first argument, and you don't want to differentiate wrt your function, but rather - parameters. Just make them the first argument.\ndef loss(params, derivative, initial_condition, model, x):\n    dfdx = jax.grad(model, 1)\n    dfdx_vect = jax.vmap(dfdx, (None, 0))\n    model_vect = jax.vmap(model, (None, 0))\n    eq_difference = dfdx_vect(params, x) - derivative(x, model(params, x))\n    condition_difference = model(params, 0) - initial_condition\n    return jnp.mean(eq_difference ** 2 - condition_difference ** 2)"
        ],
        "link": "https://stackoverflow.com/questions/76029743/is-there-a-way-to-accept-a-function-while-taking-the-gradient-using-jax-grad"
    },
    {
        "title": "JAX VMAP Parallelization Details",
        "question": "I was wondering how vmap's internals work. When I vectorize code using jax.lax.map, I know that each element is executed consecutively. However when I use vmap I execute the vectorized operation apparently in parallel. Can someone provide me with a more detailed explanation of how the parallelization works? How does Jax determine the number of parallel processes, and can this behaviour be influenced by the user?\nThanks in advance.",
        "answers": [
            "jax.vmap is a vectorizing/batching transform, not a parallelizing transform. Internally, it converts an unbatched function to a batched function, lowering to efficient primitive calls rather than an explicit map or loop.\nFor example, here is a simple function, where we create both a manually-looped and an automatically vectorized batched version:\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\n\ndef f(x, y):\n  return x @ y\n\nnum_batches = 3\nnum_entries = 5\n\nnp.random.seed(0)\nx = np.random.rand(num_batches, num_entries)\ny = np.random.rand(num_batches, num_entries)\n\n\nf_loop = lambda x, y: jnp.stack([f(xi, yi) for xi, yi in zip(x, y)])\nf_vmap = jax.vmap(f)\n\nprint(f_loop(x, y))\n# [1.3567398 2.1908383 1.6315514]\nprint(f_vmap(x, y))\n# [1.3567398 2.1908383 1.6315514]\nThese return the same results (by design), but they are quite different under the hood; by printing the jaxpr, we see that the loop version requires a long sequence of XLA calls:\nprint(jax.make_jaxpr(f_loop)(x, y))\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] a\n    d:f32[5] = squeeze[dimensions=(0,)] c\n    e:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] b\n    f:f32[5] = squeeze[dimensions=(0,)] e\n    g:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d f\n    h:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] a\n    i:f32[5] = squeeze[dimensions=(0,)] h\n    j:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] b\n    k:f32[5] = squeeze[dimensions=(0,)] j\n    l:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] i k\n    m:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] a\n    n:f32[5] = squeeze[dimensions=(0,)] m\n    o:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] b\n    p:f32[5] = squeeze[dimensions=(0,)] o\n    q:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] n p\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n    s:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] l\n    t:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] q\n    u:f32[3] = concatenate[dimension=0] r s t\n  in (u,) }\nOn the other hand, the vmap version lowers to just a single generalized dot product:\nprint(jax.make_jaxpr(f_vmap)(x, y))\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[3] = dot_general[dimension_numbers=(([1], [1]), ([0], [0]))] a b\n  in (c,) }\nNotice that there is nothing concerning parallelization here; rather we've automatically created an efficient batched version of our operation. The same is true of vmap applied to more complicated functions: it does not involve parallelization, rather it outputs an efficient batched version of your operation in an automated manner."
        ],
        "link": "https://stackoverflow.com/questions/75891826/jax-vmap-parallelization-details"
    },
    {
        "title": "Confused about evaluating vector-Jacobian-product with non-identity vectors (JAX)",
        "question": "I'm confused about the meaning of evaluating vector-Jacobian-products when the vector used for the VJP is a non-identity row vector. My question pertains to vector-valued functions, not scalar functions like loss. I will show a concrete example using Python and JAX but this is a very general question about reverse-mode automatic differentiation.\nConsider this simple vector-valued function for which the Jacobian is trivial to write down analytically:\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\nfrom jax import vjp, jacrev\n\n# Define a vector-valued function (3 inputs --> 2 outputs) \ndef vector_func(args):\n    x,y,z = args\n    a = 2*x**2 + 3*y**2 + 4*z**2\n    b = 4*x*y*z\n    return jnp.array([a, b])\n\n# Define the inputs\nx = 2.0\ny = 3.0\nz = 4.0\n\n# Compute the vector-Jacobian product at the fiducial input point (x,y,z)\nval, func_vjp = vjp(vector_func, (x, y, z))\n\nprint(val) \n# [99,96]\n\n# now evaluate the function returned by vjp along with basis row vectors to pull out gradient of 1st and 2nd output components \nv1 = jnp.array([1.0, 0.0])  # pulls out the gradient of the 1st component wrt the 3 inputs, i.e., first row of Jacobian\nv2 = jnp.array([0.0, 1.0])  # pulls out the gradient of the 1st component wrt the 3 inputs, i.e., second row of Jacobian \n\ngradient1 = func_vjp(v1)\nprint(gradient1)\n# [8, 18, 32]\n\ngradient2 = func_vjp(v2)\nprint(gradient2)\n# [48,32,24]\nThat much makes sense to me -- we're separately feeding [1,0] and [0,1] to vjp_func to respectively get the first and second rows of the Jacobian evaluated at our fiducial point (x,y,z)=(2,3,4).\nBut now what if we fed vjp_func a non-identity row vector like [2,0]? Is this asking how the fiducial (x,y,z) would need to be perturbed to double the first component of the output? If so, is there a way to see this by evaluating vector_func at the perturbed parameter values?\nI tried but I'm not sure:\n# suppose I want to know what perturbations in (x,y,z) cause a doubling of the first output and no change in second output component \nprint(func_vjp(jnp.array([2.0,0.0])))\n# [16,36,64] \n\n### Attempts to use the output of vjp_func to verify that val becomes [99*2, 96]\n### none of these work\n\nprint(vector_func([16,36,64]))\n# [20784, 147456]\n\nprint(vector_func([x*16,y*36,z*64])\n# [299184., 3538944.]\nWhat am I doing wrong in using the output of func_vjp to modify the fiducial parameters (x,y,z) and feed those back into vector_func to verify indeed that those parameter perturbations double the first component of the original output and leave the second component unchanged?",
        "answers": [
            "I think in your question you are confusing primal and tangent vector spaces. The function vector_func is a non-linear function that maps a vector in an input primal vector space (represented by (x, y, z)) to a vector in an output primal vector space (represented by val in your code).\nThe function func_vjp is a linear function that maps a vector in an output tangent vector space (represented by array([2, 0]) in your question) to a vector in an input tangent vector space ([16,36,64] in your question).\nBy construction, the tangent vectors in these transformations represent the gradients of the input function at the specified primal values. That is, if you infinitesimally perturb your output primal along the direction of your output tangent, it corresponds to infinitesimally perturbing the input primal along the direction of the input tangent.\nIf you want to check the values, you could do something like this:\ninput_primal = (x, y, z)\noutput_primal, func_vjp = vjp(vector_func, input_primal)\n\nepsilon = 1E-8  # note: small value so we're near the linear regime\noutput_tangent = epsilon * jnp.array([0.0, 1.0])\ninput_tangent, = func_vjp(output_tangent)\n\n# Compute the perturbed output given the perturbed input\nperturbed_input = [p + t for p, t in zip(input_primal, input_tangent)]\nperturbed_output_1 = vector_func(perturbed_input)\nprint(perturbed_output_1)\n# [99.00001728 96.00003904]\n\n# Perturb the output directly\nperturbed_output_2 = output_primal + output_tangent\nprint(perturbed_output_2)\n# [99.         96.00000001]\nNote that the results don't match exactly, because the VJP is valid in the locally linear limit, and your function is very nonlinear. But hopefully this helps clarify what these primal and tangent values mean in the context of the VJP computation. Mathematically, if we computed this in the limit where epsilon goes to zero, the results would match exactly – gradient computations are all about these kinds of infinitesimal limits."
        ],
        "link": "https://stackoverflow.com/questions/75883427/confused-about-evaluating-vector-jacobian-product-with-non-identity-vectors-jax"
    },
    {
        "title": "JAX performance problems",
        "question": "I am obviously not following best practices, but maybe that's because I don't know what they are. Anyway, my goal is to generate a tubular neighborhood about a curve in three dimensions. A curve is give by an array of length three f(t) = jnp.array([x(t), y(t), z(t)]).\nNow, first we compute the unit tangent:\ndef get_uvec2(f):\n  tanvec = jacfwd(f)\n  return lambda x: tanvec(x)/jnp.linalg.norm(tanvec(x))\nNext, we compute the derivative of the tangent:\ndef get_cvec(f):\n  return get_uvec2(get_uvec2(f))\nThird, we compute the orthogonal frame at a point:\ndef get_frame(f):\n  tt = get_uvec2(f)\n  tt2 = get_cvec(f)\n  def first2(t):\n    x = tt(t)\n    y = tt2(t)\n    tt3 = (jnp.cross(x, y))\n    return jnp.array([x, y, tt3])\n  return first2\nwhich we use to generate a point in the circle around a given point:\ndef get_point(frame, s):\n  v1 = frame[1, :]\n  v2 = frame[2, :]\n  return jnp.cos(s) * v1 + jnp.sin(s) * v2\nAnd now we generate the point on the tubular neighborhood corresponding to a pair of parameters:\ndef get_grid(f, eps):\n  ffunc = get_frame(f)\n  def grid(t, s):\n    base = f(t)\n    frame = ffunc(t)\n    return base + eps * get_point(frame, s)\n  return grid\nAnd finally, we put it all together:\ndef get_reg_grid(f, num1, num2, eps):\n  plist = []\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  for t in tarray:\n    for s in sarray:\n      plist.append(g(t, s))\n  return jnp.vstack(plist)\nFinally, use it to compute the tubular neighborhood around a circle in the xy-plane:\nf1 = lambda x: jnp.array([jnp.cos(2 * jnp.pi * x), jnp.sin(2 * jnp.pi * x), 0.0])\n\nfff = np.array(get_reg_grid(f1, 200, 200, 0.1))\nThe good news is that it all works. The bad news is that this computation takes well over an hour. Where did I go wrong?",
        "answers": [
            "JAX and numpy share one key rule-of-thumb for getting good performance: if you are writing for loops over array values, your code will probably be slow.\nTo make your code more performant, you should replace your loops with vectorized operations. One nice feature of JAX is jax.vmap, a vectorizing transform which makes this relatively easy. You can also use jax.jit to JIT-compile your function and get even faster execution.\nHere's a modified version of your get_reg_grid function that returns the same result with much faster execution:\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['f', 'num1', 'num2'])\ndef get_reg_grid(f, num1, num2, eps):\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  g = jax.vmap(g, in_axes=(None, 0))\n  g = jax.vmap(g, in_axes=(0, None))\n  return jnp.vstack(g(tarray, sarray))\nWith this approach, your code executes in about 300 microseconds:\n%timeit get_reg_grid(f1, 200, 200, 0.1).block_until_ready()\n# 296 µs ± 157 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
        ],
        "link": "https://stackoverflow.com/questions/75872342/jax-performance-problems"
    },
    {
        "title": "How to get value of jaxlib.xla_extension.ArrayImpl",
        "question": "Using type(z1[0]) I get jaxlib.xla_extension.ArrayImpl. Printing z1[0] I get Array(0.71530414, dtype=float32). How can I get the actual number 0.71530414?\nI tried z1[0][0] because z1[0] is a kind of array with a single value, but it gives me an error: IndexError: Too many indices for array: 1 non-None/Ellipsis indices for dim 0..\nI tried also a different approach: I searched on the web if it was possible to convert from jaxnumpy array to a python list, but I didn't find an answer.\nCan someone help me to get the value inside a jaxlib.xla_extension.ArrayImpl object?",
        "answers": [
            "You can use float(x[0]) to convert x[0] to a Python float:\nIn [1]: import jax.numpy as jnp\n\nIn [2]: x = jnp.array([0.71530414])\n\nIn [3]: x\nOut[3]: Array([0.71530414], dtype=float32)\n\nIn [4]: x[0]\nOut[4]: Array(0.71530414, dtype=float32)\n\nIn [5]: float(x[0])\nOut[5]: 0.7153041362762451\nIf you're interested in converting the entire JAX array to a list of Python floats, you can use the tolist() method:\nIn [6]: x.tolist()\nOut[6]: [0.7153041362762451]",
            "You can just use z1[0].item() to get the value. It's the same as, e.g., float(z1[0]), only you don't have to know the type in advance."
        ],
        "link": "https://stackoverflow.com/questions/75867636/how-to-get-value-of-jaxlib-xla-extension-arrayimpl"
    },
    {
        "title": "JAX: unable to jnp.where with known sizes inside a pmap",
        "question": "I wanted to do a pmap of a given function, with 2D arrays that might (or might not) contain nan values. That function must then apply some operations to the finite values that exist in each row (toy examples at the end of the post).\nI know how many points (per row) contain NaNs, even before I /jax.jit/ anything. Thus, I should be able to:\nimport jax.numpy as jnp \ninds = jnp.where(jnp.isfinite(line), size= Finite_points_number) \nbut I am not able to pass the size of the elements into the pmap-ed function.\nI have tried to:\ni) pmap over over the list with the number of good points per row:\ndata_array = jnp.array([\n                   [1,2,3,4], \n                   [4,5,6, jnp.nan]\n                  ]\n                )\n\nsizes = jnp.asarray((4, 3)) # Number of valid points per row\n\ndef jitt_function(line, N):\n    \"\"\"\n    Over-simplified function to showcase the problem\n    \"\"\"\n    inds = jnp.where(jnp.isfinite(line), size=N)\n    return jnp.sum(line[inds])\n\npmap_func = jax.pmap(jitt_function,\n                in_axes=(0, 0)\n         )\n\npmap_func(data_array, sizes)\nand it fails with\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations. The error occurred while tracing the function jitt_function at [...] for pmap. This concrete value was not available in Python because it depends on the value of the argument 'N'.\nii) I have also tried to turn the number of points (N) into a static argument:\njitt_function = jax.jit(jitt_function, static_argnames=(\"N\",))\n\npmap_func = jax.pmap(jitt_function,\n                in_axes=(0, 0)\n         )\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 1) of type <class 'jax.interpreters.partial_eval.DynamicJaxprTracer'> for function jitt_function is non-hashable.\nEven if I managed to transform this into a static argument, I would still need to \"know\" the line number, so that I could access the correct number of good points.\nQuestion: Is there any way for me to do this within jax?",
        "answers": [
            "If you have a different number of points per pmap batch, then the number is not static. The result of your intended operation would be a ragged array (i.e. a 2D array whose rows have differing numbers of elements) and ragged arrays are not supported in JAX.\nIf you actually have a static number of elements—meaning an equal number in every batch—then you can use the size argument of jnp.where to do this computation. It might look something like this:\nfrom functools import partial\n\ndef jitt_function(line, N):\n    \"\"\"\n    Over-simplified function to showcase the problem\n    \"\"\"\n    inds = jnp.where(jnp.isfinite(line), size=N, fill_value=0)\n    return jnp.sum(line[inds])\n\npmap_func = jax.pmap(partial(jitt_function, N=4))\n\npmap_func(data_array)\nIf you have fewer than the specified number of entries in each batch, then one option is to specify the fill_value argument to jnp.where to pad the output. In this case, since you are taking the sum along each dimension, a fill value of zero returns the expected result."
        ],
        "link": "https://stackoverflow.com/questions/75793881/jax-unable-to-jnp-where-with-known-sizes-inside-a-pmap"
    },
    {
        "title": "What is a pure functional version of this Jax function?",
        "question": "I am writing a simple classifier in an attempt to come to grips with Jax. I want to normalize the data (it's the Iris dataset from sklearn) and my little function works, but from what I've read in the Jax the Sharp Bits documentation, I should avoid using lambda functions and iterating over vectors. I am not versed in functional programming, and I am curious if there's a better, more Jax idomatic way to do this. Here is my code so far:\nimport jax.numpy as jnp \nfrom jax import jit, vmap \n# lots of imports ... \n\niris = load_it('data', 'iris.pkl')\n\ndef normalize(data): \n    return jnp.apply_along_axis(lambda x: x/jnp.linalg.norm(x), 1, data) \n\n# TODO: use a functional style, maybe use partial\n# and get rid of the lambda ...\n\ntic = time.perf_counter() \niris_data_normal = normalize(iris.data) \ntoc = time.perf_counter() \nprint(f\"It took jax {toc - tic:0.4f} seconds.\")\nWhen I run this I get: It took jax 0.0677 seconds. Any guidance is most appreciated!",
        "answers": [
            "Your current approach looks fine to me: it is pure (the function has no side-effects) and in JAX apply_along_axis is implemented in terms of vmap, so there's no problem in terms of computational efficiency.\nIf you wanted to write a similar function using direct array operations, you could equivalently do something like this:\ndef normalize(data):\n  return data / jnp.linalg.norm(data, axis=1, keepdims=True)"
        ],
        "link": "https://stackoverflow.com/questions/75776353/what-is-a-pure-functional-version-of-this-jax-function"
    },
    {
        "title": "using jax lax scan with inputs that don't change across iterations within scan but are different each time scan is called",
        "question": "Jax lax scan operates on a function that takes two arguments, a carry and a sequence of inputs. I am wondering how scan should be called if some inputs don't change across iterations of the scan. Naively, I could create a sequence of identical inputs, but this seems wasteful/redundant and more importantly, this isn't always possible, as scan can only scan over arrays. For example, one of the inputs I want to pass to my function is a train state (e.g. from flax.training import train_state) that contains my model and its parameters, which cannot be put into array. As I say in the title, these inputs may also change each time I call scan (e.g. the model parameters will change).\nAny ideas on how best to do this?\nThanks.",
        "answers": [
            "In general, you have three possible approaches for this:\nConvert the single value into a sequence of identical inputs to scan over\nPut the single value in the carry to carry it along to each step of the scan\nClose over the single value\nHere are three examples of a computation using these strategies:\nimport jax\nimport jax.numpy as jnp\n\na = jnp.arange(5)\nb = 2\n# Strategy 1: duplicate b across sequence\ndef f(carry, xs):\n  a, b = xs\n  result = a * b\n  return carry + result, result\n\nb_seq = jnp.full_like(a, b)\n\ntotal, cumulative = jax.lax.scan(f, 0, (a, b_seq))\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\n# Strategy 2: put b in the carry\ndef f(carry, xs):\n  carry, b = carry\n  a = xs\n  result = a * b\n  return (carry + result, b), result\n\n(total, _), cumulative = jax.lax.scan(f, (0, b), a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\n# Strategy 3: close over b\nfrom functools import partial\n\ndef f(carry, xs, b):\n  a = xs\n  result = a * b\n  return carry + result, result\n\ntotal, cumulative = jax.lax.scan(partial(f, b=b), 0, a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\nWhich you use probably depends on the context of where you are using it, but I personally think closure (option 3) is probably the cleanest approach."
        ],
        "link": "https://stackoverflow.com/questions/75776268/using-jax-lax-scan-with-inputs-that-dont-change-across-iterations-within-scan-b"
    },
    {
        "title": "Adding a new layer to a stax.serial object",
        "question": "I'd like to \"convert\" the following tensorflow code in jax:\ndef mlp(L, n_list, activation, Cb, Cw):\n    model = tf.keras.Sequential()\n\n    kernel_initializers_list = []\n    kernel_initializers_list.append(tf.keras.initializers.RandomNormal(0, math.sqrt(Cw/n_list[0])))\n    for l in range(1, L): \n        kernel_initializers_list.append(tf.keras.initializers.RandomNormal(0, math.sqrt(Cw/n_list[l])))\n    kernel_initializers_list.append(tf.keras.initializers.RandomNormal(0, math.sqrt(Cw/n_list[L])))\n    bias_initializer = tf.keras.initializers.RandomNormal(stddev=math.sqrt(Cb))\n\n\n    model.add(tf.keras.layers.Dense(n_list[1], input_shape=[n_list[0]], use_bias = True, kernel_initializer = kernel_initializers_list[0],\n          bias_initializer = bias_initializer))\n    for l in range(1, L): \n        model.add(tf.keras.layers.Dense(n_list[l+1], activation=activation, use_bias = True, kernel_initializer = kernel_initializers_list[l],\n              bias_initializer = bias_initializer))\n    model.add(tf.keras.layers.Dense(n_list[L+1], use_bias = True, kernel_initializer = kernel_initializers_list[L],\n              bias_initializer = bias_initializer))\n    print(model.summary())\n    return model\nIn jax can I add a stax.Dense to the thing I get calling stax.serial() with something equivalent to tensorflow's model.add()? How can I do it?",
        "answers": [
            "Yes, you can.\n#Create new model by jax\nnet_init, net_apply = stax.serial(\n    Conv(32, (3, 3), padding='SAME'),\n    Relu,\n    Conv(64, (3, 3), padding='SAME'),\n    Relu,\n    Conv(128, (3, 3), padding='SAME'),\n    Relu,\n    Conv(256, (3, 3), padding='SAME'),\n    Relu,\n    MaxPool((2, 2)),\n    Flatten,\n    Dense(128),\n    Relu,\n    Dense(10),\n    LogSoftmax,\n)\n\nnet_init(random.PRNGKey(111), input_shape=(-1, 32, 32, 3))    \n\n#Feedfoward\ninputs, targets = batch_data\nnet_apply(params, inputs)\nThis is my reference to help you."
        ],
        "link": "https://stackoverflow.com/questions/75743490/adding-a-new-layer-to-a-stax-serial-object"
    },
    {
        "title": "fitting a model perfectly using jax in machine learning",
        "question": "Link to text file Hi I am relatively new to Machine learning, I have managed to get a model like in the image attached, I would like to know what I can do more for the model to fit perfectly[model I made],i don't know much about choosing loss function efficiently,following code was made by adding the text file to an another program made to fit a function to data\n(https://i.sstatic.net/3ldRy.png)\nthe test file contains noisy voltage measurements\n#!/usr/bin/env python3\n#\n# Fit function to data\n\nimport matplotlib.pyplot as plt\nimport numpy  as np\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, random\n\n# load some noisy data\ntest = np.loadtxt('newe.txt')\n\n\n\nN = 200\nsigma = 0.05\nx = test[:, 0]\ny = test[:, 1]\n\n#plt.plot(x,y)\n#plt.show()\n\n# Match function to data\n\ndef func(params, x):\n  # Parameterised damped oscillation\n  l, omega = params\n  # Note, we \"normalise\" parameters\n  y_pred = jnp.exp(l*10 * x) * jnp.sin(2*jnp.pi* omega*10 * x)\n  return y_pred\n\ndef loss(params, x, y):\n  # Loss function\n  y_pred = func(params, x)\n  return jnp.mean((y - y_pred)**2)\n\n# Compile loss and gradient\nc_loss = jit(loss)\nd_loss = jit(grad(loss))\n\n# One iteration of gradient descent\ndef update_params(params, x, y):\n  grads = d_loss(params, x, y)\n  params = [param - 0.1 * grad for param, grad in zip (params, grads)]\n  return params\n\n# Initialise parameters\nkey = random.PRNGKey(0)\nparams = [random.normal(key, (1,)), random.normal(key, (1,))]\n\nerr = []\nfor epoch in range(100000):\n  err.append(c_loss(params, x, y))\n  params = update_params(params, x, y)\nerr.append(c_loss(params, x, y))\n\nprint(\"Damping:  \", params[0]*10)\nprint(\"Frequency:\", params[1]*10)\n\ny_pred = func(params, x)\n\n# Plot loss and predictions                                                           \nf, ax = plt.subplots(1,2)\nax[0].semilogy(err)\nax[0].set_title(\"History\")\nax[1].plot(x, y, label=\"ground truth\")\nax[1].plot(x, y_pred, label=\"predictions\")\nax[1].legend()\nplt.show()",
        "answers": [
            "Looking at the plot you provided and your code, it seems that your model is incabable of fitting the input data. The input data have y = 1 at x = 0, but your model is an attenuated sinusoid which will always have y = 0 at x = 0, regardless of what the parameters are.\nGiven this, I suspect your minimization is working correctly, and the results you're seeing are the closest fit your model is capable of providing for the data you're plugging in. If you were hoping for a better fit to the data, you should change to a model that's capable of fitting your data."
        ],
        "link": "https://stackoverflow.com/questions/75727749/fitting-a-model-perfectly-using-jax-in-machine-learning"
    },
    {
        "title": "How to use and interpret JAX Vector-Jacobian Product (VJP) for this example?",
        "question": "I am trying to learn how to find the Jacobian of a vector-valued ODE function using JAX. I am using the examples at https://implicit-layers-tutorial.org/implicit_functions/ That page implements its own ODE integrator and associated custom forward-mode and reverse-mode Jacobian functions. I am trying to reproduce that using the official jax odeint and diffrax libraries, but both of these primarily use reverse-mode Vector Jacobian Product (VJP) instead of the forward-mode Jacobian Vector Product (JVP) for which example code is available on that page.\nHere is a code snippet that I adapted from that page:\nimport matplotlib.pyplot as plt\n\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nimport jax.numpy as jnp\nfrom jax import jit, jvp, vjp\nfrom jax.experimental.ode import odeint\n\nfrom diffrax import diffeqsolve, ODETerm, PIDController, SaveAt, Dopri5, NoAdjoint\n\n# returns time derivatives of each of our 3 state variables (vector-valued function)\ndef f(state, t, args):\n    x, y, z = state\n    rho, sigma, beta = args \n    return jnp.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z])\n\n# convenience function that calls jax-odeint given input initial conditions and parameters (this is the function that we want Jacobian/sensitivities of)\ndef evolve(y0, rho, sigma, beta): \n    return odeint(f, y0, tarr, (rho, sigma, beta))\n\n\n# set up initial conditions, timespan for integration, and fiducial parameter values\ny0 = jnp.array([5., 5., 5.])\ntarr = jnp.linspace(0, 1., 1000)\nrho = 28.\nsigma = 10.\nbeta = 8/3. \n\n\n# first just make sure evolve() works \nys = evolve(y0, rho, sigma, beta)\n\nfig, ax = plt.subplots(1,figsize=(6,4),dpi=150,subplot_kw={'projection':'3d'})   \nax.plot(ys.T[0],ys.T[1],ys.T[2],'b-',lw=0.5)\n\n# now try to take reverse-mode vector-jacobian product (VJP) since forward-mode JVP is not defined for jax-odeint\nvjp_ys, vjp_evolve = vjp(evolve,y0,rho,sigma,beta)\n\n# vjp_ys and ys are equal -- they are the solution time series of the 3 components (state variables) of y \nprint(jnp.array_equal(ys,vjp_ys))\n\n# define some perturbation in y0 and parameters \ndelta_y0 = jnp.array([0., 0., 0.])\ndelta_rho = 0.\ndelta_sigma = 0.\ndelta_beta = 1.\n\n####### THIS FAILS \n# vjp_evolve is a function but I am not sure how to use it to get perturbations delta_ys given y0/parameter variations\nvjp_evolve(delta_y0,delta_rho,delta_sigma,delta_beta)\nThat last line raises an error:\nTypeError: The function returned by `jax.vjp` applied to evolve was called with 4 arguments, but functions returned by `jax.vjp` must be called with a single argument corresponding to the single value returned by evolve (even if that returned value is a tuple or other container).\n\nFor example, if we have:\n\n  def f(x):\n    return (x, x)\n  _, f_vjp = jax.vjp(f, 1.0)\n\nthe function `f` returns a single tuple as output, and so we call `f_vjp` with a single tuple as its argument:\n\n  x_bar, = f_vjp((2.0, 2.0))\n\nIf we instead call `f_vjp(2.0, 2.0)`, with the values 'splatted out' as arguments rather than in a tuple, this error can arise.\nI suspect I am confused at the concept of reverse-mode VJP and what the input would be in the case of this vector-valued ODE. The same problem would persist if I had used diffrax solvers.\nFor what it's worth, I can reproduce the forward-mode JVP results on that website if I use a diffrax solver while specifying adjoint=NoAdjoint, so that jax.jvp can be used:\n# I am similarly confused about how to use VJP with diffrax's default reverse-mode autodiff of the ODE system\n# however I am able to use forward-mode JVP with diffrax's ODE solver if I specify adjoint=NoAdjoint\n\n# diffrax expects reverse order for inputs (time first, then state, then args) -- opposite of jax odeint \ndef f_diffrax(t, state, args):\n    x, y, z = state\n    rho, sigma, beta = args \n    return jnp.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z])\n\n# set up diffrax inputs as closely to jax-odeint as possible \nterms = ODETerm(f_diffrax)\nt0 = 0.0\nt1 = 1.0 \ndt0 = None\nmax_steps = 16**3 # not sure if this is needed\ntsave = SaveAt(ts=tarr,dense=True)\n\ndef evolve_diffrax(y0, rho, sigma, beta):\n    return diffeqsolve(terms,Dopri5(),t0,t1,dt0,y0,jnp.array([rho,sigma,beta]),saveat=tsave,\n                       stepsize_controller=PIDController(rtol=1.4e-8,atol=1.4e-8),max_steps=max_steps,adjoint=NoAdjoint())\n\n# get solution AND differentials assuming the same changes in y0 and parameters as we tried (and failed) to get above \ndiffrax_ys, diffrax_delta_ys = jvp(evolve_diffrax, (y0,rho,sigma,beta),(delta_y0,delta_rho,delta_sigma,delta_beta))\n\n# get the actual solution arrays from the diffrax Solution objects \ndiffrax_ys = diffrax_ys.ys\ndiffrax_delta_ys = diffrax_delta_ys.ys\n\n# plot \nfig, ax = plt.subplots(1,figsize=(6,4),dpi=150,subplot_kw={'projection':'3d'})   \nax.plot(diffrax_ys.T[0],diffrax_ys.T[1],diffrax_ys.T[2],color='violet',lw=0.5)\nax.quiver(diffrax_ys.T[0][::10],diffrax_ys.T[1][::10],diffrax_ys.T[2][::10],\n          diffrax_delta_ys.T[0][::10],diffrax_delta_ys.T[1][::10],diffrax_delta_ys.T[2][::10])\n    \nThat reproduces one of the main plots of that website (showing that the ODE is very sensitive to variations in the beta parameter). So I understand the concept of forward-mode JVP (given perturbations in initial conditions and/or parameters, JVP gives the corresponding perturbation in the ODE solution as a function of time). But what does reverse-mode VJP do and what would be the correct input to the vjp_evolve function above?",
        "answers": [
            "JVP is forward-mode autodiff: given tangents of the input to the function at a primal point, it returns tangents on the outputs.\nVJP is reverse-mode autodiff: given cotangents on the output of the function at a primal point, it returns cotangents on the inputs.\nSo you can call vjp_evolve with cotangents of the same shape as vjp_ys:\nprint(vjp_evolve(jnp.ones_like(vjp_ys)))\n(Array([ 1.74762118, 26.45747015, -2.03017559], dtype=float64),\n Array(871.66349663, dtype=float64),\n Array(-83.07586548, dtype=float64),\n Array(-1754.48788565, dtype=float64))\nConceptually, JVP propagates gradients forward through a computation, while VJP propagates gradients backward. The JAX docs might be useful background for understanding the JVP & VJP transformations more deeply: https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#vector-jacobian-products-vjps-aka-reverse-mode-autodiff"
        ],
        "link": "https://stackoverflow.com/questions/75711315/how-to-use-and-interpret-jax-vector-jacobian-product-vjp-for-this-example"
    },
    {
        "title": "What is the correct way to define a vectorized (jax.vmap) function in a class?",
        "question": "I want to add a function, which is vectorized by jax.vmap, as a class method. However, I am not sure where to define this function within the class. My main goal is to avoid, that the function is being redefined each time I call the class method.\nHere is a minimal example for a class that counts how often a value occurs in a jnp.array, with a non-vectorized and vectorized version:\nimport jax.numpy as jnp\nimport jax\n\nclass ValueCounter():\n\n    def __init__(self): # for completeness, not used\n        self.attribute_1 = None\n\n    @staticmethod\n    def _count_value_in_array( # non-vectorized function\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n\n    # here comes the vectorized function\n    def count_values_in_array(self, array: jnp.array, value_array: jnp.array) -> jnp.array:\n        \"\"\"Count how often each value in an array of values occurs in an array\"\"\"\n        count_value_in_array_vec = jax.vmap(\n            self._count_value_in_array, in_axes=(None, 0)\n        ) # vectorized function is defined again each time the function is called\n        return count_value_in_array_vec(array, value_array)\nExample output & input:\nvalue_counter = ValueCounter()\nvalue_counter.count_values_in_array(jnp.array([0, 1, 2, 2, 1, 1]), jnp.array([0, 1, 2]))\nResult (correct as expected)\nArray([1, 3, 2], dtype=int32)\nThe vectorized function count_value_in_array_vecis redefined each time count_values_in_array - which seems unnecessary to me. However, I am a bit stuck on how to avoid this. Does someone know how the vectorized function could be integrated into the class in a more elegant way?",
        "answers": [
            "You can decorate the static method directly; for example:\nfrom functools import partial\n\n# ...\n\n    @staticmethod\n    @partial(jax.vmap, in_axes=(None, 0))\n    def _count_value_in_array(\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n# ..."
        ],
        "link": "https://stackoverflow.com/questions/75696639/what-is-the-correct-way-to-define-a-vectorized-jax-vmap-function-in-a-class"
    },
    {
        "title": "Returning a distribution object from a jittable function",
        "question": "I want to create a jittable function that outputs a distrax distribution object. For instance:\nimport distrax\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n   dist = distrax.Categorical(logits=jnp.sin(x))\n   return dist\n\njit_f = jax.jit(f)\na = jnp.array([1,2,3])\ndist = jit_f(a)\nCurrently this code gives me the following error:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\traceback_util.py\", line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\api.py\", line 628, in cache_miss\n    out = tree_unflatten(out_pytree_def, out_flat)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\tree_util.py\", line 75, in tree_unflatten\n    return treedef.unflatten(leaves)\n  File \"F:\\jax_env\\lib\\site-packages\\distrax\\_src\\utils\\jittable.py\", line 40, in tree_unflatten\n    obj = cls(*args, **kwargs)\n  File \"F:\\jax_env\\lib\\site-packages\\distrax\\_src\\distributions\\categorical.py\", line 60, in __init__\n    self._logits = None if logits is None else math.normalize(logits=logits)\n  File \"F:\\jax_env\\lib\\site-packages\\distrax\\_src\\utils\\math.py\", line 72, in normalize\n    return jax.nn.log_softmax(logits, axis=-1)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\traceback_util.py\", line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\api.py\", line 618, in cache_miss\n    keep_unused=keep_unused))\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\core.py\", line 2031, in call_bind_with_continuation\n    top_trace = find_top_trace(args)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\core.py\", line 1122, in find_top_trace\n    top_tracer._assert_live()\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\interpreters\\partial_eval.py\", line 1486, in _assert_live\n    raise core.escaped_tracer_error(self, None)\njax._src.traceback_util.UnfilteredStackTrace: jax._src.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[3] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was f at <stdin>:1 traced for jit.\n------------------------------\nThe leaked intermediate value was created on line <stdin>:2 (f).\n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n<stdin>:1 (<module>)\n<stdin>:2 (f)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError\nI thought that using dist = jax.block_until_ready(dist) inside f could fix the problem, but it doesn't.",
        "answers": [
            "This looks like the bug in distrax v0.1.2 reported in https://github.com/deepmind/distrax/issues/162. This wass fixed by https://github.com/deepmind/distrax/pull/177, which is part of the distrax v0.1.3 release.\nTo fix the issue, you should update to distrax v0.1.3 or later."
        ],
        "link": "https://stackoverflow.com/questions/75663449/returning-a-distribution-object-from-a-jittable-function"
    },
    {
        "title": "Python function minimization not changing optimization variables",
        "question": "I need to minimize a simple function that divides two values. The optimization paramter x is a (n,m) numpy array from which I calculate a float.\n# An initial value\nnormX0 = calculate_normX(x_start)\n\ndef objective(x) -> float:\n    \"\"\"Objective function \"\"\"\n    x = x.reshape((n,m))\n    normX = calculate_normX(x)\n    return -(float(normX) / float(normX0))\ndef calculate_normX() is a wrapper function to an external (Java-)API that takes the ndarray as an input and outputs a float, in this case, the norm of a vector. For the optimization, I was using jax and jaxopt, since it supports automatic differentiation of objective.\nsolver = NonlinearCG(fun=objective, maxiter=5, verbose=True)\nres = solver.run(x.flatten())\nor the regular scipy minimize\nobjective_jac = jax.jacrev(objective)\nminimize(objective, jac=objective_jac, x0=`x.flatten(), method='L-BFGS-B', options={'maxiter': 2})\nIn both cases, however, x is not changed during the optimization step. Even initializing x with random values the optimizer does not seem to work. I also tried other solvers like Jaxopt NonlinearCG. What am I doing wrong?",
        "answers": [
            "The external, non-JAX function call is almost certainly the source of the problem. Non-JAX function calls within JAX transforms like jacrev effectively get replaced with a trace-time constant (and in most cases will error), and so it makes sense that your optimization will not change its value.\nThe best approach would be to define your calculate_normX function using JAX rather than calling out to an external API, and then everything should work automatically.\nIf you must call to an external API, one way to do this in JAX is to use pure_callback along with custom_jvp to define the autodiff rule for your external callback. There is an example of this in the JAX docs: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html#example-pure-callback-with-custom-jvp"
        ],
        "link": "https://stackoverflow.com/questions/75651326/python-function-minimization-not-changing-optimization-variables"
    },
    {
        "title": "Caching Behavior in JAX",
        "question": "I have a function f that takes in a boolean static argument flag and performs some computation based on it's value. Below is a rough outline of this function.\n@partial(jax.jit, static_argnames=['flag'])\ndef f(x, flag):\n   # Preprocessing\n   if flag:\n      ...\n   else:\n      ...\n   # Postprocessing\nEach time f is called with a different value of flag, a recompilation of this function should be triggered. However, because flag is a boolean and can take on at most two values, it would be preferable if JAX would cache the compiled version of f for each of the possible values of flag and avoid recompilations.\nIn short, I would like JAX to compile f only two times when running following piece of code:\nflag = True\nfor i in range(100):\n   f(x, flag)\n   flag = not flag\nIs there a way to tell JAX not to throw away old compiled versions of f, each time it's called with a new value of flag? And in general, are there any caching mechanisms implemented in JAX for such scenarios? (For instance if flag is an integer, but we know beforehand that it would only ever take k distinct values, and we would like to save the compiled version of f for each of these k values)\nI know that I can use jax.lax.cond or jax.lax.switch to control the flow inside f and treat flag as a regular argument rather than a static one. But this would make the code much more bloated (and difficult to read) as there are several places within the body of f where I access flag. It would be much cleaner if I declared flag to be a static argument and then controled the caching behavior of jax.jit to avoid recompilations.",
        "answers": [
            "If I understand your question correctly, then JAX by default behaves the way you would like it to behave. Each JIT-compiled function has an LRU cache of compilations based on the shape and dtype of dynamic arguments and the hash of static arguments. You can inspect the size of this cache using the _cache_size method of the compiled function. For example:\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['flag'])\ndef f(x, flag):\n   if flag:\n       return jnp.sin(x)\n   else:\n       return jnp.cos(x)\n\nprint(f._cache_size())\n# 0\n\nx = jnp.arange(10)\nf(x, True)\nprint(f._cache_size())\n# 1\n\nf(x, False)\nprint(f._cache_size())\n# 2\n\n# Subsequent calls with the same flag value hit the cache:\nflag = True\nfor i in range(100):\n    f(x, flag)\n    flag = not flag\nprint(f._cache_size())\n# 2\nSince the size of the x argument hasn't changed, we get one cache entry for each value of flag, and the cached compilations are used in subsequent calls.\nNote however if you change the shape or dtype of the dynamic argument, you get new cache entries:\nx = jnp.arange(100)\nfor i in range(100):\n    f(x, flag)\n    flag = not flag\nprint(f._cache_size())\n# 4\nThe reason this is necessary is that, in general, functions may change its behavior based on these static quantities."
        ],
        "link": "https://stackoverflow.com/questions/75641884/caching-behavior-in-jax"
    },
    {
        "title": "Nested vmap in pmap - JAX",
        "question": "I currently can run simulations in parallel on one GPU using vmap. To speed things up, I want to batch the simulations over multiple GPU devices using pmap. However, when pmapping the vmapped function I get a tracing error.\nThe code I use to get a trajectory state is:\ntraj_state = vmap(run_trajectory, in_axes=(0, None, 0))(sim_state, timings, lambda_array)\n                                                                        \nwhere lambda_array parameterises each simulation, which is run by the function run_trajectory which runs a single simulation. I then try to nest this inside a pmap:\npmap(vmap(run_trajectory, in_axes=(0, None, 0)),in_axes=(0, None, 0))(reshaped_sim_state, timings, reshaped_lambda_array)                                                                                       \nIn doing so I get the error:\nWhile tracing the function run_trajectory for pmap, this concrete value was not available in Python because it depends on the value of the argument 'timings'.\nI'm quite new to JAX and although there are documentations on errors with traced values, I'm not very sure on how to navigate this problem.",
        "answers": [
            "vmap and pmap have slightly different APIs when it comes to in_axes. In vmap, setting in_axes=None causes inputs to be unmapped and static (i.e. un-traced), while in pmap even inputs with in_axes=None will be unmapped but still traced:\nfrom jax import vmap, pmap\nimport jax.numpy as jnp\n\ndef f(x, condition):\n  # requires untraced condition:\n  return x if condition else x + 1\n\nx = jnp.arange(4)\nvmap(f, in_axes=(0, None))(x, True)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(f, in_axes=(0, None))(x, True)\n# ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: \nTo ensure that your variable is untraced in pmap, you can partially evaluate the function; for example:\nfrom functools import partial\n\nvmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\nIn your case, applying this solution might look like this:\ndef run(sim_state, lambda_array, timings=timings):\n  return run_trajectory(sim_state, timings, lambda_array)\n\nvmap(run)(sim_state, lambda_array)\n\npmap(vmap(run))(reshaped_sim_state, reshaped_lambda_array)",
            "I can seemingly avoid this problem by passing the timing values prior to vmapping using partial, that is:\nrun_trajectory = partial(run_trajectory, starting_time=timings)\n\ntraj_state = pmap(vmap(run_trajectory, in_axes=(0, 0)))(reshaped_sim_state, reshaped_lambda_array)"
        ],
        "link": "https://stackoverflow.com/questions/75625305/nested-vmap-in-pmap-jax"
    },
    {
        "title": "looking for a tool to predict runtime of XLA-HLO computational graph",
        "question": "I'm looking for a tool to print the runtime when given the computational graph of XLA-HLO. I know there are HLO cost model (analytical model) for print the FLOPs of operator node for computational graph. But Is there any tool for print the expected runtime or any related value for runtime of XLA-HLO computational graph?\nI need a source code of it or sample usage tool for it. Thanks :)",
        "answers": [
            "If you are using JAX, you can do this using the Ahead-of-time lowering and compilation APIs to get a sense of how resource-heavy a computation is. For example:\nimport jax\nimport numpy as np\n\ndef f(M, x):\n  for i in range(10):\n    x = M @ x\n  return x\n\nM = np.random.randn(1000, 1000)\nx = np.random.randn(1000)\n\nprint(jax.jit(f).lower(M, x).compile().cost_analysis())\n[{'bytes accessed': 40080000.0,\n  'bytes accessed operand 0 {}': 40000000.0,\n  'bytes accessed operand 1 {}': 40000.0,\n  'bytes accessed output {}': 40000.0,\n  'flops': 20000000.0,\n  'optimal_seconds': 0.0,\n  'utilization operand 0 {}': 10.0,\n  'utilization operand 1 {}': 10.0}]"
        ],
        "link": "https://stackoverflow.com/questions/75552498/looking-for-a-tool-to-predict-runtime-of-xla-hlo-computational-graph"
    },
    {
        "title": "Defining the correct vectorization axes for JAX vmap with arrays of different shapes and sizes",
        "question": "Following the answer to this post, the following function that 'f_switch' that dynamically switches between multiple functions based on an index array is defined (based on 'jax.lax.switch'):\nimport jax\nfrom jax import vmap;\nimport jax.random as random\n\ndef g_0(x, y, z, u): return x + y + z + u\ndef g_1(x, y, z, u): return x * y * z * u\ndef g_2(x, y, z, u): return x - y + z - u\ndef g_3(x, y, z, u): return x / y / z / u\ng_i = [g_0, g_1, g_2, g_3]\n\n\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g)(i)\nWith input arrays: i_ar of shape (len_i,), x_ar y_ar and z_ar of shapes (len_xyz,) and u_ar of shape (len_u, len_xyz), out = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar), yields out of shape (len_i, len_xyz, len_u):\nlen_i = 50\ni_ar = random.randint(random.PRNGKey(5), shape=(len_i,), minval=0, maxval= len(g_i)) #related to \n\nlen_xyz = 3000\nx_ar = random.uniform(random.PRNGKey(0), shape=(len_xyz,))\ny_ar = random.uniform(random.PRNGKey(1), shape=(len_xyz,))\nz_ar = random.uniform(random.PRNGKey(2), shape=(len_xyz,))\n\nlen_u = 1000\nu_0 = random.uniform(random.PRNGKey(3), shape=(len_u,))\nu_1 = jnp.repeat(u_0, len_xyz)\nu_ar = u_1.reshape(len_u, len_xyz)\n\nout = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nprint('The shape of out is', out.shape)\nThis worked. **But, How can the f_switch function be defined such that the result out of out = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar) has a shape of (j_len, k_len, l_len) when the function is applied along the following axes: i_ar[j], x_ar[j], y_ar[j, k], z_ar[j, k], u_ar[l]? I am not sure about how ** Examples of these input arrays are here:\nj_len = 82;\nk_len = 20;\nl_len = 100;\ni_ar = random.randint(random.PRNGKey(0), shape=(j_len,), minval=0, maxval=len(g_i))\nx_ar = random.uniform(random.PRNGKey(1), shape=(j_len,))\ny_ar = random.uniform(random.PRNGKey(2), shape=(j_len,k_len))\nz_ar = random.uniform(random.PRNGKey(3), shape=(j_len,k_len))\nu_ar = random.uniform(random.PRNGKey(4), shape=(l_len,))\nI tried to resolve this (i.e. with given input array to get output of shape: (j_len, k_len, l_len), with a nested vmap:\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  g_map = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n  wrapper = lambda x, y, z, u: g_map(i, x, y, z, u)\n  return jax.vmap(wrapper, in_axes=(0, None, None, None, 0))(x, y, z, u)\nand to broadcast u_ar: u_ar_broadcast = jnp.broadcast_to(u_ar, (j_len, k_len, l_len)), and then apply it inside of the original f_switch. But, both of these attempts failed.",
        "answers": [
            "It looks like maybe you want something like this?\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  g = jax.vmap(g, (None, None, None, None, 0))\n  g = jax.vmap(g, (None, None, 0, 0, None))\n  g = jax.vmap(g, (0, 0, 0, 0, None))\n  return g(i, x, y, z, u)\n\nout = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nprint(out.shape)\n# (82, 20, 100)\nYou should read the in_axes from bottom to top (because the bottom vmap is the outer one, and is therefore applied to the inputs first). Schematically, you can think of the effect of the maps on the shapes as something like this:\n                               (i[82], x[82], y[82,20], z[82,20], u[100])\n(0, 0, 0, 0, None)          -> (i,     x,     y[20],    z[20],    u[100])\n(None, None, 0, 0, None)    -> (i,     x,     y,        z,        u[100])\n(None, None, None, None, 0) -> (i,     x,     y,        z,        u)\nThat said, often it is easier to rely on numpy-style broadcasting rather than on multiple nested vmaps. For example, you could also do something like this:\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g, in_axes=(0, 0, 0, 0, None))(i, x, y, z, u)\n\nout = f_switch(i_ar, x_ar[:, None, None], y_ar[:, :, None], z_ar[:, :, None], u_ar)\nprint(out.shape)\n# (82, 20, 100)"
        ],
        "link": "https://stackoverflow.com/questions/75465486/defining-the-correct-vectorization-axes-for-jax-vmap-with-arrays-of-different-sh"
    },
    {
        "title": "Efficient use of JAX for conditional function evaluation based on an array of integers",
        "question": "I want to efficiently perform conditional function evaluation based on an array of integers and other arrays with real numbers serving as input for those functions. I hope to find a JAX-based solution that provides significant performance improvements over a for-loop approach that I describe below:\nimport jax\nfrom jax import vmap;\nimport jax.numpy as jnp\nimport jax.random as random\n\ndef g_0(x, y, z, u):\n    return x + y + z + u\n\ndef g_1(x, y, z, u):\n    return x * y * z * u\n\ndef g_2(x, y, z, u):\n    return x - y + z - u\n\ndef g_3(x, y, z, u):\n    return x / y / z / u\n\ng_i = [g_0, g_1, g_2, g_3]\ng_i_jit = [jax.jit(func) for func in g_i]\n\ndef g_git(i, x, y, z, u):\n    return g_i_jit[i](x=x, y=y, z=z, u=u)\n\ndef g(i, x, y, z, u):\n    return g_i[i](x=x, y=y, z=z, u=u)\n\n\nlen_xyz = 3000\nx_ar = random.uniform(random.PRNGKey(0), shape=(len_xyz,))\ny_ar = random.uniform(random.PRNGKey(1), shape=(len_xyz,))\nz_ar = random.uniform(random.PRNGKey(2), shape=(len_xyz,))\n\nlen_u = 1000\nu_0 = random.uniform(random.PRNGKey(3), shape=(len_u,))\nu_1 = jnp.repeat(u_0, len_xyz)\nu_ar = u_1.reshape(len_u, len_xyz)\n\n\nlen_i = 50\ni_ar = random.randint(random.PRNGKey(5), shape=(len_i,), minval=0, maxval= len(g_i)) #related to g_range-1\n\n\ntotal = jnp.zeros((len_u, len_xyz))\n\nfor i in range(len_i):\n    total= total + g_git(i_ar[i], x_ar, y_ar, z_ar, u_ar)\nThe role of \"i_ar\" is to act as an index that selects one of the four functions from the list g_i. \"i_ar\" is an array of integers, with each integer representing an index into the g_i list. On the other hand, x_ar, y_ar, z_ar, and u_ar are arrays of real numbers that are inputs to the functions selected by i_ar.\nI suspect that this difference in nature between i_ar and x_ar, y_ar, z_ar, and u_ar is what could be difficult to find a JAX way that would be more efficient replacement of the for loop above'. Any ideas how to use JAX (or something else) to replace the foor loop to obtain 'total' more efficiently?\nI have tried naively using vmap:\ng_git_vmap = jax.vmap(g_git)\ntotal = jnp.zeros((len_u, len_xyz))\ntotal = jnp.sum(g_git_vmap(i_ar, x_ar, y_ar, z_ar, u_ar), axis=0)\nbut this resulted in error messages and led to nowhere.",
        "answers": [
            "Probably the best way to do this is with lax.switch, which allows dynamically switching between multiple functions based on an index array.\nHere's a comparison of your original function with an approach based on lax.switch, with timings on a Colab GPU runtime:\ndef f_original(i, x, y, z, u):\n  total = jnp.zeros((len(u), len(x)))\n  for ii in range(len(i)):\n    total += g_git(i[ii], x, y, z, u)\n  return total\n\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g)(i).sum(0)\n\nout1 = f_original(i_ar, x_ar, y_ar, z_ar, u_ar)\nout2 = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nnp.testing.assert_allclose(out1, out2, rtol=5E-3)\n\n%timeit f_original(i_ar, x_ar, y_ar, z_ar, u_ar).block_until_ready()\n# 71 ms ± 23.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n%timeit f_switch(i_ar, x_ar, y_ar, z_ar, u_ar).block_until_ready()\n# 4.69 ms ± 37.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
        ],
        "link": "https://stackoverflow.com/questions/75421210/efficient-use-of-jax-for-conditional-function-evaluation-based-on-an-array-of-in"
    },
    {
        "title": "using jax.vmap to vectorize along with broadcasting",
        "question": "Consider the following toy example:\nx = np.arange(3)\n# np.sum(np.sin(x - x[:, np.newaxis]), axis=1)\n\ncfun = lambda x: np.sum(np.sin(x - x[:, np.newaxis]), axis=1)\ncfuns = jax.vmap(cfun)\n\n# for a 2d x:\nx = np.arange(6).reshape(3,2)\ncfuns(x)\nwhere x-x[:,None] is the broadcasting part and give a 3x3 array. I want cfuns to be vectorized over each row of x.\nThe numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int64[2,2])>with<BatchTrace(level=1/0)> with\n  val = Array([[[ 0,  1],\n        [-1,  0]],\n\n       [[ 0,  1],",
        "answers": [
            "JAX transformations like vmap, jit, grad, etc. are not compatible with standard numpy operations. Instead you should use jax.numpy, which provides a similar API built on JAX-compatible operations:\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(3)\n\ncfun = lambda x: jnp.sum(jnp.sin(x - x[:, jnp.newaxis]), axis=1)\ncfuns = jax.vmap(cfun)\n\n# for a 2d x:\nx = jnp.arange(6).reshape(3,2)\n\nprint(cfuns(x))\n# [[ 0.84147096 -0.84147096]\n#  [ 0.84147096 -0.84147096]\n#  [ 0.84147096 -0.84147096]]"
        ],
        "link": "https://stackoverflow.com/questions/75386255/using-jax-vmap-to-vectorize-along-with-broadcasting"
    },
    {
        "title": "JAX best way to iterate RNGKeys?",
        "question": "In JAX I find myself needing a PRNGKey that changes on each iteration of a loop. I'm not sure of the best pattern. I've considered\na) split\nfor i in range(N):\n  rng, _ = jax.random.split(rng)\n\n  # Alternatively.\n  rng = jax.random.split(rng, 1)[0]\nb) fold_in\nfor i in range(N):\n  rng = jax.random.fold_in(rng, i)\nc) use the iterator index? seems bad since the rng doesn't depend on a prior rng.\nfor i in range(N):\n rng = jax.random.PRNGKey(i)\nWhich of these is the best pattern and why? I am leaning towards (b) as it maintains dependency on the previous rng key (e.g. passed in as an argument) but im not sure if this is really the intended use-case for jax.random.fold_in",
        "answers": [
            "JAX docs (including the PRNG design doc) recommend something similar to (a):\nfor i in range(N):\n  key, subkey = jax.random.split(key)\n  values = random.uniform(subkey, shape)\n  # key carries over to the next iteration\nThe reason this is better than splitting and throwing away the subkey is that it ensures that the streams in each iteration are independent.\nYour option (b) is also safe, and in fact is the pattern that developers had in mind when creating fold_in (see e.g. https://github.com/google/jax/discussions/12395).\nIf you have a fixed number of iterations, it may be better to do all the splits once; for example:\nfor i, key in enumerate(random.split(key, N)):\n  values = random.uniform(key, shape)\nOr if your iterations do not have sequential dependence, it's better to use vmap to vectorize the operation:\ndef f(key):\n  return random.uniform(key, shape)\n\njax.vmap(f)(random.split(key, N))"
        ],
        "link": "https://stackoverflow.com/questions/75338838/jax-best-way-to-iterate-rngkeys"
    },
    {
        "title": "Unable to import tensorflow_probability.substrates.jax",
        "question": "I am trying to import tensorflow_probability.substrates.jax (specifically to use the distributions) and getting the error shown below (it looks like a self-import). I have installed tensorflow (2.8.2), tensorflow-probability (0.14.0) and jax (0.3.25).\nTrying\nimport tensorflow_probability.substrates.jax as tfp\nI get\nImportError: cannot import name 'bijectors' from partially initialized module\n'tensorflow_probability.substrates.jax' (most likely due to a circular import)\n(/path-to-anaconda3-env/lib/python3.10/site-packages/tensorflow_probability/substrates/jax/__init__.py)\nI have tried a few different versions of tensorflow-probability with the same results.",
        "answers": [
            "This sounds like it's due to version incompatibility. tensorflow_probability v0.14 was released in Sept 2021 (history), at which point JAX's most recent release was version 0.2.20 (history). JAX has has 36 releases since then, so it's not surprising that some incompatibilities may have arisen.\nI tried in Google Colab and found that the following combination works:\nimport tensorflow\nimport tensorflow_probability\nimport jax\nprint(f\"{jax.__version__=}\")\nprint(f\"{tensorflow.__version__=}\")\nprint(f\"{tensorflow_probability.__version__=}\")\n\nimport tensorflow_probability.substrates.jax as tfp\nprint(\"loaded!\")\njax.__version__='0.3.25'\ntensorflow.__version__='2.9.2'\ntensorflow_probability.__version__='0.17.0'\nloaded!\nAnother thing that can cause similar issues is if you are working in a notebook environment and installing new versions of packages that you've already imported. If you're working in notebooks, be sure to restart your Python runtime after you install or update a package."
        ],
        "link": "https://stackoverflow.com/questions/75329624/unable-to-import-tensorflow-probability-substrates-jax"
    },
    {
        "title": "computational complexity of higher order derivatives with AD in jax",
        "question": "Let f: R -> R be an infinitely differentiable function. What is the computational complexity of calculating the first n derivatives of f in Jax? Naive chain rule would suggest that each multiplication gives a factor of 2 increase, hence the nth derivative would require at least 2^n more operations. I imagine though that clever manipulation of formal series would reduce the number of required calculations and eliminate duplications, esspecially if the derivaives are Jax jitted? Is there a different between the Jax, Tensorflow and Torch implementations?\nhttps://openreview.net/forum?id=SkxEF3FNPH discusses this topic, but doesn t provide a computational complexity.",
        "answers": [
            "What is the computational complexity of calculating the first n derivatives of f in Jax?\nThere's not much you can say in general about computational complexity of Nth derivatives. For example, with a function like jnp.sin, the Nth derivative is O[1], oscillating between negative and positive sin and cos calls as N grows. For an order-k polynomial, the Nth derivative is O[0] for N > k. Other functions may have complexity that is linear or polynomial or even exponential with N depending on the operations they contain.\nI imagine though that clever manipulation of formal series would reduce the number of required calculations and eliminate duplications, esspecially if the derivaives are Jax jitted\nYou imagine correctly! One implementation of this idea is the jax.experimental.jet module, which is an experimental transform designed for computing higher-order derivatives efficiently and accurately. It doesn't cover all JAX functions, but it may be complete enough to do what you have in mind.",
            "If L is the complexity of evaluating the scalar function f, then L*(n+1)^2 is an upper bound for the complexity of finding the first n derivatives of f as coefficients of a truncated Taylor series.\nThe general idea is that each elementary function can be implemented for truncated Taylor series in the equivalent of one or two truncated series multiplications."
        ],
        "link": "https://stackoverflow.com/questions/75324482/computational-complexity-of-higher-order-derivatives-with-ad-in-jax"
    },
    {
        "title": "How to re-write this control-flow Python function to be JAX-compatible?",
        "question": "I'm rewriting some code from Pure Python to JAX. I have a function that has if/else statements in it that depend on the value of the input variable. I know these kinds of \"control flow\" statements are not compatible with JAX. But this function is called numerous times by another larger function and that larger function will be jitted. As such, currently that larger function is raising an error when it calls the function below. Is there any way to re-write this problematic control flow function OR tell the larger function to not try to jit-compile this one?\nimport jax.numpy as jnp\nfrom jax import jit\nfrom functools import partial \n\n@partial(jit,static_argnums=(1,2,3,))\ndef test(z,z1=1.,z2=5.,z3=10.):\n    \n    if z < z1:\n        fz = 3.*jnp.sqrt(z) / (z+z1)\n    elif z >= z1 and z <= z2:\n        fz = (3./z) * z1**2 - z2 \n    elif z > z3: \n        fz = (3./z) * z3 \n        \n    return fz\n\ntest(jnp.array(2.0))\n\n\"\"\"\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function test at /tmp/ipykernel_2459178/2188894837.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument 'z'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n\"\"\"\nFollowing https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow, I tried to also include z as a static variable by doing static_argnums=(0,1,2,3) but that leads to another error:\nValueError: Non-hashable static arguments are not supported. An error occurred during a call to 'test' while trying to hash an object of type <class 'jaxlib.xla_extension.Array'>, 2.0. The error was: TypeError: unhashable type: 'Array'\nAgain, the function works fine if I comment out partial-jit decorator:\ntest(jnp.array(2.0))\n# Array(-3.5, dtype=float64, weak_type=True) \nbut the problem is that this function (jitted or not) is called within another larger function that is jitted. Is there no way to have that other function's jit bypass jitting this problematic control-flow one? Isn't there a way to only use numpy (rather than jax.numpy) inside functions so that jit basically has no effect on pure python functions called from within larger jitted functions?",
        "answers": [
            "One way to do this is by nesting calls to jax.lax.cond (although there's no way to exactly duplicate the behavior of your original function, which results in a NameError if z > z2 and z <= z3, so instead I opted to return NaN):\nimport jax.numpy as jnp\nfrom jax import lax, jit\n\n@jit\ndef test(z,z1=1.,z2=5.,z3=10.):\n  return lax.cond(\n      z < z1,\n      lambda: 3.*jnp.sqrt(z) / (z+z1),\n      lambda: lax.cond(\n          (z >= z1) & (z <= z2),\n          lambda: (3./z) * z1**2 - z2,\n          lambda: lax.cond(\n              z > z3,\n              lambda: (3./z) * z3,\n              lambda: jnp.nan\n          )\n      )\n  )\n\nprint(test(jnp.array(2.0)))\n# -3.5\nA slightly more compact approach is using jax.numpy.select:\n@jit\ndef test(z,z1=1.,z2=5.,z3=10.):\n  return jnp.select(\n      [z < z1, (z >= z1) & (z <= z2), z > z3],\n      [3.*jnp.sqrt(z) / (z+z1), (3./z) * z1**2 - z2, (3./z) * z3],\n      default=jnp.nan)\n  \nprint(test(jnp.array(2.0)))\n# -3.5\nThe difference is that in the lax.cond version, only one of the expressions is actually computed, while in the jnp.select version, all expressions are computed but only one is returned."
        ],
        "link": "https://stackoverflow.com/questions/75318915/how-to-re-write-this-control-flow-python-function-to-be-jax-compatible"
    },
    {
        "title": "Why is this JAX jitted function so much slower than the non-jitted JAX version?",
        "question": "I'm in the process of rewriting some code from pure Python to JAX. I have a function that I need to call a lot. Why is the jitted version of the following function so much slower than the non-jitted version?\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef regular(M,R,a):\n    return (3+a)*M*R**a / (4*jnp.pi * R**(3+a))\n\n@jit\ndef jitted(M,R,a):\n    return (3+a)*M*R**a / (4*jnp.pi * R**(3+a))\n\n%timeit regular(1e10,100.,-2.)\n# 346 ns ± 2.07 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n%timeit jitted(1e10,100.,-2.)\n# 4.2 µs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)",
        "answers": [
            "There is some helpful information on benchmarking in JAX's FAQ: Benchmarking JAX Code: in particular note the discussion of JAX dispatch overhead for individual operations.\nRegarding your particular example, the first thing to point out is that you're not comparing jit-compiled JAX code against non-jit-compiled JAX code; you're comparing jit-compiled JAX code against pure Python code. Because you're passing python scalars to the function, none of the operations in regular have anything to do with JAX (even jnp.pi is just a Python float), so you're just executing built-in Python arithmetic operators on Python scalars.\nIf you want to compare JAX jit to non-jit code, you can use JAX values rather than scalar values as inputs; for example:\na = jnp.array(1e10)\nb = jnp.array(100.)\nc = jnp.array(-2.)\n\n%timeit regular(a, b, c).block_until_ready()\n# 86.8 µs ± 1.56 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n%timeit jitted(a, b, c).block_until_ready()\n# 3.71 µs ± 59.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\nHere you see that JIT gives you about a 20x speedup over un-jitted JAX code.\nBut JIT or not, why is JAX so much slower than the native Python version? The reason is because each JAX function call incurs a few microseconds of dispatch overhead, while each native Python operation has much less dispatch overhead. You've written a function where the actual computations are so small they are virtually free; to first order all you are measuring is dispatch overhead.\nIn situations JAX was designed for (executing JIT-compiled sequences of operations over large arrays on accelerators), this one-time, few-microsecond dispatch cost is generally not significant in comparison to the full computation."
        ],
        "link": "https://stackoverflow.com/questions/75318282/why-is-this-jax-jitted-function-so-much-slower-than-the-non-jitted-jax-version"
    },
    {
        "title": "jax minimization with stochastically estimated gradients",
        "question": "I'm trying to use the bfgs optimizer from tensorflow_probability.substrates.jax and from jax.scipy.optimize.minimize to minimize a function f which is estimated from pseudo-random samples and has a jax.random.PRNGKey as argument. To use this function with the jax/tfp bfgs minimizer, I wrap the function inside a lambda function\nseed = 100\nkey  = jax.random.PRNGKey(seed)\nfun = lambda x: return f(x,key)\nresult = jax.scipy.optimize.minimize(fun = fun, ...)\nWhat is the best way to update the key when the minimization routine calls the function to be minimized so that I use different pseudo-random numbers in a reproducible way? Maybe a global key variable? If yes, is there an example I could follow?\nSecondly, is there a way to make the optimization stop after a certain amount of time, as one could do with a callback in scipy? I could directly use the scipy implementation of bfgs/ l-bfgs-b/ etc and use jax ony for the estimation of the function and of tis gradients, which seems to work. Is there a difference between the scipy, jax.scipy and tfp.jax bfgs implementations?\nFinally, is there a way to print the values of the arguments of fun during the bfgs optimization in jax.scipy or tfp, given that f is jitted?\nThank you!",
        "answers": [
            "There is no way to do what you're asking with jax.scipy.optimize.minimize, because the minimizer does not offer any means to track changing state between function calls, and does not provide for any inbuilt stochasticity in the optimizer.\nIf you're interested in stochastic optimization in JAX, you might try stochastic optimization in JAXOpt, which provides a much more flexible set of optimization routines.\nRegarding your second question, if you'd like to print values during the course of a jit-compiled optimization or other loop, you can use jax.debug.print."
        ],
        "link": "https://stackoverflow.com/questions/75237021/jax-minimization-with-stochastically-estimated-gradients"
    },
    {
        "title": "jax automatic differentiation",
        "question": "I have the following three functions implements in JAX.\ndef helper_1(params1):\n   ...calculations...\n   return z\n\ndef helper_2(z, params2):\n   ...calculations...\n   return y\n\ndef main(params1, params2):\n   z = helper_1(params1)\n   y = helper_2(z, params2)\n   return z,y\nI am interested in the partial derivatives of the output from main, i.e. z and y, with respect to both params1 and params2. As params1 and params2 are low dimensional and z and y are high dimensional, I am using the jax.jacfwd function.\nWhen calling\njax.jacfwd(main,argnums=(0,1))(params1,params2)\nJax computes the derivatives of z with respect to params1 (and params2, which in this case is just a bunch of zeros). My question is: does Jax recompute dz/d_param1 for the derivatives of y with respect to params1 and params2, or does it somehow figure out this has already been computed?\nI don't know if this is relevant, but the 'helper_1' function contains functions from the TensorFlow library for Jax. Thanks!",
        "answers": [
            "In general, in the situation you describe JAX's forward-mode autodiff approach will re-use the derivative of z when computing the derivative of y. If you wish, you can confirm this by looking at the jaxpr of your differentiated function:\nprint(jax.make_jaxpr(jax.jacfwd(main, (0, 1)))(params1, params2))\nThough if your function is more than moderately complicated, the output might be hard to understand.\nAs a general note, though, JAX's autodiff implementation does tend to produce a small number of unnecessary or duplicated computations. As a simple example consider this:\nimport jax\nprint(jax.make_jaxpr(jax.grad(jax.lax.sin))(1.0))\n# { lambda ; a:f32[]. let\n#     _:f32[] = sin a\n#     b:f32[] = cos a\n#     c:f32[] = mul 1.0 b\n#   in (c,) }\nHere the primal value sin(a) is computed even though it is never used in computing the final output.\nIn practice this can be addressed by wrapping your computation in jit, in which case the XLA compiler takes care of optimization, including dead code elimination and de-duplication when applicable:\nresult = jit(jax.jacfwd(main, (0, 1)))(params1, params2)"
        ],
        "link": "https://stackoverflow.com/questions/75144091/jax-automatic-differentiation"
    },
    {
        "title": "Why is JAX jit needed for jax.numpy operations?",
        "question": "From the JAX docs:\nimport jax\nimport jax.numpy as jnp\n\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(1000000)\n\nselu(x)\n\"The code above is sending one operation at a time to the accelerator. This limits the ability of the XLA compiler to optimize our functions.\"\nThe docs then proceed to wrap selu in jit:\nselu_jit = jax.jit(selu)\n\nselu_jit(x)\nAnd for some reason this improves performance significantly.\nWhy is jit even needed here? More specifically, why is the original code \"sending one operation at a time to the accelerator\"?\nI was under the impression that jax.numpy is meant for this exact purpose, oherwise we might as well be using plain old numpy? What was wrong with the original selu?\nThanks!",
        "answers": [
            "Edit: after a short discussion below I realized a more concise answer to the original question: JAX uses eager computations by default; if you want lazy evaluation—what's sometimes called graph mode in other packages—you can specify this by wrapping your function in jax.jit.\nPython is an interpreted language, which means that statements are executed one at a time. This is the sense in which the un-jitted code is sending one operation at a time to the compiler: each statement must execute and return a value before the interpreter runs the next.\nWithin a jit-compiled function, JAX replaces arrays with abstract tracers in order to determine the full sequence of operations in the function, and to send them all to XLA for compilation, where the operations may be rearranged or transformed by the compiler to make the overall execution more efficient.\nThe reason we use jax.numpy rather than normal numpy is because jax.numpy operations work with the JIT tracer machinery, whereas normal numpy operations do not.\nFor a high-level intro to how JAX and its transforms work, a good place to start is How To Think In JAX."
        ],
        "link": "https://stackoverflow.com/questions/75138443/why-is-jax-jit-needed-for-jax-numpy-operations"
    },
    {
        "title": "Given batch of samples find x,y positions - python, jax",
        "question": "I would like to find the positions of where the is a '1' in the following batch of sampled array with dimension [batch, 4,4] = [2,4,4].\nimport jax\nimport jax.numpy as jnp\n\na = jnp.array([[[0., 0., 0., 1.],\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.],\n                [0., 0., 1., 1.]],\n             \n               [[1., 0., 1., 0.],\n                [1., 0., 0., 0.],\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.]]])\nI tried going through the dimension of batches (with vmap) and use the jax function to find the coordinates with\nb = jax.vmap(jnp.where)(a)\nprint('b', b)\nbut I get an error that I don't know how to fix:\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThis Tracer was created on line /home/imi/Desktop/Backflow/backflow/src/debug.py:17 (<module>)\nI expect the following output:\nb = [[[0,3], [2,1],[2,3],[3,2],[3,3]],\n\n     [[0,0],[0,2],[1,0],[3,1],[3,3]]\nThe first line of [x,y] coordinates correspond to the positions of where there is a '1' in the first batch, and for the second line in the second batch.",
        "answers": [
            "JAX transformations like vmap require arrays to be statically-sized, so there is no way to do exactly the computation you have in mind (because the number of 1 entries, and thus the size of the output array, is data-dependent).\nBut if you know a priori that there are five entries per batch, you can do something like this:\nfrom functools import partial\nindices = jax.vmap(partial(jnp.where, size=5))(a)\nprint(jnp.stack(indices, axis=2))\n[[[0 3]\n  [2 1]\n  [2 3]\n  [3 2]\n  [3 3]]\n\n [[0 0]\n  [0 2]\n  [1 0]\n  [3 1]\n  [3 3]]]\nIf you don't know a priori how many 1 entries there are, then you have a few options: one is to avoid JAX transformations and call an un-transformed jnp.where on each batch:\nresult = [jnp.column_stack(jnp.where(b)) for b in a]\nprint(result)\n[DeviceArray([[0, 3],\n             [2, 1],\n             [2, 3],\n             [3, 2],\n             [3, 3]], dtype=int32), DeviceArray([[0, 0],\n             [0, 2],\n             [1, 0],\n             [3, 1],\n             [3, 3]], dtype=int32)]\nNote that for this case, it's not possible in general to store the results in a single array, because there may be different numbers of 1 entries in each batch, and JAX does not support ragged arrays.\nThe other option is to set the size to some maximum value, and output padded results:\nmax_size = a[0].size  # size of slice is the upper bound\nfill_value = a[0].shape  # fill with out-of-bound indices\nindices = jax.vmap(partial(jnp.where, size=max_size, fill_value=fill_value))(a)\nprint(jnp.stack(indices, axis=2))\n[[[0 3]\n  [2 1]\n  [2 3]\n  [3 2]\n  [3 3]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]]\n\n [[0 0]\n  [0 2]\n  [1 0]\n  [3 1]\n  [3 3]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]]]\nWith padded results, you could then write the remainder of your code to anticipate these padded values."
        ],
        "link": "https://stackoverflow.com/questions/75134947/given-batch-of-samples-find-x-y-positions-python-jax"
    },
    {
        "title": "Negative Sampling in JAX",
        "question": "I'm implementing a negative sampling algorithm in JAX. The idea is to sample negatives from a range excluding from this range a number of non-acceptable outputs. My current solution is close to the following:\nimport jax.numpy as jnp\nimport jax\nmax_range = 5\nn_samples = 2\ntrue_cases = jnp.array(\n    [\n        [1,2],\n        [1,4],\n        [0,5]\n    ]\n)\n# i combine the true cases in a dictionary of the following form:\nnon_acceptable_as_negatives = {\n    0: jnp.array([5]),\n    1: jnp.array([2,4]),\n    2: jnp.array([]),\n    3: jnp.array([]),\n    4: jnp.array([]),\n    5: jnp.array([])\n}\nnegatives = []\nkey = jax.random.PRNGKey(42)\nfor i in true_cases[:,0]:\n    key,use_key  = jax.random.split(key,2)\n    p = jnp.ones((max_range+1,))\n    p = p.at[non_acceptable_as_negatives[int(i)]].set(0)\n    p = p / p.sum()\n    negatives.append(\n        jax.random.choice(use_key,\n            jnp.arange(max_range+1),\n            (1, n_samples),\n            replace=False,\n            p=p,\n            )\n    )\nHowever this seems\nrather complicated and\nis not very performant as the true cases in the original contain ~200_000 entries and max range is ~ 50_000.\nHow can I improve this solution? And is there a more JAX way to store arrays of varying size which I currently store in the non_acceptable_as_negatives dict?",
        "answers": [
            "You'll generally achieve better performance in JAX (as in NumPy) if you can avoid loops and use vectorized operations instead. If I'm understanding your function correctly, I think the following does roughly the same thing, but using vmap.\nSince JAX does not support dictionary lookups based on traced values, I replaced your dict with a padded array\nimport jax.numpy as jnp\nimport jax\nmax_range = 5\nn_samples = 2\nfill_value = max_range + 1\n\ntrue_cases = jnp.array([\n  [1,2],\n  [1,4],\n  [0,5]\n])\n\nnon_acceptable_as_negatives = jnp.array([\n    [5, fill_value],\n    [2, 4],\n])\n\n@jax.vmap\ndef func(key, true_case):\n  p = jnp.ones(max_range + 1)\n  idx = true_cases[0]\n  replace = non_acceptable_as_negatives.at[idx].get(fill_value=fill_value)\n  p = p.at[replace].set(0, mode='drop')\n  return jax.random.choice(key, max_range + 1, (n_samples,), replace=False, p=p)\n\n\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, len(true_cases))\nresult = func(keys, true_cases)\nprint(result)\n[[3 1]\n [5 1]\n [1 5]]",
            "Jax array are immutable. It means that you can't edit it without copying the entire array. Here the main problem is that you create the vector p two times at each iteration. I advice you to compute the probabilities only once via numpy:\nimport numpy as np\n\nnon_acceptable_as_negatives = {\n    0: np.array([5]),\n    1: np.array([2,4]),\n    2: np.array([]),\n    3: np.array([]),\n    4: np.array([]),\n    5: np.array([])\n}\n\nprobas = np.ones((max_range+1, max_range+1))\nfor k, idx in non_acceptable_as_negatives.items():\n    for i in idx:\n        probas[k, i] = 0\nprobas = probas / probas.sum(axis=1, keepdims=True)\nprobas = jnp.array(probas)\nThen, to further speed-up the algorithm, you can compile the choice function. You can try:\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef sample(key, max_range, probas):\n    key, use_key  = jax.random.split(key, 2)\n    return jax.random.choice(use_key,\n            jnp.arange(max_range+1),\n            (1, n_samples),\n            replace=False,\n            p=probas[i],\n            ), key\nAnd finally:\nfor i in true_cases[:,0]:\n    neg, key = aux(key, max_range, probas)\n    negatives.append(neg)"
        ],
        "link": "https://stackoverflow.com/questions/75081519/negative-sampling-in-jax"
    },
    {
        "title": "jax return 0 if condition, continue if not in a jitted function",
        "question": "I want to replicate this behaviour in a jitted function (the function is an example):\ndef function(x,y):\n   if y==0:\n      return x\n   return x+1\nUsing jax.lax.cond it can be obtained with:\n@jax.jit\ndef function(x,y):\n   return jax.lax.cond(y==0, lambda x: x, lambda x: x+1, x)\nThis is simple as long as whatever needs to be done is simple when y!=0 (in this case, just add 1 to x). However, if that's complex, or there are more conditions of this sort, coding gets more convoluted.\nIs there a way to get the behavior \"if y==0, return x, if not, just keep running the function. jax.lax.cond requires a new function for every condition that is applied.\nFor example, this starts to become convoluted.\ndef function(x,y):\n    if y==0:\n       return x\n    if y>0:\n       return x-y\n    if y<0:\n       return x+y\nThis starts to be messy:\n@jax.jit\ndef function(x,y):\n    jax.lax.cond(y==0, \n             lambda x,y: x, \n             lambda x,y: jax.lax.cond(x>0, lambda x,y:x-y, lambda x,y: x+y, x,y),\n             x,y)\nIs there a better way?",
        "answers": [
            "In short, no, there's no way to return early from a Python function conditioned on traced values. The pattern I typically see to avoid messy nesting is to encapsulate logic in helper functions and call them via lax.cond.\nAlternatively, if you are branching based on multiple conditions, you may be able to better express the logic in terms of lax.switch; for example:\n@jax.jit\ndef function(x, y):\n  branches = [lambda: x, lambda: x-y, lambda: x+y]\n  conditions = jnp.array([y == 0, x > 0, True])\n  return lax.switch(jnp.argmax(conditions), branches)"
        ],
        "link": "https://stackoverflow.com/questions/75071836/jax-return-0-if-condition-continue-if-not-in-a-jitted-function"
    },
    {
        "title": "Is there a way to return tuple of mixed variables in Jax helper function?",
        "question": "On my path learning Jax, I tried to achieve something like\ndef f(x):\n    return [x + 1, [1,2,3], \"Hello\"]\n\nx = 1\nnew_x, a_list, str = jnp.where(\n    x > 0,\n    test(x),\n    test(x + 1)\n)\nWell, Jax clearly does not support this. I tried searching online and went through quite a few docs, but I couldn't find a good answer.\nAny help on how can I achieve this in Jax?",
        "answers": [
            "In general, JAX functions like jnp.where only accept array arguments, not list or string arguments. Since you're using a function that is not compatible with JAX in the first place, it might be better to just avoid JAX conditionals and just use standard Python conditionals instead:\nimport jax.numpy as jnp\n\ndef f(x):\n    return [x + 1, [1,2,3], \"Hello\"]\n\nx = 1\n\nnew_x, a_list, str_ = f(x) if x > 0 else f(x + 1)"
        ],
        "link": "https://stackoverflow.com/questions/75065495/is-there-a-way-to-return-tuple-of-mixed-variables-in-jax-helper-function"
    },
    {
        "title": "Updating entire row or column of a 2D array in JAX",
        "question": "I'm new to JAX and writing code that JIT compiles is proving to be quite hard for me. I am trying to achieve the following:\nGiven an (n,n) array mat in JAX, I would like to add a (1,n) or an (n,1) array to an arbitrary row or column, respectively, of the original array mat.\nIf I wanted to add a row array, r, to the third row, the numpy equivalent would be,\n# if mat is a numpy array\nmat[2,:] = mat[2,:] + r\nThe only way I know how to update an element of an array in JAX is using array.at[i].set(). I am not sure how one can use this to update a row or a column without explicitly using a for-loop.",
        "answers": [
            "JAX arrays are immutable, so you cannot do in-place modifications of array entries. But you can accomplish similar results with the np.ndarray.at syntax. For example, the equivalent of\nmat[2,:] = mat[2,:] + r\nwould be\nmat = mat.at[2,:].set(mat[2,:] + r)\nBut you can use the add method to be more efficient in this case:\nmat = mat.at[2:].add(r)\nHere is an example of adding a row and column array to a 2D array:\nimport jax.numpy as jnp\n\nmat = jnp.zeros((5, 5))\n\n# Create 2D row & col arrays, as in question\nrow = jnp.ones(5).reshape(1, 5)\ncol = jnp.ones(5).reshape(5, 1)\n\nmat = mat.at[1:2, :].add(row)\nmat = mat.at[:, 2:3].add(col)\n\nprint(mat)\n# [[0. 0. 1. 0. 0.]\n#  [1. 1. 2. 1. 1.]\n#  [0. 0. 1. 0. 0.]\n#  [0. 0. 1. 0. 0.]\n#  [0. 0. 1. 0. 0.]]\nSee JAX Sharp Bits: In-Place Updates for more discussion of this."
        ],
        "link": "https://stackoverflow.com/questions/75043981/updating-entire-row-or-column-of-a-2d-array-in-jax"
    },
    {
        "title": "What is the proper way to update multiple indexes of 2D (or multiple dimensions) Jax array at once?",
        "question": "what is the proper way to update multiple indexes of 2D (or multiple dimension) Jax array at once?\nThis is a follow up question to my previous on batch update for an 1D Jax array with the goal to avoid creating millions of arrays during training.\nI have tried:\nx = jnp.zeros((3,3))\n\n# Update 1 index at a time\nx = x.at[2, 2].set(1) # or x = x.at[(2, 2)].set(1)\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 1.]]\n# Nice, it works.\n# but how about 2 indexes at the same time?\nx = jnp.zeros((3,3))\nx = x.at[(1, 0), (0, 1) ].set([1, 3])\nprint(x)\n[[0. 3. 0.]\n [1. 0. 0.]\n [0. 0. 0.]]\n\nIt works again, but when I tried to update 3 or more indexes,\nx = x.at[(1, 0), (0, 1), (1,1) ].set([1, 3, 6])\nprint(x)\nIndexError: Too many indices for array: 3 non-None/Ellipsis indices for dim 2.\nI have spent some time browsing through Jax's documentation, but I couldn't find the best way. Any help?",
        "answers": [
            "The values you give in .at are rows and columns, rather than pairs of rows/columns. This is hinted at in the error message referring to dim 2 (dim 0 is rows, dim 1 is columns, there is no dim 2). This should give the desired behavior\nx = x.at[(1, 0, 1), (0, 1, 1) ].set([1, 3, 6])\n[[0. 3. 0.]\n [1. 6. 0.]\n [0. 0. 0.]]"
        ],
        "link": "https://stackoverflow.com/questions/74999715/what-is-the-proper-way-to-update-multiple-indexes-of-2d-or-multiple-dimensions"
    },
    {
        "title": "jax.lax.select vs jax.numpy.where",
        "question": "Was taking a look at the dropout implementation in flax:\ndef __call__(self, inputs, deterministic: Optional[bool] = None):\n    \"\"\"Applies a random dropout mask to the input.\n\n    Args:\n      inputs: the inputs that should be randomly masked.\n      deterministic: if false the inputs are scaled by `1 / (1 - rate)` and\n        masked, whereas if true, no mask is applied and the inputs are returned\n        as is.\n\n    Returns:\n      The masked inputs reweighted to preserve mean.\n    \"\"\"\n    deterministic = merge_param(\n        'deterministic', self.deterministic, deterministic)\n\n    if (self.rate == 0.) or deterministic:\n      return inputs\n\n    # Prevent gradient NaNs in 1.0 edge-case.\n    if self.rate == 1.0:\n      return jnp.zeros_like(inputs)\n\n    keep_prob = 1. - self.rate\n    rng = self.make_rng(self.rng_collection)\n    broadcast_shape = list(inputs.shape)\n    for dim in self.broadcast_dims:\n      broadcast_shape[dim] = 1\n    mask = random.bernoulli(rng, p=keep_prob, shape=broadcast_shape)\n    mask = jnp.broadcast_to(mask, inputs.shape)\n    return lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs))\nParticularly, I'm interested in last line lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs)). Wondering why lax.select is used here instead of:\nreturn jnp.where(mask, inputs / keep_prob, 0)\nor even more simply:\nreturn mask * inputs / keep_prob",
        "answers": [
            "jnp.where is basically the same as lax.select, except more flexible in its inputs: for example, it will broadcast inputs to the same shape or cast to the same dtype, whereas lax.select requires more strict matching of inputs:\n>>> import jax.numpy as jnp\n>>> from jax import lax\n>>> x = jnp.arange(3)\n# Implicit broadcasting\n>>> jnp.where(x < 2, x[:, None], 0)\nDeviceArray([[0, 0, 0],\n             [1, 1, 0],\n             [2, 2, 0]], dtype=int32)\n\n>>> lax.select(x < 2, x[:, None], 0)\nTypeError: select cases must have the same shapes, got [(), (3, 1)].\n# Implicit type promotion\n>>> jnp.where(x < 2, jnp.zeros(3), jnp.arange(3))\nDeviceArray([0., 0., 2.], dtype=float32)\n\n>>> lax.select(x < 2, jnp.zeros(3), jnp.arange(3))\nTypeError: lax.select requires arguments to have the same dtypes, got int32, float32. (Tip: jnp.where is a similar function that does automatic type promotion on inputs).\nLibrary code is one place where the stricter semantics can be useful, because rather than smoothing-over potential implementation bugs and returning an unexpected output, it will complain loudly. But performance-wise (especially once JIT-compiled) the two are essentially equivalent.\nAs for why the flax developers chose lax.select vs. multiplying by a mask, I can think of two reasons:\nMultiplying by a mask is subject to implicit type promotion semantics, and it takes a lot more thought to anticipate problematic outputs than a simple select, which is specifically-designed for the intended operation.\nUsing multiplication causes the compiler to treat this operation as a multiplication, which it is not. A select is a much more narrow and precise operation than a multiplication, and by specifying operations precisely it often allows the compiler to optimize the results to a greater extent."
        ],
        "link": "https://stackoverflow.com/questions/74972850/jax-lax-select-vs-jax-numpy-where"
    },
    {
        "title": "Jax vmap, in_axes doesn't work if keyword argument is passed",
        "question": "The parameter in_axes in vmap seems to only work for positional arguments.\nBut throws AssertionError (with no message) called with keyword argument.\nfrom jax import vmap\nimport numpy as np\n\ndef foo(a, b, c):\n    return a * b + c\n\nfoo = vmap(foo, in_axes=(0, 0, None))\n\naj, bj = np.random.rand(2, 100, 1)\nfoo(aj, bj, 10)  # works\nfoo(aj, bj, c=10)  # throws error\nconsole\nTraceback (most recent call last):\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\jax\\_src\\api_util.py\", line 300, in flatten_axes\n    tree_map(add_leaves, _replace_nones(proxy, axis_tree), dummy)\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\jax\\_src\\tree_util.py\", line 183, in tree_map\n    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\jax\\_src\\tree_util.py\", line 183, in <listcomp>\n    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\nValueError: Tuple arity mismatch: 2 != 3; tuple: (<object object at 0x00000187F7BF4380>, <object object at 0x00000187F7BF4380>).\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-20500a2f8a08>\", line 1, in <module>\n    runfile('C:\\\\Users\\\\Amith\\\\PycharmProjects\\\\nntp\\\\tests\\\\test2.py', wdir='C:\\\\Users\\\\Amith\\\\PycharmProjects\\\\nntp\\\\tests')\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 198, in runfile\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\tests\\test2.py\", line 11, in <module>\n    foo(aj, bj, c=10)\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\jax\\_src\\traceback_util.py\", line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\jax\\_src\\api.py\", line 1481, in vmap_f\n    in_axes_flat = flatten_axes(\"vmap in_axes\", in_tree, (in_axes, 0), kws=True)\n  File \"C:\\Users\\Amith\\PycharmProjects\\nntp\\venv\\lib\\site-packages\\jax\\_src\\api_util.py\", line 306, in flatten_axes\n    assert treedef_is_leaf(leaf)\nAssertionError\nhow would one go about running foo as foo(aj, bj, c=10) without provoking the error?",
        "answers": [
            "Yes, it's true that vmap in_axes only works for positional arguments. If you want to make a more general vmapped function, the best option currently is probably to use a wrapper function. For example:\ndef _foo(a, b, c):\n    return a * b + c\n\ndef foo(a, b, c):\n  return vmap(_foo, in_axes=(0, 0, None))(a, b, c)"
        ],
        "link": "https://stackoverflow.com/questions/74969744/jax-vmap-in-axes-doesnt-work-if-keyword-argument-is-passed"
    },
    {
        "title": "JAX: JIT compatible sparse matrix slicing",
        "question": "I have a boolean sparse matrix that I represent with row indices and column indices of True values.\nimport numpy as np\nimport jax\nfrom jax import numpy as jnp\nN = 10000\nM = 1000\nX = np.random.randint(0, 100, size=(N, M)) == 0  # data setup\nrows, cols = np.where(X == True)\nrows = jax.device_put(rows)\ncols = jax.device_put(cols)\nI want to get a column slice of the matrix like X[:, 3], but just from rows indices and column indices.\nI managed to do that by using jnp.isin like below, but the problem is that this is not JIT compatible because of the data-dependent shaped array rows[cols == m].\ndef not_jit_compatible_slice(rows, cols, m):\n  return jnp.isin(jnp.arange(N), rows[cols == m])\nI could make it JIT compatible by using jnp.where in the three-argument form, but this operation is much slower than the previous one.\ndef jit_compatible_but_slow_slice(rows, cols, m):\n  return jnp.isin(jnp.arange(N), jnp.where(cols == m, rows, -1))\nIs there any fast and JIT compatible solution to acheive the same output?",
        "answers": [
            "You can do a bit better than the first answer by using the mode argument of set() to drop out-of-bound indices, eliminating the final slice:\nout = jnp.zeros(N, bool).at[jnp.where(cols==3, rows, N)].set(True, mode='drop')",
            "I figured out that the implementation below returns the same output much faster, and it’s JIT compatible.\ndef slice(rows, cols, m):\n  res = jnp.zeros(N + 1, dtype=bool)\n  res = res.at[jnp.where(cols == m, rows, -1)].set(True)\n  return res[:-1]"
        ],
        "link": "https://stackoverflow.com/questions/74946632/jax-jit-compatible-sparse-matrix-slicing"
    },
    {
        "title": "How to isolate a subset of an array in a JAX JIT-compilable way?",
        "question": "I have some datasets I want to analyze, where the data for each dataset is stored in two arrays: one for the x values and one for the y. These arrays are generally very large (~100,000 data points or more). These data are run through several (JIT-compatible) functions which perform various calculations on the y-values. These calculations scale poorly with large datasets, so I want to create a function which isolates a subset of the data before passing it to the rest of my functions (thereby enabling me to check the entire dataset as a series of subsets, which should save time). I have a working function for isolating a subset of the data, however it is not JIT-compatible, which means I have to unjit the rest of my functions (since they're all called from some jitted master function).\nI've written up a simple dummy script which demonstrates my problem:\nimport jax\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# enable 64-bit floating point representation in JAX\njax.config.update(\"jax_enable_x64\", True)\n\n\ndef isolate_subset(indices: list) -> tuple:\n    \n    # compute difference between successive x values\n    delta = x[1] - x[0]\n    \n    # get indices of start and stop of subset\n    i_start = jax.numpy.min(jax.numpy.where(jax.numpy.isclose(x, x[indices[0]], atol=delta/2))[0])\n    i_stop = jax.numpy.max(jax.numpy.where(jax.numpy.isclose(x, x[indices[1]-1], atol=delta/2))[0])\n    \n    # get subset of x within specified bounds\n    x_subset = jax.numpy.array(x[i_start:i_stop])\n    y_subset = jax.numpy.array(y[i_start:i_stop])\n    \n    # return subset of x within specified bounds\n    return x_subset, y_subset\n\n\n# @jax.jit\ndef master_function(indices: indices) -> tuple:\n    \n    # get subset of x and y within bounds params[0] to params[1]\n    x_subset, y_subset = isolate_subset(indices)\n    \n    # call some other functions to do some calculations\n    \n    # return data subset\n    return x_subset, y_subset\n    \n    \n    \n\n# create some arbitrary x and y values\nx = np.linspace(0, 10, 1000)\ny = np.sin(2*np.pi*x)\n\n# break data down into uniform subsets\nsubset_indices = np.arange(0, len(x)+1, int(len(x)/5))\n\n# for each subset\nfor i in range(0, len(subset_indices)-1):\n    \n    # get indices of subset\n    i_sub = subset_indices[i:i+2]\n    \n    # analyse subset of data\n    x_subset, y_subset = master_function(i_sub)\n    \n    # create figure\n    fig, ax = plt.subplots(tight_layout=True, dpi=200)\n    \n    # plot original (x, y) and subset of (x, y)\n    ax.plot(x, y, \"k--\", zorder=0, label=\"original\")\n    ax.plot(x_subset, y_subset, \"r-\", zorder=1, label=\"subset\")\n    \n    # label plot\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.legend()\n\nplt.show()\nThis script works when master_function() is not jitted, but when I try to JIT it I get the following error:\nTracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)>\nThe error occurred while tracing the function master_function at untitled0.py:35 for jit. This concrete value was not available in Python because it depends on the value of the argument 'indices'.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\nI thought if I used jax.numpy instead of standard Python/NumPy, I wouldn't have any issues jitting. I've been using Python for years, but I've only been using JAX for about a month or so now, and I'm still figuring out its nuances. How can I make isolate_subset() JIT-compatible?",
        "answers": [
            "To be compatible with JIT, arrays in JAX must have static shapes: i.e. the size of an array cannot depend on the values within another array. With this in mind, it is impossible to do what you are asking, because your procedure creates subset arrays whose size depends on the values in the input arrays, so such a procedure can never be done in a JIT-compatible way.\nThe typical strategy in this case is to find a way to pre-define the expected size of the subset arrays, and pad them with extra values if necessary."
        ],
        "link": "https://stackoverflow.com/questions/74941096/how-to-isolate-a-subset-of-an-array-in-a-jax-jit-compilable-way"
    },
    {
        "title": "Using vmap in jax results in a pytree related error",
        "question": "I am trying to use vmap for batching For the following code\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nimport numpy as np\nfrom jax import grad, hessian, jacobian, jacfwd\nfrom jax import vmap\nfrom jax.lax import scan\n\ndef predict(params, input):\n    y = jnp.sin(params*input)\n    return y\n\ndef loss(params, batch):\n    inputs, targets = batch\n    predictions = predict(params, inputs)\n    return jnp.sum((predictions - targets)**2)\n\ngt_inputs = np.random.random((1,20))\ngt_targets = jnp.sin(2.3 * gt_inputs)\nbatch = np.transpose([gt_inputs, gt_targets]).squeeze()\nparam_init = 0.2\ngrads = vmap(grad(loss), (None, 0), 0)(batch)\nI am getting the following error for vmap. I am unable to understand what am I doing wrong.\nFile ~/anaconda3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/api_util.py:418, in flatten_axes(name, treedef, axis_tree, kws, tupled_args)\n    414       else:\n    415         hint += (f\" In particular, you're passing in a single argument which \"\n    416                  f\"means that {name} might need to be wrapped in \"\n    417                  f\"a singleton tuple.\")\n--> 418   raise ValueError(f\"{name} specification must be a tree prefix of the \"\n    419                    f\"corresponding value, got specification {axis_tree} \"\n    420                    f\"for value tree {treedef}.{hint}\") from None\n    421 axes = [None if a is proxy else a for a in axes]\n    422 assert len(axes) == treedef.num_leaves\n\nValueError: vmap in_axes specification must be a tree prefix of the corresponding value, got specification (None, 0) for value tree PyTreeDef((*,)).",
        "answers": [
            "The error is misleading, but the issue is that you've defined your loss function to take two arguments. Try this instead:\ngrads = vmap(grad(loss), (None, 0), 0)(param_init, batch)"
        ],
        "link": "https://stackoverflow.com/questions/74915586/using-vmap-in-jax-results-in-a-pytree-related-error"
    },
    {
        "title": "How to un-JIT-compile a function which is called by a JIT-compiled function",
        "question": "I have a script which performs some calculations on some given arrays. These calculations are performed thousands of times, so naturally I want to use JAX's JIT decorator to speed up these calculations. I have several functions which are called from some \"master function,\" and I want to JIT-compile the master function. However, there is one function I don't want to be JIT-compiled because it can't be made JIT compatible (or, at least, I don't know how to make it so). Below is an example:\nimport jax\nfrom functools import partial\nimport numpy as np\n\n\ndef function(params, X):\n    # create an array of zeros with same length as x (not X)\n    # set values to -1 if corresponding value of x (not X) is between specified limits\n    # otherwise set values to zero\n    \n    values = jax.numpy.zeros(len(x))\n    \n    for i in range(len(x)):\n        if x[i] < params[1] and x[i] > params[0]:\n            values = values.at[i].set(-1)\n            \n    X.val = values\n    return X\n\n\n# @jax.jit\ndef master_function(params):\n    # vmap previous function onto x\n    \n    partial_function = partial(function, params)\n    return jax.vmap(partial_function)(x)\n\n\n# define some variables\nparams = [4, 6]\nx = np.linspace(0, 10, 100)\n\n# run master function\nnew_x = master_function(params)\n\n# print new_x array\nprint(new_x)\nIn this simple example, I have some array x. I want to then create a copy of that array, called new_x, where each value is either a 0 or a -1. If a value in x is between some bounds (specified by params), its value in new_x should be -1, and zero otherwise. When I don't JIT-compile master_function(), this script works perfectly. However, when I JIT-compile master_function, and, by extension, function, I get the following error:\nAbstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function master_function at temp.py:28 for jit. This concrete value was not available in Python because it depends on the value of the argument 'params'.\nI understand that this error is caused by the way JIT-compilation works, so I want to un-JIT-compile function() while still JIT-compiling master_function if possible.",
        "answers": [
            "You cannot normally1 call an un-jitted function from within a jit-compiled function. In your case it looks like the best solution is to rewrite your function in a way that will be JIT-compatible. You can replace your for-loop with this:\nvalues = jnp.where((x < params[1]) & (x > params[0]), -1.0, 0.0)\nSide-note, it looks like you're doing in-place modifications of the val attribute of a batch tracer, which is not a supported operation and will probably have unexpected consequences. I'd suggest writing your code using standard operations, but the intent of your code is not clear to me so I'm not sure what change to suggest.\n1 this actually is possible using pure_callback, but probably is not what you want because it comes with performance penalties."
        ],
        "link": "https://stackoverflow.com/questions/74908032/how-to-un-jit-compile-a-function-which-is-called-by-a-jit-compiled-function"
    },
    {
        "title": "Why does jax.grad(lambda v: jnp.linalg.norm(v-v))(jnp.ones(2)) produce nans?",
        "question": "Can someone explain the following behaviour? Is it a bug?\nfrom jax import grad\nimport jax.numpy as jnp\n\nx = jnp.ones(2)\ngrad(lambda v: jnp.linalg.norm(v-v))(x) # returns DeviceArray([nan, nan], dtype=float32)\n\ngrad(lambda v: jnp.linalg.norm(0))(x) # returns DeviceArray([0., 0.], dtype=float32)\nI've tried looking up the error online but didn't find anything relevant.\nI also skimmed through https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html",
        "answers": [
            "When you compute grad(lambda v: jnp.linalg.norm(v-v))(x), your function looks roughly like this:\nf(x) = sqrt[(x - x)^2]\nso, evaluating with the chain rule, the derivative is\ndf/dx = (x - x) / sqrt[(x - x)^2]\nwhich, when you plug-in any finite x evaluates to\n0 / sqrt(0)\nwhich is undefined, and represented by NaN in floating point arithmetic.\nWhen you compute grad(lambda v: jnp.linalg.norm(0))(x), your function looks roughly like this:\ng(x) = sqrt[0.0^2]\nand because it has no dependence on x the derivative is simply\ndg/dx = 0.0\nDoes that answer your question?"
        ],
        "link": "https://stackoverflow.com/questions/74864427/why-does-jax-gradlambda-v-jnp-linalg-normv-vjnp-ones2-produce-nans"
    },
    {
        "title": "I don't know how to do automatic differentiation of a neural network with two outputs",
        "question": "I would like to create a physics-informed neural network (PINN) using JAX. I have created a neural network with one input (x) and two outputs (y1, y2), how do I differentiate y1 by x and y2 by x? I know that I can differentiate y by x in a neural network with one input (x) and one output (y) by using the following method, but I do not know how to do it in a NN with two outputs.\njax.grad(NN_model)",
        "answers": [
            "jax.grad is designed for differentiating functions that output a single scalar. For more general functions, you may be able to compute what you have in mind using jax.jacobian."
        ],
        "link": "https://stackoverflow.com/questions/74860484/i-dont-know-how-to-do-automatic-differentiation-of-a-neural-network-with-two-ou"
    },
    {
        "title": "Purpose of stop gradient in `jax.nn.softmax`?",
        "question": "jax.nn.softmax is defined as:\ndef softmax(x: Array,\n            axis: Optional[Union[int, Tuple[int, ...]]] = -1,\n            where: Optional[Array] = None,\n            initial: Optional[Array] = None) -> Array:\n  x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n  unnormalized = jnp.exp(x - lax.stop_gradient(x_max))\n  return unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)\nI'm particularly interested in the lax.stop_gradient(x_max) part. I would love an explanation for why it's needed. From a practical standpoint, it seems that stop_gradient doesn't change the gradient calculation:\nimport jax\nimport jax.numpy as jnp\n\ndef softmax_unstable(x):\n    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n\ndef softmax_stable(x):\n    x = x - jnp.max(x)\n    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n\ndef softmax_stop_gradient(x):\n    x = x - jax.lax.stop_gradient(jnp.max(x))\n    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n\n# example input\nx = jax.random.normal(jax.random.PRNGKey(123), (100,))\n\n# make sure all forward passes are equal\na = softmax_unstable(x)\nb = softmax_stable(x)\nc = softmax_stop_gradient(x)\nd = jax.nn.softmax(x)\nassert jnp.allclose(a, b) and jnp.allclose(b, c) and jnp.allclose(c, d)\n\n# make sure all gradient calculations are the same\na = jax.grad(lambda x: -jnp.log(softmax_unstable(x))[2])(x)\nb = jax.grad(lambda x: -jnp.log(softmax_stable(x))[2])(x)\nc = jax.grad(lambda x: -jnp.log(softmax_stop_gradient(x))[2])(x)\nd = jax.grad(lambda x: -jnp.log(jax.nn.softmax(x))[2])(x)\nassert jnp.allclose(a, b) and jnp.allclose(b, c) and jnp.allclose(c, d)\n\n# make sure all gradient calculations are the same, this time we use softmax functions twice\na = jax.grad(lambda x: -jnp.log(softmax_unstable(softmax_unstable(x)))[2])(x)\nb = jax.grad(lambda x: -jnp.log(softmax_stable(softmax_stable(x)))[2])(x)\nc = jax.grad(lambda x: -jnp.log(softmax_stop_gradient(softmax_stop_gradient(x)))[2])(x)\nd = jax.grad(lambda x: -jnp.log(jax.nn.softmax(jax.nn.softmax(x)))[2])(x)\nassert jnp.allclose(a, b) and jnp.allclose(b, c) and jnp.allclose(c, d)\n^ all implementations are equal, even the one where we apply the x - x_max trick but WITHOUT stop_gradient.",
        "answers": [
            "First off, the reason for subtracting x_max at all is because it prevents overflow for large inputs. For example:\nx = jnp.array([1, 2, 1000])\n\nprint(softmax_unstable(x))\n# [ 0.  0. nan]\nprint(softmax_stable(x))\n# [0. 0. 1.]\nprint(softmax_stop_gradient(x))\n# [0. 0. 1.]\nAs for why we use stop_gradient here, we can show analytically that the max(x) term cancels-out in the gradient computation, and so we know a priori that its gradient cannot affect the gradient of the overall function. Marking it as stop_gradient communicates this to JAX's autodiff machinery, leading to a more efficient gradient computation. You can see this efficiency in action by printing the jaxpr for each version of the gradient function:\nx = jnp.float32(1)\nprint(jax.make_jaxpr(jax.grad(softmax_stable))(x))\n{ lambda ; a:f32[]. let\n    b:f32[] = reduce_max[axes=()] a\n    c:f32[] = reshape[dimensions=None new_sizes=()] b\n    d:bool[] = eq a c\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n    f:f32[] = reduce_sum[axes=()] e\n    g:f32[] = sub a b\n    h:f32[] = exp g\n    i:f32[] = exp g\n    j:f32[] = reduce_sum[axes=()] i\n    _:f32[] = div h j\n    k:f32[] = integer_pow[y=-2] j\n    l:f32[] = mul 1.0 k\n    m:f32[] = mul l h\n    n:f32[] = neg m\n    o:f32[] = div 1.0 j\n    p:f32[] = mul n i\n    q:f32[] = mul o h\n    r:f32[] = add_any p q\n    s:f32[] = neg r\n    t:f32[] = div s f\n    u:f32[] = mul t e\n    v:f32[] = add_any r u\n  in (v,) }\nprint(jax.make_jaxpr(jax.grad(softmax_stop_gradient))(x))\n{ lambda ; a:f32[]. let\n    b:f32[] = reduce_max[axes=()] a\n    c:f32[] = reshape[dimensions=None new_sizes=()] b\n    d:bool[] = eq a c\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n    _:f32[] = reduce_sum[axes=()] e\n    f:f32[] = stop_gradient b\n    g:f32[] = sub a f\n    h:f32[] = exp g\n    i:f32[] = exp g\n    j:f32[] = reduce_sum[axes=()] i\n    _:f32[] = div h j\n    k:f32[] = integer_pow[y=-2] j\n    l:f32[] = mul 1.0 k\n    m:f32[] = mul l h\n    n:f32[] = neg m\n    o:f32[] = div 1.0 j\n    p:f32[] = mul n i\n    q:f32[] = mul o h\n    r:f32[] = add_any p q\n  in (r,) }\nThe second version requires fewer computations to achieve the same result, because we've essentially told the autodiff machinery it does not have to worry about differentiating max(x).",
            "That is a very good question! As you already observed subtraction of maximum does not really affect softmax, and thus gradients are the same. So why stop the gradient? Well... the answer is pretty trivial - it saves compute. This way jax does not have to trace back gradients flowing through the max computation, which eventually would cancel itself out. Shorter compilation time, less compute, less strain on graph optimizer, and less chances of some weird numerical errors to kick in.\nSo it is not needed, but it is beneficial :)"
        ],
        "link": "https://stackoverflow.com/questions/74822097/purpose-of-stop-gradient-in-jax-nn-softmax"
    },
    {
        "title": "JAX: Passing a dictionary rather than arg nums to identify variables for autodifferentiation",
        "question": "I want to use JAX as a vehicle for gradient descent; however, I have a moderately large number of parameters and would prefer to pass them as a dictionary f(func, dict) rather than f(func, x1, ...xn).\nSo instead of\n# https://www.kaggle.com/code/grez911/tutorial-efficient-gradient-descent-with-jax/notebook\ndef J(X, w, b, y):\n    \"\"\"Cost function for a linear regression. A forward pass of our model.\n\n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n        y: a target vector.\n\n    Returns:\n        scalar: a cost of this solution.    \n    \"\"\"\n    y_hat = X.dot(w) + b # Predict values.\n    return ((y_hat - y)**2).mean() # Return cost.\n\nfor i in range(100):\n    w -= learning_rate * grad(J, argnums=1)(X, w, b, y)\n    b -= learning_rate * grad(J, argnums=2)(X, w, b, y)\nSomething more like\nfor i in range(100):\n    w -= learning_rate * grad(J, arg_key='w')(arg_dict)\n    b -= learning_rate * grad(J, arg_key='b')(arg_dict)\nIs this possible?\nEDIT:\nThis is my current work around solution:\n# A features matrix.\nX = np.array([\n                 [4., 7.],\n                 [1., 8.],\n                 [-5., -6.],\n                 [3., -1.],\n                 [0., 9.]\n             ])\n\n# A target column vector.\ny = np.array([\n                 [37.],\n                 [24.],\n                 [-34.], \n                 [16.],\n                 [21.]\n             ])\n\nlearning_rate = 0.01\n\nw = np.zeros((2, 1))\nb = 0.\n\nimport jax.numpy as np\nfrom jax import grad\n\ndef J(X, w, b, y):\n    \"\"\"Cost function for a linear regression. A forward pass of our model.\n\n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n        y: a target vector.\n\n    Returns:\n        scalar: a cost of this solution.    \n    \"\"\"\n    y_hat = X.dot(w) + b # Predict values.\n    return ((y_hat - y)**2).mean() # Return cost.\n\n# Define your function arguments as a dictionary\narg_dict = {\n    'X': X,\n    'w': w,\n    'b': b,\n    'y': y\n}\nidx_dict = {idx:name for idx,name in enumerate(arg_dict.keys())}\narg_arr = [arg_dict[idx_dict[idx]] for idx in range(len(arg_dict))]\n\nfor i in range(100):\n  for idx, name in idx_dict.items():\n    var = arg_dict[idx_dict[idx]]\n    var -= learning_rate * grad(J, argnums=idx) (*arg_arr)\nThe gist is that now I don't need to write grad(...) for every single variable that needs autodifferentiation.",
        "answers": [
            "Specifying autodiff argnums by name is not currently supported in JAX, although the idea is under discussion in this issue: https://github.com/google/jax/issues/10614\nUntil that is implemented, there are ways you can convert argnames to argnums automatically using inspect.signature for your function (some examples are in the linked issue), but overall it's probably simpler to do that mapping manually for your specific function."
        ],
        "link": "https://stackoverflow.com/questions/74814437/jax-passing-a-dictionary-rather-than-arg-nums-to-identify-variables-for-autodif"
    },
    {
        "title": "JAX with JIT and custom differentiation",
        "question": "I am working with JAX through numpyro. Specially, I want to use a B-spline function (e.g. implemented in scipy.interpolate.BSpline) to transform different points into a spline where the input depends on some of the parameters in the model. Thus, I need to be able to differentiate the B-spline in JAX (only in the input argument and not in the knots or the integer order (of course!)).\nI can easily use jax.custom_vjp but not when JIT is used as it is in numpyro. I looked at the following:\nhttps://github.com/google/jax/issues/1142\nhttps://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html\nand it seems like the best hope is to use a callback. Though, I cannot figure out entirely how that would work. At https://jax.readthedocs.io/en/latest/jax.experimental.host_callback.html#using-call-to-call-a-jax-function-on-another-device-with-reverse-mode-autodiff-support\nthe TensorFlow example with reverse mode autodiff seem not to use JIT.\nThe example\nHere is Python code that works without JIT (see the b_spline_basis() function):\nfrom scipy.interpolate import BSpline\nimport numpy as np\nfrom numpy import typing as npt\nfrom functools import partial\nimport jax\n\ndoubleArray = npt.NDArray[np.double]\n\n# see\n#   https://stackoverflow.com/q/74699053/5861244\n#   https://en.wikipedia.org/wiki/B-spline#Derivative_expressions\ndef _b_spline_deriv_inner(spline: BSpline, deriv_basis: doubleArray) -> doubleArray:  # type: ignore[no-any-unimported]\n    out = np.zeros((deriv_basis.shape[0], deriv_basis.shape[1] - 1))\n\n    for col_index in range(out.shape[1] - 1):\n        scale = spline.t[col_index + spline.k + 1] - spline.t[col_index + 1]\n        if scale != 0:\n            out[:, col_index] = -deriv_basis[:, col_index + 1] / scale\n\n    for col_index in range(1, out.shape[1]):\n        scale = spline.t[col_index + spline.k] - spline.t[col_index]\n        if scale != 0:\n            out[:, col_index] += deriv_basis[:, col_index] / scale\n\n    return float(spline.k) * out\n\n\ndef _b_spline_eval(spline: BSpline, x: doubleArray, deriv: int) -> doubleArray:  # type: ignore[no-any-unimported]\n    if deriv == 0:\n        return spline.design_matrix(x=x, t=spline.t, k=spline.k).todense()\n    elif spline.k <= 0:\n        return np.zeros((x.shape[0], spline.t.shape[0] - spline.k - 1))\n\n    return _b_spline_deriv_inner(\n        spline=spline,\n        deriv_basis=_b_spline_eval(\n            BSpline(t=spline.t, k=spline.k - 1, c=np.zeros(spline.c.shape[0] + 1)), x=x, deriv=deriv - 1\n        ),\n    )\n\n\n@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2))\ndef b_spline_basis(knots: doubleArray, order: int, deriv: int, x: doubleArray) -> doubleArray:\n    return _b_spline_eval(spline=BSpline(t=knots, k=order, c=np.zeros((order + knots.shape[0] - 1))), x=x, deriv=deriv)[\n        :, 1:\n    ]\n\n\ndef b_spline_basis_fwd(knots: doubleArray, order: int, deriv: int, x: doubleArray) -> tuple[doubleArray, doubleArray]:\n    spline = BSpline(t=knots, k=order, c=np.zeros(order + knots.shape[0] - 1))\n    return (\n        _b_spline_eval(spline=spline, x=x, deriv=deriv)[:, 1:],\n        _b_spline_eval(spline=spline, x=x, deriv=deriv + 1)[:, 1:],\n    )\n\n\ndef b_spline_basis_bwd(\n    knots: doubleArray, order: int, deriv: int, partials: doubleArray, grad: doubleArray\n) -> tuple[doubleArray]:\n    return (jax.numpy.sum(partials * grad, axis=1),)\n\n\nb_spline_basis.defvjp(b_spline_basis_fwd, b_spline_basis_bwd)\n\nif __name__ == \"__main__\":\n    # tests\n\n    knots = np.array([0, 0, 0, 0, 0.25, 1, 1, 1, 1])\n    x = np.array([0.1, 0.5, 0.9])\n    order = 3\n\n    def test_jax(basis: doubleArray, partials: doubleArray, deriv: int) -> None:\n        weights = jax.numpy.arange(1, basis.shape[1] + 1)\n\n        def test_func(x: doubleArray) -> doubleArray:\n            return jax.numpy.sum(jax.numpy.dot(b_spline_basis(knots=knots, order=order, deriv=deriv, x=x), weights))  # type: ignore[no-any-return]\n\n        assert np.allclose(test_func(x), np.sum(np.dot(basis, weights)))\n        assert np.allclose(jax.grad(test_func)(x), np.dot(partials, weights))\n\n    deriv0 = np.transpose(\n        np.array(\n            [\n                0.684,\n                0.166666666666667,\n                0.00133333333333333,\n                0.096,\n                0.444444444444444,\n                0.0355555555555555,\n                0.004,\n                0.351851851851852,\n                0.312148148148148,\n                0,\n                0.037037037037037,\n                0.650962962962963,\n            ]\n        ).reshape(-1, 3)\n    )\n\n    deriv1 = np.transpose(\n        np.array(\n            [\n                2.52,\n                -1,\n                -0.04,\n                1.68,\n                -0.666666666666667,\n                -0.666666666666667,\n                0.12,\n                1.22222222222222,\n                -2.29777777777778,\n                0,\n                0.444444444444444,\n                3.00444444444444,\n            ]\n        ).reshape(-1, 3)\n    )\n    test_jax(deriv0, deriv1, deriv=0)\n\n    deriv2 = np.transpose(\n        np.array(\n            [\n                -69.6,\n                4,\n                0.8,\n                9.6,\n                -5.33333333333333,\n                5.33333333333333,\n                2.4,\n                -2.22222222222222,\n                -15.3777777777778,\n                0,\n                3.55555555555556,\n                9.24444444444445,\n            ]\n        ).reshape(-1, 3)\n    )\n    test_jax(deriv1, deriv2, deriv=1)\n\n    deriv3 = np.transpose(\n        np.array(\n            [\n                504,\n                -8,\n                -8,\n                -144,\n                26.6666666666667,\n                26.6666666666667,\n                24,\n                -32.8888888888889,\n                -32.8888888888889,\n                0,\n                14.2222222222222,\n                14.2222222222222,\n            ]\n        ).reshape(-1, 3)\n    )\n    test_jax(deriv2, deriv3, deriv=2)",
        "answers": [
            "The best way to accomplish this is probably using a combination of custom_jvp and jax.pure_callback.\nUnfortunately, pure_callback is relatively new and does not have great documentation yet, but you can find examples of its use in the JAX user forums (for example here).\nCopied here for posterity, this is an example of computing the sine and cosine via numpy callbacks in jit-compatible code with custom JVP rules for autodiff.\nimport jax\nimport numpy as np\njax.config.update('jax_enable_x64', True)\n\n@jax.custom_jvp\ndef np_sin(x):\n  # Compute the sine by calling-back to np.sin on the host.\n  return jax.pure_callback(np.sin, jax.ShapeDtypeStruct(np.shape(x), np.float64), x)\n\n@np_sin.defjvp\ndef _np_sin_jvp(primals, tangents):\n  x, = primals\n  dx, = tangents\n  return np_sin(x), np_cos(x) * dx  #d sin(x) = cos(x) dx\n\n@jax.custom_jvp\ndef np_cos(x):\n  # Compute the cosine by calling-back to np.cos on the host.\n  return jax.pure_callback(np.cos, jax.ShapeDtypeStruct(np.shape(x), np.float64), x)\n\n@np_cos.defjvp\ndef _np_cos_jvp(primals, tangents):\n  x, = primals\n  dx, = tangents\n  return np_cos(x), -np_sin(x) * dx  # d cos(x) = -sin(x) dx\n\n\nprint(np_sin(1.0))\n# 0.8414709848078965\nprint(np_cos(1.0))\n# 0.5403023058681398\nprint(jax.jit(jax.grad(np_sin))(1.0))\n# 0.5403023058681398\nNote that since pure_callback operates by sending data back to the host, it will generally have a lot of overhead on accelerators like GPU and TPU, although in a single-CPU setting this kind of approach can perform well.",
            "This is a follow up to the answer provided by jakevdp (the accepted and true answer). In the concrete example, the b_spline_basis() function and the _fwd and _bwd functions can be changed to\n@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2))\ndef b_spline_basis(knots: doubleArray, order: int, deriv: int, x: doubleArray) -> doubleArray:\n    return jax.pure_callback(\n        lambda x: _b_spline_eval(\n            spline=BSpline(t=knots, k=order, c=np.zeros((order + knots.shape[0] - 1))), x=x, deriv=deriv\n        )[:, 1:],\n        jax.ShapeDtypeStruct((x.shape[0], knots.shape[0] - order - 2), np.float64),\n        x,\n    )\n\n\ndef b_spline_basis_fwd(knots: doubleArray, order: int, deriv: int, x: doubleArray) -> tuple[doubleArray, doubleArray]:\n    spline = BSpline(t=knots, k=order, c=np.zeros(order + knots.shape[0] - 1))\n    return (\n        b_spline_basis(knots=knots, order=order, deriv=deriv, x=x),\n        jax.pure_callback(\n            lambda x: _b_spline_eval(spline=spline, x=x, deriv=deriv + 1)[:, 1:],\n            jax.ShapeDtypeStruct((x.shape[0], knots.shape[0] - order - 2), np.float64),\n            x=x,\n        ),\n    )\n\n\ndef b_spline_basis_bwd(\n    knots: doubleArray, order: int, deriv: int, partials: doubleArray, grad: doubleArray\n) -> tuple[doubleArray]:\n    return (jax.numpy.sum(partials * grad, axis=1),)\nthen the test passes even after\nassert np.allclose(test_func(x), np.sum(np.dot(basis, weights)))\nassert np.allclose(jax.grad(test_func)(x), np.dot(partials, weights))\nwas changed to\nassert np.allclose(jax.jit(test_func)(x), np.sum(np.dot(basis, weights)))\nassert np.allclose(jax.jit(jax.grad(test_func))(x), np.dot(partials, weights))\nHere is the complete code for the record:\nfrom scipy.interpolate import BSpline\nimport numpy as np\nfrom numpy import typing as npt\nfrom functools import partial\nimport jax\n\njax.config.update(\"jax_enable_x64\", True)\n\n\ndoubleArray = npt.NDArray[np.double]\n\n# see\n#   https://stackoverflow.com/q/74699053/5861244\n#   https://en.wikipedia.org/wiki/B-spline#Derivative_expressions\ndef _b_spline_deriv_inner(spline: BSpline, deriv_basis: doubleArray) -> doubleArray:  # type: ignore[no-any-unimported]\n    out = np.zeros((deriv_basis.shape[0], deriv_basis.shape[1] - 1))\n\n    for col_index in range(out.shape[1] - 1):\n        scale = spline.t[col_index + spline.k + 1] - spline.t[col_index + 1]\n        if scale != 0:\n            out[:, col_index] = -deriv_basis[:, col_index + 1] / scale\n\n    for col_index in range(1, out.shape[1]):\n        scale = spline.t[col_index + spline.k] - spline.t[col_index]\n        if scale != 0:\n            out[:, col_index] += deriv_basis[:, col_index] / scale\n\n    return float(spline.k) * out\n\n\ndef _b_spline_eval(spline: BSpline, x: doubleArray, deriv: int) -> doubleArray:  # type: ignore[no-any-unimported]\n    if deriv == 0:\n        return spline.design_matrix(x=x, t=spline.t, k=spline.k).todense()\n    elif spline.k <= 0:\n        return np.zeros((x.shape[0], spline.t.shape[0] - spline.k - 1))\n\n    return _b_spline_deriv_inner(\n        spline=spline,\n        deriv_basis=_b_spline_eval(\n            BSpline(t=spline.t, k=spline.k - 1, c=np.zeros(spline.c.shape[0] + 1)), x=x, deriv=deriv - 1\n        ),\n    )\n\n\n@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2))\ndef b_spline_basis(knots: doubleArray, order: int, deriv: int, x: doubleArray) -> doubleArray:\n    return jax.pure_callback(\n        lambda x: _b_spline_eval(\n            spline=BSpline(t=knots, k=order, c=np.zeros((order + knots.shape[0] - 1))), x=x, deriv=deriv\n        )[:, 1:],\n        jax.ShapeDtypeStruct((x.shape[0], knots.shape[0] - order - 2), np.float64),\n        x,\n    )\n\n\ndef b_spline_basis_fwd(knots: doubleArray, order: int, deriv: int, x: doubleArray) -> tuple[doubleArray, doubleArray]:\n    spline = BSpline(t=knots, k=order, c=np.zeros(order + knots.shape[0] - 1))\n    return (\n        b_spline_basis(knots=knots, order=order, deriv=deriv, x=x),\n        jax.pure_callback(\n            lambda x: _b_spline_eval(spline=spline, x=x, deriv=deriv + 1)[:, 1:],\n            jax.ShapeDtypeStruct((x.shape[0], knots.shape[0] - order - 2), np.float64),\n            x=x,\n        ),\n    )\n\n\ndef b_spline_basis_bwd(\n    knots: doubleArray, order: int, deriv: int, partials: doubleArray, grad: doubleArray\n) -> tuple[doubleArray]:\n    return (jax.numpy.sum(partials * grad, axis=1),)\n\n\nb_spline_basis.defvjp(b_spline_basis_fwd, b_spline_basis_bwd)\n\n\nif __name__ == \"__main__\":\n    # tests\n\n    knots = np.array([0, 0, 0, 0, 0.25, 1, 1, 1, 1])\n    x = np.array([0.1, 0.5, 0.9])\n    order = 3\n\n    def test_jax(basis: doubleArray, partials: doubleArray, deriv: int) -> None:\n        weights = jax.numpy.arange(1, basis.shape[1] + 1)\n\n        def test_func(x: doubleArray) -> doubleArray:\n            return jax.numpy.sum(jax.numpy.dot(b_spline_basis(knots=knots, order=order, deriv=deriv, x=x), weights))  # type: ignore[no-any-return]\n\n        assert np.allclose(jax.jit(test_func)(x), np.sum(np.dot(basis, weights)))\n        assert np.allclose(jax.jit(jax.grad(test_func))(x), np.dot(partials, weights))\n\n    deriv0 = np.transpose(\n        np.array(\n            [\n                0.684,\n                0.166666666666667,\n                0.00133333333333333,\n                0.096,\n                0.444444444444444,\n                0.0355555555555555,\n                0.004,\n                0.351851851851852,\n                0.312148148148148,\n                0,\n                0.037037037037037,\n                0.650962962962963,\n            ]\n        ).reshape(-1, 3)\n    )\n\n    deriv1 = np.transpose(\n        np.array(\n            [\n                2.52,\n                -1,\n                -0.04,\n                1.68,\n                -0.666666666666667,\n                -0.666666666666667,\n                0.12,\n                1.22222222222222,\n                -2.29777777777778,\n                0,\n                0.444444444444444,\n                3.00444444444444,\n            ]\n        ).reshape(-1, 3)\n    )\n    test_jax(deriv0, deriv1, deriv=0)\n\n    deriv2 = np.transpose(\n        np.array(\n            [\n                -69.6,\n                4,\n                0.8,\n                9.6,\n                -5.33333333333333,\n                5.33333333333333,\n                2.4,\n                -2.22222222222222,\n                -15.3777777777778,\n                0,\n                3.55555555555556,\n                9.24444444444445,\n            ]\n        ).reshape(-1, 3)\n    )\n    test_jax(deriv1, deriv2, deriv=1)\n\n    deriv3 = np.transpose(\n        np.array(\n            [\n                504,\n                -8,\n                -8,\n                -144,\n                26.6666666666667,\n                26.6666666666667,\n                24,\n                -32.8888888888889,\n                -32.8888888888889,\n                0,\n                14.2222222222222,\n                14.2222222222222,\n            ]\n        ).reshape(-1, 3)\n    )\n    test_jax(deriv2, deriv3, deriv=2)"
        ],
        "link": "https://stackoverflow.com/questions/74719636/jax-with-jit-and-custom-differentiation"
    },
    {
        "title": "Is there a way to update multiple indexes of Jax array at once?",
        "question": "Since array is immutable in Jax, so when one updates N indexes, it creates N arrays with\nx = x.at[idx].set(y)\nWith hundreds of updates per training cycle, it will ultimately create hundreds of arrays if not millions. This seems a little wasteful, is there a way to update multiple index at one go? Does anyone know if there is overhead or if it's significant? Am I overlook at this?",
        "answers": [
            "You can perform multiple updates in a single operation using the syntax you mention. For example:\nimport jax.numpy as jnp\n\nx = jnp.zeros(10)\nidx = jnp.array([3, 5, 7, 9])\ny = jnp.array([1, 2, 3, 4])\n\nx = x.at[idx].set(y)\nprint(x)\n# [0. 0. 0. 1. 0. 2. 0. 3. 0. 4.]\nYou're correct that outside JIT, each update operation will create an array copy. But within JIT-compiled functions, the compiler is able to perform such updates in-place when it is possible (for example, when the original array is not referenced again). You can read more at JAX Sharp Bits: Array Updates.",
            "This sounds very like a job for scatter update. I'm not really familiar with Jax itself, but major frameworks have it:\nhttps://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html\nWhat it does in a nutshell:\nsetup your output tensor (x)\naccumulate required updates in the other tensor (y in your case)\naccumulate in list/tensor indices where to apply you updates (create tensor/list full of index)\nfeed 1)-3) to scatter_updated"
        ],
        "link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once"
    },
    {
        "title": "Callback in JAX fori_loop",
        "question": "Is it possible to have callbacks inside a function passed to JAX fori_loop?\nIn my case, the callback will save to disk some of the intermediate results produced in the function.\nI tried something like this:\ndef callback(values):\n  # do something\n\ndef diffusion_loop(i, args):\n  # do something\n  callback(results)\n  return results\n\nfinal_result, _ = jax.lax.fori_loop(0, num_steps, diffusion_loop, (arg1, arg2))\nBut then if I use final_result or whatever was saved from the callback I get an error like this\nUnfilteredStackTrace: jax._src.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[1,4,64,64] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 traced for scan.\n------------------------------\nThe leaked intermediate value was created on line /usr/local/lib/python3.8/dist-packages/diffusers/schedulers/scheduling_pndm_flax.py:508 (_get_prev_sample). \n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n<timed exec>:81 (<module>)\n<timed exec>:67 (diffusion_loop)\n/usr/local/lib/python3.8/dist-packages/diffusers/schedulers/scheduling_pndm_flax.py:264 (step)\n/usr/local/lib/python3.8/dist-packages/diffusers/schedulers/scheduling_pndm_flax.py:472 (step_plms)\n/usr/local/lib/python3.8/dist-packages/diffusers/schedulers/scheduling_pndm_flax.py:508 (_get_prev_sample)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError",
        "answers": [
            "It sounds like you want to do a callback to the host that is impure (i.e. it has a side-effect of saving values to disk) and does not return any values to the runtime. For that, one option is jax.experimental.host_callback.id_tap, discussed in the docs here.\nFor example:\nimport jax\nfrom jax.experimental import host_callback as hcb\n\ndef callback(value, transforms):\n  # do something\n  print(f\"callback: {value}\")\n\ndef diffusion_loop(i, args):\n  hcb.id_tap(callback, i)\n  return args\n\nargs = (1, 2)\nresult, _ = jax.lax.fori_loop(0, 5, diffusion_loop, args)\ncallback: 0\ncallback: 1\ncallback: 2\ncallback: 3\ncallback: 4"
        ],
        "link": "https://stackoverflow.com/questions/74687564/callback-in-jax-fori-loop"
    },
    {
        "title": "How to improve Julia's performance using just in time compilation (JIT)",
        "question": "I have been playing with JAX (automatic differentiation library in Python) and Zygote (the automatic differentiation library in Julia) to implement Gauss-Newton minimisation method. I came upon the @jit macro in Jax that runs my Python code in around 0.6 seconds compared to ~60 seconds for the version that does not use @jit. Julia ran the code in around 40 seconds. Is there an equivalent of @jit in Julia or Zygote that results is a better performance?\nHere are the codes I used:\nPython\nfrom jax import grad, jit, jacfwd\nimport jax.numpy as jnp\nimport numpy as np\nimport time\n\ndef gaussian(x, params):\n    amp = params[0]\n    mu  = params[1]\n    sigma = params[2]\n    amplitude = amp/(jnp.abs(sigma)*jnp.sqrt(2*np.pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*jnp.exp(-0.5*(arg**2))\n\ndef myjacobian(x, params):\n    return jacfwd(gaussian, argnums = 1)(x, params)\n\ndef op(jac):\n    return jnp.matmul(\n        jnp.linalg.inv(jnp.matmul(jnp.transpose(jac),jac)),\n        jnp.transpose(jac))\n                         \ndef res(x, data, params):\n    return data - gaussian(x, params)\n@jit\ndef step(x, data, params):\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    temp = jnp.matmul(jacobian_operation, residuals)\n    return params + temp\n\nN = 2000\nx = np.linspace(start = -100, stop = 100, num= N)\ndata = gaussian(x, [5.65, 25.5, 37.23])\n\nini = jnp.array([0.9, 5., 5.0])\nt1 = time.time()\nfor i in range(5000):\n    ini = step(x, data, ini)\nt2 = time.time()\nprint('t2-t1: ', t2-t1)\nini\nJulia\nusing Zygote\n\nfunction gaussian(x::Union{Vector{Float64}, Float64}, params::Vector{Float64})\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]\n    \n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x.-mu)./sigma)\n    return amplitude.*exp.(-0.5.*(arg.^2))\n    \nend\n\nfunction myjacobian(x::Vector{Float64}, params::Vector{Float64})\n    output = zeros(length(x), length(params))\n    for (index, ele) in enumerate(x)\n        output[index,:] = collect(gradient((params)->gaussian(ele, params), params))[1]\n    end\n    return output\nend\n\nfunction op(jac::Matrix{Float64})\n    return inv(jac'*jac)*jac'\nend\n\nfunction res(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    return data - gaussian(x, params)\nend\n\nfunction step(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    \n    temp = jacobian_operation*residuals\n    return params + temp\nend\n\nN = 2000\nx = collect(range(start = -100, stop = 100, length= N))\nparams = vec([5.65, 25.5, 37.23])\ndata = gaussian(x, params)\n\nini = vec([0.9, 5., 5.0])\n@time for i in range(start = 1, step = 1, length = 5000)\n    ini = step(x, data, ini)\nend\nini",
        "answers": [
            "Your Julia code doing a number of things that aren't idiomatic and are worsening your performance. This won't be a full overview, but it should give you a good idea to start.\nThe first thing is passing params as a Vector is a bad idea. This means it will have to be heap allocated, and the compiler doesn't know how long it is. Instead, use a Tuple which will allow for a lot more optimization. Secondly, don't make gaussian act on a Vector of xs. Instead, write the scalar version and broadcast it. Specifically, with these changes, you will have\nfunction gaussian(x::Number, params::NTuple{3, Float64})\n    amp, mu, sigma = params\n    \n    # The next 2 lines should probably be done outside this function, but I'll leave them here for now.\n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*exp(-0.5*(arg^2))\nend",
            "One straightforward way to speed this up is to use ForwardDiff not Zygote, since you are taking a gradient of a vector of length 3, many times. Here this gets me from 16 to 3.5 seconds, with the last factor of 2 involving Chunk(3) to improve type-stability. Perhaps this can be improved further.\nfunction myjacobian(x::Vector, params)\n    # return rand(eltype(x), length(x), length(params))  # with no gradient, takes 0.5s\n    output = zeros(eltype(x), length(x), length(params))\n    config = ForwardDiff.GradientConfig(nothing, params, ForwardDiff.Chunk(3))\n    for (i, xi) in enumerate(x)\n        # grad = gradient(p->gaussian(xi, p), params)[1]       # original, takes 16s\n        # grad = ForwardDiff.gradient(p-> gaussian(xi, p))     # ForwardDiff, takes 7s\n        grad = ForwardDiff.gradient(p-> gaussian(xi, p), params, config)  # takes 3.5s\n        copyto!(view(output,i,:), grad)  # this allows params::Tuple, OK for Zygote, no help\n    end\n    return output\nend\n# This needs gaussian.(x, Ref(params)) elsewhere to use on many x, same params\nfunction gaussian(x::Real, params)\n    # amp, mu, sigma = params  # with params::Vector this is slower, 19 sec\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]  # like this, 16 sec\n    T = typeof(x)  # avoids having (2*pi)::Float64 promote everything\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nHowever, this is still computing many small gradient arrays in a loop. It could easily compute one big gradient array instead.\nWhile in general Julia is happy to compile loops to something fast, loops that make individual arrays tend to be a bad idea. And this is especially true for Zygote, which is fastest on matlab-ish whole-array code.\nHere's how this looks, it gets me under 1s for the whole program:\nfunction gaussian(x::Real, amp::Real, mu::Real, sigma::Real)\n    T = typeof(x)\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nfunction myjacobian2(x::Vector, params)  # with this, 0.9s\n    amp = fill(params[1], length(x))\n    mu  = fill(params[2], length(x))\n    sigma = fill(params[3], length(x))  # use same sigma & different x value at each row:\n    grads = gradient((amp, mu, sigma) -> sum(gaussian.(x, amp, mu, sigma)), amp, mu, sigma)\n    hcat(grads...)\nend\n# Check that it agrees:\nmyjacobian2(x, params) ≈ myjacobian(x, params)\nWhile this has little effect on the speed, I think you probably also want op(jac::Matrix) = Hermitian(jac'*jac) \\ jac' rather than inv."
        ],
        "link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit"
    },
    {
        "title": "jax.lax.fori_loop Abstract tracer value encountered where concrete value is expected",
        "question": "I've a JAX loop that looks like this where inside the step function I use min between the two arguments\nimport jax\n\ndef step(timestep: int, order: int = 4) -> int:\n    order = min(timestep + 1, order)\n    return order\n\nnum_steps = 10\norder = 100\norder = jax.lax.fori_loop(0, num_steps, step, order)\nThe above code fails with a jax._src.errors.ConcretizationTypeError. This is is the full stacktrace:\nWARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n---------------------------------------------------------------------------\nUnfilteredStackTrace                      Traceback (most recent call last)\n<ipython-input-4-9ec280f437cb> in <module>\n      2 order = 100\n----> 3 order = jax.lax.fori_loop(0, num_steps, step, order)\n\n16 frames\n/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)\n    161     try:\n--> 162       return fun(*args, **kwargs)\n    163     except Exception as e:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in fori_loop(lower, upper, body_fun, init_val)\n   1691 \n-> 1692     (_, result), _ = scan(_fori_scan_body_fun(body_fun), (lower_, init_val),\n   1693                           None, length=upper_ - lower_)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)\n    161     try:\n--> 162       return fun(*args, **kwargs)\n    163     except Exception as e:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in scan(f, init, xs, length, reverse, unroll)\n    258   # necessary, a second time with modified init values.\n--> 259   init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n    260   new_init_flat, changed = _promote_weak_typed_inputs(init_flat, carry_avals, carry_avals_out)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in _create_jaxpr(init)\n    244     carry_avals = tuple(_map(_abstractify, init_flat))\n--> 245     jaxpr, consts, out_tree = _initial_style_jaxpr(\n    246         f, in_tree, (*carry_avals, *x_avals), \"scan\")\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/common.py in _initial_style_jaxpr(fun, in_tree, in_avals, primitive_name)\n     59                          primitive_name: Optional[str] = None):\n---> 60   jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n     61       fun, in_tree, in_avals, primitive_name)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/common.py in _initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name)\n     53   debug = pe.debug_info(fun, in_tree, False, primitive_name or \"<unknown>\")\n---> 54   jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n     55   return jaxpr, consts, out_tree()\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py in wrapper(*args, **kwargs)\n    313     with TraceAnnotation(name, **decorator_kwargs):\n--> 314       return func(*args, **kwargs)\n    315     return wrapper\n\n/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)\n   1980     main.jaxpr_stack = ()  # type: ignore\n-> 1981     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n   1982       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n\n/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)\n   1997     in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n-> 1998     ans = fun.call_wrapped(*in_tracers_)\n   1999     out_tracers = map(trace.full_raise, ans)\n\n/usr/local/lib/python3.8/dist-packages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)\n    166     try:\n--> 167       ans = self.f(*args, **dict(self.params, **kwargs))\n    168     except:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in scanned_fun(loop_carry, _)\n   1607     i, x = loop_carry\n-> 1608     return (i + 1, body_fun()(i, x)), None\n   1609   return scanned_fun\n\n<ipython-input-2-2e3345899235> in step(timestep, order)\n      1 def step(timestep: int, order: int = 100) -> int:\n----> 2     order = min(timestep + 1, order)\n      3     return order\n\n/usr/local/lib/python3.8/dist-packages/jax/core.py in __bool__(self)\n    633   def __nonzero__(self): return self.aval._nonzero(self)\n--> 634   def __bool__(self): return self.aval._bool(self)\n    635   def __int__(self): return self.aval._int(self)\n\n/usr/local/lib/python3.8/dist-packages/jax/core.py in error(self, arg)\n   1266   def error(self, arg):\n-> 1267     raise ConcretizationTypeError(arg, fname_context)\n   1268   return error\n\nUnfilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 for scan. This concrete value was not available in Python because it depends on the values of the argument 'loop_carry'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nConcretizationTypeError                   Traceback (most recent call last)\n<ipython-input-4-9ec280f437cb> in <module>\n      1 num_steps = 10\n      2 order = 100\n----> 3 order = jax.lax.fori_loop(0, num_steps, step, order)\n\n<ipython-input-2-2e3345899235> in step(timestep, order)\n      1 def step(timestep: int, order: int = 100) -> int:\n----> 2     order = min(timestep + 1, order)\n      3     return order\n\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 for scan. This concrete value was not available in Python because it depends on the values of the argument 'loop_carry'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nEverything works fine if instead of using jax.lax.fori_loop i use a simple python loop, but my original code will end up very slow. How can I fix this issue?",
        "answers": [
            "Use jax.numpy.minimum in place of min:\ndef step(timestep: int, order: int = 4) -> int:\n    order = jax.numpy.minimum(timestep + 1, order)\n    return order\nThe reason min does not work is that in the course of executing code within jit, grad, vmap, fori_loop, etc., JAX replaces concrete values with abstract tracers, and Python functions like min don't know how to handle these abstract values. See How to Think in JAX for more background on this."
        ],
        "link": "https://stackoverflow.com/questions/74640431/jax-lax-fori-loop-abstract-tracer-value-encountered-where-concrete-value-is-expe"
    },
    {
        "title": "Why does GPU memory increase when recreating and reassigning a JAX numpy array to the same variable name?",
        "question": "When I recreate and reassign a JAX np array to the same variable name, for some reason the GPU memory nearly doubles the first recreation and then stays stable for subsequent recreations/reassignments.\nWhy does this happen and is this generally expected behavior for JAX arrays?\nFully runnable minimal example: https://colab.research.google.com/drive/1piUvyVylRBKm1xb1WsocsSVXJzvn5bdI?usp=sharing.\nFor posterity in case colab goes down:\n%env XLA_PYTHON_CLIENT_PREALLOCATE=false\nimport jax\nfrom jax import numpy as jnp\nfrom jax import random\n\n# First creation of jnp array\nx = jnp.ones(shape=(int(1e8),), dtype=float)\nget_gpu_memory() # the memory usage from the first call is 618 MB\n\n# Second creation of jnp array, reassigning it to the same variable name\nx = jnp.ones(shape=(int(1e8),), dtype=float)\nget_gpu_memory() # the memory usage is now 1130 MB - almost double!\n\n# Third creation of jnp array, reassigning it to the same variable name\nx = jnp.ones(shape=(int(1e8),), dtype=float)\nget_gpu_memory() # the memory usage is stable at 1130 MB.\nThank you!",
        "answers": [
            "The reason for this behavior comes from the interaction of several things:\nWithout pre-allocation, the GPU memory usage will grow as needed, but will not shrink when buffers are deleted.\nWhen you reassign a python variable, the old value still exists in memory until the Python garbage collector notices it is no longer referenced, and deletes it. This will take a small amount of time to occur in the background (you can call import gc; gc.collect() to force this to happen at any point).\nJAX sends instructions to the GPU asynchronously, meaning that once Python garbage-collects a GPU-backed value, the Python script may continue running for a short time before the corresponding buffer is actually removed from the device.\nAll of this means there's some delay between unassigning the previous x value, and that memory being freed on the device, and if you're immediately allocating a new value, the device will likely expand its memory allocation to fit the new array before the old one is deleted.\nSo why does the memory use stay constant on the third call? Well, by this time the first allocation has been removed, and so there is already space for the third allocation without growing the memory footprint.\nWith these things in mind, you can keep the allocation constant by putting a delay between deleting the old value and creating the new value; i.e. replace this:\nx = jnp.ones(shape=(int(1e8),), dtype=float)\nwith this:\ndel x\ntime.sleep(1)\nx = jnp.ones(shape=(int(1e8),), dtype=float)\nWhen I run it this way, I see constant memory usage at 618MiB."
        ],
        "link": "https://stackoverflow.com/questions/74628777/why-does-gpu-memory-increase-when-recreating-and-reassigning-a-jax-numpy-array-t"
    },
    {
        "title": "`jax.jit` not improving in place update performance for large arrays?",
        "question": "I am trying to apply a number of in place updates to a 2D matrix.\nIt appears that using jit to the in place update does not have any effect in computation time (which is many orders of magnitude longer than the equivalent numpy implementation).\nHere is code that demonstrates my problem and research.\nnode_count = 10000\n\n# NUMPY IMPLEMENTATION\nb = onp.zeros([node_count,node_count])\nprint(\"`numpy` in place update.\")\n%timeit b[1,1] = 1.\n# 86.9 ns ± 1.42 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\n# JAX IN PLACE IMPLEMENTATION\na = np.zeros([node_count,node_count])\nprint(\"`jax.np` in place update.\")\n%timeit a.at[1,1].set(1.)\n# 112 ms ± 14.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n## TEST JIT IMPLEMENTATION\ndef update(mat, index, val):\n    return mat.at[tuple(index)].set(val)\nupdate_jit = jit(update)\n\n# Run once for trace.\nupdate_jit(a, [1,1], 1.).block_until_ready()\n\nprint(\"`jax.np` jit in place update.\")\n%timeit update_jit(a, [1,1],1.).block_until_ready()\n# 99.6 ms ± 358 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)",
        "answers": [
            "This has nothing to do with inlining of inplace updates. This has to do with the fact that, unless otherwise requested, a JIT-compiled function will always return its result in a new, distinct buffer. The only exception to this is if you use buffer donation to explicitly mark that the input buffer can be re-used in the output:\nupdate_jit = jit(update, donate_argnums=[0])\nNote, however, that buffer donation is currently only available on GPU and TPU runtimes.\nYou'll not be able to use %timeit in this case, because the donated input buffer is no longer available for use after the first iteration, but you can confirm via %time that this improves the computation speed:\n# Following is run on a Colab T4 GPU runtime\n\nupdate_jit = jit(update)\n_ = update_jit(b, [1,1], 1.)\n%time _ = update_jit(b, [1,1], 1.).block_until_ready()\n# CPU times: user 607 µs, sys: 112 µs, total: 719 µs\n# Wall time: 5.89 ms\n\nupdate_jit_donate = jit(update, donate_argnums=[0])\nb = update_jit_donate(b, [1,1], 1.)\n%time _ = update_jit_donate(b, [1,1], 1.).block_until_ready()\n# CPU times: user 467 µs, sys: 86 µs, total: 553 µs\n# Wall time: 332 µs\nThe buffer donation version is still quite a bit slower than the NumPy version, but this is expected for the reasons discussed at FAQ: Is JAX Faster Than Numpy?.\nI suspect you're performing these micro-benchmarks to assure yourself that the compiler performs updates in-place within a JIT-compiled sequence of operations rather than making internal copies, as is mentioned in Sharp Bits: Array Updates. If so, you can confirm this by other means; for example:\n@jit\ndef sum(x):\n  return x.sum()\n\n@jit\ndef update_and_sum(x):\n  return x.at[0, 0].set(1).sum()\n\n_ = sum(b)\n%timeit sum(b).block_until_ready()\n# 1.66 ms ± 7.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n_ = update_and_sum(b)\n%timeit update_and_sum(b).block_until_ready()\n# 1.66 ms ± 20.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\nThe identical timings here show that the update operation is being performed in-place rather than causing the input buffer to be copied.",
            "The JIT version is slower because it does not operate in-place as opposed to the Numpy version: it creates a copy of the array and then modify the items. You can see that by growing the array: the execution time is proportional to the size of the array. You can also check the array a is left unmodified after the call to set. You can also see that the time of b.fill(42.0) is very close to the speed of the JAX function (but strangely not b.copy()). The out-of-place version is significantly slower because the RAM is slow and it takes a lot of time to operate on the whole array than just setting 1 value."
        ],
        "link": "https://stackoverflow.com/questions/74587875/jax-jit-not-improving-in-place-update-performance-for-large-arrays"
    },
    {
        "title": "index `jax` array with variable dimension",
        "question": "I am trying to write a general utility to update indices in a jax array that may have a different number of dimensions depending on the instance.\nI know that I have to use the .at[].set() methods, and this is what I have so far:\nb = np.arange(16).reshape([4,4])\nprint(b)\nupdate_indices = np.array([[1,1], [3,2], [0,3]])\nupdate_indices = np.moveaxis(update_indices, -1, 0)\nb = b.at[update_indices[0], update_indices[1]].set([333, 444, 555])\nprint(b)\nThis transforms:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]]\ninto\n[[  0   1   2 555]\n [  4 333   6   7]\n [  8   9  10  11]\n [ 12  13 444  15]]\nMy problem is that I have had to hard code the argument to at as update_indices[0], update_indices[1]. However, in general b could have an arbitrary number of dimensions so this will not work. (e.g. for a 3D array I would have to replace it with update_indices[0], update_indices[1], update_indices[2]).\nIt would be nice if I could write something like b.at[*update_indices] but this does not work.",
        "answers": [
            "This should work:\nb.at[tuple(update_indices)]"
        ],
        "link": "https://stackoverflow.com/questions/74566845/index-jax-array-with-variable-dimension"
    },
    {
        "title": "Execute function specifically on CPU in Jax",
        "question": "I have a function that will instantiate a huge array and do other things. I am running my code on TPUs so my memory is limited.\nHow can I execute my function specifically on the CPU?\nIf I do:\ny = jax.device_put(my_function(), device=jax.devices(\"cpu\")[0])\nI guess that my_function() is first executed on TPU and the result is put on CPU, which gives me memory error.\nand using jax.config.update('jax_platform_name', 'cpu') at the beginning of my code seems to have no effect.\nAlso please note that I can't modify my_function()\nThanks!",
        "answers": [
            "To directly specify the device on which a function should be executed, use the device argument of jax.jit. For example (using a GPU runtime because it's the accelerator I have access to at the moment):\nimport jax\n\ngpu_device = jax.devices('gpu')[0]\ncpu_device = jax.devices('cpu')[0]\n\ndef my_function(x):\n  return x.sum()\n\nx = jax.numpy.arange(10)\n\nx_gpu = jax.jit(my_function, device=gpu_device)(x)\nprint(x_gpu.device())\n# gpu:0\n\nx_cpu = jax.jit(my_function, device=cpu_device)(x)\nprint(x_cpu.device())\n# TFRT_CPU_0\nThis can also be controlled with the jax.default_device decorator around the call-site:\nwith jax.default_device(cpu_device):\n  print(jax.jit(my_function)(x).device())\n  # TFRT_CPU_0\n\nwith jax.default_device(gpu_device):\n  print(jax.jit(my_function)(x).device())\n  # gpu:0",
            "I'm going to make a guess here. I can't run it either so you may have to fiddle with it\nwith jax.default_device(jax.devices(\"cpu\")[0]):\n    y = my_function()\nSee the docs here and here."
        ],
        "link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax"
    },
    {
        "title": "Cannot compute simple gradient of lambda function in JAX",
        "question": "I'm trying to compute the gradient of a lambda function that involves other gradients of functions, but the computation is hanging and I do not understand why. In particular, the code below successfully computes f_next, but not its derivative (penultimate and last line). Any help would be appreciated\nimport jax\nimport jax.numpy as jnp\n\n# Model parameters\nγ = 1.5\nk = 0.1\nμY = 0.03\nσ = 0.03\nλ = 0.1\nωb = μY/λ\n\n# PDE params.\nσω = σ\n\ndt =0.01\n\nIC = lambda ω: jnp.exp(-(1-γ)*ω)\n\nf  = [IC]\n\nf_x= jax.grad(f[0]) #first derivative\nf_xx= jax.grad(jax.grad(f[0]))#second derivative\nf_old = f[0]\nf_next = lambda ω: f_old(ω) + 100*dt * (\n             (0.5*σω**2)*f_xx(ω) - λ*(ω-ωb)*f_x(ω) \n                - k*f_old(ω) + jnp.exp(-(1-γ)*ω))\nprint(f_next(0.))\nf.append(f_next)\n\nf_x= jax.grad(f[1]) #first derivative\nprint(f_x(0.))",
        "answers": [
            "It is because you're trying to define f_x using f_x in penultimate line so you are trying to compute gradient indefinitely. If you change it by:\nnew_f_x = jax.grad(f[1])\nit will work.\nBy the way, even if in your case the model parameters are constants, your functions have side effects (impure) and should not be grad them at this form. Instead you should add the parameters in your functions like that:\n# Model parameters\nparams = {'γ': 1.5,\n          'k': 0.1,\n          'μY': 0.03,\n          'σ': 0.03,\n          'λ': 0.1,\n          'ωb': 0.03 / 0.1}\n\nIC = lambda ω, params: jnp.exp(-(1-params['γ']) * ω)\n\n\ndef f_next(ω, params):\n    γ = params['γ']\n    k = params['k']\n    σ = params['σ']\n    λ = params['λ']\n    ωb = params['ωb']\n\n    # PDE params.\n    σω = σ\n    dt = 0.01\n\n    f_x = jax.grad(IC)\n    f_xx = jax.grad(jax.grad(IC))\n    return f_old(ω, params) + 100*dt * (\n        (0.5 * σω**2) * f_xx(ω, params) - λ * (ω-ωb) * f_x(ω, params)\n        - k * f_old(ω, params) + jnp.exp(-(1-γ) * ω)\n        )\n\nf = [IC]\nf_old = f[0]\n\nprint(f_next(0., params))\nf.append(f_next)\n\nnew_f_x = jax.grad(f[1])\nprint(new_f_x(0., params))\nNow you can compute the corrects gradients with other parameters with the same functions. You can even change the parameters inside f_next if needed. Note that using a dictionary of parameters as function input is very classic in Jax."
        ],
        "link": "https://stackoverflow.com/questions/74532784/cannot-compute-simple-gradient-of-lambda-function-in-jax"
    },
    {
        "title": "Test jax.pmap before deploying on multi-device hardware",
        "question": "I am coding on a single-device laptop and I am using jax.pmap because my code will run on multiple TPUs. I would like to \"fake\" having multiple devices to test my code and try different things.\nIs there any way to achieve this? Thanks!",
        "answers": [
            "You can spoof multiple XLA devices backed by a single device by setting the following environment variable:\n$ set XLA_FLAGS=\"--xla_force_host_platform_device_count=8\"\nIn Python, you could do it like this\n# Note: must set this env variable before jax is imported\nimport os\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\n\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3),\n#  CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n\nimport jax.numpy as jnp\nout = jax.pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\n# [ 0  1  4  9 16 25 36 49]\nNote that when a only a single physical device is present, all the \"devices\" here will be backed by the same threadpool. This will not improve performance of the code, but it can be useful for testing the semantics of parallel implementations on a single-device machine."
        ],
        "link": "https://stackoverflow.com/questions/74466352/test-jax-pmap-before-deploying-on-multi-device-hardware"
    },
    {
        "title": "Derivative in JAX and Sympy not coinciding",
        "question": "For this vectorial function I want to evaluate the jacobian:\nimport jax\nimport jax.numpy as jnp\n\ndef myf(arr, phi_0, phi_1, phi_2, lambda_0, R):\n    arr = jnp.deg2rad(arr)\n    phi_0 = jnp.deg2rad(phi_0)\n    phi_1 = jnp.deg2rad(phi_1)\n    phi_2 = jnp.deg2rad(phi_2)\n    lambda_0 = jnp.deg2rad(lambda_0)\n    \n    n = jnp.sin(phi_1)\n    \n    F = 2.0\n    rho_0 = 1.0\n    rho = R*F*(1/jnp.tan(jnp.pi/4 + arr[1]/2))**n\n    x_L = rho*jnp.sin(n*(arr[0] - lambda_0))\n    y_L = rho_0 - rho*jnp.cos(n*(arr[0] - lambda_0))\n    \n    return jnp.array([x_L,y_L])\n\n\narr = jnp.array([-18.1, 29.9])\n\njax.jacobian(myf)(arr, 29.5, 29.5, 29.5, -17.0, R=1)\nI obtain\n[[ 0.01312758  0.00014317]\n [-0.00012411  0.01514319]]\nI'm in shock with these values. Take for instance the element [0][0], 0.01312758. We know it's the partial of x_L with respect to the variable arr[0]. Whether by hand or using sympy that derivative is ~0.75.\nfrom sympy import *\n\nx, y = symbols('x y')\n\nx_L = (2.0*(1/tan(3.141592/4 + y/2))**0.492)*sin(0.492*(x + 0.2967))\nderiv = Derivative(x_L, x)\nderiv.doit()\nderiv.doit().evalf(subs={x: -0.3159, y: 0.52})\n\n0.752473089673695\n(inserting x, y, that are arr[0] and arr[1] already in radians). This is also the result I obtain by hand. What is happening with Jax results? I can't see what I'm doing bad.",
        "answers": [
            "Your JAX snippet inputs degrees, and so its gradient has units of 1/degrees, while your sympy snippet inputs radians, and so its gradient has units of 1/radians. If you convert the jax outputs to 1/radians (i.e. multiply the jax outputs by 180 / pi), you'll get the result you're looking for:\nresult = jax.jacobian(myf)(arr, 29.5, 29.5, 29.5, -17.0, R=1)\nprint(result * 180 / jnp.pi)\n[[ 0.7521549   0.00820279]\n [-0.00711098  0.8676407 ]]\nAlternatively, you could rewrite myf to accept inputs in units of radians and get the expected result by taking its gradient directly.",
            "Ok, I think I know what is happening... it is subtle.\nThe problem is that the conversion from degrees to rad done inside the function is not valid for jax. I think (but surely there're people who know more than me) that jax does the derivatives as soon as jax.jacobian(myf) is called and it evaluates only at last, when the values are passed (lazy evaluation, I think), so the transformation of values inside the function doesn't do anything. The correct code will be\ndef myf(arr, phi_0, phi_1, phi_2, lambda_0, R):\n   \n    n = jnp.sin(phi_1)\n    \n    F = 2.0\n    rho_0 = 1.0\n    rho = R*F*(1/jnp.tan(jnp.pi/4 + arr[1]/2))**n\n    x_L = (R*F*(1/jnp.tan(jnp.pi/4 + arr[1]/2))**n) *jnp.sin(n*(arr[0] - lambda_0))\n    y_L = rho_0 - (R*F*(1/jnp.tan(jnp.pi/4 + arr[1]/2))**n) *jnp.cos(n*(arr[0] - lambda_0))\n    \n    return jnp.array([x_L,y_L])\n\n\narr = jnp.array([-18.1, 29.9])\n\njax.jacobian(myf)(jnp.deg2rad(arr), jnp.deg2rad(29.5),\n                  jnp.deg2rad(29.5), jnp.deg2rad(29.5), jnp.deg2rad(-17.0),\n                  R=1)\n\n# [[ 0.7521549   0.00820279]\n#  [-0.00711098  0.8676407 ]]"
        ],
        "link": "https://stackoverflow.com/questions/74445921/derivative-in-jax-and-sympy-not-coinciding"
    },
    {
        "title": "Obtaining zeros in this derivative in Jax",
        "question": "Implementing a jacobian for a polar to cartesian coordinates, I obtain an array of zeros in Jax, which it can't be\ntheta = np.pi/4\nr = 4.0\n    \nvar = np.array([r, theta])\n\nx = var[0]*jnp.cos(var[1])\ny = var[0]*jnp.sin(var[1])\n\ndef f(var):\n    return np.array([x, y])\n    \njac = jax.jacobian(f)(var)\njac\n\n#DeviceArray([[0., 0.],\n#             [0., 0.]], dtype=float32)\nWhat am I missing?",
        "answers": [
            "Your function has no dependence on var because x, y are defined outside the function.\nThis would give the desired output instead:\ntheta = np.pi/4\nr = 4.0\n    \nvar = np.array([r, theta])\n\ndef f(var):\n    x = var[0]*jnp.cos(var[1])\n    y = var[0]*jnp.sin(var[1])\n    return jnp.array([x, y])\n    \njac = jax.jacobian(f)(var)\njac\nNote that you need to return a jax numpy array rather than a numpy array as well."
        ],
        "link": "https://stackoverflow.com/questions/74403765/obtaining-zeros-in-this-derivative-in-jax"
    },
    {
        "title": "Getting a type error while using fori_loop with JAX",
        "question": "I'm developing a code using JAX, and I wanted to JIT some parts of that had big loops. I didn't want the code to be unrolled so I used fori_loop, but I'm getting an error and can't figure out what I am doing wrong.\nThe error is:\n  self.arr = self.arr.reshape(new_shape+new_shape)\nTypeError: 'aval_method' object is not callable\nI was able to reduce the code to the following:\nimport jax.numpy as jnp\nimport jax\n\nclass UB():\n    def __init__(self, arr, new_shape):\n\n        self.arr = arr\n        self.shape = new_shape\n        if type(arr) is not object:\n            self.arr = self.arr.reshape(new_shape+new_shape)\n\n        \n    def _tree_flatten(self):\n        children = (self.arr,)  # arrays / dynamic values\n        aux_data = {\n            'new_shape': self.shape            \n        }  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\n\nclass UM():\n    def __init__(self, arr, r=None):\n\n        self.arr = arr\n        self.r = tuple(r)\n    \n    def _tree_flatten(self):\n        children = (self.arr,)  # arrays / dynamic values\n        aux_data = {\n            'r': self.r\n        }  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\n\n\nfor C in [UB, UM]:\n    jax.tree_util.register_pytree_node(\n        C,\n        C._tree_flatten,\n        C._tree_unflatten,\n    )\n\n\ndef s_w(ub, ums):\n    e  = jnp.identity(2)\n    u = UM(e, [2])\n    ums[0] = u\n    return ub, ums\n\ndef s_c(t, uns):\n    n = 20\n    ums = []\n    for un in uns:\n        ums.append(UM(un, [2]))\n\n    tub = UB(t.arr, t.r)\n    \n    s_loop_body = lambda i,x: s_w( ub=x[0], ums=x[1])\n    \n    tub, ums = jax.lax.fori_loop(0, n, s_loop_body, (tub, ums))\n    # for i in range(n):\n    #     tub, ums = s_loop_body(i, (tub, ums))\n\n    return jnp.array([u.arr.flatten() for u in ums])\n\n\nuns = jnp.array([jnp.array([1, 2, 3, 4]) for _ in range(6)])\nt = UM(jnp.array([1, 0, 0, 1]), r=[2])\nuns = s_c(t, uns)\nHas anyone encountered this issue or can explain how to fix it?",
        "answers": [
            "The issue is discussed here: https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization\nNamely, in JAX pytrees are used as general containers, and are sometimes initialized with abstract values or other place-holders, and so you cannot assume that arguments to a custom PyTree will be of array type. You might account for this by doing something like the following:\nclass UB():\n    def __init__(self, arr, new_shape):\n        self.arr = arr\n        self.shape = new_shape\n        if isinstance(arr, jnp.ndarray):\n            self.arr = self.arr.reshape(new_shape+new_shape)\nWhen I run your code with this modification, it gets past the error you asked about, but unfortunately does trigger another error due to the body function of the fori_loop not having a valid signature (namely, the arr attributes of the ums have different shapes on input and output, which is not supported by fori_loop).\nHopefully this gets you on the path toward working code!"
        ],
        "link": "https://stackoverflow.com/questions/74296697/getting-a-type-error-while-using-fori-loop-with-jax"
    },
    {
        "title": "ERROR: No matching distribution found for jax[gpu]>=0.3.4 (from -r vit_jax/requirements.txt (line 8))",
        "question": "trying to build my first dockerfile for vision transformer. ran into\nERROR: Could not find a version that satisfies the requirement jax[gpu]>=0.3.4 (from -r vit_jax/requirements.txt (line 8)) (from versions: 0.0, 0.1, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 0.1.15, 0.1.16, 0.1.18, 0.1.19, 0.1.20, 0.1.21, 0.1.22, 0.1.23, 0.1.24, 0.1.25, 0.1.26, 0.1.27, 0.1.28, 0.1.29, 0.1.30, 0.1.31, 0.1.32, 0.1.33, 0.1.34, 0.1.35, 0.1.36, 0.1.37, 0.1.38, 0.1.39, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.45, 0.1.46, 0.1.47, 0.1.48, 0.1.49, 0.1.50, 0.1.51, 0.1.52, 0.1.53, 0.1.54, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.1.77, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.9, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.14, 0.2.15, 0.2.16, 0.2.17) ERROR: No matching distribution found for jax[gpu]>=0.3.4 (from -r vit_jax/requirements.txt (line 8))\ndidn't find anyone running vit ran into this problem, so i assume it's my dockerfile's flaw not requirements.txt's? below is my dockerfile\nFROM pytorch/pytorch:1.2-cuda10.0-cudnn7-runtime\n\nENV DEBIAN_FRONTEND=noninteractive\nARG USERNAME=user\nWORKDIR /dockertest\nARG WORKDIR=/dockertest\nRUN apt-get update && apt-get install -y \\\n        automake autoconf libpng-dev nano python3-pip \\\n        sudo curl zip unzip libtool swig zlib1g-dev pkg-config \\\n        python3-mock libpython3-dev libpython3-all-dev \\\n        g++ gcc cmake make pciutils cpio gosu wget \\\n        libgtk-3-dev libxtst-dev sudo apt-transport-https \\\n        build-essential gnupg git xz-utils vim libgtk2.0-0 libcanberra-gtk-module\\\n        libva-dev libdrm-dev xorg xorg-dev protobuf-compiler \\\n        openbox libx11-dev libgl1-mesa-glx libgl1-mesa-dev \\\n        libtbb2 libtbb-dev libopenblas-dev libopenmpi-dev \\\n    && sed -i 's/# set linenumbers/set linenumbers/g' /etc/nanorc \\\n    && apt clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN git clone https://github.com/google-research/vision_transformer.git \\\n    &&cd vision_transformer \\\n    && pip3 install pip --upgrade \\\n    && pip install -r vit_jax/requirements.txt \\\n    &&python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \\\n    --config=$(pwd)/vit_jax/configs/vit.py:b16,cifar10 \\\n    --config.pretrained_dir='gs://vit_models/imagenet21k' \\\n    && pip cache purge\n\nRUN echo \"root:root\" | chpasswd \\\n    && adduser --disabled-password --gecos \"\" \"${USERNAME}\" \\\n    && echo \"${USERNAME}:${USERNAME}\" | chpasswd \\\n    && echo \"%${USERNAME}    ALL=(ALL)   NOPASSWD:    ALL\" >> /etc/sudoers.d/${USERNAME} \\\n    && chmod 0440 /etc/sudoers.d/${USERNAME}\nUSER ${USERNAME}\nRUN sudo chown -R ${USERNAME}:${USERNAME} ${WORKDIR}\nWORKDIR ${WORKDIR}",
        "answers": [
            "The issue is that you're using Python 3.6 (as specified in the docker file), which is not supported by JAX version 0.2.18 and newer (see JAX Changelog).\nTo fix the issue, you should upgrade Python to version 3.7 or newer. Python 3.6 has reached its end of life and is no longer receiving security updates.\nAlternatively, if for some reason you must continue using Python 3.6, you should install jax version 0.2.17 and jaxlib version 0.1.69, which were the last releases to be compatible with Python 3.6."
        ],
        "link": "https://stackoverflow.com/questions/74283935/error-no-matching-distribution-found-for-jaxgpu-0-3-4-from-r-vit-jax-requi"
    },
    {
        "title": "Indexing a BCOO in Jax",
        "question": "I came across another problem in my attempts to learn jax: I have a sparse BCOO array, and an array holding indices. I need to obtain all values at these indices in the BCOO array. It would be ideal if the returned array would be a sparse BCOO as well. Using the usual slicing syntax seems to not work. Is there a standard way to achieve this? e.g.\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\n\nindices = jnp.array([\n    1,1,0\n])\n\nfull_array = jnp.array(\n        [\n            [\n                [0,0,0],\n                [2,2,2],\n                [0,0,0],\n                [0,0,0]\n            ],\n            [\n                [1,1,1],\n                [0,0,0],\n                [0,0,0],\n                [0,0,0]\n            ],\n            [\n                [1,1,1],\n                [0,0,0],\n                [0,0,0],\n                [0,0,0]\n            ]\n        ]\n)\nfull_array[jnp.arange(3),indices]\n# results in:\n#    [2,2,2],\n#    [0,0,0],\n#    [1,1,1] \n\nsparse_array = sparse.bcoo_fromdense(full_array)\n\n# Trying the same thing on a sparse array:\nsparse_array[jnp.arange(3),indices]\n# produces an NotImplementedError",
        "answers": [
            "[Edit: 2022-11-15] As of jax version 0.3.25, this kind of sparse indexing is directly supported in JAX:\nimport jax\nprint(jax.__version__)\n# 0.3.25\n\nsparse_array = sparse.bcoo_fromdense(full_array)\nresult = sparse_array[jnp.arange(3),indices]\n\nprint(result.todense())\n# [[2 2 2]\n#  [0 0 0]\n#  [1 1 1]]\nOriginal answer:\nThanks for the question. Unfortunately, general indexing support has not been added yet to jax.experimental.sparse. The types of indexing operations currently supported are limited to static scalars and slices; for example:\nprint(sparse_array[0].todense())\n# [[0 0 0]\n#  [2 2 2]\n#  [0 0 0]\n#  [0 0 0]]\nWith this in mind, you may be able to build the operation you have in mind using concatenation. For example:\nresult = sparse.sparsify(jnp.vstack)([\n    sparse_array[0][1],  # only single indices supported currently\n    sparse_array[1][1],\n    sparse_array[2][0],\n])\nprint(result.todense())\n# [[2 2 2]\n#  [0 0 0]\n#  [1 1 1]]\nAdmittedly only supporting static indices is not very convenient, but we hope to add more indexing support in the future."
        ],
        "link": "https://stackoverflow.com/questions/74281298/indexing-a-bcoo-in-jax"
    },
    {
        "title": "Why is JAX's `split()` so slow at first call?",
        "question": "jax.numpy.split can be used to segment an array into equal-length segments with a remainder in the last element. e.g. splitting an array of 5000 elements into segments of 10:\narray = jnp.ones(5000)\nsegment_size = 10\nsplit_indices = jnp.arange(segment_size, array.shape[0], segment_size)\n\nsegments = jnp.split(array, split_indices)\nThis takes around 10 seconds to execute on Google Colab and on my local machine. This seems unreasonable for such a simple task on a small array. Am I doing something wrong to make this slow?\nFurther Details (JIT caching, maybe?)\nSubsequent calls to .split are very fast, provided an array of the same shape and the same split indices. e.g. the first iteration of the following loop is extremely slow, but all others fast. (11 seconds vs 40 milliseconds)\nfrom timeit import default_timer as timer\nimport jax.numpy as jnp\n\narray = jnp.ones(5000)\nsegment_size = 10\nsplit_indices = jnp.arange(segment_size, array.shape[0], segment_size)\n\nfor k in range(5):\n  start = timer()\n\n  segments = jnp.split(array, split_indices)\n  \n  end = timer()\n  print(f'call {k}: {end - start:0.2f} s')\nOutput:\ncall 0: 11.79 s\ncall 1: 0.04 s\ncall 2: 0.04 s\ncall 3: 0.05 s\ncall 4: 0.04 s\nI assume that the subsequent calls are faster because JAX is caching jitted versions of split for each combination of arguments. If that's the case, then I assume split is slow (on its first such call) because of compilation overhead.\nIs that true? If yes, how should I split a JAX array without incurring the performance hit?",
        "answers": [
            "This is slow because there are tradeoffs in the implementation of split(), and your function happens to be on the wrong side of the tradeoff.\nThere are several ways to compute slices in XLA, including XLA:Slice (i.e. lax.slice), XLA:DynamicSlice (i.e. lax.dynamic_slice), and XLA:Gather (i.e. lax.gather).\nThe main difference between these concerns whether the start and ending indices are static or dynamic. Static indices essentially mean you're specializing your computation for specific index values: this incurs some small compilation overhead on the first call, but subsequent calls can be very fast. Dynamic indices, on the other hand, don't include such specialization, so there is less compilation overhead, but each execution takes slightly longer. You may be able to guess where this is going...\njnp.split currently is implemented in terms of lax.slice (see code), meaning it uses static indices. This means that the first use of jnp.split will incur compilation cost proportional to the number of outputs, but repeated calls will execute very quickly. This seemed like the best approach for common uses of split, where a handful of arrays are produced.\nIn your case, you're generating hundreds of arrays, so the compilation cost far dominates over the execution.\nTo illustrate this, here are some timings for three approaches to the same array split, based on gather, slice, and dynamic_slice. You might wish to use one of these directly rather than using jnp.split if your program benefits from different implementations:\nfrom timeit import default_timer as timer\nfrom jax import lax\nimport jax.numpy as jnp\nimport jax\n\ndef f_slice(x, step=10):\n  return [lax.slice(x, (N,), (N + step,)) for N in range(0, x.shape[0], step)]\n\ndef f_dynamic_slice(x, step=10):\n  return [lax.dynamic_slice(x, (N,), (step,)) for N in range(0, x.shape[0], step)]\n                            \ndef f_gather(x, step=10):\n  step = jnp.asarray(step)\n  return [x[N: N + step] for N in range(0, x.shape[0], step)]\n\n\ndef time(f, x):\n  print(f.__name__)\n  for k in range(5):\n    start = timer()\n    segments = jax.block_until_ready(f(x))\n    end = timer()\n    print(f'  call {k}: {end - start:0.2f} s')\n\nx = jnp.ones(5000)\n\ntime(f_slice, x)\ntime(f_dynamic_slice, x)\ntime(f_gather, x)\nHere's the output on a Colab CPU runtime:\nf_slice\n  call 0: 7.78 s\n  call 1: 0.05 s\n  call 2: 0.04 s\n  call 3: 0.04 s\n  call 4: 0.04 s\nf_dynamic_slice\n  call 0: 0.15 s\n  call 1: 0.12 s\n  call 2: 0.14 s\n  call 3: 0.13 s\n  call 4: 0.16 s\nf_gather\n  call 0: 0.55 s\n  call 1: 0.54 s\n  call 2: 0.51 s\n  call 3: 0.58 s\n  call 4: 0.59 s\nYou can see here that static indices (lax.slice) lead to the fastest execution after compilation. However, for generating many slices, dynamic_slice and gather avoid repeated compilations. It may be that we should re-implement jnp.split in terms of dynamic_slice, but that wouldn't come without tradeoffs: for example, it would lead to a slowdown in the (possibly more common?) case of few splits, where lax.slice would be faster on both initial and subsequent runs. Also, dynamic_slice only avoids recompilation if each slice is the same size, so generating many slices of varying sizes would incur a large compilation overhead similar to lax.slice.\nThese kinds of tradeoffs are actively discussed in JAX development channels; a recent example very similar to this can be found in PR #12219. If you wish to weigh-in on this particular issue, I'd invite you to file a new jax issue on the topic.\nA final note: if you're truly just interested in generating equal-length sequential slices of an array, you would be much better off just calling reshape:\nout = x.reshape(len(x) // 10, 10)\nThe result is now a 2D array where each row corresponds to a slice from the above functions, and this will far out-perform anything that's generating a list of array slices.",
            "Jax inbult functions are also JIT compiled\nBenchmarking JAX code\nJAX code is Just-In-Time (JIT) compiled. Most code written in JAX can be written in such a way that it supports JIT compilation, which can make it run much faster (see To JIT or not to JIT). To get maximium performance from JAX, you should apply jax.jit() on your outer-most function calls.\nKeep in mind that the first time you run JAX code, it will be slower because it is being compiled. This is true even if you don’t use jit in your own code, because JAX’s builtin functions are also JIT compiled.\nSo the first time you run it, it is compiling jnp.split (Or at least, compiling some of the functions used within jnp.split)\n%%timeit -n1 -r1\njnp.split(array, split_indices)\n1min 15s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nThe second time, it is calling the compiled function\n%%timeit -n1 -r1\njnp.split(array, split_indices)\n131 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nIt is fairly complicated, calling other jax.numpy functions, so I assume it can take quite a while to compile (1 minute on my machine!)"
        ],
        "link": "https://stackoverflow.com/questions/74199437/why-is-jaxs-split-so-slow-at-first-call"
    },
    {
        "title": "How can I redefine a subfunction of a JAX-jitted function?",
        "question": "I have a function foo which is jitted with JAX. foo calls bar.\nfrom jax import jit\n\ndef bar(x):\n  return x ** 2\n\n@jit\ndef foo(x):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4)}')\nPrints foo(4) = 17 as expected.\nIf I redefine bar at runtime, what is the best way to re-jit foo?\nBonus: Is it possible to just tell jax bar changed, so that it can re-jit everything dependent upon bar?\nDetails\nIf I redefine bar and print again,\ndef bar(x):\n  return 2 * x\n\nprint(f'foo(4) = {foo(4)}')\nthe output is still foo(4) = 17. Clearly, the old bar is still in the jitted foo.\nIn order to get the desired foo with updated bar, I need to re-jit. I can do this by rewriting foo, so that it is re-jitted\n@jit\ndef foo(x):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4)}')\nwhich now prints foo(4) = 9 as desired. But rewriting all of foo is silly. I can also get the desired output by just rewrapping foo as\nfoo = jit(foo)\nBut this feels dangerous, since we're passing an already-jitted foo into the jit compiler again. I was surprised to see that it worked, and I suspect weird corner cases with this approach.\nAlso, both of these approaches require knowing that foo called bar. It's trivial in this toy example, but in more complex software, there might be many functions which need to be re-jitted. It's cumbersome to do it individually, and easy to forget some as well. Hence the 'bonus' part of my question.\nThe jit docs and jit caching docs did not answer my question, AFAICT. Nor did a reasonable search of stackoverflow or the github discussion pages.",
        "answers": [
            "The reason this is not working as expected is because the function is not pure: that is, its output depends not just on the inputs, but also on some global state. JAX transforms like JIT only work correctly for pure functions; see JAX Sharp Bits: Pure Functions for more discussion of this.\nThe best way to approach this is probably to change the function's call signature so that all relevant state is explicitly passed to the function. For example, you could pass bar explicitly:\nfrom jax import jit\nfrom functools import partial\n\ndef bar(x):\n  return x ** 2\n\n# Mark `bar` as a static argument: when it changes it will trigger re-compilation\n@partial(jit, static_argnames=['bar'])\ndef foo(x, bar):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4, bar)}')\n# foo(4) = 17\n\ndef bar(x):\n  return 2 * x\n\nprint(f'foo(4) = {foo(4, bar)}')\n# foo(4) = 9\nNow when you change bar the output of the function changes."
        ],
        "link": "https://stackoverflow.com/questions/74185207/how-can-i-redefine-a-subfunction-of-a-jax-jitted-function"
    },
    {
        "title": "multidimensional jax.isin()",
        "question": "i am trying to filter an array of triples. The criterion by which I want to filter is whether another array of triples contains at least one element with the same first and third element. E.g\nimport jax.numpy as jnp\narray1 = jnp.array(\n  [\n    [0,1,2],\n    [1,0,2],\n    [0,3,3],\n    [3,0,1],\n    [0,1,1],\n    [1,0,3],\n  ]\n)\narray2 = jnp.array([[0,1,3],[0,3,2]])\n# the mask to filter the first array1 should look like this:\njnp.array([True,False,True,False,False,False])\nWhat would be a computationally efficient way to achieve this mask using jax? I am looking forward to your input.",
        "answers": [
            "You can do this by reducing over a broadcasted equality check:\nimport jax.numpy as jnp\narray1 = jnp.array(\n  [\n    [0,1,2],\n    [1,0,2],\n    [0,3,3],\n    [3,0,1],\n    [0,1,1],\n    [1,0,3],\n  ]\n)\narray2 = jnp.array([[0,1,2],[0,3,2]])  # note adjustment to match first entry of array1\n\nmask = (array1[:, None] == array2[None, :]).all(-1).any(-1)\nprint(mask)\n# [ True False False False False False]\nXLA doesn't have any binary search-like primitive, so the best approach in general is to generate the full equality matrix and reduce. If you're running the code on an accelerator like a GPU/TPU, this sort of vectorized operation is efficiently parallelized and so it will be computed quite efficiently in practice."
        ],
        "link": "https://stackoverflow.com/questions/74154196/multidimensional-jax-isin"
    },
    {
        "title": "How to wrap a numpy function to make it work with jax.numpy?",
        "question": "I have some Jax code that requires using auto differentiation and in part of the code, I would like to call a function from a library written in NumPy. When I try this now I get\nThe numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(float32[4,22324])>with<JVPTrace(level=4/1)> with\n  primal = Traced<ShapedArray(float32[4,22324])>with<DynamicJaxprTrace(level=0/1)>\n  tangent = Traced<ShapedArray(float32[4,22324])>with<JaxprTrace(level=3/1)> with\n    pval = (ShapedArray(float32[4,22324]), None)\n    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fa89e8ffa80>, in_tracers=(Traced<ShapedArray(float32[22324,4]):JaxprTrace(level=3/1)>,), out_tracer_refs=[<weakref at 0x7fa89beb15e0; to 'JaxprTracer' at 0x7fa893b5ab80>], out_avals=[ShapedArray(float32[4,22324])], primitive=transpose, params={'permutation': (1, 0)}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fa89e9312b0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\nwhich makes sense because NumPy is not auto-differentiable.\nIs there any way to wrap a function written in NumPy such that it maps it to the jax.numpy equivalent?\nA dirty way to make this work would be to modify the library so it calls jax.numpy instead of numpy but this makes applicability harder.\nThanks!",
        "answers": [
            "Edit January 2023: JAX is now adding a number of callback methods to accomplish this kind of thing; see https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html\nNo, in general there's no way given a function that operates on NumPy arrays to automatically convert it to an equivalent function implemented in JAX. The reason for this is that JAX is not a 100% faithful one-to-one implementation of NumPy's API; rather you should think of jax.numpy as a NumPy-like wrapper around the functionality that JAX provides.\nAs a simple example, consider this code:\nnp.array(['A', 'B', 'C'])\nThis has no JAX equivalent, because JAX/XLA does not support string arrays.\nIf you want to use JAX transforms like autodiff on your code, there's not really any shortcut around rewriting your code in JAX. You can likely get a long way by replacing import numpy as np with import jax.numpy as jnp, so long as you're not using external libraries (like SciPy, Scikit-Learn, etc.) that operate on your arrays.\nAdditionally, as you do such replacements, keep in mind JAX's Sharp Bits, which are places where jax.numpy may behave differently than your original NumPy code.",
            "import numpy as np\nimport jax.numpy as jnp\nimport jax\nimport inspect\nimport re\n\ndef function_np(x):\n    return np.maximum(0, x)\n\nfunction_np_str = inspect.getsource(function_np) # getting the code as a string\nfunction_jnp_str = re.sub(r\"np\", \"jnp\", function_code) #replacing all the 'np' with 'jnp'\n# The line below creates a function defined in the 'jnp_function_str' string - which uses jnp instead of numpy\nexec(jnp_activation_str)  \nnow you have a new function called 'function_jnp' which uses jnp library.\nIt's somewhat of a crutch, but it works for simple functions."
        ],
        "link": "https://stackoverflow.com/questions/74140788/how-to-wrap-a-numpy-function-to-make-it-work-with-jax-numpy"
    },
    {
        "title": "Issues with non-hashable static arguments when forming",
        "question": "I have a vector-jacobian product that I want to compute.\nThe function func takes four arguments, the final two of which are static:\ndef func(variational_params, e, A, B):\n    ...\n    return model_params, dlogp, ...\nThe function jits perfectly fine via\nfunc_jitted = jit(func, static_argnums=(2, 3))\nThe primals are the variational_params, and the cotangents are dlogp (the second output of the function).\nCalculating the vector-jacobian product naively (by forming the jacobian) works fine:\njacobian_func = jacobian(func_jitted, argnums=0, has_aux=True)\njacobian_jitted = jit(jacobian_func, static_argnums=(2, 3))\njac, func_output = jacobian_jitted(variational_params, e, A, B)\nnaive_vjp = func_output.T @ jac \nWhen trying to form the vjp in an efficient manner via\nf_eval, vjp_function, aux_output = vjp(func_jitted, variational_params, e, A, B, has_aux=True)\nI get the following error:\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.ad.JVPTracer'> for function func is non-hashable.\nI am a little confused as the function func jitted perfectly fine... there is no option for adding static_argnums to the vjp function, so I am not too sure what this means.",
        "answers": [
            "For higher-level transformation APIs like jit, JAX generally provides a mechanism like static_argnums or argnums to allow specification of static vs. dynamic variables.\nFor lower-level transformation routines like jvp and vjp, these mechanisms are not provided, but you can still accomplish the same thing by passing partially-evaluated functions. For example:\nfrom functools import partial\n\nf_eval, vjp_function, aux_output = vjp(partial(func_jitted, A=A, B=B), variational_params, e, has_aux=True)\nThis is effectively how transformation parameters like argnums and static_argnums are implemented under the hood."
        ],
        "link": "https://stackoverflow.com/questions/74065210/issues-with-non-hashable-static-arguments-when-forming"
    },
    {
        "title": "Issue with jax.lax.scan",
        "question": "I am supposed to use Jax.lax.scan instead of a for loop with 100 iterations at line 22. I am supposed to update S and append it to S_list. I am unsure how to fix the jax.lax.scan. The error that keeps popping up is missing the required XS. When I put a value for XS it says that my length argument doesn't line up with the axis sizes. Here is my code. Can you help me?",
        "answers": [
            "You're not calling scan with the correct signature. You can find more information on the call signature in the jax.lax.scan docs. It makes clear, for example, that your step function must accept two arguments and return two arguments.\nFrom looking at your code, it looks like you're intending to do something like this:\n@jax.jit\ndef simulate():\n  key = jax.random.PRNGKey(0)\n  def step(S, _):\n    dZ = jax.random.normal(key, shape=(S.size,)) * jnp.sqrt(dt)\n    dS = r * S  * dt + σ  * S  * dZ\n    return S + dS, S\n  S0 = jnp.ones(20000)\n  _, S_array = jax.lax.scan(step, S0, xs=None, length=m)\n  return S_array\nIn particular, from the docs you can see that the S_list.append(...) and S_array = jnp.stack(S_list) are effectively part of the scan function itself, so you don't have to do that yourself after calling it.\nHope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/74061048/issue-with-jax-lax-scan"
    },
    {
        "title": "Difference between vmapping a vmapped function versus the output of a vmapped function",
        "question": "I am going through a great set of JAX notebooks that contain exercises on Eric Ma's GitHub.\nThe task that I am interested in is to replicate the following function via multiple vmap function applications:\ndata = random.normal(key, shape=(11, 31, 7))\n\ndef ex2_numpy_equivalent(data):\n    result = []\n    for d in data: \n        cp = jnp.cumprod(d, axis=-1)\n        s = jnp.sum(cp, axis=1)\n        result.append(s)\n    return jnp.stack(result)\nThe solution provided by Eric was:\ndef loopless_loop_ex2(data):\n    \"\"\"Data is three-dimensional of shape (n_datasets, n_rows, n_columns)\"\"\"\n\n    def inner(dataset):\n        \"\"\"dataset is two-dimensional of shape (n_rows, n_columns)\"\"\"\n        cp = vmap(np.cumpred)(dataset)\n        s = vmap(np.sum)(cp)\n        return s \n\n    return vmap(inner)(data)\nMy initial \"guess solution\" was to try:\nfunc1 = vmap(jnp.cumprod)\nfunc2 = vmap(jnp.sum)\nfunc3 = vmap(func2)\n\nfunc3(data)\nAlarm bells were ringing because it makes no sense for func3 to be a vmap'ped version of func2, considering that func2is already avmap'ped function. While I did get the incorrect answer (value-wise), the dimensions of the answer matched the expected sizes. What is this getting up to?\nAfter looking at Eric's solution, it seems pretty apparent that you can/should only be chaining vmap functions after you pass or evaluate the nested vmap function, but when you don't do this, and instead use my incorrect first attempt, what is going on?",
        "answers": [
            "The reason your computation is different is that you never compute the cumprod of the input. You create a function func1 = vmap(jnp.cumprod), but never call that function. Rather you're effectively computing vmap(vmap(jnp.sum))(data), which for 3-dimensional input is equivalent to the sum along the last axis:\nimport jax.numpy as jnp\nfrom jax import random, vmap\n\nkey = random.PRNGKey(1234)\ndata = random.normal(key, shape=(11, 31, 7))\n\n# func1 = vmap(jnp.cumprod) # unused\nfunc2 = vmap(jnp.sum)\nfunc3 = vmap(func2)\n\nassert jnp.allclose(func3(data), data.sum(-1))"
        ],
        "link": "https://stackoverflow.com/questions/74001537/difference-between-vmapping-a-vmapped-function-versus-the-output-of-a-vmapped-fu"
    },
    {
        "title": "Execute Markov chains with tree-structured state in parallel with JAX",
        "question": "I have a Markov chain function implemented in JAX that advances the chain from state s -> s' based on some training data (X_train).\ndef step(state: dict, key, X_train) -> dict:\n    new_state = advance(state, key, X_train)\n    return new_state\nHere, state is a fairly complicated tree-structured dict of array's that was generated by Haiku. For example,\nstate = {\n    'layer1': {\n        'weights': array(...),\n        'bias': array(...),\n    },\n    'layer2': {\n        'weights': array(...),\n        'bias': array(...),\n    },\n}\nI would like to run multiple Markov chains, with different states, in parallel. At first glance, jax.vmap function looks like a good candidate. However, state is not an array but a (tree-structured) dict.\nWhat is the best way to approach this?\nThanks!",
        "answers": [
            "Yes, you could use vmap for any pytree. But this is how you should construct it:\nstates = {'layer1':{'weights':jnp.array([[1, -2, 3],\n                                         [4, 5, 6]])},\n          'layer2':{'weights':jnp.array([[1, .2, 3],\n                                         [.4, 5, 6]])}}\nSo in your first run, your weights will be [1, -2, 3] and [1, .2, 3] for layer1 and layer2 respectively (second run will be [4, 5, 6] and [.4, 5, 6]). But markov chain should be handled by jax.lax.scan. And you could use jit compilation to speed things up. Here is a trivial example. In each step chain calculates the following:\nimport jax\nimport jax.numpy as jnp \nfrom functools import partial\n\n@jax.jit\ndef step(carry, k):\n    # this function runs a single step in the chain\n    # X_train dim:(3,3)\n    # w1 dim: (1,3)\n    # w2 dim: (3,1)\n    # X_new = log(Relu(w1@X_old)@w2) + e\n    # e~Normal(0, 1)\n    \n    state, X_train, rng = carry\n    rng, rng_input = jax.random.split(rng)\n    e = jax.random.normal(rng) # generate pseudorandom\n    w1 = state['layer1']['weights'] # it is a column vector\n    w2 = state['layer2']['weights'][None, :] # make it a row vector\n    \n    X_train = jax.nn.relu(w1@X_train)[:, None]+1\n    X_train = jnp.log(X_train@w2)\n    X_train = X_train + e\n    \n    return [state, X_train, rng], e\n\n@partial(jax.jit, static_argnums = 3)\ndef fi(state, X_train, key, number_of_steps):\n    rng = jax.random.PRNGKey(key)\n    carry = [state, X_train, rng]\n    carry, random_normals = jax.lax.scan(step, carry, xs = jnp.arange(number_of_steps))\n    state, X, rng = carry\n    return X\n\nX_train = jnp.array([[1., -1., 0.5],\n                     [1., 1, 2.],\n                     [4, 2, 0.1]])\n\nstates = {'layer1':{'weights':jnp.array([[1, -2, 3],\n                                         [4, 5, 6]])},\n          'layer2':{'weights':jnp.array([[1, .2, 3],\n                                         [.4, 5, 6]])}}\n\nvmap_fi = jax.vmap(fi, (0, None, None, None)) # only map on first argument axis 0\n\nkey = 42 # random seed\nnumber_of_steps = 100 # chain runs 100 steps\n\nlast_states = vmap_fi(states, X_train, key, number_of_steps)\nprint(last_states)\nOutput:\n[[[ 1.8478627   0.23842478  2.946475  ]\n  [ 1.3278859  -0.28155205  2.4264982 ]\n  [ 2.0921988   0.48276085  3.1908112 ]]\n\n [[ 2.9374144   5.4631433   5.645465  ]\n  [ 3.4333894   5.959118    6.1414394 ]\n  [ 3.4612248   5.9869533   6.169275  ]]]\nIn this example, you could make states dictionaries more complicated. You just need to parallelize on their 0th axis."
        ],
        "link": "https://stackoverflow.com/questions/73995540/execute-markov-chains-with-tree-structured-state-in-parallel-with-jax"
    },
    {
        "title": "jax segment_sum along array dimension",
        "question": "I am fairly new to jax and have the following problem: I need to compute functions (sum/min/max maybe more complex stuff later) across an array given an index. To solve this problem I found the jnp.ops.segment_sum function. This works great for one array, but how can I generalize this approach to a batch of arrays? E.g:\nimport jax.numpy as jnp\nindexes = jnp.array([[1,0,1],[0,0,1]])\nbatch_of_matrixes = jnp.array([\n    np.arange(9).reshape((3,3)),\n    np.arange(9).reshape((3, 3))\n])\n# The following works for one array but not multiple\njax.ops.segment_sum(\n    data=batch_of_matrixes[0],\n    segment_ids=indexes[0],\n    num_segments=2)\n# How can I get this to work with the full dataset along the 0 dimension?\n# Intended Outcome:\n[\n    [\n        [ 3  4  5],\n        [ 6  8 10]\n    ],\n    [\n        [3  5  7],\n        [6  7  8]\n   ]\n]\nIf there is a more general way to do this than the obs.segment_* family, please also let me know. Thanks in advance for help and suggestions!",
        "answers": [
            "JAX's vmap transformation is designed for exactly this kind of situation. In your case, you can use it like this:\n@jax.vmap\ndef f(data, index):\n  return jax.ops.segment_sum(data, index, num_segments=2)\n\nprint(f(batch_of_matrixes, indexes))\n# [[[ 3  4  5]\n#   [ 6  8 10]]\n\n#  [[ 3  5  7]\n#   [ 6  7  8]]]\nFor some more discussion of this, see JAX 101: Automatic Vectorization."
        ],
        "link": "https://stackoverflow.com/questions/73990561/jax-segment-sum-along-array-dimension"
    },
    {
        "title": "Error when trying to jit the computation of the Jacobian in JAX: \"ValueError: Non-hashable static arguments are not supported\"",
        "question": "This question is similar to the question here, but I cannot link with what I should alter.\nI have a function\ndef elbo(variational_parameters, eps, a, b):\n    ...\n    return theta, _\n\nelbo = jit(elbo, static_argnames=[\"a\", \"b\"])\nwhere variational_parameters is a vector (one-dimensional array) of length P, eps is a two-dimensional array of dimensions K by N, and a, b are fixed values.\nThe elbo has been successfully vmapped over the rows of eps, and has been jitted by setting by passing a and b to static_argnames, to return theta, which is a two-dimensional array of dimensions K by P.\nI want to take the Jacobian of the output theta with respect to variational_parameters through the elbo function. The first value returned by\njacobian(elbo, argnums=0, has_aus=True)(variational_parameters, eps, a, b)\ngives me a three-dimensional array of dimensions K by P by N. This is what I want. As soon as I try to jit this function\njit(jacobian(elbo, argnums=0, has_aus=True))(variational_parameters, eps, a, b)\nI get the error\nValueError: Non-hashable static arguments are not supported, which can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.partial_eval.DynamicJaxprTracer'> for function elbo is non-hashable.\nAny help would be greatly appreciated; thanks!",
        "answers": [
            "Any parameters you pass to a JIT-compiled function will no longer be static, unless you explicitly mark them as such. So this line:\njit(jacobian(elbo, argnums=0, has_aus=True))(variational_parameters, eps, a, b)\nMakes variational_parameters, eps, a, and b non-static. Then within the transformed function these non-static parameters are passed to this function:\nelbo = jit(elbo, static_argnames=[\"a\", \"b\"])\nwhich means that you are attempting to pass non-static values as static arguments, which causes an error.\nTo fix this, you should mark the static parameters as static any time they enter a jit-compiled function. In your case it might look something like this:\njit(jacobian(elbo, argnums=0, has_aus=True),\n    static_argnums=(2, 3))(variational_parameters, eps, a, b)"
        ],
        "link": "https://stackoverflow.com/questions/73941657/error-when-trying-to-jit-the-computation-of-the-jacobian-in-jax-valueerror-no"
    },
    {
        "title": "AttributeError: module 'flax' has no attribute 'nn'",
        "question": "I'm trying to run RegNeRF, which requires flax. On installing the latest version of flax==0.6.0, I got an error stating flax has no attribute optim. This answer suggested to downgrade flax to 0.5.1. On doing that, now I'm getting the error AttributeError: module 'flax' has no attribute 'nn'\nI could not find any solutions on the web for this error. Any help is appreciated.\nI'm using ubuntu 20.04",
        "answers": [
            "The flax.optim module has been moved to optax as of flax version 0.6.0; see Upgrading my Codebase to Optax for information on how to migrate your code. If you're using external code that imports flax.optim and can't update these references, you'll have to install flax version 0.5.3 or older.\nRegarding flax.nn: this module was replaced by flax.linen in flax version 0.4.0. See Upgrading my Codebase to Linen for information on this migration. If you're using external code that imports flax.nn and can't update these references, you'll have to install flax version 0.3.6 or older."
        ],
        "link": "https://stackoverflow.com/questions/73924768/attributeerror-module-flax-has-no-attribute-nn"
    },
    {
        "title": "How to use JAX vmap to efficiently calculate importance sampling estimate",
        "question": "I have code to calculate the off-policy importance sampling estimate commonly used in reinforcement learning. It is not important to know what that is, but for someone who does it might help them understand this question a little better. Basically, I have a 1D array of instances of a custom Episode class. An Episode has four attributes, all of which are arrays of floats. I have a function which loops over all episodes and for each one, it does a computation based only on the arrays in that episode. The result of that computation is a float, which I then store in a result array. Don't worry about what model.get_prob_this_action() does, you can consider it a black box that takes two floats as input and returns a float. The code for this function before optimizing with JAX is:\ndef IS_estimate(model, theta, episodes):\n    \"\"\" Calculate the unweighted importance sampling estimate\n    for each episode in episodes.\n    Return as an array, one element per episode\n    \"\"\"\n    # episodes is an array of custom Python class instances\n    \n    gamma = 1.0\n    result = np.zeros(len(episodes))\n    for ii, ep in enumerate(episodes):\n        obs = ep.observations # 1D array of floats\n        actions = ep.actions # 1D array of floats\n        rewards = ep.rewards # 1D array of floats\n        action_probs = ep.action_probs # 1D array of floats\n\n        pi_news = np.zeros(len(obs))\n        for jj in range(len(obs)):\n            pi_news[jj] = model.get_prob_this_action(obs[jj],actions[jj])\n\n        pi_ratio_prod = np.prod(pi_news / action_probs)\n\n        weighted_return = weighted_sum_gamma(rewards, gamma)\n        result[ii] = pi_ratio_prod * weighted_return\n\n    return np.array(result)\nUnfortunately, I cannot just rewrite the function to work on a single episode and then use jax.vmap to vectorize over that function. The reason is that the argument I want to vectorize is a custom Episode object, which JAX won't support.\nI can get rid of the inner loop to get pi_news using vmap, like:\ndef IS_estimate(model, theta, episodes):\n    \"\"\" Calculate the unweighted importance sampling estimate\n    for each episode in episodes.\n    Return as an array, one element per episode\n    \"\"\"\n    # episodes is an array of custom Python class instances\n    \n    gamma = 1.0\n    result = np.zeros(len(episodes))\n    for ii, ep in enumerate(episodes):\n        obs = ep.observations # 1D array of floats\n        actions = ep.actions # 1D array of floats\n        rewards = ep.rewards # 1D array of floats\n        action_probs = ep.action_probs # 1D array of floats\n\n        vmapped_get_prob_this_action = vmap(model.get_prob_this_action,in_axes=(0,0))\n        pi_news = vmapped_get_prob_this_action(obs,actions)\n\n        pi_ratio_prod = np.prod(pi_news / action_probs)\n\n        weighted_return = weighted_sum_gamma(rewards, gamma)\n        result[ii] = pi_ratio_prod * weighted_return\n\n    return np.array(result)\nand this does help some. But ideally, I'd like to vmap my outer loop as well. Does anyone know how I would do this?",
        "answers": [
            "The computation you're describing is an \"array-of-structs\" style computation; JAX's vmap does not support this. What it does support is a \"struct-of-arrays` style computation.\nAs a quick demonstration of this, here's how you might do a simple per-episode computation using first the array-of-structs pattern (with Python for-loops) and then the struct-of-arrays pattern (with jax.vmap):\nfrom typing import NamedTuple\nimport jax.numpy as jnp\nimport numpy as np\nimport jax\n\nclass Episode(NamedTuple):\n  observations: jnp.ndarray\n  actions: jnp.ndarray\n\n  def compute_result(self):\n    # stand-in for computing some value from attributes\n    return jnp.dot(self.observations, self.actions)\n\n# Computing result per episode on array of structs:\nrng = np.random.RandomState(42)\nepisodes = [\n    Episode(\n        observations=jnp.array(rng.rand(4)),\n        actions=jnp.array(rng.rand(4)))\n    for i in range(5)\n]\nresult1 = jnp.array([ep.compute_result() for ep in episodes])\nprint(result1)\n# [0.767802   0.83237386 0.49223748 0.5156544  1.1290307 ]\n\n# Computing results on struct of arrays via vmap:\nepisodes_struct_of_arrays = Episode(\n    observations = jnp.vstack([ep.observations for ep in episodes]),\n    actions = jnp.vstack([ep.actions for ep in episodes])\n)\nresult2 = jax.vmap(lambda self: self.compute_result())(episodes_struct_of_arrays)\nprint(result2)\n# [0.767802   0.83237386 0.49223748 0.5156544  1.1290307 ]\nIf you want to use JAX's vmap for this computation, you'll have to use a struct-of-arrays approach like the second one. Note that this also assumes that your Episode class is registered as a pytree (see extending pytrees) which is true by default for NamedTuple types."
        ],
        "link": "https://stackoverflow.com/questions/73921013/how-to-use-jax-vmap-to-efficiently-calculate-importance-sampling-estimate"
    },
    {
        "title": "Getting the expected dimensions of the Jacobian with JAX?",
        "question": "I am trying to get the Jacobian for a simple parameterization function within JAX. The code is as follows:\n# imports \nimport jax \nimport jax.numpy as jnp\nfrom jax import random\n\n# simple parameterization function \ndef reparameterize(v_params):\n    theta = v_params[0] + jnp.exp(v_params[1]) * eps\n    return theta \nSuppose I initialize eps to be a vector of shape (3,) and v_params to be of shape (3, 2):\nkey = random.PRNGKey(2022)\neps = random.normal(key, shape=(3,))\nkey, _ = random.split(key)\nv_params = random.normal(key, shape=(3, 2))\nI want the Jacobian to be an array of shape (3, 2) but by using\njacobian(vmap(reparameterize))(v_params)\nreturns an array of shape (3, 3, 3, 2). If I re-initialize with only a single eps:\nkey, _ = random.split(key)\neps = random.normal(key, shape=(1, ))\nkey, _ = random.split(key)\nv_params = random.normal(key, shape=(2, ))\nand call jacobian(reparameterize)(v_params) I get what I want, e.g., an array of shape (2, ). Effectively looping over all eps and stacking the results of each Jacobian gives me the desired Jacobian (and shape). What am I missing here? Thanks for your help!",
        "answers": [
            "For a function f that maps an input of shape shape_in to an output of shape shape_out, the jacobian will have shape (*shape_out, *shape_in).\nIn your case, vmap(reparameterize) takes an array of shape (3, 2) and returns an array of shape (3, 3), so the output of the jacobian is an array of shape (3, 3, 3, 2).\nIt's hard to tell from your question what computation you were intending, but if you want a jacobian the same shape as the input, you need a function that maps the input to a scalar. Perhaps the sum is what you had in mind?\nresult = jacobian(lambda x: vmap(reparameterize)(x).sum())(v_params)\nprint(result.shape)\n# (3, 2)"
        ],
        "link": "https://stackoverflow.com/questions/73915532/getting-the-expected-dimensions-of-the-jacobian-with-jax"
    },
    {
        "title": "Exception occurred while installing jaxlib on Ubuntu x86_64",
        "question": "I am unable to figure out how to troubleshoot these errors in jaxlib installation.\nIf somebody could please guide me on how to go about it, it will be much appreciated, Thanks.\nBelow are the commands I am using with the corresponding outputs.\n:~$ uname -a\nLinux pc-name-15-3567 5.15.0-47-generic #51-Ubuntu SMP Thu Aug 11 07:51:15 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n\n:~$ pip install --upgrade pip\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pip in ./.local/lib/python3.10/site-packages (22.2.2)\n\n:~$ pip install --upgrade \"jax\"\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: jax in ./.local/lib/python3.10/site-packages (0.3.17)\nCollecting jax\n  Downloading jax-0.3.19.tar.gz (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 221.8 kB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: absl-py in ./.local/lib/python3.10/site-packages (from jax[cpu]) (1.2.0)\nRequirement already satisfied: etils[epath] in ./.local/lib/python3.10/site-packages (from jax[cpu]) (0.8.0)\nRequirement already satisfied: numpy>=1.20 in /usr/lib/python3/dist-packages (from jax) (1.21.5)\nRequirement already satisfied: opt_einsum in ./.local/lib/python3.10/site-packages (from jax[cpu]) (3.3.0)\nRequirement already satisfied: scipy>=1.5 in /usr/lib/python3/dist-packages (from jax) (1.8.0)\nRequirement already satisfied: typing_extensions in ./.local/lib/python3.10/site-packages (from jax[cpu]) (4.3.0)\nCollecting jaxlib==0.3.15\n  Downloading jaxlib-0.3.15-cp310-none-manylinux2014_x86_64.whl (72.0 MB)\n     ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/72.0 MB 248.9 kB/s eta 0:04:33\nERROR: Exception:\nTraceback (most recent call last):\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 435, in _error_catcher\n    yield\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 516, in read\n    data = self._fp.read(amt) if not fp_closed else b\"\"\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n    data = self.__fp.read(amt)\n  File \"/usr/lib/python3.10/http/client.py\", line 465, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/python3.10/ssl.py\", line 1273, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/usr/lib/python3.10/ssl.py\", line 1129, in read\n    return self._sslobj.read(len, buffer)\nTimeoutError: The read operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n    status = run_func(*args)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\n    return func(self, options, args)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 369, in run\n    requirement_set = resolver.resolve(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n    result = self._result = resolver.resolve(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n    state = resolution.resolve(requirements, max_rounds=max_rounds)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\n    failure_causes = self._attempt_to_pin_criterion(name)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n    criteria = self._get_updated_criteria(candidate)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\n    self._add_to_criteria(criteria, requirement, parent=candidate)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n    if not criterion.candidates:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n    return bool(self._sequence)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n    return any(self)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n    return (c for c in iterator if id(c) not in self._incompatible_ids)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n    candidate = func()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n    self._link_candidate_cache[link] = LinkCandidate(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\n    super().__init__(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\n    self.dist = self._prepare()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\n    dist = self._prepare_distribution()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 308, in _prepare_distribution\n    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 438, in prepare_linked_requirement\n    return self._prepare_linked_requirement(req, parallel_builds)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 483, in _prepare_linked_requirement\n    local_file = unpack_url(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 165, in unpack_url\n    file = get_http_url(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 106, in get_http_url\n    from_path, content_type = download(link, temp_dir.path)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/network/download.py\", line 147, in __call__\n    for chunk in chunks:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n    for chunk in iterable:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n    for chunk in response.raw.stream(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 573, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 509, in read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 440, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\npip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.",
        "answers": [
            "The issue got solved for me by running these commands to install jaxlib:\npip install --upgrade \"jax\"\npip install --upgrade \"jaxlib\""
        ],
        "link": "https://stackoverflow.com/questions/73874153/exception-occurred-while-installing-jaxlib-on-ubuntu-x86-64"
    },
    {
        "title": "Differences in the standard error of parameters computed via the hessian inverse and via QR decomposition",
        "question": "I have solved a nonlinear optimization and I am trying to compute the standard error of the parameters obtained. I found two options: One uses the fractional covariance matrix formed from the inverse of the hessian while the other uses QR decomposition. However both errors are not the same. The standard error obtained via QR is less than that obtained from the hessian inverse. I am at a loss as to how and why both approaches differ and would like to understand better which is the more correct way. Below is the working example\n# import libraries\nimport jax\nimport jax.numpy as jnp  \nimport jaxopt\njax.config.update(\"jax_enable_x64\", True)\n\n\n# Create data\nF =  jnp.asarray([1.00e-01, 1.30e-01, 1.69e-01, 2.20e-01, 2.86e-01, 3.71e-01,\n             4.83e-01, 6.27e-01, 8.16e-01, 1.06e+00, 1.38e+00, 1.79e+00,\n             2.33e+00, 3.03e+00, 3.94e+00, 5.12e+00, 6.65e+00, 8.65e+00,\n             1.12e+01, 1.46e+01, 1.90e+01, 2.47e+01, 3.21e+01, 4.18e+01,\n             5.43e+01, 7.06e+01, 9.17e+01, 1.19e+02, 1.55e+02, 2.02e+02,\n             2.62e+02, 3.41e+02, 4.43e+02, 5.76e+02, 7.48e+02, 9.73e+02,\n             1.26e+03, 1.64e+03, 2.14e+03, 2.78e+03, 3.61e+03, 4.70e+03,\n             6.10e+03, 7.94e+03, 1.03e+04, 1.34e+04, 1.74e+04, 2.27e+04,\n             2.95e+04, 3.83e+04, 4.98e+04, 6.47e+04, 8.42e+04, 1.09e+05],dtype=jnp.float64)\n\nydata = jnp.asarray([45.1  -1.09j, 47.5  -1.43j, 46.8  -1.77j, 46.2  -2.29j,\n             46.2  -2.97j, 47.2  -3.8j , 47.   -4.85j, 45.1  -5.99j,\n             45.8  -7.33j, 42.3  -9.05j, 42.6 -10.2j , 36.5 -10.8j ,\n             34.5 -11.2j , 32.1 -10.2j , 30.   -9.18j, 29.4  -8.j  ,\n             27.3  -6.64j, 26.7  -5.18j, 25.3  -4.12j, 25.4  -3.26j,\n             25.2  -2.51j, 24.9  -1.94j, 24.9  -1.64j, 25.4  -1.35j,\n             25.5  -1.24j, 24.8  -1.1j , 24.7  -1.03j, 23.9  -1.04j,\n             25.2  -1.1j , 24.9  -1.27j, 25.   -1.46j, 25.4  -1.65j,\n             24.4  -1.98j, 24.5  -2.34j, 24.5  -2.91j, 23.8  -3.47j,\n             22.9  -4.13j, 22.3  -4.91j, 20.9  -5.66j, 20.3  -6.03j,\n             18.4  -6.96j, 17.6  -7.24j, 16.5  -7.74j, 14.3  -7.42j,\n             12.7  -7.17j, 11.2  -6.76j,  9.85 -5.89j,  8.68 -5.38j,\n              7.92 -4.53j,  7.2  -3.83j,  6.81 -3.2j ,  6.65 -2.67j,\n              6.11 -2.16j,  5.86 -1.77j], dtype=jnp.complex128)\n\nsigma = jnp.asarray([45.11316992, 47.52152039, 46.83345919, 46.25671951,\n             46.29536586, 47.35271903, 47.24957672, 45.49604488,\n             46.38285136, 43.25728262, 43.8041094 , 38.06428772,\n             36.27244133, 33.68159735, 31.37311588, 30.46900064,\n             28.09590006, 27.19783815, 25.63326745, 25.6083502 ,\n             25.32469348, 24.97545996, 24.95394959, 25.43585068,\n             25.53013122, 24.82438317, 24.72146638, 23.92261691,\n             25.22399651, 24.93236651, 25.04259571, 25.4535361 ,\n             24.48020425, 24.61149325, 24.67221312, 24.05162988,\n             23.26944133, 22.83414329, 21.65284277, 21.17665932,\n             19.67235624, 19.03096424, 18.22519136, 16.11044382,\n             14.58420036, 13.08195704, 11.47669813, 10.21209087,\n              9.12399584,  8.15529889,  7.52436708,  7.16598912,\n              6.48056325,  6.12147858], dtype=jnp.float64)\n\n# Define Model\ndef rrpwrcwo(p, x):\n    w = 2*jnp.pi*x\n    s = 1j*w\n    Rs = p[0]\n    Qh = p[1]\n    nh = p[2]\n    Rct = p[3]\n    C1 = p[4]\n    R1 = p[5]\n    Y1 = s*C1 + 1/R1\n    Z1 = 1/Y1\n    Zct = Rct + Z1\n    Ydl = (s**nh)*Qh\n    Yin = Ydl + 1/Zct\n    Zin = 1/Yin\n    Z = Rs + Zin\n    return jnp.concatenate((Z.real, Z.imag),axis = 0)\n\n\n# Define cost function\ndef obj_fun(p, x, y, yerr, lb, ub):\n    ndata = len(x)\n    dof = (2*ndata-(len(p)))\n    y_concat = jnp.concatenate([y.real, y.imag], axis = 0)\n    sigma = jnp.concatenate([yerr,yerr], axis = 0)\n    y_model = rrpwrcwo(p, x)\n    chi_sqr = (1/dof)*(jnp.sum(jnp.abs((1/sigma**2) * (y_concat - y_model)**2)))\n    return chi_sqr\n\n# Define minimization function\ndef cnls(p, x, y, yerr, lb, ub):\n    \"\"\"\n    \"\"\"\n    solver = jaxopt.ScipyMinimize(method = 'BFGS', fun= obj_fun)\n    sol = solver.run(p, x, y, yerr, lb, ub)\n    # Compute popt\n    return sol\n\n# Define initial values and bounds\np0 = jnp.asarray([5, 0.000103, 1, 20, 0.001, 20])\n\nlb = jnp.zeros(len(p0))\nlb=lb.at[2].set(0.1)\nub = jnp.full((len(p0),),jnp.inf)\nub.at[2].set(1.01)\n\n# Run optimization\nres = cnls(p0, F, ydata, sigma, lb, ub)\npopt = res.params\n# DeviceArray([5.26589219e+00, 7.46288724e-06, 8.27089860e-01,\n#              1.99066599e+01, 3.40764484e-03, 2.19277541e+01],dtype=float64)\n\n# Get the weighted residual mean square\nchisqr = res.state.fun_val\n# 0.00020399\n\n# Method 1: Error computation using the fractional covariance matrix\n\n# get hessian matrix from parameters at the minimum\nhess = jax.jacfwd(jax.jacrev(obj_fun))(popt, F, ydata, sigma, lb, ub)\n\n# Take the hessian inv\nhess_inv = jnp.linalg.inv(hess)\n\n# Form the fractional covariance matrix\ncov_mat = hess_inv * chisqr\n\n# Compute standard error of the parameters\nperr = jnp.sqrt(jnp.diag(cov_mat))\nperr\n# DeviceArray([4.60842608e-01, 3.64957208e-06, 4.59190021e-02,\n#              8.29162454e-01, 4.47488639e-04, 1.49346052e+00], dtype=float64)\n\n\n# Method 2: Error Computation using QR Decomposition\n\n# Compute gradient of function (model) with respect to the parameters\ngrads = jax.jacfwd(rrpwrcwo)(popt, F)\ngradsre = grads[:len(F)]\ngradsim = grads[len(F):]\n\n# Form diagonal weight matrices\nrtwre = jnp.diag((1/sigma))\nrtwim = jnp.diag((1/sigma))\n\nvre = rtwre@gradsre\nvim = rtwim@gradsim\n\n# Compute QR decomposition\nQ1, R1 = jnp.linalg.qr(jnp.concatenate([vre,vim], axis = 0))\n\n# Compute inverse of R1\ninvR1 = jnp.linalg.inv(R1)\n\n# Compute standard error of the parameters\nperr = jnp.linalg.norm(invR1, axis=1)*jnp.sqrt(chisqr)\nperr\n\n# DeviceArray([6.48631283e-02, 5.14577571e-07, 6.48070403e-03,\n#              1.16523404e-01, 6.28434098e-05, 2.09238133e-01],dtype=float64)",
        "answers": [
            "I believe the issue is you are computing the hessian of a chi-square per degree of freedom, when you should be computing the hessian of chi-square. If you change this line:\nchi_sqr = (1/dof)*(jnp.sum(jnp.abs((1/sigma**2) * (y_concat - y_model)**2)))\nto this:\nchi_sqr = 0.5 * (jnp.sum(jnp.abs((1/sigma**2) * (y_concat - y_model)**2)))\nthen the two approaches return approximately the same results."
        ],
        "link": "https://stackoverflow.com/questions/73812834/differences-in-the-standard-error-of-parameters-computed-via-the-hessian-inverse"
    },
    {
        "title": "How to write this function jax.jit-able>",
        "question": "def factory(points):\n  points.sort()\n  @jax.jit\n  def fwd(x):\n    for i in range(1, len(points)):\n      if x < points[i][0]:\n        return (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * x + (points[i][1] - (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * points[i][0])\n    i = len(points) - 1\n    return (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * x + (points[i][1] - (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * points[i][0])\n  return fwd\nI want to write a function that creates jitted function, given argument: points, a list contain pairs of numbers. I aware that if/else statement can't be jitted and jax.lax.cond() allow conditions but I want something like a break as you can see in the above code. Is there any way to work with conditions?",
        "answers": [
            "The challenge in converting this to JAX-compatible is that your function relies on control flow triggered by values in the array; to make this compatible with JAX you should convert it to a vector-based operation. Here's how you might express your operation in terms of np.where rather than for loops:\ndef factory_v2(points):\n  points.sort()\n  def fwd(x):\n    matches = np.where(x < points[1:, 0])[0]\n    i = matches[0] + 1 if len(matches) else len(points) - 1\n    return (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * x + (points[i, 1] - (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * points[i, 0])\n  return fwd\n\nx = 2\npoints = np.array([[4, 0], [2, 1], [6, 5], [4, 6], [5, 7]])\nprint(factory(points)(x))\n# 3.0\n\nprint(factory_v2(points)(x))\n# 3.0\nThis is closer to a JAX-compatible operation, but unfortunately it relies on creating dynamically-shaped arrays. You can get around this by using the size argument to jnp.where. Here's a JAX-compatible version that uses the JAX-only size and fill_value arguments to jnp.where to work around this dynamic size issue:\nimport jax\nimport jax.numpy as jnp\n\ndef factory_jax(points):\n  points = jnp.sort(points)\n  @jax.jit\n  def fwd(x):\n    i = 1 + jnp.where(x < points[1:, 0], size=1, fill_value=len(points) - 2)[0][0]\n    return (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * x + (points[i, 1] - (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * points[i, 0])\n  return fwd\n\nprint(factory_jax(points)(x))\n# 3.0\nIf I've understood the intended input shapes for your code, I believe this should compute the same results as your orginal function."
        ],
        "link": "https://stackoverflow.com/questions/73693195/how-to-write-this-function-jax-jit-able"
    },
    {
        "title": "If there are two functions - one with jit and other without, and when I iterate them for 100 times, unjit function gives me a less time than jit one",
        "question": "import jax\nimport numpy as np\nimport jax.numpy as jnp\na = []\na_jax = []\nfor i in range(10000):\n a.append(np.random.randint(1, 5, (5,)))\n a_jax.append(jnp.array(a[i]))\n\n# a_jax = jnp.array(a_jax)\n@jax.jit\ndef calc_add_with_jit(a, b):\n return a + b\ndef calc_add_without_jit(a, b):\n return a + b\ndef main_function_with_jit():\n for i in range(99):\n  calc_add_with_jit(a_jax[i], a_jax[i+1]) \ndef main_function_without_jit():\n for i in range(99):\n  calc_add_without_jit(a[i], a[i+1])\n\n%time calc_add_with_jit(a_jax[1], a_jax[2])\n%time main_function_with_jit()\n%time main_function_without_jit()\nNow the first %time results in 3.33 ms wall time, Second %time function results in 5.58 ms of time, Third %time results in 156 microseconds of time\nCan anyone explain why is this happening? Why is JAX-JIT slower compared to regular code? I am talking about second and third time function results",
        "answers": [
            "This question is pretty well answered in the JAX documentation; see FAQ: Is JAX Faster Than NumPy? In particular, quoting from the summary:\nif you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead. If you’re running your code on GPU or TPU, or are benchmarking more complicated JIT-compiled sequences of operations on CPU, you can generally expect JAX to outperform NumPy.\nYou are benchmarking sequences of individually-dispatched single operations on CPU, which is precisely the regime that NumPy is designed and optimized for, and so you can expect that NumPy will be faster."
        ],
        "link": "https://stackoverflow.com/questions/73681597/if-there-are-two-functions-one-with-jit-and-other-without-and-when-i-iterate"
    },
    {
        "title": "How to map the kronecker product along array dimensions?",
        "question": "Given two tensors A and B with the same dimension (d>=2) and shapes [A_{1},...,A_{d-2},A_{d-1},A_{d}] and [A_{1},...,A_{d-2},B_{d-1},B_{d}] (shapes of the first d-2 dimensions are identical).\nIs there a way to calculate the kronecker product over the last two dimensions? The shape of my_kron(A,B)should be [A_{1},...,A_{d-2},A_{d-1}*B_{d-1},A_{d}*B_{d}]. For example with d=3,\nA.shape=[2,3,3]\nB.shape=[2,4,4]\nC=my_kron(A,B)\nC[0,...] should be the kronecker product of A[0,...] and B[0,...] and C[1,...] the kronecker product of A[1,...] and B[1,...].\nFor d=2 this is simply what the jnp.kron(or np.kron) function does.\nFor d=3 this can be achived with jax.vmap. jax.vmap(lambda x, y: jnp.kron(x[0, :], y[0, :]))(A, B)\nBut I was not able to find a solution for general (unknown) dimensions. Any suggestions?",
        "answers": [
            "In numpy terms I think this is what you are doing:\nIn [104]: A = np.arange(2*3*3).reshape(2,3,3)\nIn [105]: B = np.arange(2*4*4).reshape(2,4,4)\n\nIn [106]: C = np.array([np.kron(a,b) for a,b in zip(A,B)])\nIn [107]: C.shape\nOut[107]: (2, 12, 12)\nThat treats the initial dimension, the 2, as a batch. One obvious generalization is to reshape the arrays, reducing the higher dimensions to 1, e.g. reshape(-1,3,3), etc. And then afterwards, reshape C back to the desired n-dimensions.\nnp.kron does accept 3d (and higher), but it's doing some sort of outer on the shared 2 dimension:\nIn [108]: np.kron(A,B).shape\nOut[108]: (4, 12, 12)\nAnd visualizing that 4 dimension as (2,2), I can take the diagonal and get your C:\nIn [109]: np.allclose(np.kron(A,B)[[0,3]], C)\nOut[109]: True\nThe full kron does more calculations than needed, but is still faster:\nIn [110]: timeit C = np.array([np.kron(a,b) for a,b in zip(A,B)])\n108 µs ± 2.23 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nIn [111]: timeit np.kron(A,B)[[0,3]]\n76.4 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nI'm sure it's possible to do your calculation in a more direct way, but doing that requires a better understanding of how the kron works. A quick glance as the np.kron code suggest that is does an outer(A,B)\nIn [114]: np.outer(A,B).shape\nOut[114]: (18, 32)\nwhich has the same number of elements, but it then reshapes and concatenates to produce the kron layout.\nBut following a hunch, I found that this is equivalent to what you want:\nIn [123]: D = A[:,:,None,:,None]*B[:,None,:,None,:]\nIn [124]: np.allclose(D.reshape(2,12,12),C)\nOut[124]: True\nIn [125]: timeit np.reshape(A[:,:,None,:,None]*B[:,None,:,None,:],(2,12,12))\n14.3 µs ± 184 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\nThat is easily generalized to more leading dimensions.\ndef my_kron(A,B):\n   D = A[...,:,None,:,None]*B[...,None,:,None,:]\n   ds = D.shape\n   newshape = (*ds[:-4],ds[-4]*ds[-3],ds[-2]*ds[-1])\n   return D.reshape(newshape)\n\nIn [137]: my_kron(A.reshape(1,2,1,3,3),B.reshape(1,2,1,4,4)).shape\nOut[137]: (1, 2, 1, 12, 12)"
        ],
        "link": "https://stackoverflow.com/questions/73673599/how-to-map-the-kronecker-product-along-array-dimensions"
    },
    {
        "title": "JAX - jitting functions: parameters vs \"global\" variables",
        "question": "I've have the following doubt about Jax. I'll use an example from the official optax docs to illustrate it:\ndef fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n  opt_state = optimizer.init(params)\n\n  @jax.jit\n  def step(params, opt_state, batch, labels):\n    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss_value\n\n  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n    if i % 100 == 0:\n      print(f'step {i}, loss: {loss_value}')\n\n  return params\n\n# Finally, we can fit our parametrized function using the Adam optimizer\n# provided by optax.\noptimizer = optax.adam(learning_rate=1e-2)\nparams = fit(initial_params, optimizer)\nIn this example, the function step uses the variable optimizer despite it not being passed within the function arguments (since the function is being jitted and optax.GradientTransformation is not a supported type). However, the same function uses other variables that are instead passed as parameters (i.e., params, opt_state, batch, labels). I understand that jax functions needs to be pure in order to be jitted, but what about input (read-only) variables. Is there any difference if I access a variable by passing it through the function arguments or if I access it directly since it's in the step function scope? What if this variable is not constant but modified between separate step calls? Are they treated like static arguments if accessed directly? Or are they simply jitted away and so modifications of such parameters will not be considered?\nTo be more specific, let's look at the following example:\ndef fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n  opt_state = optimizer.init(params)\n  extra_learning_rate = 0.1\n\n  @jax.jit\n  def step(params, opt_state, batch, labels):\n    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    updates *= extra_learning_rate # not really valid code, but you get the idea\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss_value\n\n  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n    extra_learning_rate = 0.1\n    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n    extra_learning_rate = 0.01 # does this affect the next `step` call?\n    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n\n  return params\nvs\ndef fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n  opt_state = optimizer.init(params)\n  extra_learning_rate = 0.1\n\n  @jax.jit\n  def step(params, opt_state, batch, labels, extra_lr):\n    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    updates *= extra_lr # not really valid code, but you get the idea\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss_value\n\n  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n    extra_learning_rate = 0.1\n    params, opt_state, loss_value = step(params, opt_state, batch, labels, extra_learning_rate)\n    extra_learning_rate = 0.01 # does this now affect the next `step` call?\n    params, opt_state, loss_value = step(params, opt_state, batch, labels, extra_learning_rate)\n\n  return params\nFrom my limited experiments, they perform differently as the second step call doesn't uses the new learning rates in the global case and also no 're-jitting' happens, however I'd like to know if there's any standard practice/rules I need to be aware of. I'm writing a library where performance is fundamental and I don't want to miss some jit optimizations because I'm doing things wrong.",
        "answers": [
            "During JIT tracing, JAX treats global values as implicit arguments to the function being traced. You can see this reflected in the jaxpr representing the function.\nHere are two simple functions that return equivalent results, one with implicit arguments and one with explicit:\nimport jax\nimport jax.numpy as jnp\n\ndef f_explicit(a, b):\n  return a + b\n\ndef f_implicit(b):\n  return a_global + b\n\na_global = jnp.arange(5.0)\nb = jnp.ones(5)\n\nprint(jax.make_jaxpr(f_explicit)(a_global, b))\n# { lambda ; a:f32[5] b:f32[5]. let c:f32[5] = add a b in (c,) }\n\nprint(jax.make_jaxpr(f_implicit)(b))\n# { lambda a:f32[5]; b:f32[5]. let c:f32[5] = add a b in (c,) }\nNotice the only difference in the two jaxprs is that in f_implicit, the a variable comes before the semicolon: this is the way that jaxpr representations indicate the argument is passed via closure rather than via an explicit argument. But the computation generated by these two functions will be identical.\nThat said, one difference to be aware of is that when an argument passed by closure is a hashable constant, it will be treated as static within the traced function (similar when explicit arguments are marked static via static_argnums or static_argnames within jax.jit):\na_global = 1.0\nprint(jax.make_jaxpr(f_implicit)(b))\n# { lambda ; a:f32[5]. let b:f32[5] = add 1.0 a in (b,) }\nNotice in the jaxpr representation the constant value is inserted directly as an argument to the add operation. The explicit way to to get the same result for a JIT-compiled function would look something like this:\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['a'])\ndef f_explicit(a, b):\n  return a + b"
        ],
        "link": "https://stackoverflow.com/questions/73621269/jax-jitting-functions-parameters-vs-global-variables"
    },
    {
        "title": "How to vmap over specific funciton in jax?",
        "question": "I have this function which works for single vector:\ndef vec_to_board(vector, player, dim, reverse=False):\n    player_board = np.zeros(dim * dim)\n    player_pos = np.argwhere(vector == player)\n    if not reverse:\n        player_board[mapping[player_pos.T]] = 1\n    else:\n        player_board[reverse_mapping[player_pos.T]] = 1\n    return np.reshape(player_board, [dim, dim])\nHowever, I want it to work for a batch of vectors.\nWhat I have tried so far:\nstates = jnp.array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]])\n batch_size = 1\nb_states = vmap(vec_to_board)((states, 1, 4), batch_size)\nThis doesn't work. However, if I understand correctly vmap should be able to handle this transformation for batches?",
        "answers": [
            "There are a couple issues you'll run into when trying to vmap this function:\nThis function is defined in terms of numpy arrays, not jax arrays. How do I know? JAX arrays are immutable, so things like arr[idx] = 1 will raise errors. You need to replace these with equivalent JAX operations (see JAX Sharp Bits: in-place updates) and ensure your function works with JAX array operations rather than numpy array operations.\nYour function makes used of dynamically-shaped arrays; e.g. player_pos, has a shape dependent on the number of nonzero entries in vector == player. You'll have to rewrite your function in terms of statically-shaped arrays. There is some discussion of this in the jnp.argwhere docstring; for example, if you know a priori how many True entries you expect in the array, you can specify the size to make this work.\nGood luck!"
        ],
        "link": "https://stackoverflow.com/questions/73588309/how-to-vmap-over-specific-funciton-in-jax"
    },
    {
        "title": "Jax.lax.scan with arguments?",
        "question": "I'm trying to speed up the execution of my code rewriting for loops into jax.lax.scan, but I ran into the issue that I need the scanFunction to handle parameters passed to the main function - but how to do it?\nHere I get NameError: name 'coefs' is not defined. from within ParseRow, but of course this is natural since I didn't pass coefs into ParseRow.\nfrom functools import partial\nimport jax.lax\nimport jax.numpy as jnp\nimport jaxopt\nimport numpy as np\n\ndataList = [[1, 1.36, 3.41, 5, 7, 2, 6, 12, 5, 10, 1, 7, 1, 12, 10, 12, 4, 10, 12, 7, 11, 5, 10, 3, 6, 12, 6, 5, 3, 5, 9, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0], [1, 1.45, 2.77, 5, 7, 2, 6, 12, 5, 10, 1, 7, 1, 12, 10, 12, 4, 10, 12, 7, 11, 5, 10, 3, 6, 12, 6, 5, 3, 5, 9, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0], [1, 1.73, 2.21, 5, 7, 2, 6, 12, 5, 10, 1, 7, 1, 12, 10, 12, 4, 10, 12, 7, 11, 5, 10, 3, 6, 12, 6, 5, 3, 5, 9, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0]]\ndataTuple = tuple(tuple(sub) for sub in dataList)\nstart = np.random.sample(532)\n\ndef ParseRow (carry, row):\n    balance = 0.0\n\n    result = row[0]\n    fOdds = row[1]\n    dOdds = row[2]\n\n    hCoefs = jnp.array(np.zeros(12))\n    \n    for p in range (0, 14):\n        s = (row[3+p]-1).astype(int)\n        h = (row[17+p]).astype(int)\n        r = (row[43+p]).astype(int)\n                \n        bCoef = coefs[p]\n        sCoef = coefs[14 + (p * 12) + s]\n        hCoef = coefs[182 + (p * 12) + h]\n            \n        rCoef = jax.lax.cond (r == 1.0, lambda x: x, lambda x: 1.0, coefs[350 +p] )\n                \n        pStrength = bCoef * sCoef * hCoef * rCoef\n\n        hCoefs = hCoefs.at[h].set(hCoefs[h] + pStrength)\n\n    for h in range (0, 12):\n        hSign = (row[31+h]-1).astype(int)\n        hCoefs = hCoefs.at[h].set(hCoefs[h] * coefs [364 + (h*12) + hSign]) \n                            \n    fPoints = 0.0\n    dPoints = 0.0\n            \n    for h in range (0, 12):\n        fPoints += hCoefs[h] * coefs[508+h]\n        dPoints += hCoefs[h] * coefs[520+h]\n    \n    balance += jax.lax.cond (result == 1.0, goodPrediction, lambda x: 0.0, [fOdds, fPoints, dPoints])\n    balance += jax.lax.cond (result == 2.0, goodPrediction, lambda x: 0.0, [dOdds, dPoints, fPoints])\n        \n    return carry + balance, balance\n\ndef goodPrediction (args):\n    return args[0] * 1000 * nn.sigmoid(args[1] - args[2])\n                        \ndef MyFuncJax(coefs, data):\n    balance = float(len(data)*-1000)\n    \n    dataJnp = jnp.asarray(data)\n\n    balance += jax.lax.scan(ParseRow, 0, dataJnp)[0]\n    \n\n    return balance\n    \n        \nmini = jaxopt.GradientDescent(MyFuncJax, stepsize=0.001, maxiter=5000, verbose=0, jit=True)\n\n@partial(jax.jit, static_argnums = (1,) )\ndef jitted_run(start, dataTuple):\n    return mini.run(start, dataTuple)\n\njitted_run(start, dataTuple)",
        "answers": [
            "Generally you can use the carry value to pass along extra data to the body function. So for example you could do something like this:\ndef ParseRow (carry, row):\n    coefs, total = carry\n    # ...\n    return (coefs, total + balance), balance\nand then construct the scan call something like this:\n(coefs, total), result = jax.lax.scan(ParseRow, (coefs, 0), dataJnp)\nbalance += total"
        ],
        "link": "https://stackoverflow.com/questions/73574199/jax-lax-scan-with-arguments"
    },
    {
        "title": "Rewriting for loop with jax.lax.scan",
        "question": "I'm having troubles understanding the JAX documentation. Can somebody give me a hint on how to rewrite simple code like this with jax.lax.scan?\nnumbers = numpy.array( [ [3.0, 14.0], [15.0, -7.0], [16.0, -11.0] ])\nevenNumbers = 0\nfor row in numbers:\n      for n in row:\n         if n % 2 == 0:\n            evenNumbers += 1",
        "answers": [
            "Assuming a solution should demonstrate the concepts rather than optimize the example shown, the function to be jax.lax.scanned must match the expected signature and any dynamic condition has to be replaced with jax.lax.cond. The code below is the closest to the original I could think of, but please be aware that I'm anything but an jaxpert.\nimport jax\nimport jax.numpy as jnp\n\ndef f(carry, row):\n\n    even = 0\n    for n in row:\n        even += jax.lax.cond(n % 2 == 0, lambda: 1, lambda: 0)\n\n    return carry + even, even\n\nnumbers = jnp.array([[3.0, 14.0], [15.0, -7.0], [16.0, -11.0]])\njax.lax.scan(f, 0, numbers)\nOutput\n(DeviceArray(2, dtype=int32, weak_type=True),\n DeviceArray([1, 0, 1], dtype=int32, weak_type=True))"
        ],
        "link": "https://stackoverflow.com/questions/73564732/rewriting-for-loop-with-jax-lax-scan"
    },
    {
        "title": "ERROR: No matching distribution found for jaxlib==0.1.67",
        "question": "I need jaxlib==0.1.67 for a project I'm working on, but I can't downgrade. At the moment I have jaxlib==0.1.75 and my program keeps failing due to an error I can't find a solution to either. I compared all versions of the important packages to another machines versions where my programs runs with no problems and the only difference is the jaxlib version (it's still 0.1.67 on the machine where it runs). I suspect that jaxlib is the issue because the error I get when it's not 0.1.67 is the following:\n    from haiku import data_structures\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/haiku/data_structures.py\", line 17, in <module>\n    from haiku._src.data_structures import to_immutable_dict\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/haiku/_src/data_structures.py\", line 30, in <module>\n    from haiku._src import utils\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/haiku/_src/utils.py\", line 24, in <module>\n    import jax\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/__init__.py\", line 108, in <module>\n    from .experimental.maps import soft_pmap\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/experimental/maps.py\", line 25, in <module>\n    from .. import numpy as jnp\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/numpy/__init__.py\", line 16, in <module>\n    from . import fft\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/numpy/fft.py\", line 17, in <module>\n    from jax._src.numpy.fft import (\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/_src/numpy/fft.py\", line 19, in <module>\n    from jax import lax\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/lax/__init__.py\", line 334, in <module>\n    from jax._src.lax.parallel import (\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/_src/lax/parallel.py\", line 36, in <module>\n    from jax._src.numpy import lax_numpy\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py\", line 51, in <module>\n    from jax import ops\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/ops/__init__.py\", line 16, in <module>\n    from jax._src.ops.scatter import (\n  File \"/net/home/justen/.local/lib/python3.10/site-packages/jax/_src/ops/scatter.py\", line 31, in <module>\n    from typing import EllipsisType\nImportError: cannot import name 'EllipsisType' from 'typing' (/usr/lib/python3.10/typing.py)\nhaiku and typing are the same version on both machines so guess it must be jaxlib. On both machines I'm on pip==20.0.2 and in a python 3.9.9 virtualenv.\nWhen I try to downgrade to jaxlib==0.1.67 I get:\nERROR: Could not find a version that satisfies the requirement jaxlib==0.1.67 (from versions: 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.10, 0.3.14, 0.3.15)\nERROR: No matching distribution found for jaxlib==0.1.67\nI even tried pip install jaxlib==0.1.67 -f https://storage.googleapis.com/jax-releases/jax_releases.html and it doesn't work.\nHas anyone experienced the same problem or maybe has a clue of what could be the issue here to help me?",
        "answers": [
            "Based on the path in the exception (/usr/lib/python3.10), it looks like you are using python 3.10. There are no python 3.10 wheels for jaxlib==0.1.67 (see pypi). You will have to use python 3.6-3.9.\nIf you think you are using python 3.9, then here's a way to clear up confusion when installing packages. Use\npython3.9 -m pip install\nto install packages into your python 3.9 environment. Replace python3.9 with whichever python interpreter you want to use.",
            "The answer by @jkr is the correct answer for your question as written (how to install jaxlib 0.1.67), but I don't think it will fix the initial error you reported.\nThis looks like a Python 3.10 only bug that briefly existed in the JAX source code on October 5, 2021, but was fixed and never actually made it into a jax release. If you're seeing this, I suspect it means you installed/imported JAX from unreleased source. Further, installing a different version of jaxlib will not fix this error, because the code is in jax itself. If you're using jaxlib 0.1.75, you might try installing jax v0.2.7 or v0.2.8, which were released around the same time, and shouldn't contain the problematic EllipsisType import.\nAnother potential issue: you reported using a Python 3.9.9 virtualenv, but your traceback indicates you're executing Python 3.10, so you probably need to check your executable paths to make sure you're executing what you think you are."
        ],
        "link": "https://stackoverflow.com/questions/73555877/error-no-matching-distribution-found-for-jaxlib-0-1-67"
    },
    {
        "title": "JAX: Getting rid of zero-gradient",
        "question": "Is there a way how to modify this function (MyFunc) so that it gives the same result, but its derivative is not zero gradient?\nfrom jax import grad\nimport jax.nn as nn\nimport numpy as np\n\ndef MyFunc(coefs):\n   a = coefs[0]\n   b = coefs[1]\n   c = coefs[2]\n   \n   if a > b:\n      return 30.0\n   elif b > c:\n      return 20.0\n   else:\n      return 10.0   \n   \nmyFuncDeriv = grad (MyFunc)   \n\n# prints [0. 0. 0.]\nprint (myFuncDeriv(np.random.sample(3)))\n# prints [0. 0. 0.]\nprint (myFuncDeriv(np.array([1.0, 2.0, 3.0])))\nEDIT: Similar function which doesn't give zero gradient - but it doesn't return 30/20/10\ndef MyFunc2(coefs):\n    a = coefs[0]\n    b = coefs[1]\n    c = coefs[2]\n    if a > b:\n        return nn.sigmoid(a)*30.0\n    if b > c:\n        return nn.sigmoid(b)*20.0\n    else:\n        return nn.sigmoid(c)*10.0\n\n\nmyFunc2Deriv = grad (MyFunc2)   \n\n# prints [0.         0.         0.45176652]\nprint (myFuncDeriv(np.array([1.0, 2.0, 3.0])))\n# prints for example [6.1160526 0.        0.       ]\nprint (myFunc2Deriv(np.random.sample(3)))",
        "answers": [
            "The gradient of your function is zero because this is the correct result for the gradient as your function is defined. For more information on this phenomenon, see FAQ: Why are gradients zero for functions based on sort order?\nIf you want a sort-based function with non-zero gradients, you can achieve this by replacing your step-wise function with a smooth approximation. The sigmoid version you included in your question seems like a reasonable approach for this approximation.\nBut note that the answer to your exact question – how to make a function that produces the same output but has nonzero gradients – is impossible, because a function returning the same outputs as yours for all inputs has a zero gradient by definition."
        ],
        "link": "https://stackoverflow.com/questions/73507935/jax-getting-rid-of-zero-gradient"
    },
    {
        "title": "How can I initialize the hidden state (carry) of a (flax linen) GRUCell as a learnable parameter (e.g. using model.init)",
        "question": "I create a GRU model in Jax using Flax and I initialize the model parameters using model.init as follows:\nimport jax.numpy as np\nfrom jax import random\nimport flax.linen as nn\nfrom jax.nn import initializers\n\nclass RNN(nn.Module):\n    n_RNN_units: int\n\n    @nn.compact\n    def __call__(self, carry, inputs):\n        \n        carry, outputs = nn.GRUCell()(carry, inputs)\n        \n        return carry, outputs\n    \n    def init_state(self):\n        \n        return nn.GRUCell.initialize_carry((), (), self.n_RNN_units, init_fn = initializers.zeros)\n\n# instantiate an RNN (GRU) model\nn_RNN_units = 200\nmodel = RNN(n_RNN_units = n_RNN_units)\n\n# initialize the parameters of the model (weights and biases)\ndata_dim = 20\nparams = model.init(carry = np.empty((n_RNN_units,)), inputs = np.empty((data_dim,)), rngs = {'params': random.PRNGKey(1)})\nUnfortuantely for me, the FrozenDict params created by model.init only contains the weight and biases of the GRU, not the initial hidden state (carry). Is there a way that I can tell model.init 1) that I also want to learn the initial hidden state and 2) specify the initializer function for the initial hidden state.\nAlternatively, if there is a better way to do this that does not involve using model.init, feel free to suggest that.\nThanks in advance",
        "answers": [
            "You can use self.param to register a tensor as parameters:\n@nn.compact\ndef __call__(self, inputs, carry=None):\n    if carry is None:\n        # Learnable initial carry\n        carry = self.param('carry_init', lambda rng, shape: jnp.zeros(shape), (self.n_RNN_units,))\n    carry, outputs = nn.GRUCell()(carry, inputs)\n    return carry, outputs\nNow carry_init is in model parameters after model.init(rng, inputs, None).\nWhat happen now is that model.apply takes parameters params with carry_init on it so gradients w.r.t to it will be computed as usual with grad.\nMore precisely when you are making a prediction of a sequence, you have to start your calls with carry, outputs = model.apply(params, inputs). It will use carry_init in params then for the following calls use carry, outputs = model.apply(params, inputs, carry). It will use carry now and the carry_init is indirectly on the computation graph of outputs and carry as the initial carry so you can propagate gradient on it. However you should care about potentially heavy gradient vanishing for it if you have long sequences so you may consider using all the values (especially the first) of your sequences to compute the loss or adapting a dedicated learning rate based on sequence length.\nDetails of linen.Module.param in the Flax documntation Managing Parameters and State."
        ],
        "link": "https://stackoverflow.com/questions/73507268/how-can-i-initialize-the-hidden-state-carry-of-a-flax-linen-grucell-as-a-lea"
    },
    {
        "title": "Recursion in JAX",
        "question": "I'm trying to write a (matrix) exponentiate-by-squaring algorithm in JAX. Unfortunately, I don't understand traced variables very well, which is complicating matters.\nMy code is:\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nimport jax.lax as jlax\n\n@partial(jax.jit, static_argnums=(1,))\ndef matpow(A, n):\n    dim = A.shape[0]\n    return jlax.switch(\n        n,\n        [lambda: jnp.identity(dim),\n         lambda: A,\n         lambda: jlax.cond(\n             jnp.floor_divide(n, 2) == jnp.true_divide(n, 2),\n             lambda: matpow(jnp.dot(A, A), jnp.floor_divide(n, 2)),\n             lambda: jnp.dot(A, matpow(jnp.dot(A, A), jnp.floor_divide(n, 2)))\n         )])\nHowever, attempting to run this with, say, matpow(2 * jnp.eye(4), 5) throws an error midway through compilation:\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 1) of type <class 'jax.interpreters.partial_eval.DynamicJaxprTracer'> for function matpow is non-hashable.\n... I have no idea what that means, to be quite honest, but it's doubly confusing because as far as I can tell n should just be an integer and therefore has a trivial hash.\nOther attempts have included: using jnp.binary_repr (not yet implemented), using np.binary_repr (TracerIntegerConversionError, even though n is marked static), and a 'wrapped' version of the above recursive function in which I defined a separate function recpow internally to matpow (hit the recursion limit.)\nWhat do I need to do to make this code work?",
        "answers": [
            "Unfortunately this kind of approach—trace-time recursion with the stopping condition depending on a traced value—is not possible in JAX, because when the code is traced, the value of n is unknown and so there's no way for the function tracing to terminate.\nYou can fix this by keeping your recursion condition static, which requires not using constructs like lax.switch. Here's an example roughly equivalent to your function:\n@partial(jax.jit, static_argnums=(1,))\ndef matpow(A, n):\n  dim = A.shape[0]\n  n = int(n)\n  if n < 0:\n    raise ValueError(\"n < 0 not implemented\")\n  elif n == 0:\n    return jnp.identity(dim)\n  elif n == 1:\n    return A\n  elif n // 2 == n / 2:\n    return matpow(A @ A, n // 2)\n  else:\n    return A @ matpow(A @ A, n // 2)\nIf you need n to be dynamic, another option is to express your logic in terms of jax.lax.while_loop, which is capable of breaking the loop based on a traced value. An example of a loop-based approach for matrix power can be found in JAX's implementation of jax.numpy.linalg.matrix_power: https://github.com/google/jax/blob/a2a84c40d526855dff158b51f5aceccbca9c953e/jax/_src/numpy/linalg.py#L72-L107"
        ],
        "link": "https://stackoverflow.com/questions/73481498/recursion-in-jax"
    },
    {
        "title": "How to install objax with GPU support?",
        "question": "I have followed the objax documentation to install the library with GPU support: https://objax.readthedocs.io/en/stable/installation_setup.html\ni.e.\npip install --upgrade objax\nCUDA_VERSION=11.6\npip install -f https://storage.googleapis.com/jax-releases/jax_releases.html jaxlib==`python3 -c 'import jaxlib; print(jaxlib.__version__)'`+cuda`echo $CUDA_VERSION | sed s:\\\\\\.::g` \nHowever the last step doesn't work. I get the following error message:\nERROR: Could not find a version that satisfies the requirement jaxlib==0.3.15+cuda116 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15) ERROR: No matching distribution found for jaxlib==0.3.15+cuda116\nI have tried with multiple versions of python/CUDA, but I always get this error.\nExecuting pip install --upgrade pip at the begining does not help.\nSystem description:\nOperating system: Ubuntu 20.04.4 LTS\nCUDA Version: 11.6\nPython version: 3.8.13",
        "answers": [
            "JAX recently updated its GPU installation instructions, which you can find here: https://github.com/google/jax#pip-installation-gpu-cuda\nIn particular, the CUDA wheels are now located at https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo, for example, you can install JAX with\n$ pip install \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nand replace cuda11 and cudnn805 respectively with the appropriate CUDA and CUDNN version for your system, ensuring that they match the versions listed in the index at the above URL.\nI've sent a pull request to the objax repository to update the instructions you were following: https://github.com/google/objax/pull/246",
            "The doc page was last updated 2 year ago. It's outdated and due to this it currently lies. Take a close look at https://storage.googleapis.com/jax-releases/jax_releases.html — there're currently no releases with CUDA.\nAs for no-CUDA versions you can install them directly from PyPI:\npip install jaxlib\nFor CUDA version — try to compile from sources. Or use conda: https://anaconda.org/search?q=jaxlib"
        ],
        "link": "https://stackoverflow.com/questions/73472417/how-to-install-objax-with-gpu-support"
    },
    {
        "title": "Local devices VS non local devices in multi GPU processing",
        "question": "I'm reading JAX documentation on jax.local_devices and in it, it is written:\nLike jax.devices(), but only returns devices local to a given process.\nAnd in jax.devices() it is written:\nReturns a list of all devices for a given backend.\nI don't know what exactly are these local and non-local devices. Could you please elaborate on the difference between these?",
        "answers": [
            "This is discussed in JAX's documentation in Using JAX in multi-host and multi-process environments:\nA process’s local devices are those that it can directly address and launch computations on. For example, on a GPU cluster, each host can only launch computations on the directly attached GPUs. On a Cloud TPU pod, each host can only launch computations on the 8 TPU cores attached directly to that host (see the Cloud TPU System Architecture documentation for more details). You can see a process’s local devices via jax.local_devices().\nThe global devices are the devices across all processes. A computation can span devices across processes and perform collective operations via the direct communication links between devices, as long as each process launches the computation on its local devices. You can see all available global devices via jax.devices(). A process’s local devices are always a subset of the global devices."
        ],
        "link": "https://stackoverflow.com/questions/73458553/local-devices-vs-non-local-devices-in-multi-gpu-processing"
    },
    {
        "title": "How to specify or set a variable to a GPU device",
        "question": "I'm new to JAX and I want to work with multiple GPUs. So far two GPUs (0 and 1) are visible to my JAX.\nimport jax\nimport os\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\nprint(jax.local_devices())\n>>>\n# prints: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]\nWhen I create a NumPy object it will always be in GPU device 0 which I assume is the default one.\nnmp = jax.numpy.ones(4)\nprint(nmp.device())\n>>>\n# Prints: gpu:0\nHow can I send my variable nmp to be stored in gpu:1, the other GPU?",
        "answers": [
            "Use .device_put()\nimport jax\nimport os\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\ndevices = jax.local_devices()\nprint(devices) # >>> [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]\n\nnmp = jax.numpy.ones(4)\nprint(nmp.device()) # >>> gpu:0\n\nnmp = jax.device_put(nmp, jax.devices()[1])\nprint(nmp.device()) # >>> gpu:1"
        ],
        "link": "https://stackoverflow.com/questions/73456614/how-to-specify-or-set-a-variable-to-a-gpu-device"
    },
    {
        "title": "JAX: average per-example gradients different from aggregated gradients",
        "question": "I want to compute per-example gradients to perform per-example clipping before the final gradient descent step.\nI wanted to ensure that the per-example gradients are correct. Therefore I implemented this minimal example below to test standard gradient computation and per-example gradient computation with JAX.\nThe problem I have is, that the average of the per-example gradients differs from the gradients of the standard computation.\nDoes someone see where I went wrong?\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\n\ndef loss(params, x, t):\n    w, b = params\n    y = jnp.dot(w, x.T) + b\n    return ((t.T - y)**2).sum()\n\n\ndef main():\n\n    n_samples = 3\n    dims_in = 7\n    dims_out = 5\n\n    key = random.PRNGKey(0)\n\n    # Random data\n    x = random.normal(key, (n_samples, dims_in), dtype=jnp.float32)\n    t = random.normal(key, (n_samples, dims_out), dtype=jnp.float32)\n    \n    # Random weights\n    w = random.normal(key, (dims_out, dims_in), dtype=jnp.float32)\n    b = random.normal(key, (dims_out, 1), dtype=jnp.float32)\n    params = (w, b)\n\n    # Standard gradient\n    reduced_grads = jax.grad(loss)\n    dw0, db0 = reduced_grads(params, x, t)\n    print(f\"{dw0.shape = }\") \n    print(f\"{db0.shape = }\") \n\n    # Per-example gradients\n    perex_grads = jax.vmap(jax.grad(loss), in_axes=((None, None), 0, 0))\n    dw1, db1 = perex_grads(params, x, t)\n    print(f\"{dw1.shape = }\") \n    print(f\"{db1.shape = }\")\n\n    # Gradients are different!\n    print(jnp.allclose(dw0, jnp.mean(dw1, axis=0)))  # should be True\n    print(jnp.allclose(db0, jnp.mean(db1, axis=0)))  # should be True\n    \n\nif __name__ == \"__main__\":\n    main()",
        "answers": [
            "THe issue is that vmap effectively passes 1D inputs to your function, and your loss function operates differently on 1D vs. 2D inputs. You can see this by comparing the following:\nprint(loss(params, x[0], t[0]))\n# 87.5574\n\nprint(loss(params, x[:1], t[:1]))\n# 18.089672\nIf you inspect the shapes of the intermediate results within the loss function, it should become clear why these differ (e.g. you're summing over a shape (5, 5) array of differences instead of a shape (5, 1) array of differences).\nTo use per-example gradients, you'll need to change the implementation of your loss function so that it returns the correct result whether the input is 1D or 2D. The easiest way to do this is probably to use jnp.atleast_2d to ensure that the inputs are two-dimensional:\ndef loss(params, x, t):\n    x = jnp.atleast_2d(x)\n    t = jnp.atleast_2d(t)\n    w, b = params\n    y = jnp.dot(w, x.T) + b\n    return ((t.T - y)**2).sum()\nAt this point the sum (not the mean) of the per-example gradients will match the full computation:\nprint(jnp.allclose(dw0, jnp.sum(dw1, axis=0)))\n# True\nprint(jnp.allclose(db0, jnp.sum(db1, axis=0)))\n# True"
        ],
        "link": "https://stackoverflow.com/questions/73435394/jax-average-per-example-gradients-different-from-aggregated-gradients"
    },
    {
        "title": "JAX GPU memory usage even with CPU allocation",
        "question": "I'm studying memory allocation of JAX to make my code faster and I found GPU memory usage of JAX even though I set it to use only CPU. My little code is:\nimport jax\nimport jax.numpy as jnp\n\njax.config.update('jax_platform_name', 'cpu')\n\nx=jnp.zeros(10)\n\nfor i in range(10000000000):\n    1+1\nThe for loop is just to see whether this program is using GPU or not.\nAfter this, what I found is it always uses 253MiB of GPU:\n303028 oh 20 0 29.5g 377844 294168 R 100.0 0.1 0:08.21 python ./test.py\nand\nActually the PID 299133 and 299522 are also using GPU memory with JAX set to use CPU. I'm not sure my actual code is much slower than my c++ code because of this but how can I set it not to use GPU at all?",
        "answers": [
            "If you have a gpu-enabled jaxlib installed, JAX will pre-reserve 90% of the GPU memory on import, unless you specify otherwise via appropriate environment variables. Examples are:\nXLA_PYTHON_CLIENT_PREALLOCATE=false disables preallocation behavior\nXLA_PYTHON_CLIENT_MEM_FRACTION=.XX specifies the fraction of memory to preallocate\nSee JAX: GPU Memory Allocation for more information on this.\nIf you want to avoid GPU memory allocation altogether, one option is to re-install a CPU-only jaxlib:\npip install --force-reinstall \"jaxlib[cpu]\""
        ],
        "link": "https://stackoverflow.com/questions/73322760/jax-gpu-memory-usage-even-with-cpu-allocation"
    },
    {
        "title": "Struggling to understand nested vmaps in JAX",
        "question": "I just about understand unnested vmaps, but try as I may, and I have tried my darnedest, nested vmaps continue to elude me. Take the snippet from this text for example\nI don't understand what the axis are in this case. Is the nested vmap(kernel, (0, None)) some sort of partial function application? Why is the function mapped twice? Can someone please explain what is going on behind the scene in other words. What does a nested vmap desugar to?? All the answers that I have found are variants of the same curt explanation: mapping over both axis, which I am struggling with.",
        "answers": [
            "Each time vmap is applied, it maps over a single axis. So say for simplicity that you have a function that takes two scalars and outputs a scalar:\ndef f(x, y):\n  assert jnp.ndim(x) == jnp.ndim(y) == 0  # x and y are scalars\n  return x + y\n\nprint(f(1, 2))\n# 0\nIf you want to apply this function to an array of x values and a single y value, you can do this with vmap:\nf_mapped_over_x = jax.vmap(f, in_axes=(0, None))\n\nx = jnp.arange(5)\nprint(f_mapped_over_x(x, 1))\n# [1 2 3 4 5]\nin_axes=(0, None) means that it is mapped along the leading axis of the first argument, x, and there is no mapping of the second argument, y.\nLikewise, if you want to apply this function to a single x value and an array of y values, you can specify this via in_axes:\nf_mapped_over_y = jax.vmap(f, in_axes=(None, 0))\n\ny = jnp.arange(5, 10)\nprint(f_mapped_over_y(1, y))\n# [ 6  7  8  9 10]\nIf you wish to map the function over both arrays at once, you can do this by specifying in_axes=(0, 0), or equivalently in_axes=0:\nf_mapped_over_x_and_y = jax.vmap(f, in_axes=(0, 0))\n\nprint(f_mapped_over_x_and_y(x, y))\n# [ 5  7  9 11 13]\nBut suppose you want to map first over x, then over y, to get a sort of \"outer-product\" version of the function. You can do this via a nested vmap, first mapping over just x, then mapping over just y:\nf_mapped_over_x_then_y = jax.vmap(jax.vmap(f, in_axes=(None, 0)), in_axes=(0, None))\n\nprint(f_mapped_over_x_then_y(x, y))\n# [[ 5  6  7  8  9]\n#  [ 6  7  8  9 10]\n#  [ 7  8  9 10 11]\n#  [ 8  9 10 11 12]\n#  [ 9 10 11 12 13]]\nThe nesting of vmaps is what lets you map over two axes separately."
        ],
        "link": "https://stackoverflow.com/questions/73212780/struggling-to-understand-nested-vmaps-in-jax"
    },
    {
        "title": "Mapping a vector to a matrix in JAX",
        "question": "I want to optimize with JAX an elements of a vector with a loss function that is a function of a matrix built by the elements of said vector. Specifically, the element of the matrix n,m correspond to the element n+m of the vector. I have tried\ndef get_F_matrix(vector):\n    N = vector.shape[0]\n    F = jnp.zeros((N//2,N//2))\n    for i in range(N//2):\n        for j in range(N//2):\n            F = F.at[i,j].set(vector[i + j])  \n    return F\nbut this is taking a very long time when the vector is of significant size. Does anyone know if there is a way to directly map the vector to the matrix efficiently?",
        "answers": [
            "Seems like you're looking for a moving window function. Code from this GitHub comment:\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, vmap\n\n@partial(jit, static_argnums=(1,))\ndef moving_window(a, size: int):\n    starts = jnp.arange(len(a) - size + 1)\n    return vmap(lambda start: jax.lax.dynamic_slice(a, (start,), (size,)))(starts)\nThis uses pure JAX while your code runs pure Python loops, so the JAX version should be much faster.\nEDIT: Indeed, it is faster:\nIn [7]: vector = jax.numpy.arange(0, 100)\n\nIn [8]: %timeit get_F_matrix(vector)\n9.14 s ± 47.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [9]: %timeit moving_window(vector, 100//2)[:-1]\n671 µs ± 15.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nExample:\n>>> vector = jax.numpy.arange(0, 10)\n>>> get_F_matrix(vector)\nDeviceArray([[0., 1., 2., 3., 4.],\n             [1., 2., 3., 4., 5.],\n             [2., 3., 4., 5., 6.],\n             [3., 4., 5., 6., 7.],\n             [4., 5., 6., 7., 8.]], dtype=float32)\n>>> moving_window(vector, 10//2)[:-1] # chop off the last row\nDeviceArray([[0, 1, 2, 3, 4],\n             [1, 2, 3, 4, 5],\n             [2, 3, 4, 5, 6],\n             [3, 4, 5, 6, 7],\n             [4, 5, 6, 7, 8]], dtype=int32)\nYour function returns an array of float32 while moving_window preserves the type of the original vector, but it should be simple to convert the resulting array to the type you need."
        ],
        "link": "https://stackoverflow.com/questions/72944343/mapping-a-vector-to-a-matrix-in-jax"
    },
    {
        "title": "Is it safe to read the value of numpy.empty or jax.numpy.empty?",
        "question": "In Flax, we typically initialize a model by passing in a random vector and let the library figure the correct shape for the parameters via shape inference. For example, this is what the tutorial did\ndef create_train_state(rng, learning_rate, momentum):\n  \"\"\"Creates initial `TrainState`.\"\"\"\n  cnn = CNN()\n  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return train_state.TrainState.create(\n      apply_fn=cnn.apply, params=params, tx=tx)\nIt is worth noting that the concrete value of jnp.ones([1, 28, 28, 1]) does not matter, as shape inference only relies on its shape. I can replace it with jnp.zeros([1, 28, 28, 1]) or jnp.random(jax.random.PRNGKey(42), [1, 28, 28, 1]), and it will give me the exactly same result.\nMy question is, can I use jnp.empty([1, 28, 28, 1]) instead? I want to use jnp.empty to clarify that we don't care about the value (and it could also be faster but the speedup is negligible). However, there is something called trap representation in C, and it looks like reading from jnp.empty without overwriting it first could trigger undefined behavior. Since Numpy is a light wrapper around C, should I worry about that?\nBonus question: let's forget about Jax and focus on vanilla Numpy. It is safe to read from np.empty([...])? Again, I don't care about the value, but I do care about not getting a segfault.",
        "answers": [
            "Because XLA does not provide a mechanism to create uninitialized memory, in JAX jnp.empty is currently (v0.3.14) equivalent to jnp.zeros (see https://github.com/google/jax/blob/jax-v0.3.14/jax/_src/numpy/lax_numpy.py#L2007-L2009)\nSo at least in the current release, it is safe to refer to the contents of jnp.empty arrays. But if you're going to rely on that property, I'd suggest using jnp.zeros instead, so that if the jnp.empty implementation changes in the future your assumptions will still be valid.\nnp.empty is different: it will include uninitialized values, and so your program's behavior may change unpredictably from run to run if you rely on those values. There's no danger of memory corruption/segfaults when accessing these uninitialized values: the memory is allocated, it's just that the contents are uninitialized and so the values will reflect whatever bits happened to be stored there at the time the block was allocated."
        ],
        "link": "https://stackoverflow.com/questions/72905359/is-it-safe-to-read-the-value-of-numpy-empty-or-jax-numpy-empty"
    },
    {
        "title": "How to reorder different sets of parameters in dm-haiku",
        "question": "In dm-haiku, parameters of neural networks are defined in dictionaries where keys are module (and submodule) names. If you would like to traverse through the values, there are multiple ways of doing so as shown in this dm-haiku issue. However, the dictionary doesn't respect the ordering of the modules and makes it hard to parse submodules. For example, if I have 2 linear layers, each followed by a mlp layer, then using hk.data_structures.traverse(params) will (roughly) return:\n['linear', 'linear_2', 'mlp/~/1', 'mlp/~/2'].\nwhereas I would like it to return:\n['linear', 'mlp/~/1', 'linear_2', 'mlp/~/2'].\nMy reason for wanting this form is if creating an invertible neural network and wanting to reverse the order the params are called, isolating substituent parts for other purposes (e.g. transfer learning), or, in general, wanting more control of how and where to (re)use trained parameters.\nTo deal with this, I've resorted to regex the names and put them in the order that I want, then using hk.data_structures.filter(predicate, params) to filter by the sorted module names. Although, this is quite tedious if I have to remake a regex every time I want to do this.\nI'm wondering if there is a way to convert a dm-haiku dictionary of params to something like a pytree with a hierarchy and ordering that makes this easier? I believe equinox handles parameters in this manner (and I'm going to look more into how that is done soon), but wanted to check to see if I'm overlooking a simple method to allow grouping, reversing, and other permutations of the params's dictionary?",
        "answers": [
            "According to source code https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/filtering.py#L42#L46 haiku use the sorted function of dict (haiku parameters are vanilla dict since 0.0.6) for hk.data_structures.traverse. Therefore you can't get the result you want without modifying the function itself. By the way, I don't get precisely what do you mean by \"to reverse the order the params are called\". All parameters are passed together in input and then the only thing that determines the order of use is the architecture of the function itself so you should manually invert the forward pass but you don't need to change something in params."
        ],
        "link": "https://stackoverflow.com/questions/72860276/how-to-reorder-different-sets-of-parameters-in-dm-haiku"
    },
    {
        "title": "What is the recommended way to do embeddings in jax?",
        "question": "So I mean something where you have a categorical feature $X$ (suppose you have turned it into ints already) and say you want to embed that in some dimension using the features $A$ where $A$ is arity x n_embed.\nWhat is the usual way to do this? Is using a for loop and vmap correct? I do not want something like jax.nn, something more efficient like\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\nFor example consider high arity and low embedding dim.\nIs it jnp.take as in the flax.linen implementation here? https://github.com/google/flax/blob/main/flax/linen/linear.py#L624",
        "answers": [
            "Indeed the typical way to do this in pure jax is with jnp.take. Given array A of embeddings of shape (num_embeddings, num_features) and categorical feature x of integers shaped (n,) then the following gives you the embedding lookup.\njnp.take(A, x, axis=0)  # shape: (n, num_features)\nIf using Flax then the recommended way would be to use the flax.linen.Embed module and would achieve the same effect:\nimport flax.linen as nn\n\nclass Model(nn.Module): \n  @nn.compact\n  def __call__(self, x):\n    emb = nn.Embed(num_embeddings, num_features)(x)  # shape",
            "Suppose that A is the embedding table and x is any shape of indices.\nA[x], which is like jnp.take(A, x, axis=0) but simpler.\nvmap-ed A[x], which parallelizes along axis 0 of x.\nnested vmap-ed A[x], which parallelizes along all axes of x.\nHere are the source code for your reference.\nimport jax\nimport jax.numpy as jnp\n\nembs = jnp.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]], dtype=jnp.float32)\n\nx = jnp.array([[3, 1], [2, 0]], dtype=jnp.int32)\n\nprint(\"\\ntake\\n\", jnp.take(embs, x, axis=0))\nprint(\"\\nuse []\\n\", embs[x])\nprint(\n    \"\\nvmap\\n\",\n    jax.vmap(lambda embs, x: embs[x], in_axes=[None, 0], out_axes=0)(embs, x),\n)\n\nprint(\n    \"\\nnested vmap\\n\",\n    jax.vmap(\n        jax.vmap(lambda embs, x: embs[x], in_axes=[None, 0], out_axes=0),\n        in_axes=[None, 0],\n        out_axes=0,\n    )(embs, x),\n)\nBTW, I learned the nested-vmap trick from the IREE GPT2 model code by James Bradbury."
        ],
        "link": "https://stackoverflow.com/questions/72817730/what-is-the-recommended-way-to-do-embeddings-in-jax"
    },
    {
        "title": "Using JAX with NetworkX",
        "question": "Can I use JIT from JAX with NetworkX algorithms? For instance, if were to compute the average clustering coefficient for a NetworkX graph object, is it possible to use the @jit decorator to speed up my analysis pipeline?",
        "answers": [
            "No, JAX's JIT and other transforms only work with functions implemented via JAX primitives (generally operations defined in jax.lax, jax.numpy, and related submodules). They cannot be used to compile/transform arbitrary Python code."
        ],
        "link": "https://stackoverflow.com/questions/72807939/using-jax-with-networkx"
    },
    {
        "title": "is there a way to trace grads through self.put_variable method in flax?",
        "question": "I would like to trace the grads through the self.put_variable. Is there anyway to make that possible? Or another way to update the param supplied to the module that is traced?\nimport jax \nfrom jax import numpy as jnp \nfrom jax import grad,random,jit,vmap \nimport flax \nfrom flax import linen as nn \n\n\nclass network(nn.Module):\n    input_size : int \n    output_size : int \n    @nn.compact\n    def __call__(self,x):\n        W = self.param('W',nn.initializers.normal(),(self.input_size,self.output_size))\n        b = self.param('b',nn.initializers.normal(),(self.output_size,))\n\n      \n        self.put_variable(\"params\",\"b\",(x@W+b).reshape(5,))  \n    \n        return jnp.sum(x+b)\n\n\nif __name__ == \"__main__\":\n    key = random.PRNGKey(0)\n    key_x,key_param,key = random.split(key,3)\n    x = random.normal(key_x,(1,5))\n\n    module = network(5,5)\n    param = module.init(key_param,x)\n    print(param)\n    #x,param = module.apply(param,x,mutable=[\"params\"])\n    #print(param)\n    print(grad(module.apply,has_aux=True)(param,x,mutable=[\"params\"]))\nmy output grads are :\nFrozenDict({\n    params: {\n        W: DeviceArray([[0., 0., 0., 0., 0.],\n                     [0., 0., 0., 0., 0.],\n                     [0., 0., 0., 0., 0.],\n                     [0., 0., 0., 0., 0.],\n                     [0., 0., 0., 0., 0.]], dtype=float32),\n        b: DeviceArray([1., 1., 1., 1., 1.], dtype=float32),\n    },\nWhat shows that it doesnt trace the grads through the self.variable_put method, as grads to W are all zero, while b clearly relies upon W.",
        "answers": [
            "Just like @jakevdp noted the test above is incorrect as b is still tied to the previous b.\nhttps://github.com/google/flax/discussions/2215 said that self.put_variable is traced.\nTesting if that is actually the case using the code below:\nimport jax \nfrom jax import numpy as jnp \nfrom jax import grad,random,jit,vmap \nimport flax \nfrom flax import linen as nn \n\nclass network(nn.Module):\n    input_size : int \n    output_size : int \n    @nn.compact\n    def __call__(self,x):\n        W = self.param('W',nn.initializers.normal(),(self.input_size,self.output_size))\n        b = self.param('b',nn.initializers.normal(),(self.output_size,))\n\n        b = x@W+b #update the b variable else it is still tied to the previous one.\n        self.put_variable(\"params\",\"b\",(b).reshape(5,))  \n     \n        return jnp.sum(x+b)\n\ndef test_update(param,x):\n    _, param = module.apply(param,x,mutable=[\"params\"])\n    return jnp.sum(param[\"params\"][\"b\"]+x),param \n\nif __name__ == \"__main__\":\n    key = random.PRNGKey(0)\n    key_x,key_param,key = random.split(key,3)\n    x = random.normal(key_x,(1,5))\n\n    module = network(5,5)\n    param = module.init(key_param,x)\n    print(param)\n\n    print(grad(test_update,has_aux=True)(param,x))\noutput:\nFrozenDict({\n    params: {\n        W: DeviceArray([[ 0.01678762,  0.00234134,  0.00906202,  0.00027337,\n                       0.00599653],\n                     [-0.00729604, -0.00417799,  0.00172333, -0.00566238,\n                       0.0097266 ],\n                     [ 0.00378883, -0.00901531,  0.01898266, -0.01733185,\n                      -0.00616944],\n                     [-0.00806503,  0.00409351,  0.0179838 , -0.00238476,\n                       0.00252594],\n                     [ 0.00398197,  0.00030245, -0.00640218, -0.00145424,\n                       0.00956188]], dtype=float32),\n        b: DeviceArray([-0.00905032, -0.00574646,  0.01621638, -0.01165553,\n                     -0.0285466 ], dtype=float32),\n    },\n})\n(FrozenDict({\n    params: {\n        W: DeviceArray([[-1.1489547 , -1.1489547 , -1.1489547 , -1.1489547 ,\n                      -1.1489547 ],\n                     [-2.0069852 , -2.0069852 , -2.0069852 , -2.0069852 ,\n                      -2.0069852 ],\n                     [ 0.98777294,  0.98777294,  0.98777294,  0.98777294,\n                       0.98777294],\n                     [ 0.9311977 ,  0.9311977 ,  0.9311977 ,  0.9311977 ,\n                       0.9311977 ],\n                     [-0.2883922 , -0.2883922 , -0.2883922 , -0.2883922 ,\n                      -0.2883922 ]], dtype=float32),\n        b: DeviceArray([1., 1., 1., 1., 1.], dtype=float32),\n    },\n}), FrozenDict({\n    params: {\n        W: DeviceArray([[ 0.01678762,  0.00234134,  0.00906202,  0.00027337,\n                       0.00599653],\n                     [-0.00729604, -0.00417799,  0.00172333, -0.00566238,\n                       0.0097266 ],\n                     [ 0.00378883, -0.00901531,  0.01898266, -0.01733185,\n                      -0.00616944],\n                     [-0.00806503,  0.00409351,  0.0179838 , -0.00238476,\n                       0.00252594],\n                     [ 0.00398197,  0.00030245, -0.00640218, -0.00145424,\n                       0.00956188]], dtype=float32),\n        b: DeviceArray([-0.01861148, -0.00523183,  0.03968921, -0.01952654,\n                     -0.06145691], dtype=float32),\n    },\n}))\nThe first FrozenDict is the original parameters.\nThe second FrozenDict is the grads, clearly being traced through self.put_variable.\nThe last FrozenDict is the parameters, where we can see that b is correctly updated.",
            "The output of your model is jnp.sum(x + b), which has no dependence on W, which in turn implies that the gradient with respect to W should be zero. With this in mind, the output you show above looks to be correct.\nEdit: It sounds like you're expecting the result of x@W+b that you used in your variable to be reflected in the value of b used in the return statement; perhaps you want something like this?\n    def __call__(self,x):\n        W = self.param('W',nn.initializers.normal(),(self.input_size,self.output_size))\n        b = self.param('b',nn.initializers.normal(),(self.output_size,))\n\n        b = x@W+b\n        self.put_variable(\"params\",\"b\",b.reshape(5,)) \n    \n        return jnp.sum(x+b)\nThat said, it's unclear to me from the question what your ultimate goal is, and given that you're asking about such an uncommon construct, I suspect this may be an XY problem. Perhaps you can edit your question to say more about what you're trying to accomplish."
        ],
        "link": "https://stackoverflow.com/questions/72705707/is-there-a-way-to-trace-grads-through-self-put-variable-method-in-flax"
    },
    {
        "title": "Can you update parameters of a module from inside the nn.compact of that module? (self modifying networks)",
        "question": "I'm quite new to flax and I was wondering what the correct way is to get this behavior:\nparam = f.init(key,x) \nnew_param, y = f.apply(param,x) \nWhere f is a nn.module instance.\nWhere f might go through multiple operations to get new_param and that those operations might rely on the intermediate param to produce their output.\nSo basically, is there a way I can access and update the parameters supplied to an instance of nn.module from within the __call__, while not losing the functional property so it can all be wrapped with the grad function transform.",
        "answers": [
            "You can treat your parameter as mutable var. Just reference to https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.BatchNorm.html\n@nn.compact\ndef __call__(self, x):\n    some_params = self.variable('mutable_params', 'some_params', init_fn)\n    # 'mutable_params' is the variable collection name\n    # at the same \"level\" as 'params'\nvars_init = model.init(key, x)\n# vars_init = {'params': nested_dict_for_params, 'mutable_params': nested_dict_for_mutable_params}\ny, mutated_vars = model.apply(vars_init, x, mutable=['mutable_params'])\nvars_new = vars_init | mutated_vars # I'm not sure frozendict support | op\n# equiv to vars_new = {'params': vars_init['params'], 'mutable_params': mutated_vars['mutable_params']}"
        ],
        "link": "https://stackoverflow.com/questions/72659535/can-you-update-parameters-of-a-module-from-inside-the-nn-compact-of-that-module"
    },
    {
        "title": "Modify an array from indexes contained in another array",
        "question": "I have an array of the shape (2,10) such as:\narr = jnp.ones(shape=(2,10)) * 2\nor\n[[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\nand another array, for example [2,4].\nI want the second array to tell from which index the elements of arr should be masked. Here the result would be:\n[[2. 2. -1. -1. -1. -1. -1. -1. -1. -1.]\n [2. 2. 2. 2.  -1. -1. -1. -1. -1. -1.]]\nI need to use jax.numpy and the answer to be vectorized and fast if possible, i.e. not using loops.",
        "answers": [
            "You can do this with a vmapped three-term jnp.where statement. For example:\nimport jax.numpy as jnp\nimport jax\n\narr = jnp.ones(shape=(2,10)) * 2\nidx = jnp.array([2, 4])\n\n@jax.vmap\ndef f(row, ind):\n  return jnp.where(jnp.arange(len(row)) < ind, row, -1)\n\nf(arr, idx)\n# DeviceArray([[ 2.,  2., -1., -1., -1., -1., -1., -1., -1., -1.],\n#              [ 2.,  2.,  2.,  2., -1., -1., -1., -1., -1., -1.]], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/72553056/modify-an-array-from-indexes-contained-in-another-array"
    },
    {
        "title": "Calculate only lower triangular elements of a matrix OR calculation on all possible pairs of the elements of a vector with jax",
        "question": "Is it possible to efficiently run some calculation on all possible pairs of the elements of a vector? I.e. I want to fill the lower triangular elements of a matrix (possibly flattened).\nI.e. I want to:\ncalculate do_my_calculation(input_vector[i], input_vector[j])\nfor all i, j in [1, length(input_vector)] and j < i\nsave all the results\nThe shape of the results is not terribly important. If I can choose however, I would prefer a vector corresponding to an unrolled of the triangular (i, j) matrix however.\nTo illustrate what I would like to do in pseudo-python:\ninput_vector = np.arange(100)\nresult_vector = []\n\nfor i in range(1, len(input_vector)):\n    for j in range(0, i):\n        result_vector.append(do_my_calculation(input_vector[i], input_vector[j])\nNote: For this question, the types of input_vector and result_vector in the above code are not pertinent. Equally, I am of course happy to preallocate result_vector if required. I am using a list for the sake of conciseness of the sample code.\nEdit 1: concrete example as requested by @ddejohn\nNote: The question is not whether I can get this to run in jax but whether I can get it to run efficiently, i.e. vectorized .\n# Set up the problem\nimport numpy as np\n\ndim = 15\ninput_vector_x = np.random.rand(dim)\ninput_vector_y = np.random.rand(dim)\noutput_vector = np.empty(np.tril_indices(dim, k=-1)[0].size)\n\nassert input_vector_x.size == input_vector_y.size\n\n# alternative implementation 1\ncounter = 0\nfor i in range(1, input_vector_x.size):\n    for j in range(0, i):\n        output_vector[counter] = (input_vector_y[j] - input_vector_y[i]) / (input_vector_x[j] - input_vector_x[i])\n        counter += 1\n\n# alternative implementation 2\n\nindices = np.tril_indices(dim, k=-1)\ni = indices[0]\nj = indices[1]\n\noutput_vector = (input_vector_y[j] - input_vector_y[i]) / (input_vector_x[j] - input_vector_x[i])",
        "answers": [
            "There are a few ways to approach this. If you want to compute the full matrix of pairwise results, you could use typical numpy-style broadcasting, assuming your function supports it. Similarly, you could use JAX's Automatic Vectorization (vmap) functionality whether or not your function is compatible with broadcasting.\nIf you really wish to only compute each value once, you can do this using the lower or upper triangular indices. Note that although this performs fewer operations, you may find that in practice it's faster, particularly on accelerators like GPU and TPU, to compute the full result. The reason for this is that multi-dimensional indexing (the gather operation) is generally relatively expensive on this kind of hardware, so the overhead of doubling the number of function calls may be preferable.\nHere's a demonstration of these three approaches:\nimport jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(5748395)\ndim = 3\nx = jax.random.uniform(key, (dim,))\n\ndef f(x1, x2):\n  return (x1 * x2) / (x1 + x2)\n\n# Option 1: full result, broadcasted operations\nprint(f(x[:, None], x[None, :]))\n# [[0.34950745 0.00658672 0.28704265]\n#  [0.00658672 0.00332469 0.00655982]\n#  [0.28704265 0.00655982 0.24352014]]\n\n# Option 2: full result, via vmap\nf_mapped = jax.vmap(jax.vmap(f, (None, 0)), (0, None))\nprint(f_mapped(x, x))\n# [[0.34950745 0.00658672 0.28704265]\n#  [0.00658672 0.00332469 0.00655982]\n#  [0.28704265 0.00655982 0.24352014]]\n\n# Option 3: explicitly computing at lower-triangular indices\ni, j = jnp.tril_indices(dim)\nout_tril = f(x[i], x[j])\nprint(out_tril)\n# [0.34950745 0.00658672 0.00332469 0.28704265 0.00655982 0.24352014]\n\nprint(jnp.zeros((dim, dim)).at[i, j].set(out_tril))\n# [[0.34950745 0.         0.        ]\n#  [0.00658672 0.00332469 0.        ]\n#  [0.28704265 0.00655982 0.24352014]]"
        ],
        "link": "https://stackoverflow.com/questions/72535103/calculate-only-lower-triangular-elements-of-a-matrix-or-calculation-on-all-possi"
    },
    {
        "title": "How to use Jax vmap over zipped arguments?",
        "question": "I have the following example code that works with a regular map\ndef f(x_y):\n    x, y = x_y\n    return x.sum() + y.sum()\n\nxs = [jnp.zeros(3) for i in range(4)]\nys = [jnp.zeros(2) for i in range(4)]\n\nlist(map(f, zip(xs, ys)))\n\n# returns:\n[DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32)]\nHow can I use jax.vmap instead? The naive thing is:\njax.vmap(f)(zip(xs, ys))\nbut this gives:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())",
        "answers": [
            "For using jax.vmap, you do not need to zip your variables. You can write what you want like below:\nimport jax.numpy as jnp\nfrom jax import vmap\n\ndef f(x_y):\n    x, y = x_y\n    return x.sum() + y.sum()\n\nxs = jnp.zeros((4,3))\nys = jnp.zeros((4,2))\nvmap(f)((xs, ys))\nOutput:\nDeviceArray([0., 0., 0., 0.], dtype=float32)",
            "vmap is designed to map over multiple variables by default, so no zip is needed. Furthermore, it can only map over array axes, not over elements of lists or tuples. So a more canonical way to to write your example would be to convert your lists to arrays and do something like this:\ndef g(x, y):\n  return x.sum() + y.sum()\n\nxs_arr = jnp.asarray(xs)\nys_arr = jnp.asarray(ys)\n\njax.vmap(g)(xs_arr, ys_arr)\n# DeviceArray([0., 0., 0., 0.], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/72509839/how-to-use-jax-vmap-over-zipped-arguments"
    },
    {
        "title": "How to get keys for jax.tree_flatten object?",
        "question": "Consider a simple nested config\np = {'a': {'b': 1.0, 'c': 2.0}}\njax.tree_flatten(p)\np = {'a': {'b': 1.0, 'c': 2.0}}\njax.tree_flatten(p)\n([1.0, 2.0], PyTreeDef({'a': {'b': *, 'c': *}}))\nHow can I get some kind of labels like ['a.b', 'a.c'] or anything else reasonable in the order is for tree_flatten?",
        "answers": [
            "There is no mechanism for this built in to jax.tree_util. In a way, the question is ill-posed: tree flattening is applicable to a far more general class of objects than nested dicts as in your example; you can even define pytree flattening for any arbitrary object (see https://jax.readthedocs.io/en/latest/pytrees.html#extending-pytrees), and it's not clear to me how you'd construct labels for flattened objects in this general case.\nIf you're only concerned with nested dicts and you want to generate these kinds of flattened labels, your best bet is probably to write your own Python code to construct the flattened keys and values; for example something like this might work:\np = {'a': {'b': 1.0, 'c': 2.0}}\n\ndef flatten(p, label=None):\n  if isinstance(p, dict):\n    for k, v in p.items():\n      yield from flatten(v, k if label is None else f\"{label}.{k}\")\n  else:\n    yield (label, p)\n\nprint(dict(flatten(p)))\n# {'a.b': 1.0, 'a.c': 2.0}"
        ],
        "link": "https://stackoverflow.com/questions/72425919/how-to-get-keys-for-jax-tree-flatten-object"
    },
    {
        "title": "Get dictionary keys for given batched values - Python",
        "question": "I defined a dictionary A and would like to find the keys given a batch of values a:\ndef dictionary(r):\n return dict(enumerate(r))\n\ndef get_key(val, my_dict):\n   for key, value in my_dict.items():\n      if np.array_equal(val,value):\n          return key\n    \n\n # dictionary\n A = jnp.array([[0, 0],[1,1],[2,2],[3,3]])\n A = dictionary(A)\n\n a = jnp.array([[[1, 1],[2, 2], [3,3]],[[0, 0],[3, 3], [2,2]]])\n keys = jax.vmap(jax.vmap(get_key, in_axes=(0,None)), in_axes=(0,None))(a, A)\nThe expected output should be: keys = [[1,2,3],[0,3,2]]\nWhy am I getting Noneas an output?",
        "answers": [
            "JAX transforms like vmap work by tracing the function, meaning they replace the value with an abstract representation of the value to extract the sequence of operations encoded in the function (See How to think in JAX for a good intro to this concept).\nWhat this means is that to work correctly with vmap, a function can only use JAX methods, not numpy methods, so your use of np.array_equal breaks the abstraction.\nUnfortunately, there's not really any replacement for it, because there's no mechanism to look up an abstract JAX value in a concrete Python dictionary. If you want to do dict lookups of JAX values, you should avoid transforms and just use Python loops:\nkeys = jnp.array([[get_key(x, A) for x in row] for row in a])\nOn the other hand, I suspect this is more of an XY problem; your goal is not to look up dictionary values within a jax transform, but rather to solve some problem. Perhaps you should ask a question about how to solve the problem, rather than how to get around an issue with the solution you have tried.\nBut if you're willing to not directly use the dict, an alternative get_key implementation that is compatible with JAX might look something like this:\ndef get_key(val, my_dict):\n  keys = jnp.array(list(my_dict.keys()))\n  values = jnp.array(list(my_dict.values()))\n  return keys[jnp.where((values == val).all(-1), size=1)]"
        ],
        "link": "https://stackoverflow.com/questions/72390628/get-dictionary-keys-for-given-batched-values-python"
    },
    {
        "title": "Check if 2D sub-array is ordered - Pyhthon JAX",
        "question": "Let us suppose that we have an array ordered. We want to check if the sub-arrays t and  t_inv are following the same order as the imposed order inorder array.\nReading from left to right: the first element is [0,0] and so on until [0,3]. t_inv is inversed because the first to elements are swapped, they do not follow the ordering as in ordered.\n# imposed order \nordered = jnp.array([[0, 0],[0,1],[0,2],[0,3]])\n\n# array with permuted order\nt = jnp.array([[[0, 0],[0, 1], [0,3]]])\nt_inv = jnp.array([[[0, 1],[0, 0], [0,3]]])\nI expect the following:\n result: ordered(t) = 1, because \"ordered\"  \nand ordered(t_inv) = -1, because \"swapped/not ordered\"\nHow can you check that the sub arrays are indeed part of the ordered array and ouput whether the order is correct or not?",
        "answers": [
            "You could do something like this:\nimport jax.numpy as jnp\n\n# imposed order \nordered = jnp.array([[0, 0],[0,1],[0,2],[0,3]])\n\n# array with permuted order\nt = jnp.array([[0, 0],[0, 1], [0,3]])\nt_inv = jnp.array([[0, 1],[0, 0], [0,3]])\n\n\ndef is_sorted(t, ordered):\n  index = jnp.where((t[:, None] == ordered).all(-1))[1]\n  return jnp.where((index == jnp.sort(index)).all(), 1, -1)\n\nprint(is_sorted(t, ordered))\n# 1\nprint(is_sorted(t_inv, ordered))\n# -1\nScaling-wise, it might be faster to use a solution based on searchsorted, but the current implementation of jnp.searchsorted in JAX is relatively slow because XLA doesn't have any native binary search algorithm, so in practice the full pairwise comparison can often be more performant."
        ],
        "link": "https://stackoverflow.com/questions/72381278/check-if-2d-sub-array-is-ordered-pyhthon-jax"
    },
    {
        "title": "Get batched indices from stacked matrices - Python Jax",
        "question": "I would like to extract the indices of stacked matrices.\nLet us say we have an array a of dimension (3, 2, 4), meaning that we have three arrays of dimension (2,4) and a list of indices (3, 2).\ndef get_cols(x,idx):  \n  x = x[:,idx]\n  return x\n\n\nidx = jnp.array([[0,1],[2,3],[1,2]])\n\na = jnp.array([[[1,2,3,4],\n            [3,2,2,4]],\n           \n           [[100,20,3,50],\n            [5,5,2,4]],\n                         \n           [[1,2,3,4],\n            [3,2,2,4]]\n           ])\n\n\n\ne = jax.vmap(get_cols, in_axes=(None,0))(a,idx)\nI want to extract the columns of the different matrices given a batch of indices. I expect the following result:\ne = [[[[1,2],\n  [3,2]],\n\n  [[100,20],\n  [5,5]],\n\n  [[1,2],\n  [3,2]]],\n \n \n \n [[[3,4],\n  [2,4]],\n  \n  [[3,50],\n  [2,4]],\n  \n  [[3,4],\n  [2,4]]],\n \n \n \n \n[[[2,3],\n[2,2]],\n           \n[[20,3],\n [5,2]],\n                         \n[[2,3],\n[2,2]]]]\nWhat am I missing?",
        "answers": [
            "It looks like you're interested in a double vmap over the inputs; e.g. something like this:\ne = jax.vmap(jax.vmap(get_cols, in_axes=(0, None)), in_axes=(None, 0))(a, idx)\nprint(e)\n[[[[  1   2]\n   [  3   2]]\n\n  [[100  20]\n   [  5   5]]\n\n  [[  1   2]\n   [  3   2]]]\n\n\n [[[  3   4]\n   [  2   4]]\n\n  [[  3  50]\n   [  2   4]]\n\n  [[  3   4]\n   [  2   4]]]\n\n\n [[[  2   3]\n   [  2   2]]\n\n  [[ 20   3]\n   [  5   2]]\n\n  [[  2   3]\n   [  2   2]]]]"
        ],
        "link": "https://stackoverflow.com/questions/72351994/get-batched-indices-from-stacked-matrices-python-jax"
    },
    {
        "title": "JAX pmap with multi-core CPU",
        "question": "What is the correct method for using multiple CPU cores with jax.pmap?\nThe following example creates an environment variable for SPMD on CPU core backends, tests that JAX recognises the devices, and attempts a device lock.\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'\n\nimport jax as jx\nimport jax.numpy as jnp\n\njx.local_device_count()\n# WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n# 2\n\njx.devices(\"cpu\")\n# [CpuDevice(id=0), CpuDevice(id=1)]\n\ndef sfunc(x): while True: pass\n\njx.pmap(sfunc)(jnp.arange(2))\nExecuting from a jupyter kernel and observing htop shows that only one core is locked\nI receive the same output from htop when omitting the first two lines and running:\n$ env XLA_FLAGS=--xla_force_host_platform_device_count=2 python test.py\nReplacing sfunc with\ndef sfunc(x): return 2.0*x\nand calling\njx.pmap(sfunc)(jnp.arange(2))\n# ShardedDeviceArray([0., 2.], dtype=float32, weak_type=True)\ndoes return a SharedDeviecArray.\nClearly I am not correctly configuring JAX/XLA to use two cores. What am I missing and what can I do to diagnose the problem?",
        "answers": [
            "As far as I can tell, you are configuring the cores correctly (see e.g. Issue #2714). The problem lies in your test function:\ndef sfunc(x): while True: pass\nThis function gets stuck in an infinite loop at trace-time, not at run-time. Tracing happens in your host Python process on a single CPU (see How to think in JAX for an introduction to the idea of tracing within JAX transformations).\nIf you want to observe CPU usage at runtime, you'll have to use a function that finishes tracing and begins running. For that you could use any long-running function that actually produces results. Here is a simple example:\ndef sfunc(x):\n  for i in range(100):\n    x = (x @ x)\n  return x\n\njx.pmap(sfunc)(jnp.zeros((2, 1000, 1000)))"
        ],
        "link": "https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu"
    },
    {
        "title": "How to generate random numbers between 0 and 1 in jax?",
        "question": "How can I generate random numbers between 0 and 1 in jax? Basically I am looking to replicate the following function from numpy in jax.\nnp.random.random(1000)",
        "answers": [
            "The equivalent in jax would be\nfrom jax import random\nkey = random.PRNGKey(758493)  # Random seed is explicit in JAX\nrandom.uniform(key, shape=(1000,))\nFor more information, see the documentation of the jax.random module.\nAlso be aware that JAX's random number generator does not maintain any sort of global state, so you'll need to think about it a bit differently than you may be accustomed to in NumPy. For more background on this, see JAX Sharp Bits: Random Numbers."
        ],
        "link": "https://stackoverflow.com/questions/72320999/how-to-generate-random-numbers-between-0-and-1-in-jax"
    },
    {
        "title": "Pickle changes type in jax",
        "question": "I have a flax struct dataclass containing a jax numpy array.\nWhen I pickle dump this object and load it again, the array is not anymore a jax numpy array and is converted to a numpy array, here is the code to reproduce it:\nimport flax\nimport jax.numpy as jnp\nimport pickle\n\n@flax.struct.dataclass\nclass A:\n    data: jnp.ndarray\n\na = A(data=jnp.zeros((2,2)))\nprint(a, type(a.data))\n\n\n\nwith open('file.pickle', 'wb') as handle:\n    pickle.dump(a, handle)\n    \nwith open('file.pickle', 'rb') as handle:\n    loaded_a = pickle.load(handle)\n\nprint(loaded_a, type(loaded_a.data))\nI don't want this behavior and I'd like it to keep its original type, is it possible ?",
        "answers": [
            "Update: this bug has been fixed in https://github.com/google/jax/pull/10659. Starting in the next release of JAX (v. 0.3.14) pickle and deepcopy should no longer convert JAX arrays to device arrays.\nThis is a known behavior in JAX; see https://github.com/google/jax/issues/2632\nIt's something that the library developers recognize as an unfortunate behavior, but a fix has not yet been prioritized. If you're interested, you might weigh-in on that issue."
        ],
        "link": "https://stackoverflow.com/questions/72191025/pickle-changes-type-in-jax"
    },
    {
        "title": "JAX return types after transformations",
        "question": "Why is the return type of jax.grad different to other jax transformations in the following scenario?\nConsider a function to be transformed by JAX which takes a custom container as an argument\nimport jax\nimport jax.numpy as jnp\nfrom collections import namedtuple\n\n# Define NTContainer using namedtuple factory\nNTContainer = namedtuple(\n    'NTContainer',['name','flag','array']\n)\n\n# Instantiate container\ncontainer = NTContainer(\n   name='composite-container',\n   flag=0,\n   array=jnp.reshape(jnp.arange(6,dtype='f4'),(3,2)),\n)\n\n# Test function\ndef test_func(container : NTContainer):\n    if container.flag == 0:\n        return container.array.sum()\n    return container.array.prod()\nJAX needs to be informed how to handle NTContainer correctly (default namedtuple pytree cannot be used)\n# Register NTContainer pytree to handle static and traceable members\n\ndef unpack_NTC(c):\n    active, passive = (c.array,), (c.name, c.flag)\n    return active, passive\n\ndef repack_NTC(passive, active):\n    (name, flag), (array,) = passive, active\n    return NTContainer(name,flag,value)\n\njax.tree_util.register_pytree_node(\n    NTContainer, unpack_NTC, repack_NTC,\n)\nNow performing several jax transforms and calling with container results in\njax.jit(test_func)(container)\n# DeviceArray(15., dtype=float32)\n\njax.vmap(test_func)(container)\n# DeviceArray([1., 5., 9.], dtype=float32)\n\njax.grad(test_func)(container)\n# NTContainer(name='composite-container',\n#     flag=0,\n#     array=DeviceArray([[1., 1.],\n#         [1., 1.],\n#         [1., 1.]], dtype=float32))\nWhy does the jax.grad transform call return a NTContainer rather than a DeviceArray?",
        "answers": [
            "The short answer is that this is the return value because this is how grad is defined, and to be honest I'm not sure how else it could be defined.\nThinking about these transforms;\njit(f)(x) computes the output of f applied to x: the result is a scalar.\nvmap(f)(x) computes the output of f applied to each element of the arrays in x: the result is a vector.\ngrad(f)(x) computes the gradient of f with respect to each component of x, and the result is one value per component of x.\nSo it makes sense that grad(f)(x) in this case has to return a collection of multiple values, but how should this collection of gradients be packaged?\nJAX could have defined grad so that it always returns these as a list or tuple, but this might make it hard to understand how the output gradients relate to the inputs, particularly for more complicated structures like nested lists, tuples, and dicts. So instead JAX takes these per-component gradients and re-bundles them in the same structure as the input argument: therefore, in this case the result is a NTContainer containing those per-component gradients."
        ],
        "link": "https://stackoverflow.com/questions/72066135/jax-return-types-after-transformations"
    },
    {
        "title": "Accumulation in JAX",
        "question": "What is the best method for handling memory when compiling an accumulation in JAX, such as jax.lax.scan, where a full buffer is excessive?\nThe following is a geometric progression example. The temptation is to recognise the accumulation only depends on an input size and implement accordingly\nimport jax.numpy as jnp\nimport jax.lax as lax\n\ndef calc_gp_size(size,x0,a):\n    scan_fun = lambda carry, i : (a*carry,)*2\n    xn, x = lax.scan(scan_fun,x0,None,length=size-1)\n    return jnp.concatenate((x0[None],x))\n\njax.config.update(\"jax_enable_x64\", True)\n\nsize = jnp.array(2**26,dtype='u8')\nx0, a = jnp.array([1.0,1.0+1.0e-08],dtype='f8')\n\njax.jit(calc_gp_size)(size,x0,a)\nHowever, attempting to use jax.jit will predictably result in a ConcretizationTypeError.\nThe correct way is to pass an argument where the buffer already exists.\ndef calc_gp_array(array,x0,a):\n    scan_fun = lambda carry, i : (a*carry,)*2\n    xn, x = lax.scan(scan_fun,x0,array)\n    return jnp.concatenate((x0[None],x))\n\narray = jnp.arange(1,2**26,dtype='u8')\nx0, a = jnp.array([1.0,1.0+1.0e-08],dtype='f8')  \n\njax.jit(calc_gp_array)(array,x0,a)\nMy concern is that there is a lot of allocated memory not being utilised (or is it?). Is there a more memory efficient approach to this example, or is the allocated memory being used somehow?\nEDIT: Incorporating the comments of @jakevdp, treating the function as main (single call - include compile and exclude caching), and profiling resulted it\n%memit jx.jit(calc_gp_size, static_argnums=0)(size,x0,a).block_until_ready()\n# peak memory: 7058.32 MiB, increment: 959.94 MiB\n\n%memit jx.jit(calc_gp_array)(jnp.arange(1,size,dtype='u8'),x0,a).block_until_ready()\npeak memory: 7850.83 MiB, increment: 1240.22 MiB\n\n%memit jnp.cumprod(jnp.full(size, a, dtype='f8').at[0].set(x0))\npeak memory: 8150.05 MiB, increment: 1539.70 MiB\nLess granular results would require line profiling the jit code (not sure how this could be done).\nSequentially initialising the array and then calling jax.jit appears to save memory\n%memit array = jnp.arange(1,size,dtype='u8'); jx.jit(calc_gp_array)(array,x0,a).block_until_ready()\n# peak memory: 6711.81 MiB, increment: 613.44 MiB\n\n%memit array = jnp.full(size, a, dtype='f8').at[0].set(x0); jnp.cumprod(array)\n# peak memory: 7675.15 MiB, increment: 1064.08 MiB",
        "answers": [
            "The first version will work if you mark the size argument as static and pass a hashable value:\nimport jax\nimport jax.numpy as jnp\nimport jax.lax as lax\n\ndef calc_gp_size(size,x0,a):\n    scan_fun = lambda carry, i : (a*carry,)*2\n    xn, x = lax.scan(scan_fun,x0,None,length=size-1)\n    return jnp.concatenate((x0[None],x))\n\njax.config.update(\"jax_enable_x64\", True)\n\nsize = 2 ** 26\nx0, a = jnp.array([1.0,1.0+1.0e-08],dtype='f8')\n\njax.jit(calc_gp_size, static_argnums=0)(size,x0,a)\n# DeviceArray([1.        , 1.00000001, 1.00000002, ..., 1.95636587,\n#              1.95636589, 1.95636591], dtype=float64)\nI think this may be slightly more memory efficient than pre-allocating the array as in your second example, though it would be worth benchmarking if that's important.\nAlso, if you're doing this sort of operation on GPU, you'll likely find built-in accumulations like jnp.cumprod to be much more performant. I believe this is more or less equivalent to your scan-based function:\nresult = jnp.cumprod(jnp.full(size, 1 + 1E-8, dtype='f8').at[0].set(1))"
        ],
        "link": "https://stackoverflow.com/questions/72043419/accumulation-in-jax"
    },
    {
        "title": "Accumulation in JAX",
        "question": "What is the best method for handling memory when compiling an accumulation in JAX, such as jax.lax.scan, where a full buffer is excessive?\nThe following is a geometric progression example. The temptation is to recognise the accumulation only depends on an input size and implement accordingly\nimport jax.numpy as jnp\nimport jax.lax as lax\n\ndef calc_gp_size(size,x0,a):\n    scan_fun = lambda carry, i : (a*carry,)*2\n    xn, x = lax.scan(scan_fun,x0,None,length=size-1)\n    return jnp.concatenate((x0[None],x))\n\njax.config.update(\"jax_enable_x64\", True)\n\nsize = jnp.array(2**26,dtype='u8')\nx0, a = jnp.array([1.0,1.0+1.0e-08],dtype='f8')\n\njax.jit(calc_gp_size)(size,x0,a)\nHowever, attempting to use jax.jit will predictably result in a ConcretizationTypeError.\nThe correct way is to pass an argument where the buffer already exists.\ndef calc_gp_array(array,x0,a):\n    scan_fun = lambda carry, i : (a*carry,)*2\n    xn, x = lax.scan(scan_fun,x0,array)\n    return jnp.concatenate((x0[None],x))\n\narray = jnp.arange(1,2**26,dtype='u8')\nx0, a = jnp.array([1.0,1.0+1.0e-08],dtype='f8')  \n\njax.jit(calc_gp_array)(array,x0,a)\nMy concern is that there is a lot of allocated memory not being utilised (or is it?). Is there a more memory efficient approach to this example, or is the allocated memory being used somehow?\nEDIT: Incorporating the comments of @jakevdp, treating the function as main (single call - include compile and exclude caching), and profiling resulted it\n%memit jx.jit(calc_gp_size, static_argnums=0)(size,x0,a).block_until_ready()\n# peak memory: 7058.32 MiB, increment: 959.94 MiB\n\n%memit jx.jit(calc_gp_array)(jnp.arange(1,size,dtype='u8'),x0,a).block_until_ready()\npeak memory: 7850.83 MiB, increment: 1240.22 MiB\n\n%memit jnp.cumprod(jnp.full(size, a, dtype='f8').at[0].set(x0))\npeak memory: 8150.05 MiB, increment: 1539.70 MiB\nLess granular results would require line profiling the jit code (not sure how this could be done).\nSequentially initialising the array and then calling jax.jit appears to save memory\n%memit array = jnp.arange(1,size,dtype='u8'); jx.jit(calc_gp_array)(array,x0,a).block_until_ready()\n# peak memory: 6711.81 MiB, increment: 613.44 MiB\n\n%memit array = jnp.full(size, a, dtype='f8').at[0].set(x0); jnp.cumprod(array)\n# peak memory: 7675.15 MiB, increment: 1064.08 MiB",
        "answers": [
            "The first version will work if you mark the size argument as static and pass a hashable value:\nimport jax\nimport jax.numpy as jnp\nimport jax.lax as lax\n\ndef calc_gp_size(size,x0,a):\n    scan_fun = lambda carry, i : (a*carry,)*2\n    xn, x = lax.scan(scan_fun,x0,None,length=size-1)\n    return jnp.concatenate((x0[None],x))\n\njax.config.update(\"jax_enable_x64\", True)\n\nsize = 2 ** 26\nx0, a = jnp.array([1.0,1.0+1.0e-08],dtype='f8')\n\njax.jit(calc_gp_size, static_argnums=0)(size,x0,a)\n# DeviceArray([1.        , 1.00000001, 1.00000002, ..., 1.95636587,\n#              1.95636589, 1.95636591], dtype=float64)\nI think this may be slightly more memory efficient than pre-allocating the array as in your second example, though it would be worth benchmarking if that's important.\nAlso, if you're doing this sort of operation on GPU, you'll likely find built-in accumulations like jnp.cumprod to be much more performant. I believe this is more or less equivalent to your scan-based function:\nresult = jnp.cumprod(jnp.full(size, 1 + 1E-8, dtype='f8').at[0].set(1))"
        ],
        "link": "https://stackoverflow.com/questions/72043419/accumulation-in-jax"
    },
    {
        "title": "jax linear solve issues",
        "question": "I am currently trying to implement my work within the jax-framework. However I am now encountering an error using the linear solve function from jax.\nHere is an example taken directly from the numpy linear algebra documentation page:\nimport numpy as np\na = np.array([[1, 2], [3, 5]])\nb = np.array([1, 2])\nx = np.linalg.solve(a, b)\nprint(x)\nNow the same example is carried out using the linear solve in jax\nimport jax.numpy as jnp\na = jnp.array([[1, 2], [3, 5]])\nb = jnp.array([1, 2])\nx = jnp.linalg.solve(a, b)\nprint(x)\nThe following error message is now generated:\nJaxStackTraceBeforeTransformation: ValueError: DenseElementsAttr could not be constructed from the given buffer. This may mean that the Python buffer layout does not match that MLIR expected layout and is a bug.\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \"C:\\Users\\mabso\\Desktop\\PhD\\bayesian NMR modelling\\prototype 1 modelling\\sequential modelling\\python implementation\\idea bucket\\jax attempt.py\", line 96, in x = jnp.linalg.solve(a, b)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\traceback_util.py\", line 162, in reraise_with_filtered_traceback return fun(*args, **kwargs)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\api.py\", line 466, in cache_miss out_flat = xla.xla_call(\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\core.py\", line 1771, in bind return call_bind(self, fun, *args, **params)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\core.py\", line 1787, in call_bind outs = top_trace.process_call(primitive, fun_, tracers, params)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\core.py\", line 660, in process_call return primitive.impl(f, *tracers, **params)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\dispatch.py\", line 149, in _xla_call_impl compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\linear_util.py\", line 285, in memoized_fun ans = call(fun, *args)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\dispatch.py\", line 197, in _xla_callable_uncached return lower_xla_callable(fun, device, backend, name, donated_invars,\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\profiler.py\", line 206, in wrapper return func(*args, **kwargs)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\dispatch.py\", line 296, in lower_xla_callable module = mlir.lower_jaxpr_to_module(\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\interpreters\\mlir.py\", line 524, in lower_jaxpr_to_module lower_jaxpr_to_fun(\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\interpreters\\mlir.py\", line 688, in lower_jaxpr_to_fun out_vals = jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack),\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax\\interpreters\\mlir.py\", line 789, in jaxpr_subcomp ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes),\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jax_src\\lax\\linalg.py\", line 1228, in _lu_cpu_gpu_lowering lu, pivot, info = getrf_impl(operand_aval.dtype, operand)\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jaxlib\\lapack.py\", line 269, in getrf_mhlo ir.DenseIntElementsAttr.get(np.arange(num_bd, -1, -1),\nValueError: DenseElementsAttr could not be constructed from the given buffer. This may mean that the Python buffer layout does not match that MLIR expected layout and is a bug.\nI have no idea what this error actually means though. I saw the following post DenseElementsAttr could not be constructed from the given buffer which seems to have similar issues. However, when I run the code from the post it works without causing any problems.\nI run Jax and Jaxlib on windows 10 with versions 0.3.7 using only CPU\nHope someone knows something:)",
        "answers": [
            "I think this is a windows-specific build issue in the jaxlib wheel that you have installed. Unfortunately, JAX does not have any official windows support at the moment, so if you're running on windows you're likely using a community-supported installation. Given the other similar question, it sounds like there are issues with a build for a recent jaxlib release.\nI would recommend reporting this issue with the source of the windows jaxlib installation you're using, and try installing older versions of jax and jaxlib to see if you can find a version that does not have this problem.\nAlternatively, if you're able to use WSL for your work, you can install and use an official Linux release that would hopefully not have this problem.",
            "See the bug report for the root cause.\nIt is a long standing platform-wise behaviour and it just bites one more time, see numpy array dtype is coming as int32 by default in a windows 10 64 bit machine"
        ],
        "link": "https://stackoverflow.com/questions/71977751/jax-linear-solve-issues"
    },
    {
        "title": "Repeating rows from array",
        "question": "I have a problem becase I would like to repeat n time all rows from array(X, Y) without using loops and get array(n*X, Y)\nimport jax.numpy as jnp\n\narr = jnp.array([[12, 14, 12, 0, 1],\n                [0, 14, 12, 0, 1],\n                [0, 0, 12, 0, 1]])\nn = 3\n\nresult = jnp.array([[12 14 12 0 1],\n                    [12 14 12 0 1],\n                    [12 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 0 12 0 1],\n                    [0 0 12 0 1],\n                    [0 0 12 0 1]])\nI haven't found any built-in method to perform this operation, tried with jnp.tile, jnp.repeat.\njnp.repeat\narr_r = jnp.repeat(arr, n, axis=1)\n\nOutput:\n[[12 12 12 14 14 14 12 12 12  0  0  0  1  1  1]\n [ 0  0  0 14 14 14 12 12 12  0  0  0  1  1  1]\n [ 0  0  0  0  0  0 12 12 12  0  0  0  1  1  1]]\n\narr_t = jnp.tile(arr, n)\n\nOutput:\n[[12 14 12  0  1 12 14 12  0  1 12 14 12  0  1]\n [ 0 14 12  0  1  0 14 12  0  1  0 14 12  0  1]\n [ 0  0 12  0  1  0  0 12  0  1  0  0 12  0  1]]\nMaybe I may construct result array from array_t...",
        "answers": [
            "You say you tried jnp.repeat but don't explain why it doesn't do what you want. I'm guessing you're neglecting the axis parameter:\njnp.repeat(arr, n, axis=0)"
        ],
        "link": "https://stackoverflow.com/questions/71974684/repeating-rows-from-array"
    },
    {
        "title": "JAX pytree for xarray",
        "question": "Is it possible to create a pytree (for JAX) based upon an xarray.DataArray?\nThe key issue seems to be that the data attribute of an xarray.DataArray is constructed as a numpy.ndarray view of the input array (such as DeviceArray)\nimport jax.numpy as jnp\nimport xarray as xr\n\nda = xr.DataArray(\n    data=jnp.array([2.0,3.0]),\n    dims=(\"var\"),\n    coords={\"var\": [\"A\",\"B\"]},\n)\n\n>>> type(da.data)\n\n    numpy.ndarray\nFlattening/Unflattening the DataArray into a pytree is relatively straight-forward (all the attributes are aux except data), but I don't know how to retrieve the DeviceArray, or even assign to it (I can't use at[.].set(.) here).\nThe alternative approach of constructing a container class (with DeviceArray member) requires that all the relevant functionality of xarray be manually implemented. For a single feature, such as labelled indexing, this is possible but redundant.",
        "answers": [
            "You may be able to do what you want by registering the xarray type as a custom PyTree node following the examples in the documentation.\nFor example, it might look like this:\nimport jax.numpy as jnp\nfrom jax import tree_util\nimport xarray as xr\n\ntree_util.register_pytree_node(\n    xr.DataArray,\n    lambda x: ((x.data,), {\"dims\": x.dims, \"coords\": x.coords}),\n    lambda kwds, args: xr.DataArray(*args, **kwds)\n)\n\nda = xr.DataArray(\n    data=jnp.array([2.0,3.0]),\n    dims=(\"var\"),\n    coords={\"var\": [\"A\",\"B\"]},\n)\nprint(da)\n# <xarray.DataArray (var: 2)>\n# array([2., 3.], dtype=float32)\n# Coordinates:\n#   * var      (var) <U1 'A' 'B'\n\ndata, tree = tree_util.tree_flatten(da)\nprint(data)\n# [array([2., 3.], dtype=float32)]\n\nda_reconstructed = tree_util.tree_unflatten(tree, data)\nprint(da_reconstructed)\n# <xarray.DataArray (var: 2)>\n# array([2., 3.], dtype=float32)\n# Coordinates:\n#   * var      (var) <U1 'A' 'B'\nI don't think there's much of a possibility that this will work as intended in any but the simplest of cases: for example, JAX transformations are restricted to functional, non-side-effecting code, and JAX arrays are immutable. xarray's operations in general violate both these constraints.\nAnother issue: you'd have to be careful to define your flatten and unflatten functions in a way that will ensure that all data and metadata is properly serialized, and if you're hoping to use this within any JAX function, be aware that you might run into issues with xarray's validation of inputs; see Custom PyTrees and Initialization for more information."
        ],
        "link": "https://stackoverflow.com/questions/71949459/jax-pytree-for-xarray"
    },
    {
        "title": "inserting rows and columns of zeros to a sparse array in python",
        "question": "I have 50ish relatively large sparse arrays (in scipy.csr_array format but that can be changed) and I would like to insert rows and columns of zeros at certain locations. An example in dense format would look like:\nA = np.asarray([[1,2,1],[2,4,5],[2,1,6]])\n# A = array([[1,2,1],\n#            [2,4,5],\n#            [2,1,6]])\nindices = np.asarray([-1, -1, 2, -1, 4, -1, -1, 7, -1])\n\n# indices =  array([-1, -1, 2, -1, 4, -1, -1, 7, -1])\n#insert rows and colums of zeros where indices[i] == -1 to get B\n\nB = np.asarray([[0,0,0,0,0,0,0,0,0],\n                [0,0,0,0,0,0,0,0,0],\n                [0,0,1,0,2,0,0,1,0],\n                [0,0,0,0,0,0,0,0,0],\n                [0,0,2,0,4,0,0,5,0],\n                [0,0,0,0,0,0,0,0,0],\n                [0,0,0,0,0,0,0,0,0],\n                [0,0,2,0,1,0,0,6,0],\n                [0,0,0,0,0,0,0,0,0]])\nA is a sparse array of shape (~2000, ~2000) with ~20000 non zero entries and indices is of shape (4096, ). I can imagine doing it in dense format but I guess I don't know enough about the way data and indices are are stored and cannot find a way to do this sort of operation for sparse arrays in a quick and efficient way.\nAnyone have any ideas or suggestions?\nThanks.",
        "answers": [
            "I would probably do this by passing the data and associated indices into a COO matrix constructor:\nimport numpy as np\nfrom scipy.sparse import coo_matrix\n\nA = np.asarray([[1,2,1],[2,4,5],[2,1,6]])\nindices = np.asarray([-1, -1, 2, -1, 4, -1, -1, 7, -1])\n\nidx = indices[indices >= 0]\ncol, row = np.meshgrid(idx, idx)\n\nmat = coo_matrix((A.ravel(), (row.ravel(), col.ravel())),\n                 shape=(len(indices), len(indices)))\nprint(mat)\n#   (2, 2)  1\n#   (2, 4)  2\n#   (2, 7)  1\n#   (4, 2)  2\n#   (4, 4)  4\n#   (4, 7)  5\n#   (7, 2)  2\n#   (7, 4)  1\n#   (7, 7)  6\n\nprint(mat.todense())\n# [[0 0 0 0 0 0 0 0 0]\n#  [0 0 0 0 0 0 0 0 0]\n#  [0 0 1 0 2 0 0 1 0]\n#  [0 0 0 0 0 0 0 0 0]\n#  [0 0 2 0 4 0 0 5 0]\n#  [0 0 0 0 0 0 0 0 0]\n#  [0 0 0 0 0 0 0 0 0]\n#  [0 0 2 0 1 0 0 6 0]\n#  [0 0 0 0 0 0 0 0 0]]",
            "You could try storing your non-zero values in one list and their respective indexes in another:\ndata_list = [[], [], [1, 2, 1], [], [2, 4, 5], [], [], [2, 1, 6], []]\nindex_list = [[], [], [2, 4, 7], [], [2, 4, 7], [], [], [2, 4, 7], []]\nThese two lists, would only then have to store the number of nonzero values each, rather than one list with 4,000,000 values.\nIf you then wanted to grab the value in position (4, 7):\ndef find_value(row, col):\n    # Check to see if the given column is in our index list\n    if col not in index_list[row]:\n        return 0\n    \n    # Otherwise return the number in the data list\n    myNum = data_list[row][index_list[row].index(col)]\n    return myNum\n    \nfind_value(4, 7)\noutput: 5\nHope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/71930665/inserting-rows-and-columns-of-zeros-to-a-sparse-array-in-python"
    },
    {
        "title": "Nested for loops in jax",
        "question": "I have a following function which processes an image\n# img : RGB image (512, 512, 3)\n# kernel : 5x5 filtering kernel \n# dilation : can take integer values 1, 2, 4, ...\n# some_data : no conditions are dependent on the value of this argument and it \n#            remains constant across multiple invocations this function\ndef filtering(img, kernel, dilation, some_data):\n    h, w, _ = img.shape\n    filtered_img = jnp.zeros(img.shape)\n\n    radius = 2\n    for i in range(radius, h-radius):\n        for j in range(radius, w-radius):\n\n            center_pos = np.array([i, j])\n            sum = jnp.array([0.0, 0.0, 0.0])\n            sum_w = 0.0\n            \n            for ii in range(-radius, radius + 1):\n                for jj in range(-radius, radius + 1):\n\n                    pos = center_pos + dilation * np.array([ii, jj])\n                    \n                    # if not for the `compute_weight` function this could have been a dilated convolution\n                    weight = kernel[ii + radius, jj + radius] * compute_weight(center_pos, pos, some_data)\n                    sum += img[pos[0], pos[1], :] * weight\n                    sum_w += weight\n\n            filtered_img = filtered_img.at[i, j].set(sum/sum_w)\n\n    return filtered_img\nThe first function call (jit compiled) takes approx. 6 hours to run (tried on both GPU and CPU). Since it is jit compiled subsequent runs may be faster, but the first run is prohibitively expensive.\nI tried removing the compute_weight function and replacing the two innermost nested loops with jnp.sum(img[i-radius:i+radius+1, j-radius:j+radius+1] * filter, axis=(0, 1)) and the first function invocation still takes around 30 minutes to run. Based on this observation and some of the other questions on SO, this seems to be due to the for loops in general.\nWill rewriting this in a more functional way and using jax constucts like loops help or is this happening due to some other issue?",
        "answers": [
            "The issue here is not execution time, the issue is compilation time. JAX's JIT compilation will flatten all Python control flow: what this means is that for your input, you are generating 512 * 512 * 5 * 5 copies of the jaxpr for the inner loop, and sending them to XLA for compilation. Since compilation costs scale as roughly the square of the length of the program, the result will be an extremely long compilation.\nYour best bet here is probably to rewrite this in terms of jax.fori_loop, which will lower the loop logic directly to XLA without the large compilation cost.\nEven better, since it looks like what you're doing is some flavor of a convolution, would be to express this in terms of something like jax.scipy.signal.convolve2d, which will be far faster than doing the looping manually."
        ],
        "link": "https://stackoverflow.com/questions/71906371/nested-for-loops-in-jax"
    },
    {
        "title": "Parameters do not converge at a lower tolerance in nonlinear least square implementation in python",
        "question": "I am translating some of my R codes to Python as a learning process, especially trying JAX for autodiff.\nIn functions to implement non-linear least square, when I set tolerance at 1e-8, the estimated parameters are nearly identical after several iterations, but the algorithm never appear to converge.\nHowever, the R codes converge at the 12th inter at tol=1e-8 and 14th inter at tol=1e-9. The estimated parameters are almost the same as the ones resulted from Python implementation.\nI think this has something to do with floating point, but not sure which step I could improve to make the converge as quickly as seen in R.\nHere are my codes, and most steps are the same as in R\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport scipy.linalg as ola\n\n\ndef update_parm(X, y, fun, dfun, parm, theta, wt):\n    len_y = len(y)\n    mean_fun = fun(X, parm)\n\n    if (type(wt) == bool):\n        if (wt):\n            var_fun = np.exp(theta * np.log(mean_fun))\n            sqrtW = 1 / np.sqrt(var_fun ** 2)\n        else:\n            sqrtW = 1\n    else:\n        sqrtW = wt\n        \n    gradX = dfun(x, parm)\n    weighted_X = sqrtW.reshape(len_y, 1) * gradX\n    z = gradX @ parm + (y - mean_fun)\n    weighted_z = sqrtW * z\n    qr_gradX = ola.qr(weighted_X, mode=\"economic\")\n    Q = qr_gradX[0]\n    R = qr_gradX[1]\n    new_parm = ola.solve(R, np.dot(Q.T, weighted_z))\n    \n    return new_parm\n\n\ndef nls_irwls(X, y, fun, dfun, init, theta = 1, tol = 1e-8, maxiter = 500):\n\n    old_parm = init\n    iter = 0\n\n    while (iter < maxiter):\n        new_parm = update_parm(X, y, fun, dfun, parm=old_parm, theta=theta, wt=True)\n        parm_diff = np.max(np.abs(new_parm - old_parm) / np.abs(old_parm))\n        print(parm_diff)\n\n        if (parm_diff < tol) :\n            break\n        else:\n            old_parm = new_parm\n            iter += 1\n            print(new_parm)\n\n    if (iter == maxiter):\n        print(\"The algorithm failed to converge\")\n    else:\n        return {\"Estimated coefficient\": new_parm}\n\n\nx = np.array([0.25, 0.5, 0.75, 1, 1.25, 2, 3, 4, 5, 6, 8])\ny = np.array([2.05, 1.04, 0.81, 0.39, 0.30, 0.23, 0.13, 0.11, 0.08, 0.10, 0.06])\n\ndef model(x, W):\n    comp1 = jnp.exp(W[0])\n    comp2 = jnp.exp(-jnp.exp(W[1]) * x)\n    comp3 = jnp.exp(W[2])\n    comp4 = jnp.exp(-jnp.exp(W[3]) * x)\n    return comp1 * comp2 + comp3 * comp4\n\n\ninit = np.array([0.69, 0.69, -1.6, -1.6])\n\n#autodiff\nmodel_grad = jax.jit(jax.jacfwd(model, argnums=1))\n\n#manual derivative\ndef dModel(x, W):\n    e1 = np.exp(W[1])\n    e2 = np.exp(W[3])\n    e5 = np.exp(-(x * e1))\n    e6 = np.exp(-(x * e2))\n    e7 = np.exp(W[0])\n    e8 = np.exp(W[2])\n    b1 = e5 * e7\n    b2 = -(x * e5 * e7 * e1) \n    b3 = e6 * e8 \n    b4 = -(x * e6 * e8 * e2)\n\n    return np.array([b1, b2, b3, b4]).T\n\nnls_irwls(x, y, model, model_grad, init=init, theta=1, tol=1e-8, maxiter=50)\nnls_irwls(x, y, model, dModel, init=init, theta=1, tol=1e-8, maxiter=50)",
        "answers": [
            "One thing to be aware of is that by default, JAX performs computations in 32-bit, while tools like R and numpy perform computations in 64-bit. Since 1E-8 is at the edge of 32-bit floating point precision, I suspect this is why your program is failing to converge.\nYou can enable 64-bit computation by putting this at the beginning of your script:\nfrom jax import config\nconfig.update('jax_enable_x64', True)\nAfter doing this, your program converges as expected. For more information, see JAX Sharp Bits: Double Precision."
        ],
        "link": "https://stackoverflow.com/questions/71902257/parameters-do-not-converge-at-a-lower-tolerance-in-nonlinear-least-square-implem"
    },
    {
        "title": "(Conv1D) Tensorflow and Jax Resulting Different Outputs for The Same Input",
        "question": "I am trying to use conv1d functions to make a transposed convlotion repectively at jax and tensorflow. I read the documentation of both of jax and tensorflow for the con1d_transposed operation but they are resulting with different outputs for the same input.\nI can not find out what the problem is. And I don't know which one produces the correct results. Help me please.\nMy Jax Implementation (Jax Code)\nx = np.asarray([[[1, 2, 3, 4, -5], [1, 2, 3, 4, 5]]], dtype=np.float32).transpose((0, 2, 1))\nfilters = np.array([[[1, 0, -1], [-1,  0,  1]], \n                    [[1, 1,  1], [-1, -1, -1]]], \n                    dtype=np.float32).transpose((2, 1, 0))\n\nkernel_rot = np.rot90(np.rot90(filters))\n\nprint(f\"x strides:  {x.strides}\\nfilters strides: {kernel_rot.strides}\\nx shape: {x.shape}\\nfilters shape: {filters.shape}\\nx: \\n{x}\\nfilters: \\n{filters}\\n\")\n\ndn1 = lax.conv_dimension_numbers(x.shape, filters.shape,('NWC', 'WIO', 'NWC'))\nprint(dn1)\n\nres = lax.conv_general_dilated(x,kernel_rot,(1,),'SAME',(1,),(1,),dn1)     \n\nres = np.asarray(res)\nprint(f\"result strides: {res.strides}\\nresult shape: {res.shape}\\nresult: \\n{res}\\n\")\nMy TensorFlow Implementation (TensorFlow Code)\nx = np.asarray([[[1, 2, 3, 4, -5], [1, 2, 3, 4, 5]]], dtype=np.float32).transpose((0, 2, 1))\nfilters = np.array([[[1, 0, -1], [-1,  0,  1]], \n                    [[1, 1,  1], [-1, -1, -1]]], \n                    dtype=np.float32).transpose((2, 1, 0))\n\nprint(f\"x strides:  {x.strides}\\nfilters strides: {filters.strides}\\nx shape: {x.shape}\\nfilters shape: {filters.shape}\\nx: \\n{x}\\nfilters: \\n{filters}\\n\")\n    \nres = tf.nn.conv1d_transpose(x, filters, output_shape = x.shape, strides = (1, 1, 1), padding = 'SAME', data_format='NWC', dilations=1)\n\nres = np.asarray(res)\nprint(f\"result strides: {res.strides}\\nresult shape: {res.shape}\\nresult: \\n{res}\\n\")\nOutput from the Jax\nresult strides: (40, 8, 4)\nresult shape: (1, 5, 2)\nresult: \n[[[ 0.  0.]\n  [ 0.  0.]\n  [ 0.  0.]\n  [10. 10.]\n  [ 0. 10.]]]\nOutput from the TensorFlow\nresult strides: (40, 8, 4)\nresult shape: (1, 5, 2)\nresult: \n[[[  5.  -5.]\n  [  8.  -8.]\n  [ 11. -11.]\n  [  4.  -4.]\n  [  5.  -5.]]]",
        "answers": [
            "Function conv1d_transpose expects filters in shape [filter_width, output_channels, in_channels]. If filters in snippet above were transposed to satisfy this shape, then for jax to return correct results, while computing dn1 parameter should be WOI (Width - Output_channels - Input_channels) and not WIO (Width - Input_channels - Output_channels). After that:\nresult.strides = (40, 8, 4)\nresult.shape = (1, 5, 2)\nresult: \n[[[ -5.,   5.],\n  [ -8.,   8.],\n  [-11.,  11.],\n  [ -4.,   4.],\n  [ -5.,   5.]]]\nResults not same as with tensorflow, but kernels for jax were flipped, so actually that was expected."
        ],
        "link": "https://stackoverflow.com/questions/71889649/conv1d-tensorflow-and-jax-resulting-different-outputs-for-the-same-input"
    },
    {
        "title": "What is the best way to index 3d matrix with vectors?",
        "question": "import jax.numpy as jnp\nvectors and array are jnp.array(dtype=jnp.int32)\nI have an array with shape [x, d, y] (3x3x3)\n[[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]]]\nand vectors x = [2 0 3], y = [ 2 0 1], d = [0 0 1]\nI want to have something like this by indexing but I tried and don't really know how, with jax.numpy.\n[[[0 0 2],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 3 0],\n [0 0 0]]]\nEdit: I would like to specify that I wanted to put number from x with its index to the array but only when x > 0. I tried with boolean mask. Something like this\nmask = x > 0\narray = array.at[mask, d, y].set(array[mask, d, y] + x)",
        "answers": [
            "You have a three-dimensional array, so you can index it with three arrays of indices. Since you want d and y to be associated with the second and third dimensions, you'll need to create another array of indices for the first dimension:\nimport jax.numpy as jnp\n\narr = jnp.zeros((3, 3, 3), dtype='int32')\nx = jnp.array([2, 0, 3])\ny = jnp.array([2, 0, 1])\nd = jnp.array([0, 0, 1])\n\ni = jnp.arange(len(x))\nmask = x > 0\n\nout = arr.at[i[mask], d[mask], y[mask]].set(x[mask])\nprint(out)\n# [[[0 0 2]\n#   [0 0 0]\n#   [0 0 0]]\n\n#  [[0 0 0]\n#   [0 0 0]\n#   [0 0 0]]\n\n#  [[0 0 0]\n#   [0 3 0]\n#   [0 0 0]]]\nIn this case the result will be the same whether or not you use the mask (i.e. arr.at[i, d, y].set(x) will give the same result) but because your question explicitly specified that you only want to use values x > 0 I included it."
        ],
        "link": "https://stackoverflow.com/questions/71880876/what-is-the-best-way-to-index-3d-matrix-with-vectors"
    },
    {
        "title": "DenseElementsAttr could not be constructed from the given buffer",
        "question": "I try to run a code written with JAX. At one part of the code, key for training set is defined as\nkey_train = random.PRNGKey(0).\nHere the type of the key jaxlib.xla_extension.DeviceArray. Then in the following part, keys are defined as keys = random.split(key_train, N). Here N is an integer which is equal to 10000. At that part of the code it gives an error like:\nDenseElementsAttr could not be constructed from the given buffer. This may mean that the Python buffer layout does not match that MLIR expected layout and is a bug.\nCould you please help me about the error?\nEdit: I try to run the code on Win10. Here (https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets/blob/main/Antiderivative/DeepONet_antideriv.ipynb) you can find the code that I try to run. For simplicity you can try to run the code below as well. You will get the exact same error.\nfrom jax import random\nN=10000\nkey_train=random.PRNGKey(0)\nkeys=random.split(key_train, N)\nJax and Jaxlib versions are 0.3.5 with cuda 11",
        "answers": [
            "I had the same error. Deleting c:\\python37\\lib\\site-packages\\jaxlib\\cuda_prng.py fixed the issue (replace the prefix by your python path). It could be cuda_prng.py was an old file.",
            "By following Erwin Coumans's advice and indeed the mistake disappeared. The version installed on Windows is ( jaxlib 0.3.7 with cuda 11.3 and cudnn 8.2 , on anaconda python 3.9.0) the latest found on : https://whls.blob.core.windows.net/unstable/index.html"
        ],
        "link": "https://stackoverflow.com/questions/71827814/denseelementsattr-could-not-be-constructed-from-the-given-buffer"
    },
    {
        "title": "Module 'jaxlib.xla_extension.jax_jit' has no attribute 'set_enable_x64_cpp_flag'",
        "question": "I've tried to install jax.lib on win10. It seems like jax.lib is installed but when I run the spyder and write 'import jax', it says\nmodule 'jaxlib.xla_extension.jax_jit' has no attribute 'set_enable_x64_cpp_flag'\nI have python 3.10 and cuda version 11.6.\nCould you please help me with it?",
        "answers": [
            "I just upgraded JAX by writing\npip install --upgrade jax jaxlib\non the anaconda command prompt and the problem is resolved."
        ],
        "link": "https://stackoverflow.com/questions/71825432/module-jaxlib-xla-extension-jax-jit-has-no-attribute-set-enable-x64-cpp-flag"
    },
    {
        "title": "How to check if a value is in an array while using jax",
        "question": "I have a negative sampling function that I want to use JAX's @jit but everything that I do makes it stop working.\nThe parameters are:\nkey: key to jax.random\nratings: a list of 3-tuples (user_id, item_id, 1);\nuser_positives: a list of lists where the i-th inner list contains the items that the i-th user has consumed;\nnum_items: the total number of items\nMy function is shown below, and its goal is to draw 100 samples from ratings and, for each sample, retrieve an item that has not been consumed by that user.\nBATCH_SIZE = 100\n\n@jit\ndef sample(key, ratings, user_positives, num_items):\n    new_key, subkey = jax.random.split(key)\n    \n    sampled_ratings = jax.random.choice(subkey, ratings, shape=(BATCH_SIZE,))\n    sampled_users = jnp.zeros(BATCH_SIZE)\n    sampled_positives = jnp.zeros(BATCH_SIZE)\n    sampled_negatives = jnp.zeros(BATCH_SIZE)\n    idx = 0\n    \n    for u, i, r in sampled_ratings:\n        negative = user_positives[u][0]\n        new_key, subkey = jax.random.split(key)\n        while jnp.isin(jnp.array([negative]), user_positives[u])[0]:\n            negative = jax.random.randint(current_subkey, (1,), 0, num_items)\n            current_subkey = jax.random.split(subkey)\n        \n        sampled_users.at[idx].set(u)\n        sampled_positives.at[idx].set(i)\n        sampled_negatives.at[idx].set(negative)\n        idx += 1\n    \n    return new_key, sampled_users, sampled_positives, sampled_negatives\nHowever, whenever I run and try to change it, new errors are generated, and I got stuck in the error below. Can anyone help me do this?\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function sample at /tmp/ipykernel_11557/2294038851.py:1 for jit, this concrete value was not available in Python because it depends on the values of the arguments 'key', 'ratings', and 'user_positives'.\nEdit 1: An input example would be:\nrng_key = \nrng_key, su, sp, sn = sample(\n    rng_key,\n    np.array([(0, 0, 1), (0, 1, 1), (1, 2, 1)]),\n    np.array([np.array([0, 1]), np.array([2])]),\n    15\n)",
        "answers": [
            "In general if you want to jit-compile a while loop whose condition depends on non-static quantities, you'll have to express it in terms of jax.lax.while_loop. For more information see JAX Sharp Bits: Structured control flow primitives.\nI'll try to edit my answer with an example based on your code if you can add an example of the expected input."
        ],
        "link": "https://stackoverflow.com/questions/71743190/how-to-check-if-a-value-is-in-an-array-while-using-jax"
    },
    {
        "title": "JAX(XLA) vs Numba(LLVM) Reduction",
        "question": "Is it possible to make CPU only reductions with JAX comparable to Numba in terms of computation time?\nThe compilers come straight from conda:\n$ conda install -c conda-forge numba jax\nHere is a 1-d NumPy array example\nimport numpy as np\nimport numba as nb\nimport jax as jx\n\n@nb.njit\ndef reduce_1d_njit_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\n@jx.jit\ndef reduce_1d_jax_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\nN = 2**10\na = np.random.randn(N)\nUsing timeit on the following\nnp.add.reduce(a) gives 1.99 µs ...\nreduce_1d_njit_serial(a) gives 1.43 µs ...\nreduce_1d_jax_serial(a).item() gives 23.5 µs ...\nNote that jx.numpy.sum(a) and using jx.lax.fori_loop gives comparable (marginally slower) comp. times to reduce_1d_jax_serial.\nIt seems there is a better way to craft the reduction for XLA.\nEDIT: compile times were not included as a print statement proceeded to check results.",
        "answers": [
            "When performing these kinds of microbenchmarks with JAX, you have to be careful to ensure you're measuring what you think you're measuring. There are some tips in the JAX Benchmarking FAQ. Implementing some of these best practices, I find the following for your benchmarks:\nimport jax.numpy as jnp\n\n# Native jit-compiled XLA sum\njit_sum = jx.jit(jnp.sum)\n\n# Avoid including device transfer cost in the benchmarks\na_jax = jnp.array(a)\n\n# Prevent measuring compilation time\n_ = reduce_1d_njit_serial(a)\n_ = reduce_1d_jax_serial(a_jax)\n_ = jit_sum(a_jax)\n\n%timeit np.add.reduce(a)\n# 100000 loops, best of 5: 2.33 µs per loop\n\n%timeit reduce_1d_njit_serial(a)\n# 1000000 loops, best of 5: 1.43 µs per loop\n\n%timeit reduce_1d_jax_serial(a_jax).block_until_ready()\n# 100000 loops, best of 5: 6.24 µs per loop\n\n%timeit jit_sum(a_jax).block_until_ready()\n# 100000 loops, best of 5: 4.37 µs per loop\nYou'll see that for these microbenchmarks, JAX is a few milliseconds slower than both numpy and numba. So does this mean JAX is slow? Yes and no; you'll find a more complete answer to that question in JAX FAQ: is JAX faster than numpy?. The short summary is that this computation is so small that the differences are dominated by Python dispatch time rather than time spent operating on the array. The JAX project has not put much effort into optimizing for Python dispatch of microbenchmarks: it's not all that important in practice because the cost is incurred once per program in JAX, as opposed to once per operation in numpy."
        ],
        "link": "https://stackoverflow.com/questions/71701041/jaxxla-vs-numballvm-reduction"
    },
    {
        "title": "Handle varying shapes in jax numpy arrays (jit compatible)",
        "question": "Important note: I need everything to be jit compatible here, otherwise my problem is trivial :)\nI have a jax numpy array such as:\na = jnp.array([1,5,3,4,5,6,7,2,9])\nFirst I filter it considering a value, let's assume that I only keep values that are < 5\na = jnp.where((a < 5), x=a, y=jnp.nan)\n# a is now [ 1. nan  3.  4. nan nan nan  2. nan]\nI want to keep only non-nan values: [ 1.  3.  4.  2.] and I will then use this array for other operations.\nBut more importantly, during execution of my program, this code will be executed multiple times with a threshold value that will change (i.e. it won't always be 5).\nHence, the shape of the final array will change too. Here is my problem with jit compilation, I don't know how to make it jit compatible since the shape depends on how many elements comply to the threshold condition.",
        "answers": [
            "JAX's JIT is not currently compatible with arrays of dynamic (data-dependent) shape, so there is no way to do what your question asks.\nThere is some experimental work in progress on handling dynamic shapes within JAX transforms like JIT (see https://github.com/google/jax/pull/9335) but I'm not certain when it will be available to use.\nThe usual workaround for this is to re-express your computations in terms of statically-shaped arrays with a fill value; for example, you could use something like this:\na = jnp.where((a < 5), size=len(a), fill_value=np.nan)\nThis will create an array of the same length as a, with non-nan values at the front, and filled with nan values at the end."
        ],
        "link": "https://stackoverflow.com/questions/71692885/handle-varying-shapes-in-jax-numpy-arrays-jit-compatible"
    },
    {
        "title": "jax: sample many observations from random.choice with replacement between them",
        "question": "I'd like to pick two indices out of an array. These indices must not be the same. One such sample can be obtained with:\nrandom.choice(next(key), num_items, (2,), replace=False)\nFor performance reasons, I'd like to batch the sampling:\nnum_samples = 100\nsamples = random.choice(next(key), num_items, (num_samples, 2), replace=False)\nThis doesn't work because of replace=False. It raises the error:\nValueError: Cannot take a larger sample than population when 'replace=False'\nFor each new sample, I'd like to have replace=True. Within one sample, I'd like to have replace=False. Is there a way to do this?\nThe next(key) in my random sampling is syntactic sugar. I'm using this snippet for convenience:\ndef reset_key(seed=42):\n    key = random.PRNGKey(seed)\n    while True:\n        key, subkey = random.split(key)\n        yield subkey\n        \nkey = reset_key()",
        "answers": [
            "The best way to do this is using jax.vmap to map across individual samples. For example:\nfrom jax import random, vmap\n\ndef sample_two(key, num_items):\n  return random.choice(key , num_items, (2,), replace=False)\n\nkey = random.PRNGKey(0)\nnum_samples = 10\nnum_items = 5\n\nkey_array = random.split(key, num_samples)\nprint(vmap(sample_two, in_axes=(0, None))(key_array, num_items))\n# [[2 0]\n#  [1 4]\n#  [2 1]\n#  [3 4]\n#  [4 2]\n#  [2 0]\n#  [1 3]\n#  [2 1]\n#  [1 0]\n#  [2 4]]\nFor more information on jax.vmap, see Automatic Vectorization in JAX."
        ],
        "link": "https://stackoverflow.com/questions/71679151/jax-sample-many-observations-from-random-choice-with-replacement-between-them"
    },
    {
        "title": "Output shape of jax's vmap",
        "question": "I'm trying to figure out how exactly jax.vmap's output work. As an example, taking the following code:\nfunc = lambda x: [[x,x],[x,x]]\n\nvfunc = vmap(func,0, out_axes=WHAT)\nexample_array = np.array([1,2,3])\nvfunc(example_array)\nI'd like vfunc's output array in the example to be something like:\n[ [[1,1],[1,1]],\n  [[2,2],[2,2]],\n  [[3,3],[3,3]]\n]\nIs there a way to get that from setting out_axes to something specifically or would I have to run some posterior shape transformations?",
        "answers": [
            "It is not possible to use vmap alone to get the result you have in mind.\nOne thing to understand about vmap is it works on axes of JAX Arrays; any time those JAX arrays are in containers (such as lists, tuples, or dicts), the in_axes and out_axes arguments refer to the individual arrays within those containers. So, for example if you return a tuple, it looks like this:\n@vmap\ndef f(x):\n  return (x, 2 * x)\n\nx = jnp.arange(3)\nprint(f(x))\n# (DeviceArray([0, 1, 2], dtype=int32), DeviceArray([0, 2, 4], dtype=int32))\nNotice each element in the tuple is vmapped separately.\nSimilarly, if you return a nested list of individual arrays, the vmap applies to each individual array and is returned in the same structure:\n@vmap\ndef f(x):\n  return [x, [x, x]]\n\nprint(f(jnp.array([1, 2])))\n# [DeviceArray([1, 2], dtype=int32),\n#  [DeviceArray([1, 2], dtype=int32), DeviceArray([1, 2], dtype=int32)]]\nNotice the nested list in the result has the same structure as the nested list of the input.\nSo, as you can see, when you return something like [[x, x], [x, x]], the output structure will always be [[y, y], [y, y]]. There is no way to use vmap to make the output structure different than the input structure as is requested in your question, so vmap by itself cannot be used for what you want to do.\nNow, if you change your question slightly and rather than returning a nested list of arrays, return an array constructed from such a nested list, then the vmap out_axes argument applies to the return value as a whole (rather than applying to each individual array in the output). You could then use that array directly, or if you actually want the nested list structure you unpack the output like this:\n@vmap\ndef func(x):\n  return jnp.array([[x,x],[x,x]])\n\nexample_array = jnp.array([1,2,3])\nresult = func(example_array)\nprint(list(map(list, result)))\n# [[DeviceArray([1, 1], dtype=int32), DeviceArray([1, 1], dtype=int32)],\n#  [DeviceArray([2, 2], dtype=int32), DeviceArray([2, 2], dtype=int32)],\n#  [DeviceArray([3, 3], dtype=int32), DeviceArray([3, 3], dtype=int32)]]",
            "You can change your code like below:\nimport jax.numpy as jnp\nfrom jax import vmap\n\nfunc = lambda x: jnp.array([[x,x],[x,x]])\n# Or\n# def func(x) :\n#     return jnp.array([[x,x],[x,x]])\n\n\nvfunc = vmap(func, in_axes=0, out_axes=0)\nexample_array = jnp.array([1,2,3])\nvfunc(example_array)\nOutout:\nDeviceArray([[[1, 1],\n              [1, 1]],\n\n             [[2, 2],\n              [2, 2]],\n\n             [[3, 3],\n              [3, 3]]], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/71638553/output-shape-of-jaxs-vmap"
    },
    {
        "title": "How to print with JAX",
        "question": "I have a JAX Boolean array and want to print a statement combined with sum of Trues:\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.host_callback import id_print\n\n@jax.jit\ndef overlaps_jax():\n    mask_cp = jnp.array([True, False, False, True, False, True, False, True, True])\n    id_print(jnp.sum(mask_cp))\n\noverlaps_jax()\nThere are 5 Trues in the mask_cp; I want to print as:\nWith jax accelerator\nThere are 5 true bools\nAs this function is jitted, I tried to print this by using id_print, but I couldn't. id_print(jnp.sum(mask_cp)) will print 5, but I couldn't use this with the strings. I have tried the followings:\nid_print(jnp.sum(mask_cp))\n# print:\n# 5\n\nid_print(\"\\nWith jax accelerator\\nThere are \" + jnp.sum(mask_cp) + \" true bools\\n\")\n# error:\n# TypeError: can only concatenate str (not \"DynamicJaxprTracer\") to str\n\nprint(\"\\nWith jax accelerator\\nThere are {} true bools\\n\".format(jnp.sum(mask_cp)))\n# print:\n# With jax accelerator\n# There are Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)> true bools\nHow could I print such statements in this code?",
        "answers": [
            "Please note that id_print is experimental, and its API and capabilities are subject to change. That said, I don't believe id_print has the capability to add text like this, but you can do it via a more general host_callback.call:\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.host_callback import call\n\n@jax.jit\ndef overlaps_jax():\n    mask_cp = jnp.array([True, False, False, True, False, True, False, True, True])\n    call(lambda x: print(f\"There are {x} true bools\"), jnp.sum(mask_cp))\n\noverlaps_jax()\nThe output is\nThere are 5 true bools"
        ],
        "link": "https://stackoverflow.com/questions/71548823/how-to-print-with-jax"
    },
    {
        "title": "Mask a numpy array after a given value",
        "question": "I have two numpy arrays like :\na = [False, False, False, False, False, True, False, False]\n\nb = [1, 2, 3, 4, 5, 6, 7, 8]\nI need to sum over b, not the full array, but only until the elements with the equivalent index in a is True\nIn other words, I want to do 1+2+3+4+5=15 instead of 1+2+3+4+5+6+7+8=36\nI need an efficient solution, I think I need to mask all elements from b that are after the first True in a and make them 0.\nSide note: My code is in jax.numpy and not original numpy but I guess it doesn't really matter.",
        "answers": [
            "You can do a cumulated sum\nnp.sum(b[np.cumsum(a)==0])",
            "I would suggest to convert the array to a list with .tolist() and then apply .index() to obtain the index of the first True: i = a.tolist().index(True). Then simple slicing and summing: total = numpy.sum(b[:i])",
            "I can think of two ways of doing this: you could do it by constructing a mask with cumsum (this will also work in regular numpy):\na = jnp.array([False, False, False, False, False, True, False, False])\nb = jnp.array([1, 2, 3, 4, 5, 6, 7, 8])\n\nmask = a.cumsum() == 0\nb.sum(where=mask) # 15\nOr you could find the first True index with jnp.where (note that the size argument only exists in JAX's version of jnp.where, not in numpy's):\nidx = jnp.where(a, size=1)[0][0]\nb[:idx].sum() # 15\nYou might do some microbenchmarks to determine which is more efficient for the size of arrays that you're concerned with."
        ],
        "link": "https://stackoverflow.com/questions/71384567/mask-a-numpy-array-after-a-given-value"
    },
    {
        "title": "vmap gives inconsistent shape error when trying to calculate gradient per sample",
        "question": "I am trying to implement a two layer neural network and get the gradient of the second layer per sample.\nMy code looks like this:\nx = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.float32)\ny = jnp.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=jnp.float32)\n\nW1 = jnp.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=jnp.float32)\nW2 = random.uniform(key, shape=(10,), minval=1, maxval=2, dtype=jnp.float32)\nb = jnp.linspace(0, -9, 10, dtype=jnp.float32)\nb = jnp.reshape(b, (1,10))\n\ndef predict(W1, W2, b, x):\n  f1 = jnp.einsum('i,j->ji', W1, x)+b\n  f1 = nn.relu(f1)\n  f2 = jnp.einsum('i,ji->j', W2, f1)\n  return f2\n\ndef loss(W1, W2, b, x, y):\n  preds = predict(W1, W2, b, x)\n  return jnp.mean(jnp.square(y-preds))\n\nperex_grads = vmap(grad(loss, argnums=1), in_axes= (0, None, 0, 0, 0))\npers_grads = perex_grads(W1, W2, b, x, y)\nI ran loss and can do grad(loss) just fine. Running vmap is the actual problem.\nThe exact error I get is:\nValueError: vmap got inconsistent sizes for array axes to be mapped:\narg 0 has shape (10,) and axis 0 is to be mapped\narg 1 has shape (10,) and axis None is to be mapped\narg 2 has shape (1, 10) and axis 0 is to be mapped\narg 3 has shape (11,) and axis 0 is to be mapped\narg 4 has shape (11,) and axis 0 is to be mapped\nso\narg 0 has an axis to be mapped of size 10\narg 2 has an axis to be mapped of size 1\nargs 3, 4 have axes to be mapped of size 11\nIt is my first time using Jax, and my google search didn't help me resolve the issue, plus the documentation was not very clear to me. I'd appreciate if anyone can help me.",
        "answers": [
            "The issue is exactly what the error message says: in order to vmap an operation over multiple arrays, the dimension of the mapped axes in each array must be equal. In your arrays, the dimensions are not equal: you passed in_axes=(0, None, 0, 0, 0) for arguments W1, W2, b, x, y, but W1.shape[0] = 10, b.shape[0] = 1, x.shape[0] = 11, and y.shape[0] = 11.\nBecause these are not equal, you get this error. To prevent this error, you should only vmap over array axes of the same length.\nFor example, if you want the gradients with respect to W2 computed per pair of W1, W2 inputs, it might look something like this (note the updated predict function and updated in_axes):\nimport jax.numpy as jnp\nfrom jax import random, nn, grad, vmap\n\nkey = random.PRNGKey(0)\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.float32)\ny = jnp.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=jnp.float32)\n\nW1 = jnp.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=jnp.float32)\nW2 = random.uniform(key, shape=(10,), minval=1, maxval=2, dtype=jnp.float32)\nb = jnp.linspace(0, -9, 10, dtype=jnp.float32)\nb = jnp.reshape(b, (1,10))\n\ndef predict(W1, W2, b, x):\n  # Since you're vmapping over W1 and W2, your function needs to\n  # handle scalar values, so we cast to 1D if necessary.\n  W1 = jnp.atleast_1d(W1)\n  W2 = jnp.atleast_1d(W2)\n\n  f1 = jnp.einsum('i,j->ji', W1, x)+b\n  f1 = nn.relu(f1)\n  f2 = jnp.einsum('i,ji->j', W2, f1)\n  return f2\n\ndef loss(W1, W2, b, x, y):\n  preds = predict(W1, W2, b, x)\n  return jnp.mean(jnp.square(y-preds))\n\nperex_grads = vmap(grad(loss, argnums=1), in_axes= (0, 0, None, None, None))\npers_grads = perex_grads(W1, W2, b, x, y)"
        ],
        "link": "https://stackoverflow.com/questions/71168030/vmap-gives-inconsistent-shape-error-when-trying-to-calculate-gradient-per-sample"
    },
    {
        "title": "Merge dataclasses in python",
        "question": "I have a dataclass like:\nimport dataclasses\nimport jax.numpy as jnp\n\n@dataclasses.dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\nIn my code, I create multiple instances of it, is there an easy way to merge two of them attribute per attribute? For example if I have:\na = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nb = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nI'd like to merge them such as having a single Metric containing:\nscore1=jnp.array([10,10,10,10,10,10]), score2=jnp.array([20,20,20,20,20,20]) and score3=jnp.array([30,30,30,30,30,30])",
        "answers": [
            "It is possible to do so in a \"jax-centric\" manner by registering the class Metric as a pytree_node. google/flax, a neural network library built on top of jax, provides the flax.struct.dataclass helper to do so. Once registered, you can use the jax.tree_util package to manipulate Metric instances:\nfrom flax.struct import dataclass as flax_dataclass\nfrom jax.tree_util import tree_multimap\nimport jax.numpy as jnp\n\n@flax_dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\n\na = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nb = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\n\ntree_multimap(lambda x, y: jnp.concatenate([x, y]), a, b)\nGives:\nMetric(score1=DeviceArray([10, 10, 10, 10, 10, 10], dtype=int32), score2=DeviceArray([20, 20, 20, 20, 20, 20], dtype=int32), score3=DeviceArray([30, 30, 30, 30, 30, 30], dtype=int32))",
            "The easiest thing is probably just to define a method:\nimport dataclasses\nimport jax.numpy as jnp\n\n\n@dataclasses.dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\n\n    def concatenate(self, other):\n        return Metric(\n            jnp.concatenate((self.score1, other.score1)),\n            jnp.concatenate((self.score2, other.score2)),\n            jnp.concatenate((self.score3, other.score3)),\n        )\nand then just do a.concatenate(b). You could also instead call the method __add__, which would make it possible just to use a + b. This is neater, but could potentially be confused with element-wise addition."
        ],
        "link": "https://stackoverflow.com/questions/71109587/merge-dataclasses-in-python"
    },
    {
        "title": "swapaxes and how it is implemented?",
        "question": "I'm wondering if someone can explain this code to me?\nc = self.config\n\nassert len(pair_act.shape) == 3\nassert len(pair_mask.shape) == 2\nassert c.orientation in ['per_row', 'per_column']\n\nif c.orientation == 'per_column':\n  pair_act = jnp.swapaxes(pair_act, -2, -3)\n  pair_mask = jnp.swapaxes(pair_mask, -1, -2)\nIt looks like pair_act is a 3D array and pair_mask is a 2D array? What are the numbers -1, -2, and -3? For 3D arrays, my initial thought is that the array is 0, column is 1, and row is 2. So where does the - sign come from? Any array examples would be appreciated. Thanks for the help.",
        "answers": [
            "The documentation for jax.numpy.swapaxes is here: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.swapaxes.html\nThe effect of swapaxes is essentially to transpose the two provided axes, resulting in a differently shaped array:\nimport jax.numpy as jnp\n\nx = jnp.arange(24).reshape((2, 3, 4))\nprint(x.shape)\n# (2, 3, 4)\n\ny = jnp.swapaxes(x, 1, 2)\nprint(y.shape)\n# (2, 4, 3)\nAs is standard in numpy indexing, negative numbers count backward from the end; here the indices refer to entries in the shape (which has length 3), so -2, -1 is equivalent to 1, 2:\ny = jnp.swapaxes(x, -2, -1)\nprint(y.shape)\n# (2, 4, 3)\nThe result of a swapaxes is equivalent to an appropriately constructed transpose operation:\ny2 = jnp.transpose(x, (0, 2, 1))\nprint((y == y2).all())\n# True"
        ],
        "link": "https://stackoverflow.com/questions/71072620/swapaxes-and-how-it-is-implemented"
    },
    {
        "title": "Error upon compilation while using jax.jit",
        "question": "Pardon me I'm still a noob with the inner workings of Jax and trying to find my way around it. I have this code which works well without the jit. But when I try to jit it, it throws an error. I initially used an if else statement within the code which also did not work and had to rewrite the code this way without an if else statement. How do I get around this?. MWE is below.\nimport jax\nimport jax.numpy as jnp\njax.config.update(\"jax_enable_x64\", True)\n\nnum_rows = 5\nnum_cols = 20\nsmf = jnp.array([jnp.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = jnp.array([1.0,2.0,3.0,4.0,5.0])\nlb = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = jnp.array([10.0, 10.0, 10.0, 10.0, 10.0])\npar = jnp.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = jnp.where(jnp.isinf(smf), 1, num_cols)\nkvals = jnp.insert(kvals, 0, 0)\nkvals = jnp.cumsum(kvals)\n\npar0_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nlb_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nub_col = jnp.zeros(num_rows*num_cols- (num_cols-1) * jnp.sum(jnp.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col = par0_col.at[kvals[i]:kvals[i+1]].set(par[i, :kvals[i+1]-kvals[i]])\n    lb_col = lb_col.at[kvals[i]:kvals[i+1]].set(lb[i])\n    ub_col = ub_col.at[kvals[i]:kvals[i+1]].set(ub[i])\n\n\n\n\npar_log = jnp.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\n\n@jax.jit  \ndef compute(p):\n    arr_1 = jnp.zeros(shape = (num_rows, num_cols))\n    arr_2 = jnp.zeros(shape = (num_rows, num_cols))\n    for i in range(num_rows):\n \n        arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\n        arr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\n\n    return arr_1\n\narr = compute(par_log)\nprint(arr)\n\n\n# WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n# Traceback (most recent call last):\n#   File \"test_7.py\", line 47, in <module>\n#     arr = compute(par_log)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 162, in reraise_with_filtered_traceback\n#     return fun(*args, **kwargs)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/api.py\", line 424, in cache_miss\n#     out_flat = xla.xla_call(\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/core.py\", line 1661, in bind\n#     return call_bind(self, fun, *args, **params)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/core.py\", line 1652, in call_bind\n#     outs = primitive.process(top_trace, fun, tracers, params)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/core.py\", line 1664, in process\n#     return trace.process_call(self, fun, tracers, params)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/core.py\", line 633, in process_call\n#     return primitive.impl(f, *tracers, **params)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/dispatch.py\", line 128, in _xla_call_impl\n#     compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/linear_util.py\", line 263, in memoized_fun\n#     ans = call(fun, *args)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/dispatch.py\", line 155, in _xla_callable_uncached\n#     return lower_xla_callable(fun, device, backend, name, donated_invars,\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/profiler.py\", line 206, in wrapper\n#     return func(*args, **kwargs)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/dispatch.py\", line 169, in lower_xla_callable\n#     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/profiler.py\", line 206, in wrapper\n#     return func(*args, **kwargs)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/interpreters/partial_eval.py\", line 1566, in trace_to_jaxpr_final\n#     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/interpreters/partial_eval.py\", line 1543, in trace_to_subjaxpr_dynamic\n#     ans = fun.call_wrapped(*in_tracers)\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/linear_util.py\", line 166, in call_wrapped\n#     ans = self.f(*args, **dict(self.params, **kwargs))\n#   File \"test_7.py\", line 42, in compute\n#     arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\", line 5704, in _rewriting_take\n#     return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\", line 5713, in _gather\n#     indexer = _index_to_gather(shape(arr), idx)  # shared with _scatter_update\n#   File \"/home/richinex/anaconda3/envs/numerical/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\", line 5956, in _index_to_gather\n#     raise IndexError(msg)\n# jax._src.traceback_util.UnfilteredStackTrace: IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\n\n# The stack trace below excludes JAX-internal frames.\n# The preceding is the original exception that occurred, unmodified.\n\n# --------------------\n\n# The above exception was the direct cause of the following exception:",
        "answers": [
            "The issue is that indexing in JAX must be done with static values, and within JIT kvals[i] is not a static value (because it is computed from a JAX array).\nOne easy way to fix this in your case is to make kvals a non-jax array; for example when you define it, do this;\nkvals = list(jnp.cumsum(kvals))\nThis works here because kvals is created outside the jit expression. In general, if your indices are also created inside a JIT expression, you can compute slices dynamically with lax.dynamic_slice, which does support dynamic start indices.\nFor more background on static vs. traced values, a useful read is How To Think In JAX."
        ],
        "link": "https://stackoverflow.com/questions/71016664/error-upon-compilation-while-using-jax-jit"
    },
    {
        "title": "Resulting array is zero when jax.numpy is used",
        "question": "I wrote the code below using numpy and got the correct output as shown in program 1. However when I switch to jax.numpy as jnp (in Program 2) the resulting output is an array of zeros. My MWE is shown below. I would like to know where I got the computation wrong? PS: the codes were run in different python files.\n#Program 1 (using numpy as np):\nimport numpy as np\n\nnum_rows = 5\nnum_cols = 20\nsmf = np.array([np.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = np.array([1,2,3,4,5])\nlb = np.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = np.array([10, 10, 10, 10, 10])\npar = np.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = np.where(np.isinf(smf), 1, num_cols)\nkvals = np.insert(kvals, 0, 0)\nkvals = np.cumsum(kvals)\n\npar0_col = np.zeros(num_rows*num_cols - (num_cols-1) * np.sum(np.isinf(smf)))\nlb_col = np.zeros(num_rows*num_cols - (num_cols-1) * np.sum(np.isinf(smf)))\nub_col = np.zeros(num_rows*num_cols- (num_cols-1) * np.sum(np.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col[kvals[i]:kvals[i+1]] = par[i, :kvals[i+1]-kvals[i]]\n    lb_col[kvals[i]:kvals[i+1]] = lb[i]\n    ub_col[kvals[i]:kvals[i+1]] = ub[i]\n\narr_1 = np.zeros(shape = (num_rows, num_cols))\narr_2 = np.zeros(shape = (num_rows, num_cols))\n\n\npar_log = np.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\nk = 0\nfor i in range(num_rows):\n\n    arr_1[i, :] = (par_log[kvals[i]:kvals[i+1]])\n    arr_2[i, :] = 10**par_log[kvals[i]:kvals[i+1]]\n  \n\nprint(arr_1)\n\n# [[0.         0.         0.         0.         0.         0.\n#   0.         0.         0.         0.         0.         0.\n#   0.         0.         0.         0.         0.         0.\n#   0.         0.        ]\n#  [0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361]\n#  [0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996]\n#  [0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336]\n#  [0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608]]\n\n# Program 2 (using jax.numpy as jnp):\n\nimport jax\nimport jax.numpy as jnp\njax.config.update(\"jax_enable_x64\", True)\n\nsmf = jnp.array([jnp.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = jnp.array([1.0,2.0,3.0,4.0,5.0])\nlb = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = jnp.array([10.0, 10.0, 10.0, 10.0, 10.0])\npar = jnp.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = jnp.where(jnp.isinf(smf), 1, num_cols)\nkvals = jnp.insert(kvals, 0, 0)\nkvals = jnp.cumsum(kvals)\n\npar0_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nlb_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nub_col = jnp.zeros(num_rows*num_cols- (num_cols-1) * jnp.sum(jnp.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col.at[kvals[i]:kvals[i+1]].set(par[i, :kvals[i+1]-kvals[i]])\n    lb_col.at[kvals[i]:kvals[i+1]].set(lb[i])\n    ub_col.at[kvals[i]:kvals[i+1]].set(ub[i])\n\narr_1 = jnp.zeros(shape = (num_rows, num_cols))\narr_2 = jnp.zeros(shape = (num_rows, num_cols))\n\n\npar_log = jnp.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\nfor i in range(num_rows):\n \n    arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\n    arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\n  \n\nprint(arr_1)\n\n# #[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]",
        "answers": [
            "The issue is that ndarray.at expressions don't operate in-place, but rather return a modified value.\nSo instead of this:\narr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\narr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\nYou should write this:\narr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\narr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\nRead more at JAX sharp bits: in-place updates.",
            "Oh I already figured it out. I needed to make an explicit assignment."
        ],
        "link": "https://stackoverflow.com/questions/71016283/resulting-array-is-zero-when-jax-numpy-is-used"
    },
    {
        "title": "TypeError: Argument 'MLP( # attributes num_neurons_per_layer = [4, 1] )' of type <class '__main__.MLP'> is not a valid JAX type",
        "question": "I'm having some issues testing a basic model on Jax. For example, I'm trying to implement the value_and_grad() function from Jax manually for a binary classification problem. Here is my model initializer:\nclass MLP(nn.Module):\n    num_neurons_per_layer: Sequence[int]\n\n    @nn.compact\n    def __call__(self, x):\n        activation = x\n        for i, num_neurons in enumerate(self.num_neurons_per_layer):\n            activation = nn.Dense(num_neurons)(activation)\n            if i != len(self.num_neurons_per_layer) - 1:\n                activation = nn.relu(activation)\n        return nn.sigmoid(activation)\nAnd here is my BCE loss which is using vmap to batch the samples quicker all wrapped in a jit:\ndef make_bce_loss(xs, ys):\n    \n    def bce_loss(params, model): \n        def cross_entropy(x, y):\n            preds = model.apply(params, x)\n            return y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds)\n        return -jnp.mean(jax.vmap(cross_entropy)(xs, ys), axis=0)\n\n    return jax.jit(bce_loss)\n\nbce_loss = make_bce_loss(X, y)\nvalue_and_grad_fn = jax.value_and_grad(bce_loss)\nThen I proceed to create the model and init the parameters:\nmodel = MLP(num_neurons_per_layer=[4, 1])\nparams = model.init(key, X)  # I create a jnp.array() to create X earlier on\nWhen I test out my jitted version of value_and_grad_fn(params, model) I get the following error:\nTypeError: Argument 'MLP( # attributes num_neurons_per_layer = [4, 1] )' of type <class '__main__.MLP'> is not a valid JAX type.\nI'm not sure what I should be doing to correct this. It is throwing an error about the [4, 1] but those aren't involved in the calculation at all, they are only used to initialize the model in the MLP class.",
        "answers": [
            "Flax models must be marked as static for use in jit:\n    return jax.jit(bce_loss, static_argnums=1)\nIt looks like flax has an issue to improve this error message: https://github.com/google/flax/issues/853"
        ],
        "link": "https://stackoverflow.com/questions/70991484/typeerror-argument-mlp-attributes-num-neurons-per-layer-4-1-of-type"
    },
    {
        "title": "Does JAX run slower than NumPy?",
        "question": "I've recently started to learn JAX. I have written a short snippet in NumPy and have written its equivalent in JAX. I was expecting JAX to be faster but when I profile the codes, the NumPy code is way faster than the JAX code. I was wondering if this is generally true or if there is an issue in my implementation.\nNumPy code:\nimport numpy as np\nfrom time import time as tm\n\n\ndef gp(x):\n    return np.maximum(np.zeros(x.shape), x)\n\n\n# -- inputs\nn_q1 = 25\nn_q2 = 5\n\nx = np.random.rand(1, n_q1)  # todo: changes w/ time\ny = np.random.rand(1, n_q2)  # todo: changes w/ time\n\n# -- activations\nn_p1 = 3\nn_p2 = 2\n\nv_p1 = np.random.rand(1, n_p1)\nv_p2 = np.random.rand(1, n_p2)\n\na_p1 = 0.5\na_p2 = 0.5\n\n# -- weights\nW_q1p1 = np.random.rand(n_q1, n_p1)\nW_p2q2 = np.random.rand(n_p2, n_q2)\nW_p1p1 = np.random.rand(n_p1, n_p1)\nW_p1p2 = np.random.rand(n_p1, n_p2)\nW_p2p1 = np.random.rand(n_p2, n_p1)\n\n# -- computation\nt1=tm()\n\nfor t in range(2000):\n\n    z_p1 = np.matmul(v_p1, W_p1p1) + np.matmul(v_p2, W_p2p1) + np.matmul(x, W_q1p1)\n    v_p1_new = a_p1 * v_p1 + (1 - a_p1) * gp(z_p1)\n\n    z_p2 = np.matmul(v_p1, W_p1p2)\n    v_p2_new = a_p2 * v_p2 + (1 - a_p2) * gp(z_p2)\n\n    v_p1, v_p2 = v_p1_new, v_p2_new\n\nprint(tm()-t1)\nThis yields: 0.02118515968322754\nJAX code:\nfrom jax import random, nn, numpy as jnp\n\nfrom time import time as tm\n\n\ndef gp(x):\n    return nn.relu(x)\n\n\n# -- inputs\nn_q1 = 25\nn_q2 = 5\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (1, n_q1))\ny = random.normal(key, (1, n_q2))  # todo: check if I need to advance \"key\" manually\n\n# -- activations\nn_p1 = 3\nn_p2 = 2\n\nv_p1 = random.normal(key, (1, n_p1))\nv_p2 = random.normal(key, (1, n_p2))\n\na_p1 = 0.5\na_p2 = 0.5\n\n# -- weights\nW_q1p1 = random.normal(key, (n_q1, n_p1))\nW_p2q2 = random.normal(key, (n_p2, n_q2))\n\nW_p1p1 = random.normal(key, (n_p1, n_p1))\nW_p1p2 = random.normal(key, (n_p1, n_p2))\nW_p2p1 = random.normal(key, (n_p2, n_p1))\n\n# -- computation\nt1=tm()\nfor t in range(2000):\n\n    z_p1 = jnp.matmul(v_p1, W_p1p1) + jnp.matmul(v_p2, W_p2p1) + jnp.matmul(x, W_q1p1)\n    v_p1_new = a_p1 * v_p1 + (1 - a_p1) * gp(z_p1)\n\n    z_p2 = jnp.matmul(v_p1, W_p1p2)\n    v_p2_new = a_p2 * v_p2 + (1 - a_p2) * gp(z_p2)\n\n    v_p1, v_p2 = v_p1_new, v_p2_new\n    \nprint(tm()-t1)\nThis yields: 2.5548229217529297",
        "answers": [
            "The JAX documentation has a useful section on this in its FAQ: https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy\nTL;DR: it's complicated. For individual matrix operations on CPU, JAX is often slower than NumPy, but JIT-compiled sequences of operations in JAX are often faster than NumPy, and once you move to GPU/TPU, JAX will generally be much faster than NumPy."
        ],
        "link": "https://stackoverflow.com/questions/70748534/does-jax-run-slower-than-numpy"
    },
    {
        "title": "Compute efficiently Hessian matrices in JAX",
        "question": "In JAX's Quickstart tutorial I found that the Hessian matrix can be computed efficiently for a differentiable function fun using the following lines of code:\nfrom jax import jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\nHowever, one can compute the Hessian also by computing the following:\ndef hessian(fun):\n  return jit(jacrev(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacfwd(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacrev(jacrev(fun)))\nHere is a minimal working example:\nimport jax.numpy as jnp\nfrom jax import jit\nfrom jax import jacfwd, jacrev\n\ndef comp_hessian():\n\n    x = jnp.arange(1.0, 4.0)\n\n    def sum_logistics(x):\n        return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\n    def hessian_1(fun):\n        return jit(jacfwd(jacrev(fun)))\n\n    def hessian_2(fun):\n        return jit(jacrev(jacfwd(fun)))\n\n    def hessian_3(fun):\n        return jit(jacrev(jacrev(fun)))\n\n    def hessian_4(fun):\n        return jit(jacfwd(jacfwd(fun)))\n\n    hessian_fn = hessian_1(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_2(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_3(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_4(sum_logistics)\n    print(hessian_fn(x))\n\n\ndef main():\n    comp_hessian()\n\n\nif __name__ == \"__main__\":\n    main()\nI would like to know which approach is best to use and when? I also would like to know if it is possible to use grad() to compute the Hessian? And how does grad() differ from jacfwd and jacrev?",
        "answers": [
            "The answer to your question is within the JAX documentation; see for example this section: https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev\nTo quote its discussion of jacrev and jacfwd:\nThese two functions compute the same values (up to machine numerics), but differ in their implementation: jacfwd uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while jacrev uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, jacfwd probably has an edge over jacrev.\nand further down,\nTo implement hessian, we could have used jacfwd(jacrev(f)) or jacrev(jacfwd(f)) or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function 𝑓:ℝⁿ→ℝ), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since ∇𝑓:ℝⁿ→ℝⁿ), which is where forward-mode wins out.\nSince your function looks like 𝑓:ℝⁿ→ℝ, then jit(jacfwd(jacrev(fun))) is likely the most efficient approach.\nAs for why you can't implement a hessian with grad, this is because grad is only designed for derivatives of functions with scalar outputs. A hessian by definition is a composition of vector-valued jacobians, not a composition of scalar gradients."
        ],
        "link": "https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax"
    },
    {
        "title": "Accelerating nested for-loops in JAX",
        "question": "I want to accelerate the nested for-loops in the example below using JAX's jit method. However, it takes very long to compile and the runtime after compilation is even slower compared to the version without using jit.\nDo I use jit correctly? Are there other features in JAX that I should use here instead?\nimport time\nimport jax.numpy as jnp\nfrom jax import jit\nfrom jax import random\n\nkey = random.PRNGKey(seed=0)\n\nwidth = 32\nheight = 64\n\nw = random.normal(key=key, shape=(height, width))\n\ndef forward():\n    a = jnp.zeros(shape=(height, width + 1))\n\n    for i in range(height):\n        a = a.at[i, 0].add(1.0)\n\n    for j in range(width):\n        for i in range(1, height-1):\n            z = a[i-1, j] * w[i-1, j] \\\n                + a[i, j] * w[i, j] \\\n                + a[i+1, j] * w[i+1, j]\n            a = a.at[i, j+1].set(z)\n\nt0 = time.time()\nforward()\nprint(time.time()-t0)\n\nfeedforward_jit = jit(forward)\n\nt0 = time.time()\nfeedforward_jit()\nprint(time.time()-t0)",
        "answers": [
            "The short answer to your question is: to optimize your loops, you should do everything you can to remove the loops from your program.\nJAX (like NumPy) is a language built on array manipulation, and any time you resort to looping over dimensions of arrays, JAX (like NumPy) will be slower than you'd probably like. This is particularly the case during JIT compilation: JAX will flatten loops before sending the operations to XLA, and XLA compilation time scales as roughly the square of the number of operations sent to it, so nested loops are a great way to quickly create very slow compilations.\nSo how can you avoid these loops? First, let's redefine your function so that it takes inputs and returns outputs (given JAX's dead code elimination and asynchronous dispatch, I don't think your initial benchmarks are telling you what you think they are; see Benchmarking JAX code for some tips):\ndef forward(w):\n  height, width = w.shape\n  a = jnp.zeros(shape=(height, width + 1))\n\n  for i in range(height):\n    a = a.at[i, 0].add(1.0)\n\n  for j in range(width):\n    for i in range(1, height-1):\n      z = (a[i-1, j] * w[i-1, j]\n           + a[i, j] * w[i, j]\n           + a[i+1, j] * w[i+1, j])\n      a = a.at[i, j+1].set(z)\n  return a\nThe first loop is a case that can be replaced by a one-line vectorized update: a = a.at[:, 0].set(1). Looking at the inner loop of the next block, it appears that the code does a convolution along each column. Let's use jnp.convolve to do that more efficiently. Using these two optimizations results in this:\ndef forward2(w):\n  height, width = w.shape\n  a = jnp.zeros((height, width + 1)).at[:, 0].set(1)\n  kernel = jnp.ones(3)\n  for j in range(width):\n    conv = jnp.convolve(a[:, j] * w[:, j], kernel, mode='valid')\n    a = a.at[1:-1, j + 1].set(conv)\n  return a\nNext let's look at the loop over width. Here it's trickier, because each iteration depends on the result of the last. One way we could express that is with lax.scan, which is one of JAX's built-in control flow operators. You might do it like this:\ndef forward3(w):\n  def body(carry, w):\n    conv = jnp.convolve(carry * w, kernel, mode='valid')\n    out = jnp.zeros_like(w).at[1:-1].set(conv)\n    return out, out\n  init = jnp.ones(w.shape[0])\n  kernel = jnp.ones(3)\n  return jnp.vstack([\n      init, lax.scan(body, jnp.ones(w.shape[0]), w.T)[1]]).T\nWe can quickly confirm that the three approaches give the same outputs:\nwidth = 32\nheight = 64\nw = random.normal(key=key, shape=(height, width))\n\nresult1 = forward(w)\nresult2 = forward2(w)\nresult3 = forward3(w)\n\nassert jnp.allclose(result1, result2)\nassert jnp.allclose(result2, result3)\nUsing IPython's %time magic we can get a rough idea of the computation time of each approach, here on a CPU backend (note the use of block_until_ready() to account for JAX's Asynchronous dispatch):\n%time forward(w).block_until_ready()\n# CPU times: user 23 s, sys: 248 ms, total: 23.3 s\n# Wall time: 22.9 s\n\n%time forward2(w).block_until_ready()\n# CPU times: user 117 ms, sys: 866 µs, total: 118 ms\n# Wall time: 118 ms\n\n%time forward3(w).block_until_ready()\n# CPU times: user 93.2 ms, sys: 2.96 ms, total: 96.1 ms\n# Wall time: 94 ms\nYou can read more about JAX and control flow at https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow."
        ],
        "link": "https://stackoverflow.com/questions/70568316/accelerating-nested-for-loops-in-jax"
    },
    {
        "title": "in_axes keyword in JAX's vmap",
        "question": "I'm trying to understand JAX's auto-vectorization capabilities using vmap and implemented a minimal working example based on JAX's documentation.\nI don't understand how in_axes is used correctly. In the example below I can set in_axes=(None, 0) or in_axes=(None, 1) leading to the same results. Why is that the case?\nAnd why do I have to use in_axes=(None, 0) and not something like in_axes=(0, )?\nimport jax.numpy as jnp\nfrom jax import vmap\n\n\ndef predict(params, input_vec):\n    assert input_vec.ndim == 1\n    activations = input_vec\n    for W, b in params:\n        outputs = jnp.dot(W, activations) + b\n        activations = jnp.tanh(outputs)\n    return outputs\n\n\nif __name__ == \"__main__\":\n\n    # Parameters\n    dims = [2, 3, 5]\n    input_dims = dims[0]\n    batch_size = 2\n\n    # Weights\n    params = list()\n    for dims_in, dims_out in zip(dims, dims[1:]):\n        params.append((jnp.ones((dims_out, dims_in)), jnp.ones((dims_out,))))\n\n    # Input data\n    input_batch = jnp.ones((batch_size, input_dims))\n\n    # With vmap\n    predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n    print(predictions)",
        "answers": [
            "in_axes=(None, 0) means that the first argument (here params) will not be mapped, while the second argument (here input_vec) will be mapped along axis 0.\nIn the example below I can set in_axes=(None, 0) or in_axes=(None, 1) leading to the same results. Why is that the case?\nThis is because input_vec is a 2x2 matrix of ones, so whether you map along axis 0 or axis 1, the input vectors are length-2 vectors of ones. In more general cases, the two specifications are not equivalent, which you can see by either (1) making batch_size differ from input_dims[0], or (2) filling your arrays with non-constant values.\nwhy do I have to use in_axes=(None, 0) and not something like in_axes=(0, )?\nIf you set in_axes=(0, ) for a function with two arguments, you get an error because the length of the in_axes tuple must match the number of arguments passed to the function. That said, it is possible to pass a scalar in_axes=0 as a shorthand for in_axes=(0, 0), but for your function this would lead to a shape error because the leading dimension of the arrays in params does not match the leading dimension of input_vec."
        ],
        "link": "https://stackoverflow.com/questions/70564419/in-axes-keyword-in-jaxs-vmap"
    },
    {
        "title": "Combine scipy.root and Jax Jacobian",
        "question": "I am having trouble using the Jacobian from JAX with scipy.root. In the below example, the root works without the Jacobian, while it fails with the Jacobian. Any ideas on what I need to rewrite in order to get the code below working with the Jacobian?\nfrom jax import jacfwd\nfrom scipy.optimize import root\nimport numpy as np\n\ndef objectFunction(valuesEndo, varNamesEndo, valuesExo, varNamesExo, equations): \n  for i in range(len(varNamesExo)):\n      exec(\"%s = %.10f\" %(varNamesExo[i], valuesExo[i]))\n\n  for i in range(len(varNamesEndo)):\n    exec(\"%s = %.10f\" %(varNamesEndo[i], valuesEndo[i]))\n    \n  equationVector = np.zeros(len(equations))\n  for i in range(len(equations)):\n      exec('equationVector[%d] = eval(equations[%d])' %(i, i))    \n      \n  return equationVector\n\nvarNamesEndo = ['x', 'y']\nvaluesEndoInitialGuess = [1., 1.]\n\nvarNamesExo = ['a', 'b']\nvaluesExo = [1., 1.]\n\nequations = ['a*x+b*y**2-4',\n            'np.exp(x) + x*y - 3']\n\nequations = ['a*x**2 + b*y**2',\n            'a*x**2 - b*y**2']\n\n# Without Jacobian\nsol1 =  root(fun=objectFunction,\n            x0=valuesEndoInitialGuess, \n            args=(varNamesEndo, valuesExo, varNamesExo, equations))\n#----> Works\n\n# With Jacobian\njac  = jacfwd(objectFunction)\nsol2 =  root(fun=objectFunction,\n            x0=valuesEndoInitialGuess, \n            args=(varNamesEndo, valuesExo, varNamesExo, equations),\n            jac=jac)\n#----> Not woring\nAt least there seems to be problems with the line\nfor i in range(len(varNamesEndo)):\n        exec(\"%s = %.10f\" %(varNamesEndo[i], valuesEndo[i]))",
        "answers": [
            "There are two issues:\nto perform automatic differentiation, JAX relies on replacing values with tracers. This means your approach of printing and evaluating the string representation of the value will not work.\nadditionally, you are attempting to assign traced values to a standard numpy array. You should use a JAX array instead, as it knows how to handle traced values.\nWith this in mind, you can rewrite your function this way and it should work, so long as your equations only use Python arithmetic operations and jax functions (not things like np.exp):\nimport jax.numpy as jnp\n\ndef objectFunction(valuesEndo, varNamesEndo, valuesExo, varNamesExo, equations): \n  for i in range(len(varNamesExo)):\n      exec(\"%s = valuesExo[%d]\" %(varNamesExo[i], i))\n\n  for i in range(len(varNamesEndo)):\n    exec(\"%s = valuesEndo[%d]\" %(varNamesEndo[i], i))\n    \n  equationVector = jnp.zeros(len(equations))\n  for i in range(len(equations)):\n      equationVector = equationVector.at[i].set(eval(equations[i]))\n      \n  return equationVector\nSide-note: this kind of approach based on setting variable names using exec is really brittle and error-prone; I'd suggest an approach based on building explicit namespaces for evaluating your equations. For example something like this:\ndef objectFunction(valuesEndo, varNamesEndo, valuesExo, varNamesExo, equations):\n  namespace = {\n      **dict(zip(varNamesEndo, valuesEndo)),\n      **dict(zip(varNamesExo, valuesExo))\n  }\n  return jnp.array([eval(eqn, namespace) for eqn in equations])"
        ],
        "link": "https://stackoverflow.com/questions/70409729/combine-scipy-root-and-jax-jacobian"
    },
    {
        "title": "Fastest way to multiply and sum 4D array with 2D array in python?",
        "question": "Here's my problem. I have two matrices A and B, with complex entries, of dimensions (n,n,m,m) and (n,n) respectively.\nBelow is the operation I perform to get a matrix C -\nC = np.sum(B[:,:,None,None]*A, axis=(0,1))\nComputing the above once takes about 6-8 seconds. Since I have to compute many such Cs, it takes a lot of time. Is there a faster way to do this? (I'm doing these using JAX NumPy on a multi-core CPU; normal NumPy takes even longer)\nn=77 and m=512, if you are wondering. I can parallelize as I'm working on a cluster, but the sheer size of the arrays consumes a lot of memory.",
        "answers": [
            "It looks like you want einsum:\nC = np.einsum('ijkl,ij->kl', A, B)\nWith numpy on a Colab CPU I get this:\nimport numpy as np\nx = np.random.rand(50, 50, 500, 500)\ny = np.random.rand(50, 50)\n\ndef f1(x, y):\n  return np.sum(y[:,:,None,None]*x, axis=(0,1))\n\ndef f2(x, y):\n  return np.einsum('ijkl,ij->kl', x, y)\n\nnp.testing.assert_allclose(f1(x, y), f2(x, y))\n\n%timeit f1(x, y)\n# 1 loop, best of 5: 1.52 s per loop\n%timeit f2(x, y)\n# 1 loop, best of 5: 620 ms per loop"
        ],
        "link": "https://stackoverflow.com/questions/70394566/fastest-way-to-multiply-and-sum-4d-array-with-2d-array-in-python"
    },
    {
        "title": "Is there a module to convert a tensorflow NN to Jax?",
        "question": "There is a libary to convert Jax functions to Tensorflow functions. Is there a similar library to convert TensorFlow functions to Jax functions?",
        "answers": [
            "No, there is no library supported by the JAX team to convert tensorflow into JAX in a manner similar to how jax.experimental.jax2tf converts JAX code to tensorflow, and I have not seen any such library developed by others.",
            "See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md.\n\"jax2tf.call_tf: for using TensorFlow functions in a JAX context, e.g., to call a TensorFlow library or a SavedModel inside a JAX function.\"\nThat is what you need. So you can call tf function under jax context.\nfor example:\n# Compute cos with TF and sin with JAX\ndef cos_tf_sin_jax(x):\n  return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))",
            "Is https://github.com/google-deepmind/tf2jax what you were looking for? It only works for TF v2 though.",
            "To my knowledge there is no library similar to the one you mentioned to convert TensorFlow functions to Jax functions. I'm sorry"
        ],
        "link": "https://stackoverflow.com/questions/70356126/is-there-a-module-to-convert-a-tensorflow-nn-to-jax"
    },
    {
        "title": "Automatic Differentiation with respect to rank-based computations",
        "question": "I'm new to automatic differentiation programming, so this maybe a naive question. Below is a simplified version of what I'm trying to solve.\nI have two input arrays - a vector A of size N and a matrix B of shape (N, M), as well a parameter vector theta of size M. I define a new array C(theta) = B * theta to get a new vector of size N. I then obtain the indices of elements that fall in the upper and lower quartile of C, and use them to create a new array A_low(theta) = A[lower quartile indices of C] and A_high(theta) = A[upper quartile indices of C]. Clearly these two do depend on theta, but is it possible to differentiate A_low and A_high w.r.t theta?\nMy attempts so far seem to suggest no - I have using the python libraries of autograd, JAX and tensorflow, but they all return a gradient of zero. (The approaches I have tried so far involve using argsort or extracting the relevant sub-arrays using tf.top_k.)\nWhat I'm seeking help with is either a proof that the derivative is not defined (or cannot be analytically computed) or if it does exist, a suggestion on how to estimate it. My eventual goal is to minimize some function f(A_low, A_high) wrt theta.",
        "answers": [
            "This is the JAX computation that I wrote based on your description:\nimport numpy as np\nimport jax.numpy as jnp\nimport jax\n\nN = 10\nM = 20\n\nrng = np.random.default_rng(0)\nA = jnp.array(rng.random((N,)))\nB = jnp.array(rng.random((N, M)))\ntheta = jnp.array(rng.random(M))\n\ndef f(A, B, theta, k=3):\n  C = B @ theta\n  _, i_upper = lax.top_k(C, k)\n  _, i_lower = lax.top_k(-C, k)\n  return A[i_lower], A[i_upper]\n\nx, y = f(A, B, theta)\ndx_dtheta, dy_dtheta = jax.jacobian(f, argnums=2)(A, B, theta)\nThe derivatives are all zero, and I believe this is correct, because the change in value of the outputs does not depend on the change in value of theta.\nBut, you might ask, how can this be? After all, theta enters into the computation, and if you put in a different value for theta, you get different outputs. How could the gradient be zero?\nWhat you must keep in mind, though, is that differentiation doesn't measure whether an input affects an output. It measures the change in output given an infinitesimal change in input.\nLet's use a slightly simpler function as an example:\nimport jax\nimport jax.numpy as jnp\n\nA = jnp.array([1.0, 2.0, 3.0])\ntheta = jnp.array([5.0, 1.0, 3.0])\n\ndef f(A, theta):\n  return A[jnp.argmax(theta)]\n\nx = f(A, theta)\ndx_dtheta = jax.grad(f, argnums=1)(A, theta)\nHere the result of differentiating f with respect to theta is all zero, for the same reasons as above. Why? If you make an infinitesimal change to theta, it will in general not affect the sort order of theta. Thus, the entries you choose from A do not change given an infinitesimal change in theta, and thus the derivative with respect to theta is zero.\nNow, you might argue that there are circumstances where this is not the case: for example, if two values in theta are very close together, then certainly perturbing one even infinitesimally could change their respective rank. This is true, but the gradient resulting from this procedure is undefined (the change in output is not smooth with respect to the change in input). The good news is this discontinuity is one-sided: if you perturb in the other direction, there is no change in rank and the gradient is well-defined. In order to avoid undefined gradients, most autodiff systems will implicitly use this safer definition of a derivative for rank-based computations.\nThe result is that the value of the output does not change when you infinitesimally perturb the input, which is another way of saying the gradient is zero. And this is not a failure of autodiff – it is the correct gradient given the definition of differentiation that autodiff is built on. Moreover, were you to try changing to a different definition of the derivative at these discontinuities, the best you could hope for would be undefined outputs, so the definition that results in zeros is arguably more useful and correct."
        ],
        "link": "https://stackoverflow.com/questions/70214451/automatic-differentiation-with-respect-to-rank-based-computations"
    },
    {
        "title": "Python - time difference (JAX library)",
        "question": "I'm trying to compare execution times between functions:\nsimpleExponentialSmoothing - which is my implementation of SES in JAX library\nsimpleExponentialSmoothingJax - as above, but boosted with JIT from JAX library\nSimpleExpSmoothing - implementation from Statsmodels library\nI have tried using %timeit, time and writing my own function to measure time using datetime, however I'm quite confused. My function to measure time and %timeit are returning the same exec time, however %time is showing much, much different exec time. I have found that %time checks only single run and is less accurate than %timeit, but how does it apply to asynchronous functions like those in JAX? Although I've blocked them until finishing calculations, I'm not sure if that is enough.\nI need advice about this measure, which should I take as actual execution time?\n%time\n%time timeSeriesSes = simpleExponentialSmoothing(params, timeSeries, initState).block_until_ready()\n%time timeSeriesSesJit = simpleExponentialSmoothingJit(params, timeSeries, initState).block_until_ready()\n%time timeSeriesSesSm = SimpleExpSmoothing(timeSeries).fit()\nCPU times: user 82.4 ms, sys: 4.03 ms, total: 86.4 ms\nWall time: 97.6 ms\nCPU times: user 199 µs, sys: 0 ns, total: 199 µs\nWall time: 214 µs\nCPU times: user 6.12 ms, sys: 0 ns, total: 6.12 ms\nWall time: 6.2 ms\n%timeit\n%timeit timeSeriesSes = simpleExponentialSmoothing(params, timeSeries, initState).block_until_ready()\n%timeit timeSeriesSesJit = simpleExponentialSmoothingJit(params, timeSeries, initState).block_until_ready()\n%timeit timeSeriesSesSm = SimpleExpSmoothing(timeSeries).fit()\n48.8 ms ± 904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n15.5 µs ± 222 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n3.4 ms ± 62.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)",
        "answers": [
            "For your JAX-specific question: using block_until_ready() should be enough to account for JAX's asynchronous execution.\nBe careful also about JIT compilation: the first time you call a JIT-compiled function with arguments of a particular shape, the compilation time will affect the speed of execution. After that, the cached compilation will be used.\nAs to your more general question: the difference between %timeit and %time is covered in the IPython docs:\nBy default, timeit() temporarily turns off garbage collection during the timing. The advantage of this approach is that it makes independent timings more comparable. The disadvantage is that GC may be an important component of the performance of the function being measured.\n(From https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit)\nSo if you want to measure performance with Python garbage collection, and with only a single execution, use %time. If you want to measure performance without Python garbage collection, and with multiple executions for more statistical rigor, use %timeit."
        ],
        "link": "https://stackoverflow.com/questions/70161804/python-time-difference-jax-library"
    },
    {
        "title": "JAX hook / information / warning when a JIT function is re-compiled",
        "question": "Is it possible in JAX to get a notification whenever a function has to be re-compiled by the JAX just-in-time compiler (because the input changed and the cached compiled version cannot be evaluated)?\nFor now, I utilize a hacky workaround for being informed on the recompilation. In the current implementation, the tracer executes the function once when it needs to be compiled, and sideeffects are allowed that are thus executed only when the function is recompiled:\nimport jax\nrecompilation_count: int = 0\n\n@jax.jit\ndef func(z):\n    global recompilation_count\n    recompilation_count += 1\n    return z * z + 100 / z\n\n\nfunc(1)\nprint(recompilation_count)\nfunc(2)\nprint(recompilation_count)\nfunc(jax.numpy.arange(10))\nprint(recompilation_count)\nfunc(jax.numpy.arange(10, 20))\nprint(recompilation_count)\nfunc(jax.numpy.arange(10) ** 2)\nprint(recompilation_count)\n\nassert recompilation_count == 2\nHowever, this is an internal of the implementation of JAX, and hence cannot be used in a reliable manner. Is there another way to be informed and potentially prevent recompilation of a function if it happens to frequently?",
        "answers": [
            "I don't believe there is any built-in API to do what you are asking. But similar functionality is currently under active discussion (see e.g. https://github.com/google/jax/issues/8655)\nBut note there is a built-in way to track compilation count, if you wish:\nimport jax\n\n@jax.jit\ndef f(x):\n  return x\n\nprint(f._cache_size())\n# 0\n\n_ = f(jnp.arange(3))\nprint(f._cache_size())\n# 1\n\n_ = f(jnp.arange(3))  # should not trigger a recompilation\nprint(f._cache_size())\n# 1\n\n_ = f(jnp.arange(100))  # should trigger a recompilation\nprint(f._cache_size())\n# 2"
        ],
        "link": "https://stackoverflow.com/questions/70126574/jax-hook-information-warning-when-a-jit-function-is-re-compiled"
    },
    {
        "title": "JAX: avoid just-in-time recompilation for a function evaluated with a varying number of elements along one axis",
        "question": "Is it possible to avoid recompiling a JIT function when the structure of its input remains essentially unchanged, aside from one axis having a varying number of elements?\nimport jax\n\n@jax.jit\ndef f(x):\n    print('recompiling')\n    return (x + 10) * 100\n\na = f(jax.numpy.arange(300000000).reshape((-1, 2, 2)).block_until_ready()) # recompiling\nb = f(jax.numpy.arange(300000000).reshape((-1, 2, 2)).block_until_ready())\nc = f(jax.numpy.arange(450000000).reshape((-1, 2, 2)).block_until_ready()) # recompiling. It would be nice if it weren't\nRequirements: pip install jax, jaxlib",
        "answers": [
            "No, there is no way to avoid recompilation when you call a function with arrays of a different shape. Fundamentally, JAX compiles functions for statically-shaped inputs and outputs, and calling a JIT-compiled function with an array of a new shape will always trigger re-compilation.\nThere is some ongoing work on relaxing this requirement (search \"dynamic shapes\" in JAX's github repository) but no such APIs are available at the moment."
        ],
        "link": "https://stackoverflow.com/questions/70126391/jax-avoid-just-in-time-recompilation-for-a-function-evaluated-with-a-varying-nu"
    },
    {
        "title": "Jax: Take derivative with respect to index of vector-valued argument",
        "question": "Does Jax support taking the derivate w.r.t. an index of a vector-valued variable? Consider this example (where a is a vector/array):\ndef test_func(a):\n  return a[0]**a[1]\nI can pass in the argument number into grad(..), but I cannot seem to pass the index of a vector-valued argument like in the example above. I tried passing a tuple of tuples, i.e.,\ngrad(test_func, argnums=((0,),))\nbut that does not work.",
        "answers": [
            "There's no built-in transform that can take gradients with respect to certain elements of arrays, but you can straightforwardly do this via a wrapper function that splits the array into individual elements; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef test_func(a):\n  return a[0]**a[1]\n\na = jnp.array([1.0, 2.0])\nfgrad = jax.grad(lambda *args: test_func(jnp.array(args)), argnums=0)\nprint(fgrad(*a))\n# 2.0\nIf you want to take a gradient with respect to all the inputs individually (returning a vector of gradients with respect to each entry), you can use jax.jacobian:\nprint(jax.jacobian(test_func)(a))\n# [2. 0.]"
        ],
        "link": "https://stackoverflow.com/questions/70068234/jax-take-derivative-with-respect-to-index-of-vector-valued-argument"
    },
    {
        "title": "How to compute the joint probability density function from a joint cumulative density function in Jax?",
        "question": "I have a joint cumulative density function defined in python as a function of a jax array and returning a single value. Something like:\ndef cumulative(inputs: array) -> float:\n    ...\nTo have the gradient, I know I can just do grad(cumulative), but that is only giving me the first-order partial derivatives of cumulative with respect to the input variables. Instead, what I would like to do is to compute is this, assuming F is my function and f the joint probability density function:\nThe order of the partial derivation doesn't matter.\nSo, I have several questions:\nhow to compute this efficiently in Jax? I assume I cannot just call grad n times\nonce the resulting function is computed, will the resulting function have a higher call complexity than the original function (is it increased by O(n), or is it constant, or something else)?\nalternatively, how can I compute a single partial derivative with respect to only one of the variable of the input array, as opposed to the entire array? (And I will just repeat this n times, once per variable)",
        "answers": [
            "JAX generally treats gradients as being with respect to individual arguments, not elements within arguments. Within this context, one built-in function that is similar to what you want to do (but not exactly the same) is jax.hessian, which computes the hessian matrix of second derivatives; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  return jnp.prod(x ** 2)\n\nx = jnp.arange(1.0, 4.0)\nprint(jax.hessian(f)(x))\n# [[72. 72. 48.]\n#  [72. 18. 24.]\n#  [48. 24.  8.]]\nFor higher-order derivatives with respect to individual elements of the array, I think you'll have to manually nest the gradients. You could do so with a helper function that looks something like this:\ndef grad_all(f):\n  def gradfun(x):\n    args = tuple(x)\n    f_args = lambda *args: f(jnp.array(args))\n    for i in range(len(args)):\n      f_args = jax.grad(f_args, argnums=i)\n    return f_args(*args)\n  return gradfun\n\nprint(grad_all(f)(x))\n# 48.0"
        ],
        "link": "https://stackoverflow.com/questions/69987613/how-to-compute-the-joint-probability-density-function-from-a-joint-cumulative-de"
    },
    {
        "title": "Vectorise nested vmap",
        "question": "Here's some data I have:\nimport jax.numpy as jnp\nimport numpyro.distributions as dist\nimport jax\n\nxaxis = jnp.linspace(-3, 3, 5)\nyaxis = jnp.linspace(-3, 3, 5)\nI'd like to run the function\ndef func(x, y):\n    return dist.MultivariateNormal(jnp.zeros(2), jnp.array([[.5, .2], [.2, .1]])).log_prob(jnp.asarray([x, y]))\nover each pair of values from xaxis and yaxis.\nHere's a \"slow\" way to do:\nresults = np.zeros((len(xaxis), len(yaxis)))\n\nfor i in range(len(xaxis)):\n    for j in range(len(yaxis)):\n        results[i, j] = func(xaxis[i], yaxis[j])\nWorks, but it's slow.\nSo here's a vectorised way of doing it:\njax.vmap(lambda axis: jax.vmap(func, (None, 0))(axis, yaxis))(xaxis)\nMuch faster, but it's hard to read.\nIs there a clean way of writing the vectorised version? Can I do it with a single vmap, rather than having to nest one within another one?\nEDIT\nAnother way would be\njax.vmap(func)(xmesh.flatten(), ymesh.flatten()).reshape(len(xaxis), len(yaxis)).T\nbut it's still messy.",
        "answers": [
            "I believe Vectorization guidelnes for jax is quite similar to your question; to replicate the logic of nested for-loops with vmap requires nested vmaps.\nThe cleanest approach using jax.vmap is probably something like this:\nfrom functools import partial\n\n@partial(jax.vmap, in_axes=(0, None))\n@partial(jax.vmap, in_axes=(None, 0))\ndef func(x, y):\n    return dist.MultivariateNormal(jnp.zeros(2), jnp.array([[.5, .2], [.2, .1]])).log_prob(jnp.asarray([x, y]))\n\nfunc(xaxis, yaxis)\nAnother option here is to use the jnp.vectorize API (which is implemented via multiple vmaps), in which case you can do something like this:\nprint(jnp.vectorize(func)(xaxis[:, None], yaxis))"
        ],
        "link": "https://stackoverflow.com/questions/69846536/vectorise-nested-vmap"
    },
    {
        "title": "Turn a tf.data.Dataset to a jax.numpy iterator",
        "question": "I am interested about training a neural network using JAX. I had a look on tf.data.Dataset, but it provides exclusively tf tensors. I looked for a way to change the dataset into JAX numpy array and I found a lot of implementations that use Dataset.as_numpy_generator() to turn the tf tensors to numpy arrays. However I wonder if it is a good practice, as numpy arrays are stored in CPU memory and it is not what I want for my training (I use the GPU). So the last idea I found is to manually recast the arrays by calling jnp.array but it is not really elegant (I am afraid about the copy in GPU memory). Does anyone have a better idea for that?\nQuick code to illustrate:\nimport os\nimport jax.numpy as jnp\nimport tensorflow as tf\n\ndef generator():\n    for _ in range(2):\n        yield tf.random.uniform((1, ))\n\nds = tf.data.Dataset.from_generator(generator, output_types=tf.float32,\n                                    output_shapes=tf.TensorShape([1]))\n\nds1 = ds.take(1).as_numpy_iterator()\nds2 = ds.skip(1)\n\nfor i, batch in enumerate(ds1):\n    print(type(batch))\n\nfor i, batch in enumerate(ds2):\n    print(type(jnp.array(batch)))\n\n# returns:\n\n<class 'numpy.ndarray'> # not good\n<class 'jaxlib.xla_extension.DeviceArray'> # good but not elegant",
        "answers": [
            "Both tensorflow and JAX have the ability to convert arrays to dlpack tensors without copying memory, so one way you can create a JAX array from a tensorflow array without copying the underlying data buffer is to do it via dlpack:\nimport numpy as np\nimport tensorflow as tf\nimport jax.dlpack\n\ntf_arr = tf.random.uniform((10,))\ndl_arr = tf.experimental.dlpack.to_dlpack(tf_arr)\njax_arr = jax.dlpack.from_dlpack(dl_arr)\n\nnp.testing.assert_array_equal(tf_arr, jax_arr)\nBy doing the round-trip to JAX, you can compare unsafe_buffer_pointer() to ensure that the arrays point at the same buffer, rather than copying the buffer along the way:\ndef tf_to_jax(arr):\n  return jax.dlpack.from_dlpack(tf.experimental.dlpack.to_dlpack(arr))\n\ndef jax_to_tf(arr):\n  return tf.experimental.dlpack.from_dlpack(jax.dlpack.to_dlpack(arr))\n\njax_arr = jnp.arange(20.)\ntf_arr = jax_to_tf(jax_arr)\njax_arr2 = tf_to_jax(tf_arr)\n\nprint(jnp.all(jax_arr == jax_arr2))\n# True\nprint(jax_arr.unsafe_buffer_pointer() == jax_arr2.unsafe_buffer_pointer())\n# True",
            "From Flax example:\nhttps://github.com/google/flax/blob/6ae22681ef6f6c004140c3759e7175533bda55bd/examples/imagenet/train.py#L183\ndef prepare_tf_data(xs):\n  local_device_count = jax.local_device_count()\n  def _prepare(x):\n    x = x._numpy() \n    return x.reshape((local_device_count, -1) + x.shape[1:])\n  return jax.tree_util.tree_map(_prepare, xs)\n\nit = map(prepare_tf_data, ds)\nit = jax_utils.prefetch_to_device(it, 2)"
        ],
        "link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator"
    },
    {
        "title": "Vectorization guidelnes for jax",
        "question": "suppose I have a function (for simplicity, covariance between two series, though the question is more general):\ndef cov(x, y):\n   return jnp.dot((x-jnp.mean(x)), (y-jnp.mean(y)))\nNow I have a \"dataframe\" D (a 2-dimenisonal array, whose columns are my series) and I want to vectorize cov in such a way that the application of the vectorized function produces the covariance matrix. Now, there is an obvious way of doing it:\ncov1 = jax.vmap(cov, in_axes=(None, 1))\ncov2 = jax.vmap(cov1, in_axes=(1, None))\nbut that seems a little clunky. Is there a \"canonical\" way of doing this?",
        "answers": [
            "If you want to express logic equivalent to nested for loops with vmap, then yes it requires nested vmaps. I think what you've written is probably as canonical as you can get for an operation like this, although it might be slightly more clear if written using decorators:\nfrom functools import partial\n\n@partial(jax.vmap, in_axes=(1, None))\n@partial(jax.vmap, in_axes=(None, 1))\ndef cov(x, y):\n   return jnp.dot((x-jnp.mean(x)), (y-jnp.mean(y)))\nFor this particular function, though, note that you can express the same thing using a single dot product if you wish:\nresult = jnp.dot((x - x.mean(0)).T, (y - y.mean(0)))"
        ],
        "link": "https://stackoverflow.com/questions/69772134/vectorization-guidelnes-for-jax"
    },
    {
        "title": "Jax/Flax (very) slow RNN-forward-pass compared to pyTorch?",
        "question": "I recently implemented a two-layer GRU network in Jax and was disappointed by its performance (it was unusable).\nSo, i tried a little speed comparison with Pytorch.\nMinimal working example\nThis is my minimal working example and the output was created on Google Colab with GPU-runtime. notebook in colab\nimport flax.linen as jnn \nimport jax\nimport torch\nimport torch.nn as tnn\nimport numpy as np \nimport jax.numpy as jnp\n\ndef keyGen(seed):\n    key1 = jax.random.PRNGKey(seed)\n    while True:\n        key1, key2 = jax.random.split(key1)\n        yield key2\nkey = keyGen(1)\n\nhidden_size=200\nseq_length = 1000\nin_features = 6\nout_features = 4\nbatch_size = 8\n\nclass RNN_jax(jnn.Module):\n\n    @jnn.compact\n    def __call__(self, x, carry_gru1, carry_gru2):\n        carry_gru1, x = jnn.GRUCell()(carry_gru1, x)\n        carry_gru2, x = jnn.GRUCell()(carry_gru2, x)\n        x = jnn.Dense(4)(x)\n        x = x/jnp.linalg.norm(x)\n        return x, carry_gru1, carry_gru2\n\nclass RNN_torch(tnn.Module):\n    def __init__(self, batch_size, hidden_size, in_features, out_features):\n        super().__init__()\n\n        self.gru = tnn.GRU(\n            input_size=in_features, \n            hidden_size=hidden_size,\n            num_layers=2\n            )\n        \n        self.dense = tnn.Linear(hidden_size, out_features)\n\n        self.init_carry = torch.zeros((2, batch_size, hidden_size))\n\n    def forward(self, X):\n        X, final_carry = self.gru(X, self.init_carry)\n        X = self.dense(X)\n        return X/X.norm(dim=-1).unsqueeze(-1).repeat((1, 1, 4))\n\nrnn_jax = RNN_jax()\nrnn_torch = RNN_torch(batch_size, hidden_size, in_features, out_features)\n\nXj = jax.random.normal(next(key), (seq_length, batch_size, in_features))\nYj = jax.random.normal(next(key), (seq_length, batch_size, out_features))\nXt = torch.from_numpy(np.array(Xj))\nYt = torch.from_numpy(np.array(Yj))\n\ninitial_carry_gru1 = jnp.zeros((batch_size, hidden_size))\ninitial_carry_gru2 = jnp.zeros((batch_size, hidden_size))\n\nparams = rnn_jax.init(next(key), Xj[0], initial_carry_gru1, initial_carry_gru2)\n\ndef forward(params, X):\n    \n    carry_gru1, carry_gru2 = initial_carry_gru1, initial_carry_gru2\n\n    Yhat = []\n    for x in X: # x.shape = (batch_size, in_features)\n        yhat, carry_gru1, carry_gru2 = rnn_jax.apply(params, x, carry_gru1, carry_gru2)\n        Yhat.append(yhat) # y.shape = (batch_size, out_features)\n\n    #return jnp.concatenate(Y, axis=0)\n\njitted_forward = jax.jit(forward)\nResults\n# uncompiled jax version\n%time forward(params, Xj)\nCPU times: user 7min 17s, sys: 8.18 s, total: 7min 25s Wall time: 7min 17s\n# time for compiling\n%time jitted_forward(params, Xj)\nCPU times: user 8min 9s, sys: 4.46 s, total: 8min 13s Wall time: 8min 12s\n# compiled jax version\n%timeit jitted_forward(params, Xj)\nThe slowest run took 204.20 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 115 µs per loop\n# torch version\n%timeit lambda: rnn_torch(Xt)\n10000000 loops, best of 5: 65.7 ns per loop\nQuestions\nWhy is my Jax-implementation so slow? What am i doing wrong?\nAlso, why is compiling taking so long? The sequence is not that long..\nThank you :)",
        "answers": [
            "The reason the JAX code compiles slowly is that during JIT compilation JAX unrolls loops. So in terms of XLA compilation, your function is actually very large: you call rnn_jax.apply() 1000 times, and compilation times tend to be roughly quadratic in the number of statements.\nBy contrast, your pytorch function uses no Python loops, and so under the hood it is relying on vectorized operations that run much faster.\nAny time you use a for loop over data in Python, a good bet is that your code will be slow: this is true whether you're using JAX, torch, numpy, pandas, etc. I'd suggest finding an approach to the problem in JAX that relies on vectorized operations rather than relying on slow Python looping."
        ],
        "link": "https://stackoverflow.com/questions/69767707/jax-flax-very-slow-rnn-forward-pass-compared-to-pytorch"
    },
    {
        "title": "jax woes (on an NVDIA DGX box, no less)",
        "question": "I am trying to run jax on an nvidia dgx box, but am failing miserably, thus:\n>>> import jax\n>>> import jax.numpy as jnp\n>>> x = jnp.arange(10)\n2021-10-25 13:00:05.863667: W \nexternal/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't \nget ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n2021-10-25 13:00:05.864713: F \nexternal/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:435] \nptxas returned an error during compilation of ptx to sass: 'INTERNAL: Failed to \nlaunch ptxas'  If the error message indicates that a file could not be written, \nplease verify that sufficient filesystem space is provided.\nAborted (core dumped)\nAny suggestions would be much appreciated.",
        "answers": [
            "This means that your CUDA installation is not configured correctly, and can generally be fixed by ensuring that the CUDA toolkit binaries (including ptxas) are present in your $PATH. See https://github.com/google/jax/discussions/6843 and https://github.com/google/jax/issues/7239 for responses to users reporting similar issues.",
            "For this problem you need to install nvidia-driver, cuda and cudnn correctly and the risky command here would be: sudo apt install nvidia-cuda-toolkit avoid this command if you have installed those 3 already.\nthe way which works for me:\nInstall nvidia-driver: follow this and proper version also. you can try sudo ubuntu-drivers devices in ubuntu\nInstall cuda : for finding which cuda version works for you run nvidia-smi and on top-left you will see compatible version for the cuda then go nvidia cuda archive and follow the instructions there. at this step you should be able to see cuda foder when you type ls /usr/local. if you want to install header also you can find useful command from nvidia installation guide for cuda.\nInstall cudnn which means copy paste some files into /usr/local/cuda directory if you go through cuDNN nvidia guide you would find the best way.\nthe last step you need to refer to the cuda path (/usr/local/cuda if you follow above). for example if you use docker you need to mount it like here. avoid install nvidia-cuda-toolkit it would remove your previous installation and instead you can install it in conda-env by conda install -c nvidia cuda-nvcc which doesn't interfere your cuda installation."
        ],
        "link": "https://stackoverflow.com/questions/69712084/jax-woes-on-an-nvdia-dgx-box-no-less"
    },
    {
        "title": "how to do curve fitting using google jax?",
        "question": "Extending the examples from http://implicit-layers-tutorial.org/neural_odes/ I am tying to mimic the curve fitting function in scipy , scipy.optimize.curve_fit ,using google jax. The function to be fitted is a first order ODE.\n#Generate toy data for first order ode.\n\nimport jax.numpy as jnp\nimport jax\nimport numpy as np\n\n\n#input  data \nu = np.zeros(100)  \nu[10:50] = 1\nt = np.arange(len(u))\nu = jnp.array(u)\n\n#first order ODE\ndef f(y,t,k,tau,u):\n \n  return (k*u[t]-y)/tau\n  \n#Euler integration\ndef odeint_euler(f, y0, t, *args):\n  def step(state, t):\n    y_prev, t_prev = state\n    dt = t - t_prev\n    y = y_prev + dt * f(y_prev, t_prev, *args)\n    return (y, t), y\n  _, ys = jax.lax.scan(step, (y0, t[0]), t[1:])\n  return ys\n\npred = odeint_euler(f, jnp.array([0.0]),t,2.,5.,u) \npred_noise = pred.reshape(-1) +  0.05* np.random.randn(len(pred)) # this is the  data to be fitted\n\n# define loss function \ndef loss_function(params,u,targets):\n  k,tau = params\n  \n  pred = odeint_euler(f, jnp.array([0.0]),t,k,tau,u)\n  return jnp.sum((pred-targets)**2)      \n\n\ndef update(params, u, targets):\n  grads = jax.grad(loss_function)(params,u, targets)\n  return [w - 0.0001 * dw for w,dw  in zip(params, grads)] \n\n\nupdated_params = jnp.array([1.0,2.0]) #initial parameters\nfor i in range(100):\n  updated_params = update(updated_params, u, pred_noise)\nprint(updated_params)\nThe code works fine. However , this runs pretty slow when compared to scipy curve fit. The accuracy of the solution is not good even after 500, 1000 iterations. What is wrong with the above code ? Any idea how to make the code run faster and to get more accurate solution? Is there any better way of doing the curve fitting with jax?",
        "answers": [
            "I see two overall issues with your approach:\nThe reason your code is running slowly is because you are doing your looping in Python, which incurs JAX's dispatch overhead every loop. I'd recommend using JAX's built-in tools for minimization of loss functions; for example:\nfrom jax.scipy.optimize import minimize\nresult = minimize(\n    loss_function, x0=jnp.array([1.0,2.0]),\n    method='BFGS', args=(u, pred_noise))\nThe reason your accuracy does not approach that of scipy is likely because JAX defaults to 32-bit computations (See Double (64 bit) Precision). To run your code in 64-bit, you can run this block before any other imports:\nfrom jax import config\nconfig.update('jax_enable_x64', True)"
        ],
        "link": "https://stackoverflow.com/questions/69641423/how-to-do-curve-fitting-using-google-jax"
    },
    {
        "title": "JIT a least squares loss function in Jax",
        "question": "I have a simple loss function that looks like this\n        def loss(r, x, y):\n            resid = f(r, x) - y\n            return jnp.mean(jnp.square(resid))\nI would like to optimize over the parameter r and use some static parameters x and y to compute the residual. All parameters in question are DeviceArrays.\nIn order to JIT this, I tried doing the following\n        @partial(jax.jit, static_argnums=(1, 2))\n        def loss(r, x, y):\n            resid = f(r, x) - y\n            return jnp.mean(jnp.square(resid))\nbut I get this error\njax._src.traceback_util.UnfilteredStackTrace: ValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 1) of type <class 'numpy.ndarray'> for function loss is non-hashable.\nI understand that from #6233 that this is by design but I was wondering what the workaround here is, as this seems like a very common use case where you have some fixed (input, output) training data pairs and some free variable.\nThanks for any tips!\nEDIT: this is the error I get when I just try to use jax.jit\njax._src.traceback_util.UnfilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function loss at /path/to/my/script:9 for jit, this concrete value was not available in Python because it depends on the value of the argument 'r'.`",
        "answers": [
            "It sounds like you're thinking of static arguments as \"values that don't vary between computations\". In JAX's JIT, static arguments can better be thought of as \"hashable compile-time constants\". In your case, you don't have hashable compile-time constants; you have arrays, so you can just JIT-compile with no static args:\n@jit\ndef loss(r, x, y):\n    resid = f(r, x) - y\n    return jnp.mean(jnp.square(resid))\nIf you really want the JAX machinery to know that your arrays are constant, you can do so by passing them via a closure or a partial; for example:\nfrom functools import partial\n\ndef loss(r, x, y):\n    resid = f(r, x) - y\n    return jnp.mean(jnp.square(resid))\nloss = jit(partial(loss, x=x, y=y))\nHowever, for the type of computation you are doing, where the constants are arrays operated on by JAX array functions, these two approaches lead to basically identical lowered XLA code, so you may as well use the simpler one."
        ],
        "link": "https://stackoverflow.com/questions/69593968/jit-a-least-squares-loss-function-in-jax"
    },
    {
        "title": "How to use grad convolution in google-jax?",
        "question": "Thanks for reading my question!\nI was just learning about custom grad functions in Jax, and I found the approach JAX took with defining custom functions is quite elegant.\nOne thing troubles me though.\nI created a wrapper to make lax convolution look like PyTorch conv2d.\nfrom jax import numpy as jnp\nfrom jax.random import PRNGKey, normal \nfrom jax import lax\nfrom torch.nn.modules.utils import _ntuple\nimport jax\nfrom jax.nn.initializers import normal\nfrom jax import grad\n\ntorch_dims = {0: ('NC', 'OI', 'NC'), 1: ('NCH', 'OIH', 'NCH'), 2: ('NCHW', 'OIHW', 'NCHW'), 3: ('NCHWD', 'OIHWD', 'NCHWD')}\n\ndef conv(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n    n = len(input.shape) - 2\n    if type(stride) == int:\n        stride = _ntuple(n)(stride)\n    if type(padding) == int: \n        padding = [(i, i) for i in _ntuple(n)(padding)]\n    if type(dilation) == int:\n        dilation = _ntuple(n)(dilation)\n    return lax.conv_general_dilated(lhs=input, rhs=weight, window_strides=stride, padding=padding, lhs_dilation=dilation, rhs_dilation=None, dimension_numbers=torch_dims[n], feature_group_count=1, batch_group_count=1, precision=None, preferred_element_type=None)\nThe problem is that I could not find a way to use its grad function:\ninit = normal()\nrng = PRNGKey(42)\nx = init(rng, [128, 3, 224, 224])\nk = init(rng, [64, 3, 3, 3])\ny = conv(x, k)\ngrad(conv)(y, k)\nThis is what I got.\nValueError: conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 64 // 1 != 3.\nPlease help!",
        "answers": [
            "When I run your code with the most recent releases of jax and jaxlib (jax==0.2.22; jaxlib==0.1.72), I see the following error:\nTypeError: Gradient only defined for scalar-output functions. Output had shape: (128, 64, 222, 222).\nIf I create a scalar-output function that uses conv, the gradient seems to work:\nresult = grad(lambda x, k: conv(x, k).sum())(x, k)\nprint(result.shape)\n# (128, 3, 224, 224)\nIf you are using an older version of JAX, you might try updating to a more recent version – perhaps the error you're seeing is due to a bug that has already been fixed."
        ],
        "link": "https://stackoverflow.com/questions/69571976/how-to-use-grad-convolution-in-google-jax"
    },
    {
        "title": "Is there a CUDA threadId alike in Jax (google)?",
        "question": "I'm trying to understand the behaviour of jax.vmap/pmap, (jax: https://jax.readthedocs.io/). CUDA has threadId to let you know which thread is executing the code, is there a similar concept in jax? (jax.process_id is not)",
        "answers": [
            "No, there is no real analog to CUDA threadid in JAX. Details about GPU thread assignment are handled at a lower level by the XLA compiler, and I don't know of any straightforward API to plumb this information back to JAX's Python runtime.\nOne case where JAX does offer higher-level handling of device assignment is when using pmap; in this case you can explicitly pass a set of device IDs to the pmapped function if you want logic that depends on the device on which the mapped code is being executed. For example, I ran the following on an 8-device system:\nimport jax\nimport jax.numpy as jnp\n\nnum_devices = jax.device_count()\n\ndef f(device, data):\n  return data + device\n\ndevice_index = jnp.arange(num_devices)\ndata = jnp.zeros((num_devices, 10))\n\njax.pmap(f)(device_index, data)\n\n# ShardedDeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n#                     [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n#                     [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n#                     [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n#                     [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n#                     [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.],\n#                     [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.],\n#                     [7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/69450145/is-there-a-cuda-threadid-alike-in-jax-google"
    },
    {
        "title": "How to use jax vmap for nested loops?",
        "question": "I want to use vmap to vectorise this code for performance.\ndef matrix(dataA, dataB):\n    return jnp.array([[func(a, b) for b in dataB] for a in dataA])\nmatrix(data, data)\nI tried this:\ndef f(x, y):\n    return func(x, y)\nmapped = jax.vmap(f)\nmapped(data, data)\nBut this only gives the diagonal entries.\nBasically I have a vector data = [1,2,3,4,5] (example), I want to get a matrix such that each entry (i, j) of the matrix is f(data[i], data[j]). Thus, the resulting matrix shape will be (len(data), len(data)).",
        "answers": [
            "jax.vmap maps across one set of axes at a time. If you want to map across two independent sets of axes, you can do so by nesting two vmap transformations:\nmapped = jax.vmap(jax.vmap(f, in_axes=(None, 0)), in_axes=(0, None))\nresult = mapped(data, data)"
        ],
        "link": "https://stackoverflow.com/questions/69429846/how-to-use-jax-vmap-for-nested-loops"
    },
    {
        "title": "How can I utilize JAX library to speed up my code?",
        "question": "I have written a code that gets some vertex and rearranged them based on some rules. When the input contains big data, the code runs very slowly e.g. for 60000 loops it will take about 15 hours on google colab TPU runtime. I have found JAX is one of the best libraries to do so and trying to use it, but due to lack of experience in dealing with such big data and its related methods such as parallelization, I have faced to some problems. The following small sample is created to show what does the code doing:\nimport numpy as np\n\n# <class 'numpy.ma.core.MaskedArray'> <class 'numpy.ma.core.MaskedArray'> (m, 4) <class 'numpy.int64'>\nnodes = np.ma.masked_array(np.array([[0, 1, 2, 3], [4, 0, 5, 1], [6, 4, 7, 5], [8, 6, 9, 7]],\n                                    dtype=np.int64), mask=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# <class 'numpy.ndarray'> <class 'numpy.ndarray'> (n, 3) <class 'numpy.float64'>\nvert = np.array([[0.06944111, -0.12027553, -0.3], [0., -0.13888221, -0.3], [0.05, -0.08660254, -0.3],\n                [0.06944111, -0.12027553, -0.5], [0.06944111, -0.12027553, -0.1], [0., -0.13888221, -0.1],\n                [0.06944111, -0.12027553,  0.1], [0., -0.13888221,  0.1], [0.06944111, -0.12027553,  0.3],\n                [0., -0.13888221,  0.3]])\n\n\ndef ali_sh():\n    mod_array = []\n    mod_idx = []\n\n    for cell in range(len(nodes)):\n        vertex_idx = []\n\n        B_face = sorted(nodes[cell], key=lambda v: [vert[v][0]], reverse=True)\n        if round(vert[B_face[1]][0], 7) == round(vert[B_face[2]][0], 7):\n            if vert[B_face[1]][1] > vert[B_face[2]][1]:\n                B_face[1], B_face[2] = B_face[2], B_face[1]\n\n        mod_array.append(B_face)\n\n        for vertex in B_face:\n            vertex_idx.append(np.where(nodes[cell] == vertex)[0][0])\n\n        mod_idx.append(vertex_idx)\n\n    return mod_idx\n\nmod_idx = ali_sh()\nThe above code is just a view of my code and have some differences e.g. in this code jnp.where run correctly but using the main code and the big data it will stuck and must use np.where instead. In my first try, I have added jax_r = jit(ali_sh) and mod_idx = jax_r().block_until_ready() to the end of the code, but I did not get any better performance. I have used FiPy library and its methods, which where in numpy types e.g. 'fipy.mesh.vertexCoords.T' is a numpy ndarray. I have tried to convert the used fipy numpy arrays to JAX ones by jnp.array(fipy numpy arrays) to check if it could help, but I get errors due to using lambda by sorted command. How can I implement JAX on my code to get a better run time.\nDoes colab need anything to do to get its maximum capability on TPU or GPU for such codes? Does using JAX could have significant effects on my code to speeding up? I would be appreciated if someone could help to find out how to speed up the code.",
        "answers": [
            "Writing efficient JAX code is very similar to writing efficient NumPy code: generally if you are using a for loop over rows of your data, your code will not be very efficient. Instead, you should strive to write your computations in terms of vectorized operations.\nIn your code, it looks like you are relying on many non-JAX elements (e.g. NumPy masked arrays, operations in FiPy, etc.) so it's unlikely that JAX will be able to improve your runtime. I'd focus instead on rewriting your code to make efficient use of NumPy, replacing the for-loop logic with NumPy vectorized operations.\nHere is an example of expressing your function in terms of vectorized operations:\ndef ali_sh_vectorized():\n  i_sort = np.argsort(vert[nodes, 0], axis=1)[:, ::-1]\n  B_face = nodes[np.arange(nodes.shape[0])[:, None], i_sort]\n  close = np.isclose(vert[B_face[:, 1],1], vert[B_face[:, 2], 2])\n  larger = np.greater(vert[B_face[:, 1],1], vert[B_face[:, 2], 2])\n  col_1 = np.where(close & larger, B_face[:, 2], B_face[:, 1])\n  col_2 = np.where(close & larger, B_face[:, 1], B_face[:, 2])\n  B_face[:, 1] = col_1\n  B_face[:, 2] = col_2\n  mod_idx = np.where(nodes[:, :, None] == B_face[:, None, :])[2].reshape(nodes.shape)\n  return mod_idx\nThe differences in the output compared to the original function are due to differences in how the Python sort and the NumPy sort handle equivalent elements, but I believe the overall logic is the same."
        ],
        "link": "https://stackoverflow.com/questions/69422078/how-can-i-utilize-jax-library-to-speed-up-my-code"
    },
    {
        "title": "How does one get a parameter from a params (pytree) in haiku? (jax framework)",
        "question": "For example you set up a module and that has params. But if you want do regularize something in a loss what is the pattern?\nimport jax.numpy as jnp\nimport jax\ndef loss(params, x, y):\n   l = jnp.sum((y - mlp.apply(params, x)) ** 2)\n   w = hk.get_params(params, 'w') # does not work like this\n   l += jnp.sum(w ** w)\n   return l\nThere is some pattern missing in the examples.",
        "answers": [
            "params is essentially a read-only dictionary, so you can get the value of a parameter by treating it as a dictionary:\nprint(params['w'])\nIf you want to update the parameters, you cannot do it in-place, but have to first convert it to a mutable dictionary:\nparams_mutable = hk.data_structures.to_mutable_dict(params)\nparams_mutable['w'] = 3.14\nparams_new = hk.data_structures.to_immutable_dict(params_mutable)"
        ],
        "link": "https://stackoverflow.com/questions/69031947/how-does-one-get-a-parameter-from-a-params-pytree-in-haiku-jax-framework"
    },
    {
        "title": "Is there a way to speed up indexing a vector with JAX?",
        "question": "I am indexing vectors and using JAX, but I have noticed a considerable slow-down compared to numpy when simply indexing arrays. For example, consider making a basic array in JAX numpy and ordinary numpy:\nimport jax.numpy as jnp\nimport numpy as onp \njax_array = jnp.ones((1000,))\nnumpy_array = onp.ones(1000)\nThen simply indexing between two integers, for JAX (on GPU) this gives a time of:\n%timeit jax_array[435:852]\n1000 loops, best of 5: 1.38 ms per loop\nAnd for numpy this gives a time of:\n%timeit numpy_array[435:852]\n1000000 loops, best of 5: 271 ns per loop\nSo numpy is 5000 times faster than JAX. When JAX is on a CPU, then\n%timeit jax_array[435:852]\n1000 loops, best of 5: 577 µs per loop\nSo faster, but still 2000 times slower than numpy. I am using Google Colab notebooks for this, so there should not be a problem with the installation/CUDA.\nAm I missing something? I realise that indexing is different for JAX and numpy, as given by the JAX 'sharp edges' documentation, but I cannot find any way to perform assignment such as\nnew_array = jax_array[435:852]\nwithout a considerable slowdown. I cannot avoid indexing the arrays as it is necessary in my program.",
        "answers": [
            "The short answer: to speed things up in JAX, use jit.\nThe long answer:\nYou should generally expect single operations using JAX in op-by-op mode to be slower than similar operations in numpy. This is because JAX execution has some amount of fixed per-python-function-call overhead involved in pushing compilations down to XLA.\nEven seemingly simple operations like indexing are implemented in terms of multiple XLA operations, which (outside JIT) will each add their own call overhead. You can see this sequence using the make_jaxpr transform to inspect how the function is expressed in terms of primitive operations:\nfrom jax import make_jaxpr\nf = lambda x: x[435:852]\nmake_jaxpr(f)(jax_array)\n# { lambda  ; a.\n#   let b = broadcast_in_dim[ broadcast_dimensions=(  )\n#                             shape=(1,) ] 435\n#       c = gather[ dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(), start_index_map=(0,))\n#                   indices_are_sorted=True\n#                   slice_sizes=(417,)\n#                   unique_indices=True ] a b\n#       d = broadcast_in_dim[ broadcast_dimensions=(0,)\n#                             shape=(417,) ] c\n#   in (d,) }\n(See Understanding Jaxprs for info on how to read this).\nWhere JAX outperforms numpy is not in single small operations (in which JAX dispatch overhead dominates), but rather in sequences of operations compiled via the jit transform. So, for example, compare the JIT-compiled versus not-JIT-compiled version of the indexing:\n%timeit f(jax_array).block_until_ready()\n# 1000 loops, best of 5: 612 µs per loop\n\nf_jit = jit(f)\nf_jit(jax_array)  # trigger compilation\n%timeit f_jit(jax_array).block_until_ready()\n# 100000 loops, best of 5: 4.34 µs per loop\n(note that block_until_ready() is required for accurate micro-benchmarks because of JAX's asynchronous dispatch)\nJIT-compiling this code gives a 150x speedup. It's still not as fast as numpy because of JAX's few-millisecond dispatch overhead, but with JIT that overhead is incurred only once. And when you move past microbenchmarks to more complicated sequences of real-world computations, those few milliseconds will no longer dominate, and the optimization provided by the XLA compiler can make JAX far faster than the equivalent numpy computation."
        ],
        "link": "https://stackoverflow.com/questions/68951669/is-there-a-way-to-speed-up-indexing-a-vector-with-jax"
    },
    {
        "title": "Error message in Python with differentiation",
        "question": "I am computing these derivatives using the Montecarlo approach for a generic call option. I am interested in this combined derivative (with respect to both S and Sigma). Doing this with the algorithmic differentiation, I get an error that can be seen at the end of the page. What could be a possible solution? Just to explain something regarding the code, I am going to attach the formula used to compute the \"X\" in the code below:\nfrom jax import jit, grad, vmap\nimport jax.numpy as jnp\nfrom jax import random\nUnderlying_asset = jnp.linspace(1.1,1.4,100)\nvolatilities = jnp.linspace(0.5,0.6,100)\ndef second_derivative_mc(S,vol):\n    N = 100\n    j,T,q,r,k = 10000,1.,0,0,1.\n    S0 = jnp.array([S]).T #(Nx1) vector underlying asset\n    C = jnp.identity(N)*vol    #matrix of volatilities with 0 outside diagonal \n    e = jnp.array([jnp.full(j,1.)])#(1xj) vector of \"1\"\n    Rand = np.random.RandomState()\n    Rand.seed(10)\n    U= Rand.normal(0,1,(N,j)) #Random number for Brownian Motion\n    sigma2 = jnp.array([vol**2]).T #Vector of variance Nx1\n\n    first = jnp.dot(sigma2,e) #First part equation\n    second = jnp.dot(C,U)     #Second part equation\n\n    X = -0.5*first+jnp.sqrt(T)*second\n\n    St = jnp.exp(X)*S0\n\n    P = jnp.maximum(St-k,0)\n    payoff = jnp.average(P, axis=-1)*jnp.exp(-q*T)\n    return payoff \n\n\ngreek = vmap(grad(grad(second_derivative_mc, argnums=1), argnums=0)(Underlying_asset,volatilities)\nThis is the error message:\n> UnfilteredStackTrace                      Traceback (most recent call\n> last) <ipython-input-78-0cc1da97ae0c> in <module>()\n>      25 \n> ---> 26 greek = vmap(grad(grad(second_derivative_mc, argnums=1), argnums=0))(Underlying_asset,volatilities)\n> \n> 18 frames UnfilteredStackTrace: TypeError: Gradient only defined for\n> scalar-output functions. Output had shape: (100,).\nThe stack trace below excludes JAX-internal frames. The preceding is the original exception that occurred, unmodified.\nThe above exception was the direct cause of the following exception:\n> TypeError                                 Traceback (most recent call\n> last) /usr/local/lib/python3.7/dist-packages/jax/_src/api.py in\n> _check_scalar(x)\n>     894     if isinstance(aval, ShapedArray):\n>     895       if aval.shape != ():\n> --> 896         raise TypeError(msg(f\"had shape: {aval.shape}\"))\n>     897     else:\n>     898       raise TypeError(msg(f\"had abstract value {aval}\"))\n\n> TypeError: Gradient only defined for scalar-output functions. Output had shape: (100,).",
        "answers": [
            "As the error message indicates, gradients can only be computed for functions that return a scalar. Your function returns a vector:\nprint(len(second_derivative_mc(1.1, 0.5)))\n# 100\nFor vector-valued functions, you can compute the jacobian (which is similar to a multi-dimensional gradient). Is this what you had in mind?\nfrom jax import jacobian\ngreek = vmap(jacobian(jacobian(second_derivative_mc, argnums=1), argnums=0))(Underlying_asset,volatilities)\nAlso, this is not what you asked about, but the function above will probably not work as you intend even if you solve the issue in the question. Numpy RandomState objects are stateful, and thus will generally not work correctly with jax transforms like grad, jit, vmap, etc., which require side-effect-free code (see Stateful Computations In JAX). You might try using jax.random instead; see JAX: Random Numbers for more information."
        ],
        "link": "https://stackoverflow.com/questions/68908160/error-message-in-python-with-differentiation"
    },
    {
        "title": "Why does Mypy think adding two Jax arrays returns a numpy array?",
        "question": "Consider the following file:\nimport jax.numpy as jnp\n\ndef test(a: jnp.ndarray, b: jnp.ndarray) -> jnp.ndarray:\n    return a + b\nRunning mypy mypytest.py returns the following error:\nmypytest.py:4: error: Incompatible return value type (got \"numpy.ndarray[Any, dtype[bool_]]\", expected \"jax._src.numpy.lax_numpy.ndarray\")\nFor some reason it believes adding two jax.numpy.ndarrays returns a NumPy array of bools. Am I doing something wrong? Or is this a bug in MyPy, or Jax's type annotations?",
        "answers": [
            "At least statically, jnp.ndarray is a subclass of np.ndarray with very minimal modifications\nclass ndarray(np.ndarray, metaclass=_ArrayMeta):\n  dtype: np.dtype\n  shape: Tuple[int, ...]\n  size: int\n\n  def __init__(shape, dtype=None, buffer=None, offset=0, strides=None,\n               order=None):\n    raise TypeError(\"jax.numpy.ndarray() should not be instantiated explicitly.\"\n                    \" Use jax.numpy.array, or jax.numpy.zeros instead.\")\nAs such, it inherits np.ndarray's method type signatures.\nI guess the runtime behaviour is achieved via the jnp.array function. Unless I've missed some stub files or type trickery, the result of jnp.array matches jnp.ndarray simply because jnp.array is untyped. You can test this out with\ndef foo(_: str) -> None:\n   pass\n\nfoo(jnp.array(0))\nwhich passes mypy.\nSo to answer your questions, I don't think you're doing anything wrong. It's a bug in the sense that it's probably not what they mean, but it's not actually incorrect because you do get an np.ndarray when you add jnp.ndarrays because a jnp.ndarray is an np.ndarray.\nAs for why bools, that's likely because your jnp.arrays are missing generic parameters and the first valid overload for __add__ on np.ndarray is\n    @overload\n    def __add__(self: NDArray[bool_], other: _ArrayLikeBool_co) -> NDArray[bool_]: ...  # type: ignore[misc]\nso it's just defaulted to bool.",
            "In general, JAX has very poor compatibility with mypy, because it's very difficult to satisfy mypy's constraints with JAX's transformation model, which often calls functions with transform-specific tracer values that act as stand-ins for arrays (See How To Think in JAX: JIT Mechanics for a brief discussion of this mechanism).\nThis use of tracer types as standins for arrays means that mypy will raise errors when strictly-typed JAX functions are transformed, and for this reason throughout the JAX codebase we tend to alias Array to Any, and use this as the return type annotation for JAX functions that return arrays.\nIt would be good to improve on this, because an Any return type is not very useful for effective type checking, but it's just the first of many challenges for making mypy play well with JAX. If you want to read some of the last few years worth of discussions surrounding this issue, I would start here: https://github.com/google/jax/issues/943\nAnd in the meantime, my suggestion would be to use Any as a type annotation for JAX arrays.",
            "As of late 2023, it appears that jax has greatly improved its typing annotations. mypy is fine with the new syntax:\nfrom jax import Array\nfrom jax.typing import ArrayLike\n\nimport jax.numpy as jnp\n\ndef test(a: ArrayLike, b: ArrayLike) -> Array:\n    return a + b"
        ],
        "link": "https://stackoverflow.com/questions/68884215/why-does-mypy-think-adding-two-jax-arrays-returns-a-numpy-array"
    },
    {
        "title": "Websockets messages only sent at the end and not in instances using async / await, yield in nested for loops",
        "question": "I have a computationally heavy process that takes several minutes to complete in the server. So I want to send the results of every iteration to the client via websockets.\nThe overall application works but my problem is that all the messages are arriving at the client in one big chunk after the entire simulation finishes. I must be missing something here as I expect the await websocket.send_json() to send the message during the process and not all of them at the end.\nServer python (FastAPI)\n# A very simplified abstraction of the actual app.\n\ndef simulate_intervals(data):\n  for t in range(data.n_intervals):\n    state = interval(data) # returns a JAX NumPy array\n    yield state\n\ndef simulate(data):\n  for key in range(data.n_trials):\n     trial = simulate_intervals(data)\n     yield trial\n\n@app.websocket(\"/ws\")\nasync def socket(websocket: WebSocket):\n\n  await websocket.accept()\n  while True:\n    # Get model inputs from client\n    data = await websocket.receive_text()\n    # Minimal computation\n    nodes = distributions(data)\n\n    nodosJson = json.dumps(nodes, cls=NumpyEncoder)\n    # I expect this message to be sent early on,\n    # but the client gets it at the end with all the other messages. \n    await websocket.send_json({\"tipo\": \"nodos\", \"datos\": json.loads(nodosJson)})\n    \n    # Heavy computation\n    trials = simulate(data)\n\n    for trialI, trial in enumerate(trials):\n      for stateI, state in enumerate(trial):\n        stateString = json.dumps(state, cls=NumpyEncoder)\n\n        await websocket.send_json(\n          {\n            \"tipo\": \"estado\",\n            \"datos\": json.loads(stateString),\n            \"trialI\": trialI,\n            \"stateI\": stateI,\n          }\n        )\n\n    await websocket.send_json({\"tipo\": \"estado\", \"msg\": \"fin\"})\nFor completeness, here is the basic client code.\nClient\nconst ws = new WebSocket('ws://localhost:8000/ws');\n\nws.onopen = () => {\n  console.log('Conexión exitosa');\n};\n\nws.onmessage = (e) => {\n  const mensaje = JSON.parse(e.data);\n  console.log(mensaje);\n};\n\nbotonEnviarDatos.onclick = () => {\n   ws.send(JSON.stringify({...}));\n}",
        "answers": [
            "I got a similar issue, and was able to resolve it by adding a small await asyncio.sleep(0.1) after sending json messages. I have not dived into asyncios internals yet, but my guess is that websocker.send shedules a message to be sent, but since the async function continues to run it never has a chance to do it in the background. Sleeping the async function makes asyncio pick up other tasks while it is waiting.",
            "I was not able to make it work as posted in my question, still interested in hearing from anyone who understands why it is not possible to send multiple async messages without them getting blocked.\nFor anyone interested, here is my current solution:\nPing pong messages from client and server\nI changed the logic so the server and client are constantly sending each other messages and not trying to stream the data in a single request from the client.\nThis actually works much better than my original attempt because I can detect when a sockets gets disconnected and stop processing in the server. Basically, if the client disconnects, no new requests for data are sent from that client and the server never continues the heavy computation.\nServer\n# A very simplified abstraction of the actual app.\n\ndef simulate_intervals(data):\n  for t in range(data.n_intervals):\n    state = interval(data) # returns a JAX NumPy array\n    yield state\n\ndef simulate(data):\n  for key in range(data.n_trials):\n     trial = simulate_intervals(data)\n     yield trial\n\n@app.websocket(\"/ws\")\nasync def socket(websocket: WebSocket):\n\n  await websocket.accept()\n  while True:\n    # Get messages from client\n    data = await websocket.receive_text()\n    \n    # \"tipo\" is basically the type of data being sent from client or server to the other one.\n    # In this case, \"tipo\": \"inicio\" is the client sending inputs and requesting for a certain data in response.\n    if data[\"tipo\"] == \"inicio\":\n      # Minimal computation\n      nodes = distributions(data)\n\n      nodosJson = json.dumps(nodes, cls=NumpyEncoder)\n      # In this first interaction, the client gets the first message without delay. \n      await websocket.send_json({\"tipo\": \"nodos\", \"datos\": json.loads(nodosJson)})\n\n      # Since this is a generator (def returns yield) it does not actually\n      # trigger that actual computationally heavy process. \n      trials = simulate(data)\n      \n      # define some initial variables to count the iterations\n      trialI = 0\n      stateI = 0\n      trialsLen = args.number_trials\n      statesLen = 600\n      \n      # load the first trial (also a generator)\n      # without the for loop used before, the counters and next()\n      # allow us to do the same as being done before in the for loop\n      trial = next(trials)\n\n      # With the use of generators and next() it is possible to keep\n      # this first message light on the server and send the first response\n      # as quickly as possible.\n    \n    # This type of message asks for the next instance of the simluation\n    # without processing the entire model.\n    elif data[\"tipo\"] == \"sim\":\n      # check if we are within the limits (before this was a nested for loop)\n      if trialI < trialsLen and stateI < statesLen:\n        # Trigger the next instance of the simulation\n        state = next(trial)\n        # update counter\n        stateI = stateI + 1\n        \n        # Send the message with 1 instance of the simulation.\n        # \n        stateString = json.dumps(state, cls=NumpyEncoder)\n        await websocket.send_json(\n          {\n             \"tipo\": \"estado\",\n             \"datos\": json.loads(stateString),\n             \"trialI\": trialI,\n             \"stateI\": stateI,\n          }\n        )\n        \n        # Check if the second loop is done\n        if stateI == statesLen:\n          # update counter of first loop\n          trialI = trialI + 1\n          # update counter of second loop\n          stateI = 0\n          \n          # Check if there are more pending trials,\n          # otherwise stop and notify the client we are done.\n          try:\n            trial = next(trials)\n          except StopIteration:\n            await websocket.send_json({\"tipo\": \"fin\"})\nClient\nJust the part that actually changed:\nws.onmessage = (e) => {\n  const mensaje = JSON.parse(e.data);\n  \n  // Simply check the type of incoming message so it can be processed\n  if (mensaje.tipo === 'fin') {\n    viz.calcularResultados();\n  } else if (mensaje.tipo === 'nodos') {\n    viz.pintarNodos(mensaje.datos);\n  } else if (mensaje.tipo === 'estado') {\n    viz.sumarEstado(mensaje.datos);\n  }\n\n  // After receiving a message, ping the server for the next one \n  ws.send(\n    JSON.stringify({\n      tipo: 'sim',\n    })\n  );\n};\nThis seems like reasonable solution to keep the server and client working together. I am able to show in the client the progress of a long simulation and the user experience is much better than having to wait for a long time for the server to respond. Hope it helps other with a similar problem."
        ],
        "link": "https://stackoverflow.com/questions/68884040/websockets-messages-only-sent-at-the-end-and-not-in-instances-using-async-awai"
    },
    {
        "title": "JAX - Problem in differentiating of function",
        "question": "I am trying to perform a Montecarlo Simulation on a call and after that compute in Python its first derivative with respect to the underlying asset, but it still does not works\nfrom jax import random\nfrom jax import jit, grad, vmap\nimport jax.numpy as jnp\n\nxi = jnp.linspace(1,1.2,5)\ndef Simulation(xi):\n    K,T,number_sim,sigma,r,q = 1.,1.,100,0.4,0,0\n    S = jnp.broadcast_to(xi,(number_sim,len(xi))).T\n\n    mean = -.5 * sigma * sigma * T\n    volatility = sigma*jnp.sqrt(T)\n    r_numb = random.PRNGKey(10)\n    BM = mean + volatility * random.normal(r_numb, shape=(number_sim,))\n\n    product = S*jnp.exp(BM)\n\n    payoff = jnp.maximum(product-K,0)\n\n    result = jnp.average(payoff, axis=1)*jnp.exp(-q*T)\n\n    return result\n\nfirst_derivative = vmap(grad(Simulation))(xi)\nI do not know if the way that is implemented the algorithm is the best one to compute the derivative using \"AD method\"; this algorithm works in this way:\nS = Simulate a matrix containing all the underlying; for each row I have each underlying generated with the \"xi = jnp.linspace\", and inside each row of the matrix I have the same value for a number of times equal to \"number_sim\"\nproduct = After generating the BM ( vector containing normal number ) I need to multiply each element of BM (with exp) with each element of each row of S\nSo this is a short explanation of the algorithm, I really appreciate any kind of advice or tips to manage this problem, and compute the derivative with AD method! Thanks in advance",
        "answers": [
            "It appears that your function maps a vector Rᴺ→Rᴺ. There are two notions of a derivative that make sense in this case: an elementwise derivative (which in JAX you can compute by composing jax.vmap and jax.grad). This will return a derivative vector of length N, where element i contains the derivative of the ith output with respect to the ith input.\nAlternatively, you can compute the jacobian matrix (using jax.jacobian) which will return a shape [N, N] matrix, where element i,j contains the derivative of the ith output with respect to the jth input.\nThe issue you're having is that your function is written assuming a vector input (you ask for the length of xi), which implies you're interested in the jacobian, but you are asking for the elementwise derivative, which requires a scalar-valued function.\nSo you have two possible ways of solving this, depending on what derivative you're interested in. If you're interested in the jacobian, you can use the function as written and use the jax.jacobian transform:\nfrom jax import jacobian\nprint(jacobian(Simulation)(xi))\n# [[0.6528027 0.        0.        0.        0.       ]\n#  [0.        0.6819291 0.        0.        0.       ]\n#  [0.        0.        0.7003516 0.        0.       ]\n#  [0.        0.        0.        0.7181915 0.       ]\n#  [0.        0.        0.        0.        0.7608434]]\nAlternatively, if you're interested in the elementwise gradient, you can rewrite your function to be compatible with scalar inputs, and use vmap of grad as you did in your example. Only two lines need to be changed:\ndef Simulation_scalar(xi):\n    K,T,number_sim,sigma,r,q = 1.,1.,100,0.4,0,0\n\n    # S = jnp.broadcast_to(xi,(number_sim,len(xi))).T\n    S = jnp.broadcast_to(xi,(number_sim,) + xi.shape).T\n\n    mean = -.5 * sigma * sigma * T\n    volatility = sigma*jnp.sqrt(T)\n    r_numb = random.PRNGKey(10)\n    BM = mean + volatility * random.normal(r_numb, shape=(number_sim,))\n\n    product = S*jnp.exp(BM)\n\n    payoff = jnp.maximum(product-K,0)\n\n    # result = jnp.average(payoff, axis=1)*jnp.exp(-q*T)\n    result = jnp.average(payoff, axis=-1)*jnp.exp(-q*T)\n\n    return result\n\nprint(vmap(grad(Simulation_scalar))(xi))\n# [0.6528027 0.6819291 0.7003516 0.7181915 0.7608434]"
        ],
        "link": "https://stackoverflow.com/questions/68609584/jax-problem-in-differentiating-of-function"
    },
    {
        "title": "Is there a way to disable forward evaluation while using VJP in JAX?",
        "question": "I use VJP frequently in my project. It runs the function that is subject to Jacobian computation and returns a primals_out together with the callable vjp function. For example, custom VJP definition in JAX documentation is given like this:\nfrom jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n# Returns primal output and residuals to be used in backward pass by f_bwd.\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res # Gets residuals computed in f_fwd\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\nIn this example, we see that evaluation of the forward function is required when using VJP. This is also the case when using regular VJP instead of a custom defined one. However, when the evaluation of function costs highly and since I have already run that function somewhere in my code, I don't want VJP to evaluate that function one more time.\nSo, is there any way to indicate that a function will not be evaluated when computing its VJP?",
        "answers": [
            "I don't think there is any way to explicitly disable forward evaluation in this context, but if you wrap your computation in a jit compilation, the XLA compiler will automatically do dead code elimination and trim unused branches from the computation graph."
        ],
        "link": "https://stackoverflow.com/questions/68491225/is-there-a-way-to-disable-forward-evaluation-while-using-vjp-in-jax"
    },
    {
        "title": "Apply function only on slice of array under jit",
        "question": "I am using JAX, and I want to perform an operation like\n@jax.jit\ndef fun(x, index):\n    x[:index] = other_fun(x[:index])\n    return x\nThis cannot be performed under jit. Is there a way of doing this with jax.ops or jax.lax? I thought of using jax.ops.index_update(x, idx, y) but I cannot find a way of computing y without incurring in the same problem again.",
        "answers": [
            "The previous answer by @rvinas using dynamic_slice works well if your index is static, but you can also accomplish this with a dynamic index using jnp.where. For example:\nimport jax\nimport jax.numpy as jnp\n\ndef other_fun(x):\n    return x + 1\n\n@jax.jit\ndef fun(x, index):\n  mask = jnp.arange(x.shape[0]) < index\n  return jnp.where(mask, other_fun(x), x)\n\nx = jnp.arange(5)\nprint(fun(x, 3))\n# [1 2 3 3 4]",
            "It seems there are two issues in your implementation. First, the slices are producing dynamically shaped arrays (not allowed in jitted code). Second, unlike numpy arrays, JAX arrays are immutable (i.e. the contents of the array cannot be changed).\nYou can overcome the two problems by combining static_argnums and jax.lax.dynamic_update_slice. Here is an example:\ndef other_fun(x):\n    return x + 1\n\n@jax.partial(jax.jit, static_argnums=(1,))\ndef fun(x, index):\n    update = other_fun(x[:index])\n    return jax.lax.dynamic_update_slice(x, update, (0,))\n\nx = jnp.arange(5)\nprint(fun(x, 3))  # prints [1 2 3 3 4]\nEssentially, the example above uses static_argnums to indicate that the function should be recompiled for different index values and jax.lax.dynamic_update_slice creates a copy of x with updated values at :len(update)."
        ],
        "link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit"
    },
    {
        "title": "sum matrix elementwise using vmap (jax)?",
        "question": "I'm trying to understand the in_axes and out_axes options in vmap. For example, I want to sum two matrix and get the output with the same shape.\nX = np.arange(9).reshape(3,3)\nY = np.arange(0,-9,-1).reshape(3,3)\ndef sum2(x,y):\n    return x + y\nvmap(sum2,in_axes=((0,1),(0,1)))(X,Y)\nI think I mapped both axes 0 and 1 for X and Y respectively. The output will have the same shape as X,Y. But i get the error,\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-403-103694166574> in <module>\n      3 def sum2(x,y):\n      4     return x + y\n----> 5 vmap(sum2,in_axes=((0,1),(0,1)))(X,Y)\n\n    [... skipping hidden 2 frame]\n\n~/anaconda3/lib/python3.8/site-packages/jax/api_util.py in flatten_axes(name, treedef, axis_tree, kws)\n    276       assert treedef_is_leaf(leaf)\n    277       axis_tree, _ = axis_tree\n--> 278     raise ValueError(f\"{name} specification must be a tree prefix of the \"\n    279                      f\"corresponding value, got specification {axis_tree} \"\n    280                      f\"for value tree {treedef}.\") from None\n\nValueError: vmap in_axes specification must be a tree prefix of the corresponding value, got specification ((0, 1), (0, 1)) for value tree PyTreeDef((*, *)).",
        "answers": [
            "First of all, the easiest way to do an element-wise sum is to use the built-in broadcasting of binary operations, and call sum2(X, Y) directly.\nThat said, if you're trying to understand vmap: the issue is that vmap can only map one axis at a time. If you want to map multiple axes, you can nest multiple vmaps. I believe what you intended to do can be expressed this way:\nfrom jax import vmap\nimport jax.numpy as np\n\nX = np.arange(9).reshape(3,3)\nY = np.arange(0,-9,-1).reshape(3,3)\n\ndef sum2(x,y):\n    assert x.ndim == y.ndim == 0\n    return x + y\n\nvmap(vmap(sum\n  vmap(sum2, in_axes=(0, 0), out_axes=0),\n  in_axes=(1, 1), out_axes=1\n)(X,Y)\nNote: I added the assertion about number of dimensions to demonstrate that the mapped function is being called on scalar values.\nAlso, notice that when the mapped axes match, e.g. in_axes=(0, 0) can be equivalently written in_axes=0, but I left it as a tuple because it was closer to the syntax you were trying.\nIn fact, a far more concise way to do the same computation with nested vmap would be to use the default arguments: vmap(vmap(sum2))(X, Y) will do the same elementwise sum."
        ],
        "link": "https://stackoverflow.com/questions/68332924/sum-matrix-elementwise-using-vmap-jax"
    },
    {
        "title": "Not able to import python package jax in Google TPU",
        "question": "I am working on linux console and typing python takes me into the python console. When I use the following command in TPU machine\nimport jax\nthen it generates following mss and get out of the python prompt.\nparamjeetsingh80@t1v-n-1c883486-w-0:~$ python3\nPython 3.8.5 (default, Jan 27 2021, 15:41:15)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import jax\n2021-07-08 17:41:39.660523: F external/org_tensorflow/tensorflow/core/tpu/tpu_executor_init_fns.inc:110] TpuTransferManager_ReadDynamicShapes not available in this library.\nAborted (core dumped)\nparamjeetsingh80@t1v-n-1c883486-w-0:~$\nThis issue is causing problem in my code so I would like to figure out, what is this issue and how to get rid of this?",
        "answers": [
            "It may be that your system does not have the correct version of libtpu. Try installing the version listed here.\nYou should be able to do this automatically with\n$ pip install -U pip  # older pip may not support extra requirements\n$ pip install -U jax  # newer jax required for [tpu] extras declaration\n$ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/jax_releases.html",
            "Above command give some error but I researched and below command worked for me. But your answer give me the direction that it is a package issue.\npip install --upgrade pip\npip install \"jax[tpu]>=0.2.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/68306484/not-able-to-import-python-package-jax-in-google-tpu"
    },
    {
        "title": "Jax - autograd of a sigmoid always returns nan",
        "question": "I am trying to differentiate a function that approximates the fraction of a gaussian that is contained within 2 limits (a truncated gaussian) given a shifted mean. jnp.grad does not let me differentiate adding up boolean filters (the commented line) so I've had to improvise with a sigmoid instead.\nHowever, now the the gradient is always nan when the truncation boundary is high and I don't understand why.\nIn the example below I'm calculating the gradient of a gaussian with 0 mean and std=1, which I then shift around with x.\nIf I decrease the boundary, then the function behaves as expected. But this is not a solution. When the boundary is high then belows becomes 1 all the time. But is this is the case and x has no impact on below, then its contribution to the gradient should be 0 not nan. But if I return belows[0][0] instead of the jnp.mean(filt, axis=0), I still get nan.\nAny ideas? Thanks in advance (there is an issue open onf github as well)\nimport os\n\nfrom tqdm import tqdm\n\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=4' # Use 8 CPU devices\nimport numpy as np\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nfrom jax import vmap\n\nfrom functools import reduce\n\ndef sigmoid(x, scale=100):\n    return 1 / (1 + jnp.exp(-x*scale))\n\ndef above_lower(x, l, scale=100):\n    return sigmoid(x - l, scale)\n\ndef below_upper(x, u, scale=100):\n    return 1 - sigmoid(x - u, scale)\n\ndef combine_soft_filters(a):\n    return jnp.prod(jnp.stack(a), axis=0)\n\n\ndef fraction_not_truncated(mu, v, limits, stdnorm_samples):\n    L = jnp.linalg.cholesky(v)\n    y = vmap(lambda x: jnp.dot(L, x))(stdnorm_samples) + mu\n    # filt = reduce(jnp.logical_and, [(y[..., i] > l) & (y[..., i] < u) for i, (l, u) in enumerate(limits)])\n    aboves = [above_lower(y[..., i], l) for i, (l, u) in enumerate(limits)]\n    belows = [below_upper(y[..., i], u) for i, (l, u) in enumerate(limits)]\n    filt = combine_soft_filters(aboves+belows)\n    return jnp.mean(filt, axis=0)\n\nlimits = np.array([\n        [0.,1000],\n])\n\nstdnorm_samples = np.random.multivariate_normal([0], np.eye(1), size=1000)\n\ndef func(x):\n    return fraction_not_truncated(jnp.zeros(1)+x, jnp.eye(1), limits, stdnorm_samples)\n\n_x = np.linspace(-2, 2, 500)\ngradfunc = jax.grad(func)\nvals = [func(x) for x in tqdm(_x)]\ngrads = [gradfunc(x) for x in tqdm(_x)]\nprint(vals)\nprint(grads)\nimport matplotlib.pyplot as plt\nplt.plot(_x, np.asarray(vals))\nplt.ylabel('f(x)')\nplt.twinx()\nplt.plot(_x, np.asarray(grads), c='r')\nplt.ylabel(\"f(x)'\")\nplt.title('Fraction not truncated')\nplt.axhline(0, color='k', alpha=0.2)\nplt.xlabel('shift')\nplt.tight_layout()\nplt.show()\n[DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64), DeviceArray(1., dtype=float64)]\n[DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64), DeviceArray(nan, dtype=float64)]",
        "answers": [
            "The issue is that your sigmoid function is implemented in such a way that the automatically determined gradient is not stable for large negative values of x:\nimport jax.numpy as jnp\nimport jax\n\ndef sigmoid(x, scale=100):\n    return 1 / (1 + jnp.exp(-x*scale))\n\nprint(jax.grad(sigmoid)(-1000.0))\n# nan\nYou can see why this is happening using the jax.make_jaxpr function to introspect the operations produced by the automatically determined gradient (comments are my annotations):\n>>> jax.make_jaxpr(jax.grad(sigmoid))(-1000.0)\n{ lambda  ; a.                    # a = -1000\n  let b = neg a                   # b = 1000\n      c = mul b 100.0             # c = 100,000\n      d = exp c                   # d = inf\n      e = add d 1.0\n      _ = div 1.0 e\n      f = integer_pow[ y=-2 ] e   # f = 0\n      g = mul 1.0 f               # g = 0\n      h = mul g 1.0               # h = 0\n      i = neg h                   # i = 0\n      j = mul i d                 # j = 0 * inf = NaN\n      k = mul j 100.0             # k = NaN\n      l = neg k                   # l = NaN\n  in (l,) }                       # return NaN\nThis is one of those cases where 64-bit floating point arithmetic fails you: it doesn't have the range to work with numbers like exp(100000).\nSo what can you do? One heavy-weight option would be to use a custom derivative rule to tell autodiff how to handle the sigmoid function in a more stable way. In this case, though, an easier option is to re-express the sigmoid function in terms of something that is better behaved under autodiff transformations. One option is this:\ndef sigmoid(x, scale=100):\n    return 0.5 * (jnp.tanh(x * scale / 2) + 1)\nUsing this version in your script fixes the issue."
        ],
        "link": "https://stackoverflow.com/questions/68290850/jax-autograd-of-a-sigmoid-always-returns-nan"
    },
    {
        "title": "How to install just XLA?",
        "question": "I want to use XLA as a backend for my project. Is there a recommended way to install it on its own (without the rest of TensorFlow). Jax probably does this, but looking in their repository it's not obvious how.",
        "answers": [
            "There is no supported way to install XLA on its own apart from tensorflow.\nThat said, JAX does extract, build, and bundle XLA separately from tensorflow within the jaxlib package. You can see the relevant build scripts for jaxlib on various platforms here: https://github.com/google/jax/tree/main/build\nIn particular, take a look at build_wheel.py, which contains the scripts that extract relevant pieces of XLA from the tensorflow source as part of the jaxlib build.",
            "XLA has been moved to the OpenXLA GitHub organization. It can be installed on its own from there."
        ],
        "link": "https://stackoverflow.com/questions/68290128/how-to-install-just-xla"
    },
    {
        "title": "Gradient Accumulation with JAX",
        "question": "I made a simple script to try to do gradient accumulation with JAX. The idea is to have large batch size (e.g. 64) that are split in small chunks (e.g. 4) that fit in the GPU's memory. For each chunck, the resulting gradient, stored in a pytree, is added to the current batch gradient. The update is done only when all chunks of the large batch are computed. In this particular example, we simply try to fit random 512-dimensional vectors to random booleans with a linear layer. Here is the script:\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, random\nfrom jax.experimental import optimizers\nfrom functools import partial\nfrom jax.nn.initializers import normal, zeros\nfrom typing import Callable\nfrom dataclasses import dataclass\n\n@dataclass\nclass Jax_model:\n    init_fun: Callable\n    apply_fun: Callable\n\n\ndef Dense(input_size: int, output_size: int, init_kernel=normal(), init_bias=zeros):\n\n    def init_fun(key):\n        key, sub_key1, sub_key2 = jax.random.split(key, 3)\n        params = {\n            'I': init_kernel(sub_key1, (input_size, output_size) ),\n            'I_b': init_bias(sub_key2, (1,output_size) ),\n        }\n        return params\n\n    def apply_fun(params, inputs):\n        I, I_b, = params['I'], params['I_b']\n        logits = inputs @ I + I_b\n        return logits\n\n    return Jax_model(init_fun, apply_fun)\n\n\ndef divide_pytree(pytree, div):\n    for pt in jax.tree_util.tree_leaves(pytree):\n        pt = pt / div\n    return pytree\n\n\ndef add_pytrees(pytree1, pytree2):\n    for pt1, pt2 in zip( jax.tree_util.tree_leaves(pytree1), jax.tree_util.tree_leaves(pytree2) ):\n        pt1 = pt1 + pt2\n    return pytree1\n\n\nrng_key = random.PRNGKey(42)\nbatch_size = 64\naccumulation_size = 4\nmodel_dim = 512\nn_iter = 50\n\nmodel = Dense(model_dim, 1)\nrng_key, sub_key = random.split(rng_key)\ninit_params = model.init_fun(sub_key)\nopt_init, opt_update, get_params = optimizers.adam(0.001)\nopt_state = opt_init(init_params)\n\n@jit\ndef update(i, current_opt_state, current_batch):\n    N = current_batch[0].shape[0]\n    K = accumulation_size\n    num_gradients = N//K\n    accumulation_batch = (current_batch[ib][0:K] for ib in range(len(current_batch)))\n    value, grads = jax.value_and_grad(loss_func)(get_params(current_opt_state), accumulation_batch)\n    value = value / num_gradients\n    grads = divide_pytree(grads, num_gradients)\n    for k in range(K,N,K):\n        accumulation_batch = (current_batch[ib][k:k+K] for ib in range(len(current_batch)))\n        new_value, new_grads = jax.value_and_grad(loss_func)(get_params(current_opt_state), accumulation_batch)\n        value = value + (new_value / num_gradients)\n        grads = add_pytrees(grads, divide_pytree(new_grads, num_gradients))\n    return opt_update(i, grads, current_opt_state), value\n\ndef loss_func(current_params, current_batch):\n    inputs, labels = current_batch\n    predictions = model.apply_fun(current_params, inputs)\n    loss = jnp.square(labels-predictions).sum()\n    return loss\n\nfor i in range(n_iter):\n    rng_key, sub_key1, sub_key2 = random.split(rng_key, 3)\n    inputs = jax.random.uniform(sub_key1, (batch_size, model_dim))\n    labels = jax.random.uniform(sub_key2, (batch_size, 1)) > 0.5\n    batch = inputs, labels\n    opt_state, batch_loss = update(i, opt_state, batch)\n    print(i, batch_loss)\nI have doubts about the divide_pytree and add_pytrees. Does it actually modify the current batch gradient or am I missing something ? Moreover, do you see any speed issue with this code ? In particular, should I use the jax.lax.fori_loop in place of the traditional python for loop ?\nRelated links:\nhttps://github.com/google/jax/issues/1488\nhttps://github.com/google-research/long-range-arena/issues/4",
        "answers": [
            "Regarding the pytree computations: as written your functions are returning the input unmodified. The better approach for this is to use jax.tree_util.tree_map; for example:\nfrom jax.tree_util import tree_map\n\ndef divide_pytree(pytree, div):\n  return tree_map(lambda pt: pt / div, pytree)\n\ndef add_pytrees(pytree1, pytree2):\n  return tree_map(lambda pt1, pt2: pt1 + pt2, pytree1, pytree2)\nRegarding performance: anything in the for loop will be flattened when JIT-compiled, with one repeated copy of all XLA instructions per iteration of the loop. If you have 5 iterations, that's not really an issue. If you have 5000, that would significantly slow down compilation times (because XLA needs to analyze & optimize 5000 explicit copies of the instructions in the loop).\nfori_loop can help, but does not lead to optimal code, particularly when running on CPU and GPU.\nBetter would be to use broadcasted or vmapped operations where possible to express the logic of the loops without explicit looping."
        ],
        "link": "https://stackoverflow.com/questions/68016425/gradient-accumulation-with-jax"
    },
    {
        "title": "derivatives by a and b, using using algorithmic differentiation",
        "question": "I've been tasked to find the derivatives by a and b, using jax, for this function\nnow, the reason I'm here is because I don't know enough Python, and this for the course in question, we haven't been thought python either.\nthe assignment is:\nreturn a tuple (dfa, dfb) such that dfa is the partial derivatives of f by a,\n           and dfb is the partial derivative of f by b\nnow, I was able to do it the normal way:\ndef function(a, b):\n   dfa = sym.diff((2/b)*sym.cos(a)*sym.exp(-a*a/b*b), a)\n   dfb = sym.diff((2/b)*sym.cos(a)*sym.exp(-a*a/b*b), a)\n   return (dfa, dfb)\nbut im not familiar with algorithmic differentiation, using the example we were given, i've tried this:\ndef foo():\n\n   x = (2/b)*sym.cos(a)\n   y = sym.exp(-sym.Pow(a/b,2))\n   return (x*y)\n\ndef f_partial_derviatives_algo():\n   return jax.grad(foo)\nbut I'm getting this error:\ncannot unpack non-iterable function object\nIf anyone can help with understanding how i can do something like that, It would be greatly appreciated",
        "answers": [
            "JAX and sympy are not compatible. You should either use one or the other, and not try to combine the two.\nIf you want to compute the partial derivatives of this function at some value using JAX, you can write something like this:\nimport jax.numpy as jnp\nfrom jax import grad\n\ndef f(a, b):\n  return (2 / b) * jnp.cos(a) * jnp.exp(- a ** 2 / b ** 2)\n\ndf_da = grad(f, argnums=0)\ndf_db = grad(f, argnums=1)\n\nprint(df_da(1.0, 1.0), df_db(1.0, 1.0))\n# -1.4141841 0.3975322"
        ],
        "link": "https://stackoverflow.com/questions/67922307/derivatives-by-a-and-b-using-using-algorithmic-differentiation"
    },
    {
        "title": "Best way to compute the moving average of word vectors in JAX",
        "question": "Let say I have a matrix W of shape (n_words, model_dim) where n_words is the number of words in a sentence and model_dim is the dimension of the space where the word vectors are represented. What is the fastest way to compute the moving average of these vectors ?\nFor example, with a window size of 2 (window length = 5), I could have something like this (which raises an error TypeError: JAX 'Tracer' objects do not support item assignment):\nfrom jax import random\nimport jax.numpy as jnp\n\n# Fake word vectors (17 words vectors of dimension 32)\nW = random.normal(random.PRNGKey(0), shape=(17, 32)) \n\nws = 2          # window size\nN = W.shape[0]  # number of words\n\nnew_W = jnp.zeros(W.shape)\n\nfor i in range(N):\n    window = W[max(0, i-ws):min(N, i+ws+1)]\n    n = window.shape[0]\n    for j in range(n):\n        new_W[i] += W[j] / n\nI guess there is a faster solution with jnp.convolve but I'm not familiar with it.",
        "answers": [
            "This looks like you're trying to do a convolution, so jnp.convolve or similar would likely be a more performant approach.\nThat said, your example is a bit strange because n is never larger than 4, so you never access any but the first four elements of W. Also, you overwrite the previous value in each iteration of the inner loop, so each row of new_W just contained a scaled copy of one of the first four rows of W.\nChanging your code to what I think you meant and using index_update to make it compatible with JAX's immutable arrays gives this:\nfrom jax import random\nimport jax.numpy as jnp\n\n# Fake word vectors (17 words vectors of dimension 32)\nW = random.normal(random.PRNGKey(0), shape=(17, 32)) \n\nws = 2          # window size\nN = W.shape[0]  # number of words\n\nnew_W = jnp.zeros(W.shape)\n\nfor i in range(N):\n    window = W[max(0, i-ws):min(N, i+ws)]\n    n = window.shape[0]\n    for j in range(n):\n      new_W = new_W.at[i].add(window[j] / n)\nand here is the equivalent in terms of a much more efficient convolution:\nfrom jax.scipy.signal import convolve\nkernel = jnp.ones((4, 1))\nnew_W_2 = convolve(W, kernel, mode='same') / convolve(jnp.ones_like(W), kernel, mode='same')\n\njnp.allclose(new_W, new_W_2)\n# True"
        ],
        "link": "https://stackoverflow.com/questions/67906145/best-way-to-compute-the-moving-average-of-word-vectors-in-jax"
    },
    {
        "title": "How to write a JAX custom vector-Jacobian product (vjp) for softmax",
        "question": "In order to understand JAX's reverse mode auto-diff I tried to write a custom_vjp for softmax like this:\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\n@jax.custom_vjp\ndef stablesoftmax(x):\n    print(f\"input: {x} shape: {x.shape}\")\n    expc = jnp.exp(x - jnp.amax(x))\n    return expc / jnp.sum(expc)\n\ndef ssm_fwd(x):\n    s = stablesoftmax(x)\n    return s, s\n\ndef ssm_bwd(acts, d_dacts):\n    dacts_dinput = jnp.diag(acts) - jnp.outer(acts, acts)  # Jacobian\n    d_dinput = jnp.dot(d_dacts, dacts_dinput)  # Vector-Jacobian product\n    print(f\"Saved activations:\\n{acts} shape: {acts.shape}\")\n    print(f\"d/d_acts:\\n{d_dacts} shape: {d_dacts.shape}\")\n    print(f\"d_acts/d_input (Jacobian of softmax):\\n{dacts_dinput} shape: {dacts_dinput.shape}\")\n    print(f\"d/d_input:\\n{d_dinput} shape: {d_dinput.shape}\")\n    return d_dinput\n\nstablesoftmax.defvjp(ssm_fwd, ssm_bwd)\n\nprint(f\"JAX version: {jax.__version__}\")\ny = np.array([1., 2., 3.])\na = stablesoftmax(y)\nsoftmax_jac_fun = jax.jacrev(stablesoftmax)\ndsoftmax_dy = softmax_jac_fun(y)\nprint(f\"Softmax Jacobian: {dsoftmax_dy}\")\nBut when I call jacrev I get errors about the structure of the VJP result not matching the structure of the input to the softmax:\nJAX version: 0.2.13\ninput: [1. 2. 3.] shape: (3,)\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\ninput: [1. 2. 3.] shape: (3,)\nSaved activations:\n[0.09003057 0.24472848 0.66524094] shape: (3,)\nd/d_acts:\nTraced<ShapedArray(float32[3])>with<BatchTrace(level=1/0)>\n  with val = array([[1., 0., 0.],\n                    [0., 1., 0.],\n                    [0., 0., 1.]], dtype=float32)\n       batch_dim = 0 shape: (3,)\nd_acts/d_input (Jacobian of softmax):\n[[ 0.08192507 -0.02203305 -0.05989202]\n [-0.02203305  0.18483645 -0.1628034 ]\n [-0.05989202 -0.1628034   0.22269544]] shape: (3, 3)\nd/d_input:\nTraced<ShapedArray(float32[3])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([[ 0.08192507, -0.02203305, -0.05989202],\n                          [-0.02203305,  0.18483645, -0.1628034 ],\n                          [-0.05989202, -0.1628034 ,  0.22269544]], dtype=float32)\n       batch_dim = 0 shape: (3,)\nTraceback (most recent call last):\n  File \"analysis/vjp_test.py\", line 30, in <module>\n    dsoftmax_dy = softmax_jac_fun(y)\njax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Custom VJP rule must produce an output with the same container (pytree) structure as the args tuple of the primal function, and in particular must produce a tuple of length equal to the number of arguments to the primal function, but got VJP output structure PyTreeDef(*) for primal input structure PyTreeDef((*,)).\nHowever you can see when I print the shapes they both have shape (3,) but JAX doesn't seem to agree? (Actually the input and output are 3 by 3 matrices but this is because JAX is trying to vmap the JVPs in jacrev so pulling back the entire basis for R(3) in one go (i.e. a 3x3 identity matrix).\nNote: I get the same error if I use jax.grad or jax.vjp directly.",
        "answers": [
            "According to the custom_vjp docs:\nThe output of bwd must be a tuple of length equal to the number of arguments of the primal function\nSo the return statement in the backward pass should look like this:\ndef ssm_bwd(acts, d_dacts):\n    ...\n    return (d_dinput,)"
        ],
        "link": "https://stackoverflow.com/questions/67790625/how-to-write-a-jax-custom-vector-jacobian-product-vjp-for-softmax"
    },
    {
        "title": "Jax and train Neural Networks",
        "question": "I am a beginner in JAX and I am trying to learn how to train a neural network. I saw some blogs, but as I understood there isn't a library that you can train it easily, like 'fit' as in sklearn. I am interested about classification task, could you please reccommend me any blogs in order to adopt his/her algorithm into my problem?",
        "answers": [
            "JAX is an array manipulation library, not a deep learning library: in that respect, you should think of it as more similar to NumPy than similar to scikit-learn. If you want neural networks built on JAX, there are several other good projects available such as haiku and flax."
        ],
        "link": "https://stackoverflow.com/questions/67778124/jax-and-train-neural-networks"
    },
    {
        "title": "indexing into numpy array with jax array: faulty error messages",
        "question": "The following numpy code is perfectly fine:\narr = np.arange(50)\nprint(arr.shape) # (50,)\n\nindices = np.zeros((30,), dtype=int)\nprint(indices.shape) # (30,)\n\narr[indices]\nIt also works after migrating to jax:\nimport jax.numpy as jnp\n\narr = jnp.arange(50)\nprint(arr.shape) # (50,)\n\nindices = jnp.zeros((30,), dtype=int)\nprint(indices.shape) # (30,)\n\narr[indices]\nNow let's try a mix of numpy and jax:\narr = np.arange(50)\nprint(arr.shape) # (50,)\n\nindices = jnp.zeros((30,), dtype=int)\nprint(indices.shape) # (30,)\n\narr[indices]\nThis produces the following error:\nIndexError: too many indices for array: array is 1-dimensional, but 30 were indexed\nIf indexing into a numpy array with a jax array is not supported, that's fine by me. But the error message seems wrong. And things get even more confusing. If you change the shapes a bit, the code works fine. In the following sample I've only edited the shape of indices from (30,) to (40,). No more error message:\narr = np.arange(50)\nprint(arr.shape) # (50,)\n\nindices = jnp.zeros((40,), dtype=int)\nprint(indices.shape) # (40,)\n\narr[indices]\nI'm running jax version '0.2.12', on the cpu. What is happening here?",
        "answers": [
            "This is a long-standing known issue (see https://github.com/google/jax/issues/620); it's not a bug that can be easily fixed by JAX itself, but will require changes to how NumPy treats non-ndarray indices. The good news is that the fix is on the horizon: your problematic code above is accompanied by the following warning, which originates from NumPy:\nFutureWarning: Using a non-tuple sequence for multidimensional indexing is\n deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this\n will be interpreted as an array index, `arr[np.array(seq)]`, which will result\n either in an error or a different result.\nOnce this deprecation cycle is complete, JAX arrays will work correctly in NumPy indexing.\nUntil then, you can work around it by explicitly calling np.asarray when using JAX arrays to index into NumPy arrays."
        ],
        "link": "https://stackoverflow.com/questions/67741305/indexing-into-numpy-array-with-jax-array-faulty-error-messages"
    },
    {
        "title": "is it possible to jit a function which uses jax.numpy.unique?",
        "question": "The following code does not work:\ndef get_unique(arr):\n    return jnp.unique(arr)\n\nget_unique = jit(get_unique)\nget_unique(jnp.ones((10,)))\nThe error message compains about the use of jnp.unique:\nFilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=0/1)>\nThe error arose in jnp.unique()\nThe documentation on sharp bits explains that jit doesn't work if the shape of internal arrays depends on argument values. This is exactly the case here.\nAccording to the docs, a potential workaround is to specify static parameters. But this doesn't apply to my case. The parameters will change for almost every function call. I have split up my code into a preprocessing step, which performs calculations such as this jnp.unique, and a computation step which can be jitted.\nBut still I'd like to ask, is there some workaround that I'm not aware of?",
        "answers": [
            "No, for the reasons you mention, there's currently no way to use jnp.unique on a non-static value.\nIn similar cases, JAX sometimes adds extra parameters that can be used to specify a static size for the output (for example, the size parameter in jax.numpy.nonzero) but nothing like that is currently implemented for jnp.unique. If that is something you'd like, it would be worth filing a feature request."
        ],
        "link": "https://stackoverflow.com/questions/67739742/is-it-possible-to-jit-a-function-which-uses-jax-numpy-unique"
    },
    {
        "title": "Jax vmap for simple array update",
        "question": "I am new to Jax, and I am working on transforming someone else's code, which used the numba \"fastmath\" feature and relied on many nested for-loops without much performance loss. I am trying to recreate the same behavior using Jax's vmap function. However, I am currently struggling a lot with some fundamental questions. Here's a dumbed-down example of what I am trying to vectorized using vmap:\nimport jax.numpy as jnp\nfrom jax import vmap\nimport jax.ops\n\na = jnp.arange(20).reshape((4, 5))\nb = jnp.arange(5)\nc = jnp.arange(4)\nd = jnp.zeros(20)\ne = jnp.zeros((4, 5))\n\nfor i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n        a = jax.ops.index_add(a, jax.ops.index[i, j], b[j] + c[i])\n        d = jax.ops.index_update(d, jax.ops.index[i*a.shape[1] + j], b[j] * c[i])\n        e = jax.ops.index_update(e, jax.ops.index[i, j], 2*b[j])\nHow would I rewrite such a code using vmap? While this code would be relatively easy to vectorize manually, I wish to understand better how vmap works and hope that any answer would help me. The docs don't seem to really help me right now. I really appreciate any help you can provide.",
        "answers": [
            "Here is how you might achieve roughly the same computation using vmap:\nfrom jax import vmap, partial\n\n@partial(vmap, in_axes=(0, None, 0))\n@partial(vmap, in_axes=(0, 0, None))\ndef f(a, b, c):\n  return a + b + c, b * c, 2 * b\n\na, d, e = f(a, b, c)\nd = d.ravel()"
        ],
        "link": "https://stackoverflow.com/questions/67655607/jax-vmap-for-simple-array-update"
    },
    {
        "title": "What is JaxNumpy-compatible equivalent to this Python function?",
        "question": "How do I implement the below in a JAX-compatable way (e.g., using jax.numpy)?\ndef actions(state: tuple[int, ...]) -> list[tuple[int, ...]]:\n    l = []\n    iterables = [range(1, i+1) for i in state]\n    ns = list(range(len(iterables)))\n    for i, iterable in enumerate(iterables):\n        for value in iterable:\n            action = tuple(value if n == i else 0 for n in ns)\n            l.append(action)\n    return l\n\n>>> state = (3, 1, 2)\n>>> actions(state)\n[(1, 0, 0), (2, 0, 0), (3, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 2)]",
        "answers": [
            "Jax, like numpy, cannot efficiently operate on Python container types like lists and tuples, so there's not really any JAX-compatible way to create a function with the exact signature you specify above.\nBut if you're alright with the return value being a two-dimensional array, you could do something like this, based on jnp.vstack:\nfrom typing import Tuple\nimport jax.numpy as jnp\nfrom jax import jit, partial\n\n@partial(jit, static_argnums=0)\ndef actions(state: Tuple[int, ...]) -> jnp.ndarray:\n  return jnp.vstack([\n    jnp.zeros((val, len(state)), int).at[:, i].set(jnp.arange(1, val + 1))\n    for i, val in enumerate(state)])\n>>> state = (3, 1, 2)\n>>> actions(state)\nDeviceArray([[1, 0, 0],\n             [2, 0, 0],\n             [3, 0, 0],\n             [0, 1, 0],\n             [0, 0, 1],\n             [0, 0, 2]], dtype=int32)\nNote that because the size of the output array depends on the content of state, state must be a static quantity, so a tuple is a good option for the input."
        ],
        "link": "https://stackoverflow.com/questions/67290650/what-is-jaxnumpy-compatible-equivalent-to-this-python-function"
    },
    {
        "title": "why when i have a np.power in my function jax.grad can't give me the derivitives?",
        "question": "I want to train a simple linear model. these below x and y are my data.\nimport numpy as np\nx = np.linspace(0,1,100)\ny = 2 * x + 3 + np.random.randn(100)\nf is a function that calculates mean square error over all data.\ndef f(params, x, y):\n  return np.mean(np.power((params['w'] * x + params['b'])-y , 2))\nfrom jax import grad\ndf = grad(f)\nparams = dict()\n#initialize parameters\nparams['w'] = 2.4\nparams['b'] = 10.\ndf(params, x, y) # I will do this in a loop (implementing gradient decent part\nthis gives me an error:\nFilteredStackTrace: jax._src.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray\nwhen I clear np.power code works. why?",
        "answers": [
            "JAX cannot compute gradients of numpy functions, but it can compute gradients of jax.numpy functions. If you rewrite your code in terms of jax.numpy, it should work for you:\nimport numpy as np\nx = np.linspace(0,1,100)\ny = 2 * x + 3 + np.random.randn(100)\n\nimport jax.numpy as jnp\ndef f(params, x, y):\n  return jnp.mean(jnp.power((params['w'] * x + params['b'])-y , 2))\n\nfrom jax import grad\ndf = grad(f)\nparams = dict()\n\nparams['w'] = 2.4\nparams['b'] = 10.\ndf(params, x, y)\n# {'b': DeviceArray(14.661432, dtype=float32),\n#  'w': DeviceArray(7.3792152, dtype=float32)}\nYou can read more details in the TracerArrayConversionError documentation page."
        ],
        "link": "https://stackoverflow.com/questions/67285326/why-when-i-have-a-np-power-in-my-function-jax-grad-cant-give-me-the-derivitives"
    },
    {
        "title": "All pairwise cross products of the rows of two matrices",
        "question": "I would like to efficiently calculate all pairwise cross products of the rows of two matrices, A and B, which are nx3 and mx3 in size. And would ideally like to achieve this in einsum notation.\ni.e. the output Matrix C, would be (n X m x 3),\nwhere\nC[0][0] = cross(n[0],m[0])\nC[0][1] = cross(n[0],m[1])\n...\nC[1][0] = cross(n[1],m[0])\n...\nDue to the approach I am taking, using for loops aren't an option.\nAny help would be much appreciated.",
        "answers": [
            "Looks like cross broadcasts the leading dimensions.\nnp.cross(A[:, None,:], B[None, :,:])"
        ],
        "link": "https://stackoverflow.com/questions/67205068/all-pairwise-cross-products-of-the-rows-of-two-matrices"
    },
    {
        "title": "Partial derivatives using Jax?",
        "question": "I'm confused by Jax documentation, here's what I'm trying to do:\ndef line(m,x,b):\n  return m*x + b\n\ngrad(line)(1,2,3)\nAnd the error:\n---------------------------------------------------------------------------\nFilteredStackTrace                        Traceback (most recent call last)\n<ipython-input-48-d14b17620b30> in <module>()\n      3 \n----> 4 grad(line)(1,2,3)\n\nFilteredStackTrace: TypeError: grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.floating or np.complexfloating), but got int32. If you want to use integer-valued inputs, use vjp or set allow_int to True.\n\nThe stack trace above excludes JAX-internal frames.\nThe following is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\n6 frames\n/usr/local/lib/python3.7/dist-packages/jax/api.py in _check_input_dtype_revderiv(name, holomorphic, allow_int, x)\n    844   elif not allow_int and not (dtypes.issubdtype(aval.dtype, np.floating) or\n    845                               dtypes.issubdtype(aval.dtype, np.complexfloating)):\n--> 846     raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype that \"\n    847                     \"is a sub-dtype of np.floating or np.complexfloating), \"\n    848                     f\"but got {aval.dtype.name}. If you want to use integer-valued \"\n\nTypeError: grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.floating or np.complexfloating), but got int32. If you want to use integer-valued inputs, use vjp or set allow_int to True.\nI'm referencing the official tutorial code:\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n\nkey = random.PRNGKey(0)\n\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12,  0.77],\n                   [0.88, -1.08, 0.15],\n                   [0.52, 0.06, -1.30],\n                   [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ())\n\nW_grad = grad(loss, argnums=0)(W, b)\nprint('W_grad', W_grad)\nAnd the result:\nW_grad [-0.16965576 -0.8774648  -1.4901345 ]\nWhat am I doing wrong here? I gather key is being used in some important way, but I can't figure out why/how it's necessary. To answer this question, please adjust code in the first block as necessary to remove the error.",
        "answers": [
            "Jax is telling you it doesn't like integers. grad(line)(1.,2.,3.) (using floats) fixes the problem.",
            "I think the Error here is clear:\nTypeError: grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.floating or np.complexfloating), but got int32. If you want to use integer-valued inputs, use vjp or set allow_int to True.\nTo use grad(line)(1,2,3) with Int32, change it to grad(line, allow_int=True)(1,2,3)"
        ],
        "link": "https://stackoverflow.com/questions/67119321/partial-derivatives-using-jax"
    },
    {
        "title": "Jax, jit and dynamic shapes: a regression from Tensorflow?",
        "question": "The documentation for JAX says,\nNot all JAX code can be JIT compiled, as it requires array shapes to be static & known at compile time.\nNow I am somewhat surprised because tensorflow has operations like tf.boolean_mask that does what JAX seems incapable of doing when compiled.\nWhy is there such a regression from Tensorflow? I was under the assumption that the underlying XLA representation was shared between the two frameworks, but I may be mistaken. I don't recall Tensorflow ever having troubles with dynamic shapes, and functions such as tf.boolean_mask have been around forever.\nCan we expect this gap to close in the future? If not, why makes it impossible to do in JAX' jit what Tensorflow (among others) enables?\nEDIT\nThe gradient passes through tf.boolean_mask (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nx1 = tf.placeholder(tf.float32, (3,))\nx2 = tf.placeholder(tf.float32, (3,))\ny = tf.boolean_mask(x1, x2 > 0)\nprint(y.shape)  # prints \"(?,)\"\ndydx1, dydx2 = tf.gradients(y, [x1, x2])\nassert dydx1 is not None and dydx2 is None",
        "answers": [
            "Currently, you can't (as discussed here)\nThis is not a limitation of JAX jit vs TensorFlow, but a limitation of XLA or rather how the two compile.\nJAX uses simply XLA to compile the function. XLA needs to know the static shape. That's an inherent design choice within XLA.\nTensorFlow uses the function: this creates a graph which can have shapes that are not statically known. This is not as efficient as using XLA, but still fine. However, tf.function offers an option jit_compile, which will compile the graph inside the function with XLA. While this offers often a decent speedup (for free), it comes with restrictions: shapes need to be statically known (surprise, surprise,...)\nThis is overall not too surprising behavior: computations in computers are in general faster (given a decent optimizer went over it) the more is previously known as more parameters (memory layout,...) can be optimally scheduled. The less is known, the slower the code (on this end is normal Python).",
            "I don't think JAX isn't more incapable of doing this than TensorFlow. Nothing forbid you to do this in JAX:\nnew_array = my_array[mask]\nHowever, mask should be indices (integers) and not booleans. This way, JAX is aware of the shape of new_array (the same as mask). In that sens, I'm pretty sure that tf.boolean_mask is not differentiable i.e. it will raise an error if you try to compute its gradient at some point.\nMore generally, if you need to mask an array, whatever library you are using, there are two approaches:\nif you know in advance what indices need to be selected and you need to provide these indices such that the library can compute the shape before compilation;\nif you can't define these indices, for whatever reason, then you need to design your code in order to avoid the prevent the padding to affect your result.\nExamples for each situation\nLet say you're writing a simple embedding layer in JAX. The input is a batch of token indices corresponding to several sentences. To get word embeddings corresponding to these indices, I will simply write word_embeddings = embeddings[input]. Since I don't know the length of the sentences in advance, I need to pad all token sequences to the same length beforehand, such that input is of shape (number_of_sentences, sentence_max_length). Now, JAX will compile the masking operation every time this shape changes. To minimize the number of compilations, you can provide the same number of sentences (also called batch size) and you can set the sentence_max_length to the maximum sentence length in the entire corpus. This way, there will be only one compilation during training. Of course, you need to reserve one row in word_embeddings that corresponds to the pad index. But still, the masking works.\nLater in the model, let say you want to express each word of each sentence as a weighted average of all other words in the sentence (like a self-attention mechanism). The weights are computed in parallel for the entire batch and are stored in the matrix A of dimension (number_of_sentences, sentence_max_length, sentence_max_length). The weighted averages are computed with the formula A @ word_embeddings. Now, you need to make sure the pad tokens don't affect this previous formula. To do so, you can zero out the entries of A corresponding to the pad indices to remove their influence in the averaging. If the pad token index is 0, you would do:\n    mask = jnp.array(input > 0, dtype=jnp.float32)\n    A = A * mask[:, jnp.newaxis, :]\n    weighted_mean = A @ word_embeddings \nSo here we used a boolean mask, but the masking is somehow differentiable since we multiply the mask with another matrix instead of using it as an index. Note that we should proceed the same way to remove the rows of weighted_mean that also correspond to pad tokens."
        ],
        "link": "https://stackoverflow.com/questions/66711706/jax-jit-and-dynamic-shapes-a-regression-from-tensorflow"
    },
    {
        "title": "sampling univariate gausssian with specific mean and standard deviation using jax.random.normal",
        "question": "I'm trying to sample from a gaussian with specific standard deviation and mean, I know the following function is sampling from a gaussian with zero mean and standard deviation equals to 1:\nimport jax\nfrom jax import random\n\nkey = random.PRNGKey(0)\nmu = 20\nstd = 4\n\nx1 = jax.random.normal(key, (1000,))\nAnd I can adjust the mean by doing: x1 = x1 + mu, but how can I adjust the standard deviation?",
        "answers": [
            "Create your samples this way:\nx1 = mu + std * jax.random.normal(key, (1000,))\nIf you do this, the histogram of samples will follow the expected distribution:\nimport jax\nfrom jax import random\nfrom jax.scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nkey = random.PRNGKey(0)\nmu = 20\nstd = 4\n\nx1 = mu + std * jax.random.normal(key, (1000,))\nplt.hist(x1, bins=50, density=True)\n\nx = jnp.linspace(5, 35, 100)\ny = norm.pdf(x, loc=mu, scale=std)\nplt.plot(x, y)",
            "This\nx1 = std * x1 + mu\nwill give you want you want"
        ],
        "link": "https://stackoverflow.com/questions/66664455/sampling-univariate-gausssian-with-specific-mean-and-standard-deviation-using-ja"
    },
    {
        "title": "How to perform non-integer index arithmetic in jit-compiled jax code?",
        "question": "If we perform a non-integer calculation on array indices (followed by casting to int() ), it seems we still cannot use the result as a valid index in jit-compiled jax code. How can we work around this issue?\nFollowing is a minimal example. Specific question: can the command jnp.diag_indices(d) be made to work without passing extra parameters to fun()\nRun this in a Jupiter cell:\nimport jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef fun(t):\n    d = jnp.sqrt(t.size**2)\n    d = jnp.array(d,int)\n    \n    jnp.diag_indices(t.size)   # this line works\n    jnp.diag_indices(d)        # this line breaks. Comment it out to see that d and t.size have the same dtype=int32 \n\n    return t.size, d\n    \nfun(jnp.array([1,2]))    ",
        "answers": [
            "The issue is not the type of d, it is the fact that d is the result of a jax operation and therefore is traced within a JIT context. In JAX, the shapes and sizes of arrays cannot depend on traced quantities, which is why your code results in an error.\nTo work around this, a useful pattern is to use np operations rather than jnp operations to ensure that d is static and not traced:\nimport jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef fun(t):\n    d = np.sqrt(t.size**2)\n    d = np.array(d, int)\n    \n    jnp.diag_indices(t.size)\n    jnp.diag_indices(d)\n\n    return t.size, d\n    \nprint(fun(jnp.array([1,2])))\n# (DeviceArray(2, dtype=int32), DeviceArray(2, dtype=int32))\nFor a brief background on tracing, static values, and similar topics, the How To Think In JAX documentation page may be helpful."
        ],
        "link": "https://stackoverflow.com/questions/66661970/how-to-perform-non-integer-index-arithmetic-in-jit-compiled-jax-code"
    },
    {
        "title": "JAX vmap behaviour",
        "question": "I'm trying to understand the behaviour of JAX vmap, so I wrote the following code:\nimport jax.numpy as jnp\nfrom jax import vmap\n\ndef what(a,b,c):\n  z = jnp.dot(a,b)\n  return z + c\n\nv_what = vmap(what, in_axes=(None,0,None))\n\na = jnp.array([1,1,3])\nb = jnp.array([2,2])\nc = 1.0\n\nv_what(a,b,c)\nAnd the output is:\nDeviceArray([[3., 3., 7.],\n             [3., 3., 7.]], dtype=float32)\nI understand that the only input that is being altered is b, but Can someone shed some light on why this is the result? And how the dot product behaves after I vectorized the function?",
        "answers": [
            "You have specified that the transformed function should map over the first axis of b, and not map over any axis of a or c. So roughly, you've created a mapped function that does this:\ndef v_what(a, b, c):\n  return jnp.stack([what(a, b_i, c) for b_i in b], axis=0)\nFor your inputs, within each row the dot product looks like jnp.dot(a, 2), and the result is equivalent to a * 2."
        ],
        "link": "https://stackoverflow.com/questions/66548897/jax-vmap-behaviour"
    },
    {
        "title": "Understanding JAX argnums parameter in its gradient function",
        "question": "I'm trying to understand the behaviour of argnums in JAX's gradient function. Suppose I have the following function:\ndef make_mse(x, t):  \n  def mse(w,b): \n    return np.sum(jnp.power(x.dot(w) + b - t, 2))/2\n  return mse \nAnd I'm taking the gradient in the following way:\nw_gradient, b_gradient = grad(make_mse(train_data, y), (0,1))(w,b)\nargnums= (0,1) in this case, but what does it mean? With respect to which variables the gradient is calculated? What will be the difference if I will use argnums=0 instead? Also, can I use the same function to get the Hessian matrix?\nI looked at JAX help section about it, but couldn't figure it out",
        "answers": [
            "When you pass multiple argnums to grad, the result is a function that returns a tuple of gradients, equivalent to if you had computed each separately:\ndef f(x, y):\n  return x ** 2 + x * y + y ** 2\n\ndf_dxy = grad(f, argnums=(0, 1))\ndf_dx = grad(f, argnums=0)\ndf_dy = grad(f, argnums=1)\n\nx = 3.0\ny = 4.25\nassert df_dxy(x, y) == (df_dx(x, y), df_dy(x, y))\nIf you want to compute a mixed second derivatives, you can do this by repeatedly applying the gradient:\nd2f_dxdy = grad(grad(f, argnums=0), argnums=1)\nassert d2f_dxdy(x, y) == 1"
        ],
        "link": "https://stackoverflow.com/questions/66445754/understanding-jax-argnums-parameter-in-its-gradient-function"
    },
    {
        "title": "Create a 3D tensor of zeros with exactly one '1' randomly placed on every slice in numpy/jax",
        "question": "I need to create a 3D tensor like this (5,3,2) for example\narray([[[0, 0],\n        [0, 1],\n        [0, 0]],\n\n       [[1, 0],\n        [0, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [1, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [0, 0],\n        [1, 0]],\n\n       [[0, 0],\n        [0, 1],\n        [0, 0]]])\nThere should be exactly one 'one' placed randomly in every slice (if you consider the tensor to be a loaf of bread). This could be done using loops, but I want to vectorize this part.",
        "answers": [
            "Try generate a random array, then find the max:\na = np.random.rand(5,3,2)\nout = (a == a.max(axis=(1,2))[:,None,None]).astype(int)",
            "The most straightforward way to do this is probably to create an array of zeros, and set a random index to 1. In NumPy, it might look like this:\nimport numpy as np\n\nK, M, N = 5, 3, 2\ni = np.random.randint(0, M, K)\nj = np.random.randint(0, N, K)\nx = np.zeros((K, M, N))\nx[np.arange(K), i, j] = 1\nIn JAX, it might look something like this:\nimport jax.numpy as jnp\nfrom jax import random\n\nK, M, N = 5, 3, 2\nkey1, key2 = random.split(random.PRNGKey(0))\ni = random.randint(key1, (K,), 0, M)\nj = random.randint(key2, (K,), 0, N)\nx = jnp.zeros((K, M, N)).at[jnp.arange(K), i, j].set(1)\nA more concise option that also guarantees a single 1 per slice would be to use broadcasted equality of a random integer with an appropriately constructed range:\nr = random.randint(random.PRNGKey(0), (K, 1, 1), 0, M * N)\nx = (r == jnp.arange(M * N).reshape(M, N)).astype(int)",
            "You can create a zero array where the first element of each sub-array is 1, and then permute it across the final two axes:\nx = np.zeros((5,3,2)); x[:,0,0] = 1\n\nrng = np.random.default_rng()\nx = rng.permuted(rng.permuted(x, axis=-1), axis=-2)"
        ],
        "link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice"
    },
    {
        "title": "Sampling from multivariate normal distribution in JAX gives type error",
        "question": "I'm trying to use JAX to generate samples from multivariate normal distribution using:\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nkey = random.PRNGKey(0)\ncov = np.array([[1.2, 0.4], [0.4, 1.0]])\nmean = np.array([3,-1])\nx1,x2 = jax.random.multivariate_normal(key, mean, cov, 5000).T\nHowever when I run the code I get the following error:\nTypeError                                 Traceback (most recent call last)\n<ipython-input-25-1397bf923fa4> in <module>()\n      2 cov = np.array([[1.2, 0.4], [0.4, 1.0]])\n      3 mean = np.array([3,-1])\n----> 4 x1,x2 = jax.random.multivariate_normal(key, mean, cov, 5000).T\n\n1 frames\n/usr/local/lib/python3.6/dist-packages/jax/core.py in canonicalize_shape(shape)\n   1159          \"got {}.\")\n   1160   if any(isinstance(x, Tracer) and isinstance(get_aval(x), ShapedArray)\n-> 1161          and not isinstance(get_aval(x), ConcreteArray) for x in shape):\n   1162     msg += (\"\\nIf using `jit`, try using `static_argnums` or applying `jit` to \"\n   1163             \"smaller subfunctions.\")\n\nTypeError: 'int' object is not iterable\nI'm not sure what the problem is since the same syntax works for the equivalent function in Numpy",
        "answers": [
            "In the jax.random module, most shapes must explicitly be tuples. So instead of shape 5000, use (5000,):\nx1,x2 = jax.random.multivariate_normal(key, mean, cov, (5000,)).T"
        ],
        "link": "https://stackoverflow.com/questions/66014763/sampling-from-multivariate-normal-distribution-in-jax-gives-type-error"
    },
    {
        "title": "Non-hashable static arguments are not supported in Jax when using vmap",
        "question": "This is related to this question. After some work, I managed to change it down to the last error. The code looks like this now.\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, 0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha, T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n\n@jax.partial(jax.jit, static_argnums=(0,2,))    \ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\nThis leads to (what I hope) is the final error, that is\nNon-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.batching.BatchTracer'> for function map_row is non-hashable.\nThis is very strange, as I think I have marked every everywhere axis appears to be static, yet it still tells me that it is traced.",
        "answers": [
            "When you write a partial function with positional arguments, those arguments are passed first. So this:\njax.partial(map_row, alpha, T)\nis essentially equivalent to this:\nlambda z_input: map_row(alpha, T, z_input)\nNotice the incorrect order of the arguments – this is what's causing your error: you're passing z_input, a non-hashable tracer, to an argument that is expected to be static.\nYou can fix this by replacing the partial statement above with:\nlambda z: map_row(z, alpha, T)\nand then your code will run correctly."
        ],
        "link": "https://stackoverflow.com/questions/65685211/non-hashable-static-arguments-are-not-supported-in-jax-when-using-vmap"
    },
    {
        "title": "A JAX custom VJP function for multiple input variable does not work for NumPyro/HMC-NUTS",
        "question": "I am trying to use a custom VJP (vector-Jacobian product) function as a model for a HMC-NUTS in numpyro. I was able to make a single variable function that works for HMC-NUTS as follows:\nimport jax.numpy as jnp\nfrom jax import custom_vjp\n\n@custom_vjp\ndef h(x):\n    return jnp.sin(x)\n\ndef h_fwd(x):\n    return h(x), jnp.cos(x)\n\ndef h_bwd(res, u):\n    cos_x  = res \n    return (cos_x * u,)\n\nh.defvjp(h_fwd, h_bwd)\nHere, I defined h(x)=sin(x) by manual. Then, I made a test data as\nimport numpy as np\nnp.random.seed(32)\nsigin=0.3\nN=20\nx=np.sort(np.random.rand(N))*4*np.pi\ndata=hv(x)+np.random.normal(0,sigin,size=N)\ntest data\nI was able to perform a HMC-NUTS in NumPyro in this case as\nimport numpyro\nimport numpyro.distributions as dist\n\ndef model(x,y):\n    sigma = numpyro.sample('sigma', dist.Exponential(1.))\n    x0 = numpyro.sample('x0', dist.Uniform(-1.,1.))\n    #mu=jnp.sin(x-x0)\n    #mu=hv(x-x0)\n    mu=h(x-x0)\n    numpyro.sample('y', dist.Normal(mu, sigma), obs=y)\n\nfrom jax import random\nfrom numpyro.infer import MCMC, NUTS\n\nrng_key = random.PRNGKey(0)\nrng_key, rng_key_ = random.split(rng_key)\nnum_warmup, num_samples = 1000, 2000\nkernel = NUTS(model)\nmcmc = MCMC(kernel, num_warmup, num_samples)\nmcmc.run(rng_key_, x=x, y=data)\nmcmc.print_summary()\nIt works.\nsample: 100%|██████████| 3000/3000 [00:15<00:00, 193.84it/s, 3 steps of size 7.67e-01. acc. prob=0.92]\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n     sigma      0.35      0.06      0.34      0.26      0.45   1178.07      1.00\n        x0      0.07      0.11      0.07     -0.11      0.26   1243.73      1.00\n\nNumber of divergences: 0\nHowever, if I define a multi variable function as,\n@custom_vjp\ndef h(x,A):\n    return A*jnp.sin(x)\n\ndef h_fwd(x, A):\n    res = (A*jnp.cos(x), jnp.sin(x))\n    return h(x,A), res\n\ndef h_bwd(res, u):\n    A_cos_x, sin_x = res\n    return (A_cos_x * u, sin_x * u)\n\nh.defvjp(h_fwd, h_bwd)\nthen perform a HMC-NUTS as\ndef model(x,y):\n    sigma = numpyro.sample('sigma', dist.Exponential(1.))\n    x0 = numpyro.sample('x0', dist.Uniform(-1.,1.))\n    A = numpyro.sample('A', dist.Exponential(1.))\n    mu=h(x-x0,A)\n    numpyro.sample('y', dist.Normal(mu, sigma), obs=y)\n\nrng_key = random.PRNGKey(0)\nrng_key, rng_key_ = random.split(rng_key)\nnum_warmup, num_samples = 1000, 2000\nkernel = NUTS(model)\nmcmc = MCMC(kernel, num_warmup, num_samples)\nmcmc.run(rng_key_, x=x, y=data)\nmcmc.print_summary()\nthen I got an error as\nTypeError: mul got incompatible shapes for broadcasting: (3,), (22,).\nI suspect that output shape(s) in my function was wrong. But, I could not figure out what was wrong after various trials changing shapes.",
        "answers": [
            "def model(x,y):\nsigma = numpyro.sample('sigma', dist.Exponential(1.))\nx0 = numpyro.sample('x0', dist.Uniform(-1.,1.))\nA = numpyro.sample('A', dist.Exponential(1.))\nhv=vmap(h,(0,None),0)\nmu=hv(x-x0,A)\nnumpyro.sample('y', dist.Normal(mu, sigma), obs=y)\nvmap solved this problem."
        ],
        "link": "https://stackoverflow.com/questions/65684271/a-jax-custom-vjp-function-for-multiple-input-variable-does-not-work-for-numpyro"
    },
    {
        "title": "Jax cannot find the static argnums",
        "question": "This is related with this question. I manage to make the most of the code work, except one of the strange thing.\nHere is the modified code.\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, a_min=0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha=alpha, T=T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n    \n@jax.partial(jax.jit, static_argnums=(0,2,))\ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, axis=0, alpha=1.5, T=20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\nThis code will produce an error as follows:\ntuple index out of range\nwhich is cause by this line of code\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\nEven if I replace the function body with nothing but an entity function, the error persists. This is a really strange behavior. However, it is very important for me to get this thing to be static as it will help to unrolled loops.",
        "answers": [
            "This error is due to a wart that I hope will be fixed soon in JAX: static arguments cannot be passed by keyword. In other words, you should change this:\ndef toy(input, weight):\n    return (weight*entmax(input, axis=0, alpha=1.5, T=20)).sum()\nto this:\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\nThe same fix should be applied in calls to max_row.\nAt this point, you end up with a ValueError because of passing traced variables to functions that require static arguments; the solution will be similar to that in How to handle JAX reshape with JIT.\nOne additional note: this static_argnums error has recently been improved, and in the next release will be a bit more clear:\nValueError: jitted function has static_argnums=(2,), donate_argnums=() but was called with only 1 positional arguments."
        ],
        "link": "https://stackoverflow.com/questions/65612989/jax-cannot-find-the-static-argnums"
    },
    {
        "title": "How to handle JAX reshape with JIT",
        "question": "I am trying to implement entmax-alpha as is described in here.\nHere is the code.\nimport jax\nimport jax.numpy as jnp\nfrom jax import custom_jvp\nfrom jax import jit\nfrom jax import lax\nfrom jax import vmap\n\n\n@jax.partial(jit, static_argnums=(2,))\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, a_min=0) ** (1 / (alpha - 1))\n\n\n@jit\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\n@jit\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n\n@jax.partial(jit, static_argnums=(1, 2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    reduce_length = input.shape[axis]\n    input = jnp.swapaxes(input, -1, axis)\n    input = input.reshape(input.size / reduce_length, reduce_length)\n    result = vmap(jax.partial(map_row, alpha=alpha, T=T), 0)(input)\n    return jnp.swapaxes(result, -1, axis)\n\n\n@jax.partial(jit, static_argnums=(1, 2,))\ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\nWhen I call it with the following code:\nimport numpy as np\nfrom jax import value_and_grad\ninput = jnp.array(np.random.randn(64, 10))\nweight = jnp.array(np.random.randn(64, 10))\n\ndef toy(input, weight):\n    return (weight*entmax(input, axis=-1, alpha=1.5, T=20)).sum()\n\nvalue_and_grad(toy)(input, weight)\nI got the following error.\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-3a62e54c67d2> in <module>()\n      7     return (weight*entmax(input, axis=-1, alpha=1.5, T=20)).sum()\n      8 \n----> 9 value_and_grad(toy)(input, weight)\n\n35 frames\n<ipython-input-1-d85b1daec668> in entmax(input, axis, alpha, T)\n     49 @jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\n     50 def entmax(input, axis=-1, alpha=1.5, T=10):\n---> 51     reduce_length = input.shape[axis]\n     52     input = jnp.swapaxes(input, -1, axis)\n     53     input = input.reshape(input.size / reduce_length, reduce_length)\n\nTypeError: tuple indices must be integers or slices, not DynamicJaxprTracer\nIt seems to be always connected to the reshape operations. I am not sure why this happens, and any help will be really appreciated.\nTo recreate the problem, here is the colab notebook\nThanks a lot.",
        "answers": [
            "The error comes from the fact that you are attempting to index a Python tuple with a traced quantity, axis. You can fix this error by making axis a static argument:\n@jax.partial(jit, static_argnums=(0, 1, 2,))\ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    ...\nUnfortunately, this uncovers another problem: p_tau declares that the alpha parameter is static, but body() calls this with a traced quantity. This quantity cannot be easily marked static in body because it is passed within a dictionary of parameters that contains the input that is being traced.\nTo fix this, you'll have to rewrite your function signatures, carefully marking in each one which inputs are static and which are not, and making sure the two do not mix across the layers of function calls."
        ],
        "link": "https://stackoverflow.com/questions/65505103/how-to-handle-jax-reshape-with-jit"
    },
    {
        "title": "Unable to Install Specific JAX jaxlib GPU version",
        "question": "I'm trying to install a particular version of jaxlib to work with my CUDA and cuDNN versions. Following the README, I'm trying\npip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html\nThis returns the following error:\nERROR: Requested jaxlib==0.1.52+cuda101 from https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.52%2Bcuda101-cp37-none-manylinux2010_x86_64.whl has different version in metadata: '0.1.52'\nDoes anyone know what causes this or how to get around the error?",
        "answers": [
            "This error appears to be from a new check in pip version 20.3.X and higher, likely related to the new dependency resolver. I can reproduce this error with pip version 20.3.3, but the package installs correctly with pip version 20.2.4.\nThe easiest way to proceed would probably be to first downgrade pip; i.e.\npip install pip==20.2.4\nand then proceed with your jaxlib install.",
            "Note that both the versions of jax and jaxlib has to match. You can use something like:\n$ pip install --upgrade jax==0.3.2 jaxlib==0.3.2+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nAnother workaround would be to first choose a specific version of jax and jaxlib from the available wheel files and then install those.\n$ pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.76+cuda11.cudnn82-cp39-none-manylinux2010_x86_64.whl",
            "Maybe you need to change the link to: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html. So, pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version"
    },
    {
        "title": "How to resolve ValueError `vector::reserve` in JAX/Python?",
        "question": "EDIT: GitHub issue here: https://github.com/google/jax/issues/5190\nI am trying to optimize the following function using jit:\n@partial(jit, static_argnums=(0, 1,))\ndef coocurrence_helper(pairs: np.array, label_map: Dict) -> lil_matrix:\n    uniques = lil_matrix(np.zeros((len(label_map), len(label_map))).astype(\"int32\"))\n    for item in pairs:\n        if item[0]!=item[1]:\n            uniques[label_map[item[0]], label_map[item[1]]] += 1\n    return uniques\nthe routine above is used here:\ndef _get_pairwise_frequencies(\n     data: pd.DataFrame, crosstab=False\n    ) -> pd.DataFrame:\n        values = data.stack()\n        values.index = values.index.droplevel(1)\n        values.name = \"vals\"\n        values = optimize(values.to_frame())\n        pair = optimize(values.join(values, rsuffix=\"_2\"))\n        label_map = dict()\n        for lbl, each in enumerate(values.vals.unique()):\n            label_map[each] = lbl\n        if not crosstab:\n            freq = coocurrence_helper(pairs = pair.values, label_map=label_map)\n            return ((freq / freq.sum(1).ravel()).astype(np.float32))\n        else:\n            freq = pd.crosstab(pair[\"vals\"], pair[\"vals_2\"])\n            self.index = freq.index\n            return csr_matrix((freq / freq.sum(1)).astype(np.float32))\nBut i get the following error:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-42-f8e638fc2bb6> in <module>\n----> 1 _get_pairwise_frequencies(data)\n\n<ipython-input-30-43adeb39c76c> in _get_pairwise_frequencies(data, crosstab)\n     25             label_map[each] = lbl\n     26         if not crosstab:\n---> 27             freq = coocurrence_helper(pairs = pair.values, label_map=label_map)\n     28             return csr_matrix((freq / freq.sum(1).ravel()).astype(np.float32))\n     29         else:\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/jax/api.py in f_jitted(*args, **kwargs)\n    369         return cache_miss(*args, **kwargs)[0]  # probably won't return\n    370     else:\n--> 371       return cpp_jitted_f(*args, **kwargs)\n    372   f_jitted._cpp_jitted_f = cpp_jitted_f\n    373 \n\nValueError: vector::reserve\nWhat can be the source of the issue here? Without using static_argnums the error message is\nRuntimeError: Invalid argument: Unknown NumPy type O size 8\nwith the same traceback.",
        "answers": [
            "The issue is that you are returning a scipy.sparse.lil_matrix, which is not a valid JAX type. The JAX jit decorator cannot be used as a compiler for arbitrary Python code; it is designed to optimize sequences of operations on JAX arrays.\nThe best way to proceed in this case would probably be to remove the @partial(jit, ...) decorator from your function; if you wanted to use JAX jit compilation here, you would first have to rewrite your code to avoid scipy.sparse matrices and use JAX arrays instead."
        ],
        "link": "https://stackoverflow.com/questions/65307334/how-to-resolve-valueerror-vectorreserve-in-jax-python"
    },
    {
        "title": "Higher-order multivariate derivatives in jax",
        "question": "I am confused about how to compute higher-order multivariate derivatives in jax.\nFor example, how do you compute d^2f / dx dy for\ndef f(x, y):\n     return jnp.sin(jnp.dot(x, y.T))\nwhere x, y in R^n, n >= 1?\nI've been experimenting with jax.jvp and jax.partial, but I haven't had any success.",
        "answers": [
            "Since x and y are vector-valued and f(x, y) is a scalar, I believe you can compute what you're after by combining the jax.jacfwd and jax.jacrev functions with appropriate argnums:\nimport jax.numpy as jnp\nfrom jax import jacfwd, jacrev\n\ndef f(x, y):\n     return jnp.sin(jnp.dot(x, y.T))\n\nd2f_dxdy = jacfwd(jacrev(f, argnums=1), argnums=0)\n  \nx = jnp.arange(4.0)\ny = jnp.ones(4)\n\nprint(d2f_dxdy(x, y))\n\n# DeviceArray([[0.96017027, 0.        , 0.        , 0.        ],\n#              [0.2794155 , 1.2395858 , 0.2794155 , 0.2794155 ],\n#              [0.558831  , 0.558831  , 1.5190012 , 0.558831  ],\n#              [0.83824646, 0.83824646, 0.83824646, 1.7984167 ]],\n#             dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/65270359/higher-order-multivariate-derivatives-in-jax"
    },
    {
        "title": "No library found under: /usr/local/cuda-9.0/targets/aarch64-linux/lib/libcublasLt.so.9.0",
        "question": "I'm trying to install JAX on the NVIDIA Jetson TX2 and I'm facing considerable issues.\nI have CUDA 9.0 and it gives me the following error:\nNo library found under: /usr/local/cuda-9.0/targets/aarch64-linux/lib/libcublasLt.so.9.0\nSo I go looking and of course that library does not exist. Does anyone have any pointers on how I can about installing that library? I've tried searching google and it does not appear to exist at all.",
        "answers": [
            "The cublasLt library did not come into existence until cuda 10.1 here is the cublas 10.0 doc and here is the cublas 10.1 doc.\nTherefore you won't be able to use cublasLt with CUDA 9.0\nOn a Jetson the correct way to get the latest CUDA install including libraries like cublas is to install the latest JetPack."
        ],
        "link": "https://stackoverflow.com/questions/64921435/no-library-found-under-usr-local-cuda-9-0-targets-aarch64-linux-lib-libcublasl"
    },
    {
        "title": "Jaxlib pip installation failure",
        "question": "From the command line, I've tried following this installation tutorial I'd like to avoid building from source if at all possible. Currently, I'm not sure what the issue is. Could anyone verify that they get the same/different response when trying to install Jaxlib?\nFor awareness, Jax installed fine without any issues, but some supporting components are found in Jaxlib which is installed separately.\nC:\\Users\\john.smith>pip install jaxlib\nERROR: Could not find a version that satisfies the requirement jaxlib (from versions: none)\nERROR: No matching distribution found for jaxlib",
        "answers": [
            "It appears you are using Windows. JAX currently does not provide jaxlib builds for Windows (see google/jax#438) though there is work in progress to rectify this; see google/jax#4843.\nThere are some comments in the above issue that suggest a possible approach to build jaxlib for Windows if you wish to build it yourself."
        ],
        "link": "https://stackoverflow.com/questions/64863194/jaxlib-pip-installation-failure"
    },
    {
        "title": "Comparison of two approaches of exponentiating elements of a matrix",
        "question": "I have two approaches of exponentiating a matrix in jnp = jax.numpy. A straightforward one:\njnp.exp(-X/reg)\nAnd with some additional actions:\ndef exp_reg(X, reg):\n    K = jnp.empty_like(X)\n    K = jnp.divide(X, -reg)\n    return jnp.exp(K)\nHowever, when I tested them:\n%timeit jnp.exp(-X/reg).block_until_ready()\n%timeit exp_reg(X, reg).block_until_ready()\nThe second approach turned to outperform, despite having superficially some additional overhead. I've run a %timeit with a matrix of size 2000 x 2000:\n7.85 ms ± 567 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n5.19 ms ± 52.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nWhy it may be the case?",
        "answers": [
            "The difference here is order of operations.\nIn jnp.exp(-X/reg), you are negating every entry of X, and then dividing each entry of the result by reg. That's two passes over the array X.\nin exp_reg you are negating reg (which presumably is a scalar value?) and then dividing X by the result. That's one pass over X.\nIf X is large, I would expect the first approach to be slightly slower than the second, due to the multiple passes over X.\nFortunately, since you're using JAX, you can jit compile your code, in which case XLA generally can optimize over equivalent orders of operation like these. Indeed, for your two functions, compilation eliminates the discrepancy:\nfrom jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\ndef exp_reg1(X, reg):\n  return jnp.exp(-X/reg)\n\ndef exp_reg2(X, reg):\n  K = jnp.divide(X, -reg)\n  return jnp.exp(K)\n\nX = jnp.array(np.random.rand(1000, 1000))\nreg = 2.0\n\n%timeit exp_reg1(X, reg)\n# 100 loops, best of 3: 3.17 ms per loop\n%timeit exp_reg2(X, reg)\n# 100 loops, best of 3: 2.2 ms per loop\n\n# Trigger compilation\njit(exp_reg1)(X, reg)\njit(exp_reg2)(X, reg)\n\n%timeit jit(exp_reg1)(X, reg)\n# 1000 loops, best of 3: 1.92 ms per loop\n%timeit jit(exp_reg2)(X, reg)\n# 100 loops, best of 3: 1.84 ms per loop\n(side note: there's no reason to pre-allocate an empty array K before assigning the result of an operation to a variable of the same name)."
        ],
        "link": "https://stackoverflow.com/questions/64685368/comparison-of-two-approaches-of-exponentiating-elements-of-a-matrix"
    },
    {
        "title": "vmap ops.index_update in Jax",
        "question": "I have the following code below and it's using a simple for loop. I was just wondering if there was a way to vmap it? Here is the original code:\nimport numpy as np \nimport jax.numpy as jnp\nimport jax.scipy.signal as jscp\nfrom scipy import signal\nimport jax\n\ndata = np.random.rand(192,334)\n\na = [1,-1.086740193996892,0.649914553946275,-0.124948974636730]\nb = [0.054778173164082,0.164334519492245,0.164334519492245,0.054778173164082]\nimpulse = signal.lfilter(b, a, [1] + [0]*99) \nimpulse_20 = impulse[:20]\nimpulse_20 = jnp.asarray(impulse_20)\n\n@jax.jit\ndef filter_jax(y):\n    for ind in range(0, len(y)):\n      y = jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19])\n    return y\n\njnpData = jnp.asarray(data)\n\n%timeit filter_jax(jnpData).block_until_ready()\nAnd here is my attempt at using vmap:\ndef paraUpdate(y, ind):\n    return jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19])\n\n@jax.jit\ndef filter_jax2(y):\n  ranger = range(0, len(y))\n  return jax.vmap(paraUpdate, y)(ranger)\nBut I receive the following error:\nTypeError: vmap in_axes must be an int, None, or (nested) container with those types as leaves, but got Traced<ShapedArray(float32[192,334])>with<DynamicJaxprTrace(level=0/1)>.\nI'm a little confused since the range is of type int so I'm not too sure what's going on.\nIn the end, I'm trying to get this little piece optimized as best as possible to get the lowest time.",
        "answers": [
            "jax.vmap can express functionality in which a single operation is independently applied across multiple axes of an input. Your function is a bit different: you have a single operation iteratively applied to a single input.\nFortunately JAX provides lax.scan which can handle this situation. The implementation would look something like this:\nfrom jax import lax\n\ndef paraUpdate(y, ind):\n    return jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19]), ind\n\n@jax.jit\ndef filter_jax2(y):\n  ranger = jnp.arange(len(y))\n  return lax.scan(paraUpdate, y, ranger)[0]\n\nprint(np.allclose(filter_jax(jnpData), filter_jax2(jnpData)))\n# True\n\n%timeit filter_jax(jnpData).block_until_ready()\n# 10 loops, best of 3: 28.6 ms per loop\n\n%timeit filter_jax2(jnpData).block_until_ready()\n# 1000 loops, best of 3: 519 µs per loop\nIf you change your algorithm so that you'e applying the operation to every column in the array rather than the first N columns, it can be expressed with vmap like this:\n@jax.jit\ndef filter_jax3(y):\n  f = lambda col: jscp.convolve(impulse_20, col)[:-19]\n  return jax.vmap(f, in_axes=1, out_axes=1)(y)"
        ],
        "link": "https://stackoverflow.com/questions/64655749/vmap-ops-index-update-in-jax"
    },
    {
        "title": "Nontransitive subclassing with numpy and jax",
        "question": "My question is simple:\n>>> isinstance(x, jax.numpy.ndarray)\nTrue\n>>> issubclass(jax.numpy.ndarray, numpy.ndarray)\nTrue\n>>> isinstance(x, numpy.ndarray)\nFalse\n?\nAnd now I will ramble so SE will accept my reasonable question.",
        "answers": [
            "The reason this is the case is because jax.numpy.ndarray overrides instance checks with a metaclass:\nclass _ArrayMeta(type(np.ndarray)):  # type: ignore\n  \"\"\"Metaclass for overriding ndarray isinstance checks.\"\"\"\n\n  def __instancecheck__(self, instance):\n    try:\n      return isinstance(instance.aval, _arraylike_types)\n    except AttributeError:\n      return isinstance(instance, _arraylike_types)\n\nclass ndarray(np.ndarray, metaclass=_ArrayMeta):\n  dtype: np.dtype\n  shape: Tuple[int, ...]\n  size: int\n\n  def __init__(shape, dtype=None, buffer=None, offset=0, strides=None,\n               order=None):\n    raise TypeError(\"jax.numpy.ndarray() should not be instantiated explicitly.\"\n                    \" Use jax.numpy.array, or jax.numpy.zeros instead.\")\n(view source)\nThe reason your code returns what it does is because you have an x value which is not an instance of numpy.ndarray, but for which this __instancecheck__ method returns true.\nWhy this kind of subterfuge in JAX? Well, for the purpose of JIT compilation, auto-differentiation, and other transforms, JAX uses stand-in objects called tracers that are meant to look and act like an array, despite not actually being an array. This overriding of instance checks is one of the tricks JAX uses to make such tracing work."
        ],
        "link": "https://stackoverflow.com/questions/64654717/nontransitive-subclassing-with-numpy-and-jax"
    },
    {
        "title": "What is the fastest way of selecting a subset of a JAX matrix?",
        "question": "Let's say I have a 2D matrix and I want to plot its values in a histogram. For that, I need to do something like:\nlist_1d = matrix_2d.reshape((-1,)).tolist()\nAnd then use the list to plot the histogram. So far so good, it's just that there are items in the original matrix that I want to exclude. For simplicity, let's say I have a list like this:\nexclude = [(2, 5), (3, 4), (6, 1)]\nSo, the list_1d should have all the items in the matrix without the items pointed to by the exclude (the items of exclude are row and column indices).\nAnd BTW, the matrix_2d is a JAX array which means its content is in GPU.",
        "answers": [
            "One way to do this is to create a mask array that you use to select the desired subset of the array. The mask indexing operation returns a 1D copy of the selected data:\nimport jax.numpy as jnp\nfrom jax import random\nmatrix_2d = random.uniform(random.PRNGKey(0), (10, 10))\nexclude = [(2, 5), (3, 4), (6, 1)]\n\nind = tuple(jnp.array(exclude).T)\nmask = jnp.ones_like(matrix_2d, dtype=bool).at[ind].set(False)\n\nlist_1d = matrix_2d[mask].tolist()\nlen(list_1d)\n# 97"
        ],
        "link": "https://stackoverflow.com/questions/64616206/what-is-the-fastest-way-of-selecting-a-subset-of-a-jax-matrix"
    },
    {
        "title": "Why is this function slower in JAX vs numpy?",
        "question": "I have the following numpy function as seen below that I'm trying to optimize by using JAX but for whatever reason, it's slower.\nCould someone point out what I can do to improve the performance here? I suspect it has to do with the list comprehension taking place for Cg_new but breaking that apart doesn't yield any further performance gains in JAX.\nimport numpy as np \n\ndef testFunction_numpy(C, Mi, C_new, Mi_new):\n    Wg_new = np.zeros((len(Mi_new[:,0]), len(Mi[0])))\n    Cg_new = np.zeros((1, len(Mi[0])))\n    invertCsensor_new = np.linalg.inv(C_new)\n\n    Wg_new = np.dot(invertCsensor_new, Mi_new)\n    Cg_new = [np.dot(((-0.5*(Mi_new[:,m].conj().T))), (Wg_new[:,m])) for m in range(0, len(Mi[0]))] \n\n    return C_new, Mi_new, Wg_new, Cg_new\n\nC = np.random.rand(483,483)\nMi = np.random.rand(483,8)\nC_new = np.random.rand(198,198)\nMi_new = np.random.rand(198,8)\n\n%timeit testFunction_numpy(C, Mi, C_new, Mi_new)\n#1000 loops, best of 3: 1.73 ms per loop\nHere's the JAX equivalent:\nimport jax.numpy as jnp\nimport numpy as np\nimport jax\n\ndef testFunction_JAX(C, Mi, C_new, Mi_new):\n    Wg_new = jnp.zeros((len(Mi_new[:,0]), len(Mi[0])))\n    Cg_new = jnp.zeros((1, len(Mi[0])))\n    invertCsensor_new = jnp.linalg.inv(C_new)\n\n    Wg_new = jnp.dot(invertCsensor_new, Mi_new)\n    Cg_new = [jnp.dot(((-0.5*(Mi_new[:,m].conj().T))), (Wg_new[:,m])) for m in range(0, len(Mi[0]))] \n\n    return C_new, Mi_new, Wg_new, Cg_new\n\nC = np.random.rand(483,483)\nMi = np.random.rand(483,8)\nC_new = np.random.rand(198,198)\nMi_new = np.random.rand(198,8)\n\nC = jnp.asarray(C)\nMi = jnp.asarray(Mi)\nC_new = jnp.asarray(C_new)\nMi_new = jnp.asarray(Mi_new)\n\njitter = jax.jit(testFunction_JAX) \n\n%timeit jitter(C, Mi, C_new, Mi_new)\n#1 loop, best of 3: 4.96 ms per loop",
        "answers": [
            "For general considerations on benchmark comparisons between JAX and NumPy, see https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy\nAs for your particular code: when JAX jit compilation encounters Python control flow, including list comprehensions, it effectively flattens the loop and stages the full sequence of operations. This can lead to slow jit compile times and suboptimal code. Fortunately, the list comprehension in your function is readily expressed in terms of native numpy broadcasting. Additionally, there are two other improvements you can make:\nthere is no need to forward declare Wg_new and Cg_new before computing them\nwhen computing dot(inv(A), B), it is much more efficient and precise to use np.linalg.solve rather than explicitly computing the inverse.\nMaking these three improvements to both the numpy and JAX versions result in the following:\ndef testFunction_numpy_v2(C, Mi, C_new, Mi_new):\n    Wg_new = np.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return C_new, Mi_new, Wg_new, Cg_new\n\n@jax.jit\ndef testFunction_JAX_v2(C, Mi, C_new, Mi_new):\n    Wg_new = jnp.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return C_new, Mi_new, Wg_new, Cg_new\n\n%timeit testFunction_numpy_v2(C, Mi, C_new, Mi_new)\n# 1000 loops, best of 3: 1.11 ms per loop\n%timeit testFunction_JAX_v2(C_jax, Mi_jax, C_new_jax, Mi_new_jax)\n# 1000 loops, best of 3: 1.35 ms per loop\nBoth functions are a fair bit faster than they were previously due to the improved implementation. You'll notice, however, that JAX is still slower than numpy here; this is somewhat to be expected because for a function of this level of simplicity, JAX and numpy are both generating effectively the same short series of BLAS and LAPACK calls executed on a CPU architecture. There's simply not much room for improvement over numpy's reference implementation, and with such small arrays JAX's overhead is apparent.",
            "I tested the problem with perfplot across a range of problem sizes. Result: jax is ever so slightly faster. The reason why jax doesn't outperform numpy here is that it's run on a CPU (just like NumPy) and here, NumPy already pretty optimized. (It uses BLAS/LAPACK under the hood.)\nCode to reproduce the plot:\nimport jax.numpy as jnp\nimport jax\nimport numpy as np\nimport perfplot\n\n\ndef setup(n):\n    C_new = np.random.rand(n, n)\n    Mi_new = np.random.rand(n, 8)\n    return C_new, Mi_new\n\n\ndef testFunction_numpy_v2(C_new, Mi_new):\n    Wg_new = np.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return Wg_new, Cg_new\n\n\n@jax.jit\ndef testFunction_JAX_v2(C_new, Mi_new):\n    Wg_new = jnp.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return Wg_new, Cg_new\n\n\nb = perfplot.bench(\n    setup=setup,\n    kernels=[testFunction_numpy_v2, testFunction_JAX_v2],\n    n_range=[2 ** k for k in range(14)],\n    equality_check=None\n)\nb.save(\"out.png\")\nb.show()"
        ],
        "link": "https://stackoverflow.com/questions/64517793/why-is-this-function-slower-in-jax-vs-numpy"
    },
    {
        "title": "Alternative to scipy stats zmap function",
        "question": "Is there any alternative to scipy stats module of the zmap function? I'm currently using it to obtain the zmap scores for two really large arrays and it's taking quite some time.\nAre there any libraries or alternatives that could boost its performance? Or even another of obtaining what the zmap function does?\nYour ideas and comments would be appreciated!\nHere's my minimal reproducible code below:\nfrom scipy import stats\nimport numpy as np\n\nFeatureData = np.random.rand(483, 1)\ngoodData = np.random.rand(4640, 483)\nFeatureNorm= stats.zmap(FeatureData, goodData)\nAnd here's what the scipy stats.zmap does under the hood:\ndef zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\nAny ideas on how to optimize it for my use case? Could I use libraries like numba or JAX to boost this further?",
        "answers": [
            "Fortunately, the zmap code is pretty straightforward. The overhead in numpy, however, will come from the fact that it must instantiate intermediate arrays. If you use a numerical compiler such as that available in numba or jax, it can fuse these operations and do the computation with less overhead.\nUnfortunately, numba doesn't support optional arguments to mean and std, so let's take a look at JAX. For reference, here are benchmarks for scipy and for the raw numpy version of the function, computed on a Google Colab CPU runtime:\nimport numpy as np\nfrom scipy import stats\n\nFeatureData = np.random.rand(483, 1)\ngoodData = np.random.rand(4640, 483)\n\n%timeit stats.zmap(FeatureData, goodData)\n# 100 loops, best of 3: 13.9 ms per loop\n\ndef np_zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\n%timeit np_zmap(FeatureData, goodData)\n# 100 loops, best of 3: 13.8 ms per loop\nHere is the equivalent code executed in JAX, both eager mode and JIT compiled:\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef jnp_zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(jnp.asarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\njit_jnp_zmap = jit(jnp_zmap)\n\nFeatureData = jnp.array(FeatureData)\ngoodData = jnp.array(goodData)\n%timeit jnp_zmap(FeatureData, goodData).block_until_ready()\n# 100 loops, best of 3: 8.59 ms per loop\n\njit_jnp_zmap(FeatureData, goodData)  # trigger compilation\n%timeit jit_jnp_zmap(FeatureData, goodData).block_until_ready()\n# 100 loops, best of 3: 2.78 ms per loop\nThe JIT-compiled version is about a factor of 5 faster than the scipy or numpy code. On a Colab T4 GPU runtime, the compiled version gains another factor of 10:\n%timeit jit_jnp_zmap(FeatureData, goodData).block_until_ready()\n1000 loops, best of 3: 286 µs per loop\nIf this kind of operation is a bottleneck in your analysis, a compiler like JAX might be a good option."
        ],
        "link": "https://stackoverflow.com/questions/64437380/alternative-to-scipy-stats-zmap-function"
    },
    {
        "title": "Conditional update in JAX?",
        "question": "In autograd/numpy I could do:\nq[q<0] = 0.0\nHow can I do the same thing in JAX?\nI tried import numpy as onp and using that to create arrays, but that doesn't seem to work.",
        "answers": [
            "JAX arrays are immutable, so in-place index assignment statements cannot work. Instead, jax provides the jax.ops submodule, which provides functionality to create updated versions of arrays.\nHere is an example of a numpy index assignment and the equivalent JAX index update:\nimport numpy as np\nq = np.arange(-5, 5)\nq[q < 0] = 0\nprint(q)\n# [0 0 0 0 0 0 1 2 3 4]\n\nimport jax.numpy as jnp\nq = jnp.arange(-5, 5)\nq = q.at[q < 0].set(0)  # NB: this does not modify the original array,\n                        # but rather returns a modified copy.\nprint(q)\n# [0 0 0 0 0 0 1 2 3 4]\nNote that in op-by-op mode, the JAX version does create multiple copies of the array. However when used within a JIT compilation, XLA can often fuse such operations and avoid copying of data.",
            "To address @truth's comment, jnp.where would generalise to many dimensions. Here's an example:\nfrom jax import numpy as jnp\nx = jnp.arange(4).reshape((2, 2)) - 2\nprint(x)\n# Array([[-2, -1], [0, 1]], dtype=int32)\nprint(jnp.where(q < 0, 0, q))\n# Array([[0, 0], [0, 1]], dtype=int32)\nThis is jit compatible too (when jnp.where is used with 3 arguments)."
        ],
        "link": "https://stackoverflow.com/questions/64410133/conditional-update-in-jax"
    },
    {
        "title": "Efficiently fill an array from a function",
        "question": "I want to construct a 2D array from a function in such a way that I can utilize jax.jit.\nThe way I would normally do this using numpy is to create an empty array, and then fill that array in-place.\nxx = jnp.empty((num_a, num_b))\nyy = jnp.empty((num_a, num_b))\nzz = jnp.empty((num_a, num_b))\n\nfor ii_a in range(num_a):\n    for ii_b in range(num_b):\n        a = aa[ii_a, ii_b]\n        b = bb[ii_a, ii_b]\n\n        xyz = self.get_coord(a, b)\n\n        xx[ii_a, ii_b] = xyz[0]\n        yy[ii_a, ii_b] = xyz[1]\n        zz[ii_a, ii_b] = xyz[2]\nTo make this work within jax I have attempted to use the jax.opt.index_update.\n        xx = xx.at[ii_a, ii_b].set(xyz[0])\n        yy = yy.at[ii_a, ii_b].set(xyz[1])\n        zz = zz.at[ii_a, ii_b].set(xyz[2])\nThis runs without errors but is very slow when I try to use a @jax.jit decorator (at least an order of magnitude slower than the pure python/numpy version).\nWhat is the best way to fill a multi-dimensional array from a function using jax?",
        "answers": [
            "JAX has a vmap transform that is designed specifically for this kind of application.\nAs long as your get_coords function is compatible with JAX (i.e. is a pure function with no side-effects), you can accomplish this in one line:\nfrom jax import vmap\nxx, yy, zz = vmap(vmap(get_coord))(aa, bb)",
            "This can be achieved efficiently by using either the jax.vmap or the jax.numpy.vectorize functions.\nAn example using vectorize:\nimport jax.numpy as jnp\n\ndef get_coord(a, b):\n    return jnp.array([a, b, a+b])\n\nf0 = jnp.vectorize(get_coord, signature='(),()->(i)')\nf1 = jnp.vectorize(f0, excluded=(1,), signature='()->(i,j)')\n\nxyz = f1(a,b)\nThe vectorize function uses vmap under the hood, so this should be exactly equivalent to:\nf0 = jax.vmap(get_coord, (None, 0))\nf1 = jax.vmap(f0, (0, None)) \nThe advantage of using vectorize is that the code can be still be run in standard numpy. The disadvantage is less concise code and possibly a small amount of overhead because of the wrapper."
        ],
        "link": "https://stackoverflow.com/questions/64221771/efficiently-fill-an-array-from-a-function"
    },
    {
        "title": "Efficient way to compute Jacobian x Jacobian.T",
        "question": "Assume J is the Jacobian of some function f with respect to some parameters. Are there efficient ways (in PyTorch or perhaps Jax) to have a function that takes two inputs (x1 and x2) and computes J(x1)*J(x2).transpose() without instantiating the entire J matrices in memory?\nI have come across something like jvp(f, input, v=vjp(f, input)) but don't quite understand it and not sure is what I want.",
        "answers": [
            "In JAX, you can compute a full jacobian matrix using jax.jacfwd or jax.jacrev, or you can compute a jacobian operator and its transpose using jax.jvp and jax.vjp.\nSo, for example, say you had a function Rᴺ → Rᴹ that looks something like this:\nimport jax.numpy as jnp\nimport numpy as np\n\nnp.random.seed(1701)\nN, M = 10000, 5\nf_mat = np.array(np.random.rand(M, N))\ndef f(x):\n  return jnp.sqrt(f_mat @ x / N)\nGiven two vectors x1 and x2, you can evaluate the Jacobian matrix at each using jax.jacfwd\nimport jax\nx1 = np.array(np.random.rand(N))\nx2 = np.array(np.random.rand(N))\nJ1 = jax.jacfwd(f)(x1)\nJ2 = jax.jacfwd(f)(x2)\nprint(J1 @ J2.T)\n# [[3.3123782e-05 2.5001222e-05 2.4946943e-05 2.5180108e-05 2.4940484e-05]\n#  [2.5084497e-05 3.3233835e-05 2.4956826e-05 2.5108084e-05 2.5048916e-05]\n#  [2.4969209e-05 2.4896170e-05 3.3232871e-05 2.5006309e-05 2.4947023e-05]\n#  [2.5102483e-05 2.4947576e-05 2.4906987e-05 3.3327218e-05 2.4958186e-05]\n#  [2.4981882e-05 2.5007204e-05 2.4966144e-05 2.5076926e-05 3.3595043e-05]]\nBut, as you note, along the way to computing this 5x5 result, we instantiate two 5x10,000 matrices. How might we get around this?\nThe answer is in jax.jvp and jax.vjp. These have somewhat unintuitive call signatures for the purposes of your question, as they are designed primarily for use in forward-mode and reverse-mode automatic differentiation. But broadly, you can think of them as a way to compute J @ v and J.T @ v for a vector v, without having to actually compute J explicitly.\nFor example, you can use jax.jvp to compute the effect of J1 operating on a vector, without actually computing J1:\nJ1_op = lambda v: jax.jvp(f, (x1,), (v,))[1]\n\nvN = np.random.rand(N)\nnp.allclose(J1 @ vN, J1_op(vN))\n# True\nSimilarly, you can use jax.vjp to compute the effect of J2.T operating on a vector, without actually computing J2:\nJ2T_op = lambda v: jax.vjp(f, x2)[1](v)[0]\n\nvM = np.random.rand(M)\nnp.allclose(J2.T @ vM, J2T_op(vM))\n# True\nPutting these together and operating on an identity matrix gives you the full jacobian matrix product that you're after:\ndef direct(f, x1, x2):\n  J1 = jax.jacfwd(f)(x1)\n  J2 = jax.jacfwd(f)(x2)\n  return J1 @ J2.T\n\ndef indirect(f, x1, x2, M):\n  J1J2T_op = lambda v: jax.jvp(f, (x1,), jax.vjp(f, x2)[1](v))[1]\n  return jax.vmap(J1J2T_op)(jnp.eye(M)).T\n\nnp.allclose(direct(f, x1, x2), indirect(f, x1, x2, M))\n# True\nAlong with the memory savings, this indirect method is also a fair bit faster than the direct method, depending on the sizes of the jacobians involved:\n%time direct(f, x1, x2)\n# CPU times: user 1.43 s, sys: 14.9 ms, total: 1.44 s\n# Wall time: 886 ms\n%time indirect(f, x1, x2, M)\n# CPU times: user 311 ms, sys: 0 ns, total: 311 ms\n# Wall time: 158 ms"
        ],
        "link": "https://stackoverflow.com/questions/63559139/efficient-way-to-compute-jacobian-x-jacobian-t"
    },
    {
        "title": "Not able to install jaxlib",
        "question": "I am trying to install jaxlib on my windows 10 by the following command which I found on the documentation..\npip install jaxlib\nIt shows the following error\nCollecting jaxlib\n  Could not find a version that satisfies the requirement jaxlib (from versions: None)\nNo matching distribution found for jaxlib",
        "answers": [
            "UPDATE 20240514\nJax is now supported on Windows, because Jaxlib has been so since March 2023.\nThanks to cloudhan's jax-windows-builder, it is now rather easy to install JAX and jaxlib on windows. E.g.\npip install jax==0.3.13 https://whls.blob.core.windows.net/unstable/cuda111/jaxlib-0.3.7+cuda11.cudnn82-cp38-none-win_amd64.whl\nThat's all. As explained there, I had to copy the jaxlib's link address of the whl file I was interested in, i.e. the https://whls.blob.core... above). But I also had to take care that JAX's version and Jaxlib's were compatible, which compatibility is easy to assess at github.com/google/jax/releases: just pick the last two of each (!)the version numbers! nothing to download from there.\ntested with the versions explicited above. I.e. python3.8-64 & jax==0.3.13 & jaxlib-0.3.7",
            "Jaxlib is not supported on windows you can see it here.. https://github.com/google/jax/issues/438",
            "I went through the process the last two days myself so here is what i did:\ndownload and install the latest version of microsoft visual studio ( to get a c++ compiler and toolchain)\ndownload and install python\ncreate a virtual python environment with the packages pip, setuptools, six, wheel and numpy (i did it in the GUI of pycharm)\ndownload jax\nopen up a windows powershell as administrator, change to the jax directory and complete the following steps (commands are in quotes)\ninstall chocolatey (package manager for easy bazel installation)\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\ninstall bazel (for building jaxlib)\nchoco install bazel\ninstall msys2 (linux utilities for bazel)\nchoco install msys2\npermamently link the python environment in your powershell\n[System.Environment]::SetEnvironmentVariable(\"PATH\", $Env:Path + \";C:\\path\\to\\venv\", \"Machine\")\nstill beeing in the jax folder in powershell actually build and compile jax\n./build/build.py\nafter installation execute the command you're ask to to install jaxlib, it's something like\npip install c:\\...\\jax\\dist\\jaxlib-0.1.72-cp39-none-win_amd64.whl\nand finally install jax with it\npip install -e .\nThis way it worked for me, since the documentation on the jax homepage utterly confused me.",
            "if someone still have same issue, jaxlib is not supported on alpine linux. If you're using a docker image try changing from alpine to debian based images",
            "JAX does not provide jaxlib builds for Windows at this moment of time.\nIssue 1, issue 2\nBut you can build it yourself if you wish.There are some comments in the above issue that might help you.",
            "This worked for me:\npowershell -ExecutionPolicy ByPass -NoExit -Command \"& 'C:\\users\\<username>\\Anaconda3\\condabin\\conda_hook.bat' ; conda activate <yourcondaEnvironment>\""
        ],
        "link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib"
    },
    {
        "title": "Get and Post API call in java with basic authentication",
        "question": "I want to call GET and POST API in java without using any framework. I need to use basic authentication. Can anybody help me with some tutorial link. In google I found code only in spring framework, But I am not using Spring. I am looking for code to call API with basic authentication.\nI have to add new url with authentication in the below code. What modification is required if API is secured with basic auth and it is POST method. I am new to java so not much aware.\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.Proxy;\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class NetClientGet {\n\n    public static void main(String[] args)  {\n        \n        try\n        {\n            System.out.println(\"Inside the main function\");\n             URL weburl=new URL(\"http://dummy.restapiexample.com/api/v1/employees\");\n             HttpURLConnection conn = (HttpURLConnection) weburl.openConnection();\n             conn.setRequestMethod(\"GET\");\n             conn.setRequestProperty(\"Accept\", \"application/json\");\n             System.out.println(\"Output is: \"+conn.getResponseCode());\n             System.out.println(\"Output is: \");\n             System.setProperty(\"http.proxyHost\", null);\n             //conn.setConnectTimeout(60000);\n             if(conn.getResponseCode()!=200)\n             {\n                 System.out.println(conn.getResponseCode());\n                 throw new RuntimeException(\"Failed : HTTP Error Code: \"+conn.getResponseCode());\n             }\n             System.out.println(\"After the 2 call \");\n             InputStreamReader in=new InputStreamReader(conn.getInputStream());\n             BufferedReader br =new BufferedReader(in);\n             String output;\n             while((output=br.readLine())!=null)\n             {\n                 System.out.println(output);\n             }\n             conn.disconnect();\n             \n        }\n        catch(Exception e)\n        {\n            System.out.println(e.getMessage());\n        }\n        \n    }\n}",
        "answers": [
            "Basic Authentication\nSee the RFC #2617 section 2: Basic Authentication Scheme\nAdd Authentication header into the request. Here's an example:\nString username = \"john\";\nString password = \"pass\";\n// ...\nURL weburl=new URL(\"http://dummy.restapiexample.com/api/v1/employees\");\nHttpURLConnection conn = (HttpURLConnection) weburl.openConnection();\nconn.setRequestMethod(\"GET\");\nconn.setRequestProperty(\"Accept\", \"application/json\");\n// snippet begins\nconn.setRequestProperty(\"Authorization\",\n  \"Basic \" + Base64.getEncoder().encodeToString(\n    (username + \":\" + password).getBytes()\n  )\n);\n// snippet ends\nSystem.out.println(\"Output is: \"+conn.getResponseCode());\nPOST Method\nSee this answer for more information about using POST method with HttpURLConnection."
        ],
        "link": "https://stackoverflow.com/questions/62564502/get-and-post-api-call-in-java-with-basic-authentication"
    },
    {
        "title": "vmap over a list in jax",
        "question": "Using jax, I try to calculate gradients per sample, process them and then bring them in the normal form to calculate a normal parameter update. My working code looks like\ndifferentiate_per_sample = jit(vmap(grad(loss), in_axes=(None, 0, 0)))\ngradients = differentiate_per_sample(params, x, y)\n\n# some code\n\ngradients_summed_over_samples = []\n    for layer in gradients:\n        (dw, db) = layer\n        (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n        gradients_summed_over_samples.append((dw, db))\nwhere gradients is of the form list(tuple(DeviceArray(...), DeviceArray(...)), ...).\nNow I tried to rewrite the loop as vmap (not sure if it brings a speedup in the end)\ndef sum_samples(layer):\n    (dw, db) = layer\n    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n\nvmap(sum_samples)(gradients)\nbut sum_samples is called only once and not for each element in the list.\nIs the list the problem or do I understand something else wrong?",
        "answers": [
            "jax.vmap will only be mapped over jax array inputs, not inputs that are lists of arrays or tuples. In addition, vmapped functions cannot modify inputs in-place; the functions should return a value, and this return value will be stacked with other return values to construct the output\nFor example, you could modify the function you defined and use it like this:\nimport jax.numpy as np\nfrom jax import random\n\ndef sum_samples(layer):\n    (dw, db) = layer\n    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n    return np.array([dw, db])\n\nkey = random.PRNGKey(1701)\ndata = random.uniform(key, (10, 2, 20))\n\nresult = vmap(sum_samples)(data)\nprint(result.shape)\n# (10, 2)\nSide note: if you're using this approach, the vmapped function above can be more concisely expressed as:\ndef sum_samples(layer):\n    return layer.sum(1)"
        ],
        "link": "https://stackoverflow.com/questions/61786831/vmap-over-a-list-in-jax"
    },
    {
        "title": "What's the best way to compute row-wise (or axis-wise) dot products with jax?",
        "question": "I have two numerical arrays of shape (N, M). I'd like to compute a row-wise dot product. I.e. produce an array of shape (N,) such that the nth row is the dot product of the nth row from each array.\nI'm aware of numpy's inner1d method. What would the best way be to do this with jax? jax has jax.numpy.inner, but this does something else.",
        "answers": [
            "You can define your own jit-compiled version of inner1d in a few lines of jax code:\nimport jax\n@jax.jit\ndef inner1d(X, Y):\n  return (X * Y).sum(-1)\nTesting it out:\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.core import umath_tests\n\n\nX = np.random.rand(5, 10)\nY = np.random.rand(5, 10)\n\nprint(umath_tests.inner1d(X, Y))\nprint(inner1d(jnp.array(X), jnp.array(Y)))\n# [2.23219571 2.1013316  2.70353783 2.14094973 2.62582531]\n# [2.2321959 2.1013315 2.703538  2.1409497 2.6258256]",
            "You can try jax.numpy.einsum. Here the implementaion using numpy einsum\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\narr1 = np.random.randint(0,10,[5,5])\narr2 = np.random.randint(0,10,[5,5])\n\narr = np.inner1d(arr1,arr2)\narr\narray([ 87, 200, 229,  81,  53])\nnp.einsum('...i,...i->...',arr1,arr2)\narray([ 87, 200, 229,  81,  53])"
        ],
        "link": "https://stackoverflow.com/questions/61314443/whats-the-best-way-to-compute-row-wise-or-axis-wise-dot-products-with-jax"
    },
    {
        "title": "What is the difference between JAX, Trax, and TensorRT, in simple terms?",
        "question": "I have been using TensorRT and TensorFlow-TRT to accelerate the inference of my DL algorithms.\nThen I have heard of:\nJAX https://github.com/google/jax\nTrax https://github.com/google/trax\nBoth seem to accelerate DL. But I am having a hard time to understand them. Can anyone explain them in simple terms?",
        "answers": [
            "Trax is a deep learning framework created by Google and extensively used by the Google Brain team. It comes as an alternative to TensorFlow and PyTorch when it comes to implementing off-the-shelf state of the art deep learning models, for example Transformers, Bert etc. , in principle with respect to the Natural Language Processing field.\nTrax is built upon TensorFlow and JAX. JAX is an enhanced and optimised version of Numpy. The important distinction about JAX and NumPy is that the former using a library called XLA (advanced linear algebra) which allows to run your NumPy code on GPU and TPU rather than on CPU like it happens in the plain NumPy, thus speeding up computation."
        ],
        "link": "https://stackoverflow.com/questions/60766116/what-is-the-difference-between-jax-trax-and-tensorrt-in-simple-terms"
    },
    {
        "title": "Jacobian determinant of vector-valued function with Python JAX/Autograd",
        "question": "I have a function that maps vectors onto vectors\nand I want to calculate its Jacobian determinant\n,\nwhere the Jacobian is defined as\n.\nSince I can use numpy.linalg.det, to compute the determinant, I just need the Jacobian matrix. I know about numdifftools.Jacobian, but this uses numerical differentiation and I'm after automatic differentiation. Enter Autograd/JAX (I'll stick to Autograd for now, it features an autograd.jacobian() method, but I'm happy to use JAX as long as I get what I want). How do I use this autograd.jacobian()-function correctly wit ha vector-valued function?\nAs a simple example, let's look at the function\n![f(x)=(x_0^2, x_1^2)](https://chart.googleapis.com/chart?cht=tx&chl=f(x%29%20%3D%20(x_0%5E2%2C%20x_1%5E2%29 )\nwhich has the Jacobian\n![J_f = diag(2 x_0, 2 x_1)](https://chart.googleapis.com/chart?cht=tx&chl=J_f%20%3D%20%5Coperatorname%7Bdiag%7D(2x_0%2C%202x_1%29 )\nresulting in a Jacobian determinant\n>>> import autograd.numpy as np\n>>> import autograd as ag\n>>> x = np.array([[3],[11]])\n>>> result = 4*x[0]*x[1]\narray([132])\n>>> jac = ag.jacobian(f)(x)\narray([[[[ 6],\n         [ 0]]],\n\n\n       [[[ 0],\n         [22]]]])\n>>> jac.shape\n(2, 1, 2, 1)\n>>> np.linalg.det(jac)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/python3.8/site-packages/autograd/tracer.py\", line 48, in f_wrapped\n    return f_raw(*args, **kwargs)\n  File \"<__array_function__ internals>\", line 5, in det\n  File \"/usr/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 2113, in det\n    _assert_stacked_square(a)\n  File \"/usr/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 213, in _assert_stacked_square\n    raise LinAlgError('Last 2 dimensions of the array must be square')\nnumpy.linalg.LinAlgError: Last 2 dimensions of the array must be square\nA first approach gives me correct values, but the wrong shape. Why does .jacobian() return such a nested array? If I reshape it correctly, I get the correct result:\n>>> jac = ag.jacobian(f)(x).reshape(-1,2,2)\narray([[[ 6,  0],\n        [ 0, 22]]])\n>>> np.linalg.det(jac)\narray([132.])\nBut now let's take a look at how this works out with array broadcasting, when I try to evaulate the Jacobian determinant for multiple values of x\n>>> x = np.array([[3,5,7],[11,13,17]])\narray([[ 3,  5,  7],\n       [11, 13, 17]])\n>>> result = 4*x[0]*x[1]\narray([132, 260, 476])\n>>> jac = ag.jacobian(f)(x)\narray([[[[ 6,  0,  0],\n         [ 0,  0,  0]],\n\n        [[ 0, 10,  0],\n         [ 0,  0,  0]],\n\n        [[ 0,  0, 14],\n         [ 0,  0,  0]]],\n\n\n       [[[ 0,  0,  0],\n         [22,  0,  0]],\n\n        [[ 0,  0,  0],\n         [ 0, 26,  0]],\n\n        [[ 0,  0,  0],\n         [ 0,  0, 34]]]])\n>>> jac = ag.jacobian(f)(x).reshape(-1,2,2)\n>>> jac\narray([[[ 6,  0],\n        [ 0,  0]],\n\n       [[ 0,  0],\n        [ 0, 10]],\n\n       [[ 0,  0],\n        [ 0,  0]],\n\n       [[ 0,  0],\n        [14,  0]],\n\n       [[ 0,  0],\n        [ 0,  0]],\n\n       [[ 0, 22],\n        [ 0,  0]],\n\n       [[ 0,  0],\n        [ 0,  0]],\n\n       [[26,  0],\n        [ 0,  0]],\n\n       [[ 0,  0],\n        [ 0, 34]]])\n>>> jac.shape\n(9,2,2)\nHere obviously both shapes are wrong, correct (as in the Jacobian matrix I want) woule be\n[[[ 6,  0],\n  [ 0, 22]],\n [[10,  0],\n  [ 0, 26]],\n [[14,  0],\n  [ 0, 34]]]\nwith shape=(6,2,2)\nHow do I need to use autograd.jacobian (or jax.jacfwd/jax.jacrev) in order to make it handle multiple vector inputs correctly?\nNote: Using an explicit loop and treating every point manually, I get the correct result. But is there a way to do it in place?\n>>> dets = []\n>>> for v in zip(*x):\n>>>    v = np.array(v)\n>>>    jac = ag.jacobian(f)(v)\n>>>    print(jac, jac.shape, '\\n')\n>>>    det = np.linalg.det(jac)\n>>>    dets.append(det)\n [[ 6.  0.]\n [ 0. 22.]] (2, 2)\n\n [[10.  0.]\n [ 0. 26.]] (2, 2)\n\n [[14.  0.]\n [ 0. 34.]] (2, 2)\n\n>>> dets\n [131.99999999999997, 260.00000000000017, 475.9999999999998]",
        "answers": [
            "\"How do I use this autograd.jacobian()-function correctly with a vector-valued function?\"\nYou've written\nx = np.array([[3],[11]])\nThere are two issues with this. The first is that this is a vector of vectors, while autograd is designed for vector to vector functions. The second is that autograd expects floating point numbers, rather than ints. If you try to differentiate with respect to ints, you'll get an error. You aren't seeing an error with a vector of vectors because autograd automatically converts your lists of ints to lists of floats.\nTypeError: Can't differentiate w.r.t. type <class 'int'>\nThe following code should give you the determinant.\nimport autograd.numpy as np\nimport autograd as ag\n\ndef f(x):\n    return np.array([x[0]**2,x[1]**2])\n\nx = np.array([3.,11.])\njac = ag.jacobian(f)(x)\nresult = np.linalg.det(jac)\nprint(result)\n\"How do I need to use autograd.jacobian (or jax.jacfwd/jax.jacrev) in order to make it handle multiple vector inputs correctly?\"\nThere is a way to do it in place, it is called jax.vmap. See the JAX docs. (https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\nIn this case, I could compute a vector of Jacobian determinants with the following code. Note that I can define the function f in exactly the same way as before, vmap does the work for us behind the scenes.\nimport jax.numpy as np\nimport jax\n\ndef f(x):\n    return np.array([x[0]**2,x[1]**2])\n\nx = np.array([[3.,11.],[5.,13.],[7.,17.]])\n\njac = jax.jacobian(f)\nvmap_jac = jax.vmap(jac)\nresult = np.linalg.det(vmap_jac(x))\nprint(result)"
        ],
        "link": "https://stackoverflow.com/questions/59737069/jacobian-determinant-of-vector-valued-function-with-python-jax-autograd"
    },
    {
        "title": "JAX: time to jit a function grows superlinear with memory accessed by function",
        "question": "Here is a simple example, which numerically integrates the product of two Gaussian pdfs. One of the Gaussians is fixed, with mean always at 0. The other Gaussian varies in its mean:\nimport time\n\nimport jax.numpy as np\nfrom jax import jit\nfrom jax.scipy.stats.norm import pdf\n\n# set up evaluation points for numerical integration\nintegr_resolution = 6400\nlower_bound = -100\nupper_bound = 100\nintegr_grid = np.linspace(lower_bound, upper_bound, integr_resolution)\nproba = pdf(integr_grid)\nintegration_weight = (upper_bound - lower_bound) / integr_resolution\n\n\n# integrate with new mean\ndef integrate(mu_new):\n    x_new = integr_grid - mu_new\n\n    proba_new = pdf(x_new)\n    total_proba = sum(proba * proba_new * integration_weight)\n\n    return total_proba\n\n\nprint('starting jit')\nstart = time.perf_counter()\nintegrate = jit(integrate)\nintegrate(1)\nstop = time.perf_counter()\nprint('took: ', stop - start)\nThe function looks seemingly simple, but it doesn't scale at all. The following list contains pairs of (value for integr_resolution, time it took to run the code):\n100 | 0.107s\n200 | 0.23s\n400 | 0.537s\n800 | 1.52s\n1600 | 5.2s\n3200 | 19s\n6400 | 134s\nFor reference, the unjitted function, applied to integr_resolution=6400 takes 0.02s.\nI thought that this might be related to the fact that the function is accessing a global variable. But moving the code to set up the integration points inside of the function has no notable influence on the timing. The following code takes 5.36s to run. It corresponds to the table entry with 1600 which previously took 5.2s:\n# integrate with new mean\ndef integrate(mu_new):\n    # set up evaluation points for numerical integration\n    integr_resolution = 1600\n    lower_bound = -100\n    upper_bound = 100\n    integr_grid = np.linspace(lower_bound, upper_bound, integr_resolution)\n    proba = pdf(integr_grid)\n    integration_weight = (upper_bound - lower_bound) / integr_resolution\n\n    x_new = integr_grid - mu_new\n\n    proba_new = pdf(x_new)\n    total_proba = sum(proba * proba_new * integration_weight)\n\n    return total_proba\nWhat is happening here?",
        "answers": [
            "I also answered this at https://github.com/google/jax/issues/1776, but adding the answer here too.\nIt's because the code uses sum where it should use np.sum.\nsum is a Python built-in that extracts each element of a sequence and sums them one by one using the + operator. This has the effect of building a large, unrolled chain of adds which XLA takes a long time to compile.\nIf you use np.sum, then JAX builds a single XLA reduction operator, which is much faster to compile.\nAnd just to show how I figured this out: I used jax.make_jaxpr, which dumps JAX's internal trace representation of a function. Here, it shows:\nIn [3]: import jax\n\nIn [4]: jax.make_jaxpr(integrate)(1)\nOut[4]:\n{ lambda b c ;  ; a.\n  let d = convert_element_type[ new_dtype=float32\n                                old_dtype=int32 ] a\n      e = sub c d\n      f = sub e 0.0\n      g = pow f 2.0\n      h = div g 1.0\n      i = add 1.8378770351409912 h\n      j = neg i\n      k = div j 2.0\n      l = exp k\n      m = mul b l\n      n = mul m 2.0\n      o = slice[ start_indices=(0,)\n                 limit_indices=(1,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      p = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] o\n      q = add p 0.0\n      r = slice[ start_indices=(1,)\n                 limit_indices=(2,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      s = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] r\n      t = add q s\n      u = slice[ start_indices=(2,)\n                 limit_indices=(3,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      v = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] u\n      w = add t v\n      x = slice[ start_indices=(3,)\n                 limit_indices=(4,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      y = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] x\n      z = add w y\n... similarly ...\nand it's then obvious why this is slow: the program is very big.\nContrast the np.sum version:\nIn [5]: def integrate(mu_new):\n   ...:     x_new = integr_grid - mu_new\n   ...:\n   ...:     proba_new = pdf(x_new)\n   ...:     total_proba = np.sum(proba * proba_new * integration_weight)\n   ...:\n   ...:     return total_proba\n   ...:\n\nIn [6]: jax.make_jaxpr(integrate)(1)\nOut[6]:\n{ lambda b c ;  ; a.\n  let d = convert_element_type[ new_dtype=float32\n                                old_dtype=int32 ] a\n      e = sub c d\n      f = sub e 0.0\n      g = pow f 2.0\n      h = div g 1.0\n      i = add 1.8378770351409912 h\n      j = neg i\n      k = div j 2.0\n      l = exp k\n      m = mul b l\n      n = mul m 2.0\n      o = reduce_sum[ axes=(0,)\n                      input_shape=(100,) ] n\n  in [o] }\nHope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/59068666/jax-time-to-jit-a-function-grows-superlinear-with-memory-accessed-by-function"
    }
]