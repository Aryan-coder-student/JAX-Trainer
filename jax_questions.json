[
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Is it expected that vmapping over different input sizes for the same function impacts the accuracy of the result?",
        "question": "I was suprised to see that depending on the size of an input matrix, which is vmapped over inside of a function, the output of the function changes slightly. That is, not only does the size of the output change (which is what I would expect from vmapping) but also the numerics changed slightly. (Note that this only occurs in float32 and only on the GPU)\nI wrote a minimally reproducible example to illustrate the behaviour:\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\ndef equinox_vmap(x, mlp):\n    out = eqx.filter_vmap(mlp.__call__)(x)\n    return out\n\nkey = jax.random.PRNGKey(0)\nkey, network_key = jax.random.split(key, 2)\nmlp = eqx.nn.MLP(2, 2, 10, 2, key=network_key)\n\nkey, key_x = jax.random.split(key, 2)\nx = jax.random.normal(key_x, (10000, 2))\n\nerror_eqx = equinox_vmap(x[:10], mlp) - equinox_vmap(x, mlp)[:10]\nprint(\"eqx error:\", error_eqx)\nWhen running this example I get the output:\neqx error: [[-1.4442205e-04  1.0999292e-04]\n [-5.9515238e-05 -9.1716647e-06]\n [ 1.4841557e-05  5.6132674e-05]\n [ 0.0000000e+00  0.0000000e+00]\n [-9.1642141e-06 -2.5466084e-05]\n [ 3.8832426e-05 -3.3110380e-05]\n [ 3.3825636e-05 -2.4946406e-05]\n [ 4.0918589e-05 -3.2216311e-05]\n [ 1.3601780e-04  8.7693334e-06]\n [ 0.0000000e+00  0.0000000e+00]]\nI understand that the numerics of float32 are not fully accurate and some error is to be expected. However, I was suprised that the result changes depending on how much of the input array is put into the function. I was expecting that the first row of the x array, i.e., x[0,:] would still be filled with the same values and therefore the first row in the output would be the same.\nFurther notes:\nI enabled the use of float64 (jax.config.update(\"jax_enable_x64\", False)) which completely removed this from occuring. I understand that this is a numerical problem, but I am a little bit confused how the vmapping interacts with the example.\nWhen I run the same example on the CPU (using jax.config.update(\"jax_platform_name\", \"cpu\")) this problem also disappears which I also find difficult to understand.\nQuestions:\nIs this to be expected?\nWhere does this \"inconsistency\" come from?\nWhy does it not occur on the CPU and only on the GPU?\nSetup:\nGPU: NVIDIA RTX 6000 Ada Generation 48 GB\nPython 3.11.11 with\nequinox                  0.13.0\njax                      0.7.0\njax-cuda12-pjrt          0.7.0\njax-cuda12-plugin        0.7.0\njaxlib                   0.7.0\njaxtyping                0.3.2\nml_dtypes                0.5.3\nnumpy                    2.3.2\nnvidia-cublas-cu12       12.9.1.4\nnvidia-cuda-cupti-cu12   12.9.79\nnvidia-cuda-nvcc-cu12    12.9.86\nnvidia-cuda-nvrtc-cu12   12.9.86\nnvidia-cuda-runtime-cu12 12.9.79\nnvidia-cudnn-cu12        9.11.0.98\nnvidia-cufft-cu12        11.4.1.4\nnvidia-cusolver-cu12     11.7.5.82\nnvidia-cusparse-cu12     12.5.10.65\nnvidia-nccl-cu12         2.27.6\nnvidia-nvjitlink-cu12    12.9.86\nnvidia-nvshmem-cu12      3.3.9\nopt_einsum               3.4.0\npip                      24.0\nscipy                    1.16.1\nsetuptools               65.5.0\ntyping_extensions        4.14.1\nwadler_lindig            0.1.7\nAny explanations are greatly appreachiated.",
        "answers": [
            "This is behaving as expected. This is not fundamentally about vmap; this is about floating point math. Whenever you're doing floating point operations, you will accumulate rounding errors, and when you do the \"same\" computation in two different ways, you will accumulate rounding errors differently (see Is floating-point math broken? for some discussion of this).\nRunning vmap over different batch sizes results in different sequences of operations, which in turn results in different rounding errors.\nAs for why this differs between CPU and GPU, it's all about how the floating point operations are sequenced. CPU is a serial architecture, so it's likely computing matrix products row-by-row with the same accumulation orders regardless of input size. GPU is a parallel architecture, and will generally distribute and accumulate results differently depending on the size of the inputs."
        ],
        "link": "https://stackoverflow.com/questions/79726091/is-it-expected-that-vmapping-over-different-input-sizes-for-the-same-function-im"
    },
    {
        "title": "Are JAX operations already vectorized?",
        "question": "In the documentation, JAX provides vectorization. However, aren't JAX operations already vectorized? For example, to add two vectors, I thought that the element-wise additions were vectorized internally already.\nMy guess is that vectorization is useful when: it's hard for us to add a dimension for broadcasting, so we resort to a more explicit vectorization.\nEDIT: for example, instead of vectorizing convolution2d with different kernels, I simply stack the kernels, copy and stack the channel, then perform the convolution2d with this stack of kernels.",
        "answers": [
            "I have also raised a similar question here: https://github.com/jax-ml/jax/issues/26212 By now I think there is no universal answer to this and it will remain a matter of taste to a certain degree. However in some cases there is a clearer answer:\nSome operations in JAX are not natively vectorized, such as e.g. jnp.histogram or jnp.bincount, in this case you can use vmap to get a \"batched\" version of that function (for example search for \"batched_histogram\" here http://axeldonath.com/jax-diffusion-models-pydata-boston-2025/). This is really convenient and avoids loops to improve readability as well as performance.\nvmap works over PyTrees. Some libraries (most notably equinox) use this to avoid the need for handling a batch axis in models completely and just finally vmap over the whole parameter tree by convention. This frees developers from thinking about the batch axis at all, but when working with equinox you have to stick to that convention. It also only works if operations are independent across different batches. It does not work for operations such as a \"batch norm\" (see also https://docs.kidger.site/equinox/examples/stateful/)\nIn some cases one introduces a local(!) extra dimension to an array to avoid writing a Python loop and optionally reduce after. This can often be implemented more shortly and with clearer intent using vmap (basically what you said).\nAs broadcasting and batch axes are universally accepted convention in deep learning I mostly stick with them. But I rely on vmap whenever there is no native vectorization, whenever I work with libraries that rely on vmap by convention, or whenever I need to vectorize operations along non-conventional axes (basically everything except batch axis)."
        ],
        "link": "https://stackoverflow.com/questions/79718029/are-jax-operations-already-vectorized"
    },
    {
        "title": "Does vmap correctly split the RNG keys?",
        "question": "In the following code, when I remove the vmap, I have the right randomized behavior. However, with vmap, I don't anymore. Isn't this supposed to be one of the features of nnx.vmap?\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\n@nnx.vmap(\n    in_axes=(None, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((4, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")",
        "answers": [
            "To make dropout work together with vmap in flax, we need to use split_rngs and StateAxes :\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\nbs = 4\n\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=bs, only='dropout')\n@nnx.vmap(\n    in_axes=(state_axes, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\nmodel.train()\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\nOutput with jax: 0.7.0.dev20250704, flax: 0.10.6\nOutput batch:\n[[0.         0.1736668  1.6533196  0.         0.        ]\n [0.         0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.",
            "I'm not sure nnx.vmap and nnx.split_rngs are necessary in vfdev's answer. Also, having a train kwarg is unnecessary in most situations since NNX models can dynamically jump between train=True, train=False with .train() and .eval()\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    x = self.linear(x)\n    x = self.dropout(x)\n    return x\n\nkey = jax.random.PRNGKey(42)\n\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n@nnx.jit\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  return model(x)\n\nbs = 4\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Enable training. This works because Dropout layers have a .deterministic property\n# that can be modified.\nmodel.train()\n\noutput_batch = batched_apply(model, batch_input)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\noutput:\nModel initialized successfully.\nDropout Rate: 0.5\n------------------------------\nInput batch shape: (4, 10)\nInput batch:\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n------------------------------\nRunning the batched model in training mode (dropout is active)...\nOutput batch shape: (4, 5)\n\nOutput batch:\n[[0.         0.1736668  0.         0.         0.        ]\n [0.         0.         1.6533196  1.0752656  0.        ]\n [0.         0.         0.         0.         0.7218913 ]\n [0.09358063 0.         0.         1.0752656  0.        ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.\nand if instead you do model.eval()\nOutput batch:\n[[0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]]\n------------------------------\n❌ Verification failed: All outputs were the same."
        ],
        "link": "https://stackoverflow.com/questions/79698307/does-vmap-correctly-split-the-rng-keys"
    },
    {
        "title": "Configuration options varying between jax installs?",
        "question": "I have a laptop I do work on for a program that includes jax, the program ends up getting run here on a small scale to test it, then it is sent off to a server for batch processing.\nIn the program I have set these flags for jax:\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\n(as well as others but these are the relevant ones)\nThis runs fine on my laptop (using sharding to CPU parallelise), but when running on the server on GPU, I get an error message:\nAttributeError: Unrecognized config option: jax_captured_constants_report_frames\n(and the same for jax_captured_constants_warn_bytes if that were to run first)\nWhy is there this discrepancy? Can I use these flags some other way that is generalised between different jax installs?\npip list | grep jax, on laptop:\njax                       0.6.2\njaxlib                    0.6.2\njaxtyping                 0.3.2\non server:\njax                       0.6.0\njax-cuda12-pjrt           0.6.0\njax-cuda12-plugin         0.6.0\njaxlib                    0.6.0\njaxtyping                 0.3.2\nEDIT: As a side note, what is the scope of jax flags? I have a jax initialisation function to set os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=\" + str(cpu_count()) before the rest of the code runs, if I set jax.config.update(..., ...) options in here, will they hold in files called after it that also import jax? Or do I have to set them again? Is there a function to check the current value of these flags?",
        "answers": [
            "The jax_captured_constants_report_frames and jax_captured_constants_warn_bytes configurations were added in JAX version 0.6.1 (Relevant PR: https://github.com/jax-ml/jax/pull/28157) If you want to use them on your server, you'll have to update JAX to v0.6.1 or later."
        ],
        "link": "https://stackoverflow.com/questions/79693916/configuration-options-varying-between-jax-installs"
    },
    {
        "title": "Unable to set cpu device count for jax parallelisation?",
        "question": "I have been trying to generalise this jax program for solving on both CPU and GPU depending on the machine it's running on (essentially need cpu parallelisation to speed up testing versus gpu for production). I can get jax to parallelise on the GPU, but no matter what I do jax will not detect my cpu_count and thus cannot be sharded across cores (for context am running on 8 core, 16 thread laptop processor).\nI found out that XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT had to be set before jax was initialised (was previously set in the if statement included in the code), but it is still not working. I also tried setting at the very start of my code (this is a snippet from the only file using jax itself, but some other files use jnp as a jax drop in for numpy).\nCan anyone tell me why jax will not pick up on the flag? (Relevant code snippet and jupyter notebook output included below). Thanks.\nRelevant code snippet:\nfrom multiprocessing import cpu_count\ncore_count = cpu_count()\n\n### THIS NEEDS TO BE SET BEFORE JAX IS INITIALISED IN ANY WAY, INCLUDING IMPORTING\n# - XLA_FLAGS are read WHEN jax is IMPORTED\n\n# you can see other ways of setting the environment variable that I've tried here\n\n#jax.config.update('xla_force_host_platform_device_count', core_count)\n#os.environ[\"XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT\"] = '16'#str(core_count)\n#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=' + str(core_count)\nos.environ[\"XLA_FLAGS\"] = f\"--xla_force_host_platform_device_count={cpu_count()}\"\n\nimport jax\n\n# defaults float data types to 64-bit instead of 32 for greater precision\njax.config.update('jax_enable_x64', True)\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\njax.config.update('jax_traceback_filtering', 'off')\n# https://docs.jax.dev/en/latest/gpu_memory_allocation.html\n#jax.config.update('xla_python_client_allocator', '\\\"platform\\\"')\n# can't set via jax.config.update for some reason\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = '\\\"platform\\\"'\n\nprint(\"\\nDefault jax backend:\", jax.default_backend())\n\navailable_devices = jax.devices()\nprint(f\"Available devices: {available_devices}\")\n\nrunning_device = xla_bridge.get_backend().platform\nprint(\"Running device:\", running_device, end='')\n\nif running_device == 'cpu':\n    print(\", with:\", core_count, \"cores.\")\n\n    from jax.sharding import PartitionSpec as P, NamedSharding\n\n    # Create a Sharding object to distribute a value across devices:\n    # Assume core_count is the no. of core devices available\n    mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n\n    # Example matrix shape (9, N), e.g., N = 1e7\n    #x = jax.random.normal(jax.random.key(0), (9, Np))\n\n    # Specify sharding: don't split axis 0 (rows), split axis 1 (columns) across devices\n    # then apply sharding to produce a sharded array from the matrix input\n    # and use jax.device_put to distribute it across devices:\n    s0_sharded = jax.device_put(s0, NamedSharding(mesh, P(None, 'cols')))  # 'None' means don't shard axis 0\n\n    print(s0_sharded.sharding)            # See the sharding spec\n    print(s0_sharded.addressable_shards)  # Check each device's shard\n    jax.debug.visualize_array_sharding(s0_sharded)\nOutput:\nDefault jax backend: cpu\nAvailable devices: [CpuDevice(id=0)]\nRunning device: cpu, with: 16 cores.\n\n...\n\nrelevant line of my code: --> 258 mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n... jax backend trace\nValueError: Number of devices 1 must be >= the product of mesh_shape (16,)",
        "answers": [
            "I tried running your snippet and got a number of errors related to missing imports and undefined names (os is not defined, xla_bridge is not defined, s0 is undefined). This, along with the fact that you're running in Jupyter notebook, makes me think that you've already imported JAX in your runtime before running this cell.\nAs mentioned in your code comments, the XLA device count must be set before JAX is imported in your runtime. You should try restarting the Jupyter kernel, then fix the missing imports and variables and rerun your cell as the first execution in your fresh runtime.\nHere's a simple recipe that should work to set the device count while asserting that you've not already imported JAX in another cell in your notebook:\nimport os\nimport sys\n\nassert \"jax\" not in sys.modules, \"jax already imported: you must restart your runtime\"\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\nIf running this results in an assertion error, then you'll have to restart your kernel/runtime before running it again."
        ],
        "link": "https://stackoverflow.com/questions/79691728/unable-to-set-cpu-device-count-for-jax-parallelisation"
    },
    {
        "title": "Jax vmapping while loop [closed]",
        "question": "Closed. This question needs debugging details. It is not currently accepting answers.\nEdit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\nClosed 2 months ago.\nImprove this question\nI have a function that has jax.lax.while_loop. Now, I want to vmap it. However, vmap makes the execution time very slow compared to the original one.\nI understand that in the case of lax.cond, it is transformed into select, which evaluates all branches and thus may decrease the computational speed.\nIs a similar thing happening here? If so, what is the best practice to do do xx while y is true with vmap?",
        "answers": [
            "A while_loop under vmap becomes a single while_loop over a batched body_fun and cond_fun, meaning effectively that every loop in the batch executes for the same number of iterations. If different batches lead to vastly different iteration times, this can result in extra computation compared to executing individual while_loops in sequence."
        ],
        "link": "https://stackoverflow.com/questions/79660448/jax-vmapping-while-loop"
    },
    {
        "title": "Looking for an efficent JAX function to reconstruct an image from patches",
        "question": "I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.\nI am trying to reconstruct the original images from the patches. Here is vanilla python code that works:\nkernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)\n\ndef fwd(x):\n    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')\n\n    # return channels last\n    return jnp.transpose(xcv, [0,2,3,1])\n\npatches = fwd(bfrc)\n\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\n\n# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT\n# H_PATCHES == IMG_WIDTH // PATCH_WIDTH\n\nreconstructed = np.zeros(EXPECTED_IMG_SHAPE)\n\nfor vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):\n    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):\n        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):\n            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):\n                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):\n                    row = vpatch * PATCH_HEIGHT + prow\n                    col = hpatch * PATCH_WIDTH + pcol\n                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]\n\n# This assert passes\nassert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0\nOf course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?",
        "answers": [
            "I'm not sure what happened here:\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\nbut I assume it's some kind of mistake.\nAssuming bfrc has shape of (batch, channels, height, width), and\nV_PATCHES = IMG_HEIGHT // PATCH_HEIGHT\nH_PATCHES = IMG_WIDTH // PATCH_WIDTH\nthen patch_reshaped_pn_c_h_w will have the shape of (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH).\nKeeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.\nV, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape\n\nH_total = V * PH\nW_total = H * PW\n\npatches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)\n\nreconstructed = patches.reshape(V, H, PH, PW, C)\nreconstructed = reconstructed.transpose(0, 2, 1, 3, 4)\nreconstructed = reconstructed.reshape(H_total, W_total, C)\nreconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)\nYou can additionally wrap it into @jax.jit, which should be slightly faster."
        ],
        "link": "https://stackoverflow.com/questions/79647350/looking-for-an-efficent-jax-function-to-reconstruct-an-image-from-patches"
    },
    {
        "title": "Would using lists rather than jax.numpy arrays lead to more accurate numerical transformations?",
        "question": "I am doing a project with RNNs using jax and flax and I have noticed some behavior that I do not really understand.\nMy code is basically an optimization loop where the user provides the initial parameters for the system they want to optimize. This system is divided onto several time steps. He feeds the initial input into the first time step of the the system, gets a certain output, feeds this output into a RNN which returns the parameters for the following time step and so on. Then it is optimized using adam (particularly using optax).\nNow the user inputs his initial parameters as a dict and then there is a function called prepare_parameters_from_dict that basically converts this dict into a list of lists (or a list of jnp arrays for that matter).\nMy question/observation is when I make this function return a list of jnp.arrays instead of a list of lists, the property I am optimizing is an order of magnitude worse!\nFor example, using a list of lists outputs 0.9997 and a list of jnp.arrays outputs 0.998 (the closer to one the better).\nNoting: the RNN output a list of jnp.arrays (it is using flax linnen) and everything in the code remains the same.\nHere are said function:\nOutputing list of lists:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(flat_params)\n        shapes.append(len(flat_params))\n    return res, shapes\nUsing list of jnp.arrays:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(jnp.array(flat_params))\n        shapes.append(jnp.array(flat_params).shape[0])\n    return res, shapes\nand this is an example of the users input initial params:\ninitial_params = {\n    \"param1\": {\n        \"gamma\": 0.1,\n        \"delta\": -3 * jnp.pi / 2,\n    }\n}\nThe rest of the code remains exactly the same for both.\nAfter optimization if for example there were five time steps, this is how the final optimized params for each time step would look like:\nusing list of jnp.arrays:\n[[Array([ 0.1       , -4.71238898], dtype=float64)],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nusing list of lists:\n[[ [0.1       , -4.71238898] ]],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nWould such a difference in behavior be due to how jax handles grad and jit and others with lists compared to jnp.arrays or am I missing something?",
        "answers": [
            "The main operative difference between these two cases is that Python floats are treated as weakly-typed, meaning that the list version of your code could result in operations being performed at a lower precision. For example:\nIn [1]: import jax\n\nIn [2]: import jax.numpy as jnp\n\nIn [3]: jax.config.update('jax_enable_x64', True)\n\nIn [4]: list_values = [0.1, -4.71238898]\n\nIn [5]: array_values = jax.numpy.array(list_values)\n\nIn [6]: x = jax.numpy.float32(1.0)\n\nIn [7]: x + list_values[1]\nOut[7]: Array(-3.712389, dtype=float32)\n\nIn [8]: x + array_values[1]\nOut[8]: Array(-3.71238898, dtype=float64)\nNotice that the array version leads to higher-precision computations in this case. If I had to guess what the main difference is in your two runs, I'd guess something to do with the precision implied by strict vs weak types."
        ],
        "link": "https://stackoverflow.com/questions/79634990/would-using-lists-rather-than-jax-numpy-arrays-lead-to-more-accurate-numerical-t"
    },
    {
        "title": "How to select between using a `jax.lax.scan` vs a `for` loop when using JAX?",
        "question": "I am a JAX beginner and someone experienced with JAX told me that if we have repeated calls to a scan/for loop (e.g. when these are themselves wrapped by another for loop), it might be better to leave the loop as a for instead of converting it to a scan because the for loop is unrolled completely and only has the 1-time huge compilation cost while the scan is not unrolled by default and even though its compilation cost will be small, the fact that it is rolled will mean that the cost of repeatedly running this loop will end up making the scan more expensive than the for. This did not strike me immediately when I started writing my code, but made sense upon thinking about it.\nSo, I tested this assumption using code based on the following pseudo-code (the full code is really long and I hope these relevant parts I provide here are easier to understand):\nfor i in range(num_train_steps):  # Currently fixed to be a for loop\n  for j in range(num_env_steps):  # Currently fixed to be a for loop\n    act()\n\ndef act():\n  for k in range(num_algo_iters):  # Currently playing around with making this one either a scan or a for loop\n    jax.lax.scan(rollout_func)  # Currently fixed to be a scan\nThe only loop in the above code that I tested switching between scan and for was the k loop and then I varied the variable num_env_steps to be 1, 100, 1000 and 10000 to see whether increasing the number of times the act() (and thus the k loop) was executed made a difference to the timing. (The testing was done with 5 iterations for the k for loop and 2 iterations for the innermost scan although these are variable in general, if that matters.) The times taken for act() for the different repeats were 1.5, 11.3,, 99.0, 956.2 seconds for the scan version and 5.1, 14.5, 103.6, 972.7 seconds for the for version. So the for version never ended up faster for the number of repeats I tried.\nSo, now I am wondering if for any number of repeats (i.e. num_env_steps), the unrolling of the for actually makes the program faster than with scan. My questions:\nWould maybe increasing the repeats even more by setting num_env_steps to 100k or 1 million make it faster or can we always just replace a for with a scan? I have this question because I wonder if I am trying to over-optimise my code by converting every for to a scan.\nIf I set unroll = True for the scan, would it then always be fine to replace all fors with scans and expect speed-ups?\nIs there a rule of thumb that can help me decide when to use for and when to use scan if I am only interested in such speed-ups?\nact was jitted by the way.",
        "answers": [
            "scan vs for loop is essentially a tradeoff between compilation cost and runtime cost.\nJAX unrolls Python control flow, meaning that a for loop with 100 iterations leads to a single linear program with 100 copies of the loop body. The benefit of this is that it leaves the compiler free to optimize code across loop iterations, e.g. fusing operations between one iteration and the next; or noticing that one output is unused and eliding every computation in its graph. The downside is that compilation cost grows super-linearly with the size of the program, so for loops with large loop bodies and/or many iterations can lead to very long compilation times.\nWith scan or fori_loop on the other hand, the looping logic is pushed into the HLO, and the loop body is only parsed and compiled once. This results in much more efficient compilation, but may leave some runtime performance on the table compared to a for loop, because the compiler has fewer degrees of freedom to work with.\nThe best option will depend on the details of your program, and the relative importance of runtime and compile time costs in your particular application. Speaking very generally, though: for a smaller loop body with fewer iterations, for loops are often the better choice. For a larger loop body with more iterations, scan / fori_loop is likely better.\nNote also that scan has an unroll parameter that gives you the ability to tune the tradeoff between these extremes: unroll=True is effectively equivalent to a for loop, while unroll=n for 1 < n < N_iterations effectively puts a small for loop within each step of the larger scan."
        ],
        "link": "https://stackoverflow.com/questions/79633608/how-to-select-between-using-a-jax-lax-scan-vs-a-for-loop-when-using-jax"
    },
    {
        "title": "JAX Point Cloud Processing: Slow index_points_3d operation causing extreme XLA fusion loops in backpropagation",
        "question": "I'm trying to use JAX for implementing point cloud processing. However, I found that training becomes extremely slow due to my implementation of the following index_points_3d operation, which performs selection of features based on 3D indices.\nHere's my current implementation:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    features_expanded = features[..., None, :]\n    idx_expanded = indices[..., None]\n    return jnp.take_along_axis(features_expanded, idx_expanded, axis=1)\nWhen I traced the profiler, I found that this operation triggers extreme repetitions of loop_dynamic_update_slice_fusion, loop_add_fusion, input_reduce_fusion, and loop_select_fusion in the backpropagation stage as in following.\nThe forward pass is not a problem since the learning went fast when I stopped the gradient of the output features.\nI've tried different implementations such as using vmap on the batch dimension, but failed to achieve any performance gains.\nI'm not deeply familiar with JAX's low-level operations, so I'm unsure if this is a fundamental limitation of JAX/XLA or if there's a more efficient approach. Any help or guidance on optimizing this operation would be greatly appreciated!",
        "answers": [
            "Thanks to jakevdp's comment, I got a significant speedup using one-hot matrix multiplication. I changed to the following code:\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    B, N, C = features.shape\n    _, S, K = indices.shape\n    one_hot = jax.nn.one_hot(indices, num_classes=N, dtype=features.dtype)\n    return jnp.einsum('bskn,bnc->bskc', one_hot, features)"
        ],
        "link": "https://stackoverflow.com/questions/79631678/jax-point-cloud-processing-slow-index-points-3d-operation-causing-extreme-xla-f"
    },
    {
        "title": "Why some nested python functions are defined as `def _():`",
        "question": "I understand internal functions are prefixed with '_' to indicate they are helper/internal functions. It also helps with tooling etc. But I find some functions with just '_' as their name. Can't even find where they are called from. e.g., from\nhttps://github.com/jax-ml/jax/blob/7412adec21c534f8e4bcc627552f28d162decc86/jax/_src/pallas/mosaic/helpers.py#L72\ndef run_on_first_core(core_axis_name: str):\n  \"\"\"Runs a function on the first core in a given axis.\"\"\"\n  num_cores = jax.lax.axis_size(core_axis_name)\n  if num_cores == 1:\n    return lambda f: f()\n\n  def wrapped(f):\n    core_id = jax.lax.axis_index(core_axis_name)\n\n    @pl_helpers.when(core_id == 0)\n    @functools.wraps(f)\n    def _(): ## How is this called?\n      return f()\n\n  return wrapped\nThere are several of them in an internal code base but here are some references\nhttps://github.com/search?q=repo%3Ajax-ml%2Fjax%20def%20_()%3A&type=code\nhttps://github.com/jax-ml/jax/blob/7412adec21c534f8e4bcc627552f28d162decc86/docs/pallas/tpu/distributed.ipynb#L1125",
        "answers": [
            "A name of _ is different from a name prefixed with _. A name that is only _ means, by convention, \"I need to supply a name to satisfy the syntax, but I don't actually need to use the name\"*. That would be the case here, since the _ is never actually used anywhere.\nIn terms of how this function is actually called, the when decorator appears to be here:\ndef when(condition):\n  def _wrapped(f):\n    if isinstance(condition, bool):\n      if condition:\n        f()\n    else:\n      jax.lax.cond(condition, f, lambda: None)\n  return _wrapped\nYou can see that the decorator has a handle on the function via f, and calls it internally if condition is satisfied.\n* I could have sworn that this convention comes from PEP8, but I've skimmed the document twice now, and can't find where it says it."
        ],
        "link": "https://stackoverflow.com/questions/79625948/why-some-nested-python-functions-are-defined-as-def"
    },
    {
        "title": "Why is array manipulation in JAX much slower?",
        "question": "I'm working on converting a transformation-heavy numerical pipeline from NumPy to JAX to take advantage of JIT acceleration. However, I’ve found that some basic operations like broadcast_to and moveaxis are significantly slower in JAX—even without JIT—compared to NumPy, and even for large batch sizes like 3,000,000 where I would expect JAX to be much quicker.\n### Benchmark: moveaxis + broadcast_to ###\nNumPy: moveaxis + broadcast_to → 0.000116 s\nJAX: moveaxis + broadcast_to → 0.204249 s\nJAX JIT: moveaxis + broadcast_to → 0.054713 s\n\n### Benchmark: broadcast_to only ###\nNumPy: broadcast_to → 0.000059 s\nJAX: broadcast_to → 0.062167 s\nJAX JIT: broadcast_to → 0.057625 s\nAm I doing something wrong? Are there better ways of performing these kind of manipulations?\nHere's a minimal benchmark ChatGPT generated, comparing broadcast_to and moveaxis in NumPy, JAX, and JAX with JIT:\nimport timeit\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit\n\n# Base transformation matrix\nM_np = np.array([[1, 0, 0, 0.5],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1]])\n\nM_jax = jnp.array(M_np)\n\n# Batch size\nn = 1_000_000\n\nprint(\"### Benchmark: moveaxis + broadcast_to ###\")\n\n# NumPy\nt_numpy = timeit.timeit(\n    lambda: np.moveaxis(np.broadcast_to(M_np[:, :, None], (4, 4, n)), 2, 0),\n    number=10\n)\nprint(f\"NumPy: moveaxis + broadcast_to → {t_numpy:.6f} s\")\n\n# JAX\nt_jax = timeit.timeit(\n    lambda: jnp.moveaxis(jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)), 2, 0).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: moveaxis + broadcast_to → {t_jax:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_and_move_jax(M):\n    return jnp.moveaxis(jnp.broadcast_to(M[:, :, None], (4, 4, n)), 2, 0)\n\n# Warm-up\nbroadcast_and_move_jax(M_jax).block_until_ready()\n\nt_jit = timeit.timeit(\n    lambda: broadcast_and_move_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: moveaxis + broadcast_to → {t_jit:.6f} s\")\n\nprint(\"\\n### Benchmark: broadcast_to only ###\")\n\n# NumPy\nt_numpy_b = timeit.timeit(\n    lambda: np.broadcast_to(M_np[:, :, None], (4, 4, n)),\n    number=10\n)\nprint(f\"NumPy: broadcast_to → {t_numpy_b:.6f} s\")\n\n# JAX\nt_jax_b = timeit.timeit(\n    lambda: jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: broadcast_to → {t_jax_b:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_only_jax(M):\n    return jnp.broadcast_to(M[:, :, None], (4, 4, n))\n\nbroadcast_only_jax(M_jax).block_until_ready()\n\nt_jit_b = timeit.timeit(\n    lambda: broadcast_only_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: broadcast_to → {t_jit_b:.6f} s\")",
        "answers": [
            "There are a couple things happening here that come from the different execution models of NumPy and JAX.\nFirst, NumPy operations like broadcasting, transposing, reshaping, slicing, etc. typically return views of the original buffer. In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies. I suspect this is the largest contribution to the timing difference here.\nSecond, NumPy tends to have very fast dispatch time for individual operations. JAX has much slower dispatch time for individual operations, and this can become important when the operation itself is very cheap (like \"return a view of the array with different strides/shape\")\nYou might wonder given these points how JAX could ever be faster than NumPy. The key is JIT compilation of sequences of operations: within JIT-compiled code, sequences of operations are fused so that the output of each individual operation need not be allocated (or indeed, need not even exist at all as a buffer of intermediate values). Additionally, for JIT compiled sequences of operations the dispatch overhead is paid only once for the whole program. Compare this to NumPy where there's no way to fuse operations or to avoid paying the dispatch cost of each and every operation.\nSo in microbenchmarks like this, you can expect JAX to be slower than NumPy. But for real-world sequences of operations wrapped in JIT, you should often find that JAX is faster, even when executing on CPU.\nThis type of question comes up enough that there's a section devoted to it in JAX's FAQ: FAQ: is JAX faster than NumPy?\nAnswering the followup question:\nIs the statement \"In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies\", within a jitted environment?\nThis question is not really well-formulated, because in a jitted environment, array objects do not necessarily correspond to buffers of values. Let's make this more concrete with a simple example:\nimport jax\n\n@jax.jit\ndef f(x):\n  y = x[::2]\n  return y.sum()\nYou might ask: in this program, is y a copy or a view of x? The answer is neither, because y is never explicitly created. Instead, JIT fuses the slice and the sum into a single operation: the array x is the input, and the array y.sum() is the output, and the intermediate array y is never actually created.\nYou can see this by printing the compiled HLO for this function:\nx = jax.numpy.arange(10)\nprint(f.lower(x).compile().as_text())\nHloModule jit_f, is_scheduled=true, entry_computation_layout={(s32[10]{0})->s32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.9 (Arg_0.10: s32[], Arg_1.11: s32[]) -> s32[] {\n  %Arg_0.10 = s32[] parameter(0), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  %Arg_1.11 = s32[] parameter(1), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  ROOT %add.12 = s32[] add(s32[] %Arg_0.10, s32[] %Arg_1.11), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\n%fused_computation (param_0.2: s32[10]) -> s32[] {\n  %param_0.2 = s32[10]{0} parameter(0)\n  %iota.0 = s32[5]{0} iota(), iota_dimension=0, metadata={op_name=\"jit(f)/jit(main)/iota\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.1 = s32[] constant(2)\n  %broadcast.0 = s32[5]{0} broadcast(s32[] %constant.1), dimensions={}\n  %multiply.0 = s32[5]{0} multiply(s32[5]{0} %iota.0, s32[5]{0} %broadcast.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %bitcast.1 = s32[5,1]{1,0} bitcast(s32[5]{0} %multiply.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %gather.0 = s32[5]{0} gather(s32[10]{0} %param_0.2, s32[5,1]{1,0} %bitcast.1), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}, indices_are_sorted=true, metadata={op_name=\"jit(f)/jit(main)/gather\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.0 = s32[] constant(0)\n  ROOT %reduce.0 = s32[] reduce(s32[5]{0} %gather.0, s32[] %constant.0), dimensions={0}, to_apply=%region_0.9, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\nENTRY %main.14 (Arg_0.1: s32[10]) -> s32[] {\n  %Arg_0.1 = s32[10]{0} parameter(0), metadata={op_name=\"x\"}\n  ROOT %gather_reduce_fusion = s32[] fusion(s32[10]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\nThe output is complicated, but the main thing to look at here is the ENTRY %main section, which is the \"main\" program generated by compilation. It consists of two steps: %Arg0.1 identifies the input argument, and ROOT %gather_reduce_fusion is essentially a single compiled kernel that sums every second element of the input. No intermediate arrays are generated. The blocks above this (e.g. the %fused_computation (param_0.2: s32[10]) -> s32[] definition) give you information about what operations are done within this kernel, but represent a single fused operation.\nNotice that the sliced array represented by y in the Python code never actually appears in the main function block, so questions about its memory layout cannot be answered except by saying \"y doesn't exist in the compiled program\".",
            "According to the Jax Docs (emphasis mine):\nif you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead"
        ],
        "link": "https://stackoverflow.com/questions/79615872/why-is-array-manipulation-in-jax-much-slower"
    },
    {
        "title": "Freezing filtered parameter collections with Flax.nnx",
        "question": "I'm trying to work out how to do transfer learning with flax.nnx. Below is my attempt to freeze the kernel of my nnx.Linear instance and optimize the bias. I think maybe I'm not correctly setting up the 'wrt' argument to my optimizer.\nfrom jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\ndef f(x,m=2.234,b=-1.123):\n    return m*x+b\n\ndef compute_loss(model, inputs, obs):\n    prediction = model(inputs)\n    error = obs - prediction\n    loss = jnp.mean(error ** 2)\n    mae = jnp.mean(jnp.abs(error ) )\n    return loss, mae\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    model.kernel.value = jnp.array([[2.0]]) #load pretrained kernel  \n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 ) \n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n    \n    loss_grad = nnx.value_and_grad(compute_loss, has_aux = True)\n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None,),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( optimizer, x, obs ):\n        (loss,mae), grads = loss_grad( optimizer.model, x, obs )        \n        optimizer.update( grads )\n        return optimizer, (loss,mae)\n\n    transfer_params = nnx.All(nnx.PathContains(\"bias\"))\n    optimizer_transfer = nnx.Optimizer(model, optax.adam(learning_rate=1e-3), wrt = transfer_params)\n\n    optimizer, (losses,maes) = optimizer_scan( optimizer_transfer, x1, obs1 )\n\n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = optimizer.model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', optimizer.model.kernel.value )\n    print( 'b approximation:', optimizer.model.bias.value )\nAnd this results in the following error:\nValueError: Mismatch custom node data: ('bias', 'kernel') != ('bias',); value: State({\n  'bias': VariableState(\n    type=Param,\n    value=Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=1/0)>\n  )\n}).",
        "answers": [
            "The missing link for me was nnx.DiffState. For clarification on DiffState, see the documentation for nnx.grad() on the nnx \"transforms\" page:\nhttps://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html\nAnyway, effectively the only changes that need be made for the code to work as intended are:\nMove the declaration of transfer_params to before the value_and_grad call,\nCreate an nnx.DiffState object diff_state = nnx.DiffState(0,transfer_params)\nGive diff_state as the argnums keyword for nnx.value_and_grad.\nAnd that does it!\nAnother helpful example of how to use nnx.DiffState with parameter filtering can be found here:\nhttps://github.com/google/flax/issues/4167\nAnd lastly here is the complete fixed example:\nfrom jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\ndef f(x,m=2.234,b=-1.123):\n    return m*x+b\n\ndef compute_loss(model, inputs, obs):\n    prediction = model(inputs)\n    error = obs - prediction\n    loss = jnp.mean(error ** 2)\n    mae = jnp.mean(jnp.abs(error ) )\n    return loss, mae\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    model.kernel.value = jnp.array([[2.0]]) #load pretrained kernel\n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 ) \n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n\n    transfer_params = nnx.All(nnx.PathContains(\"bias\"))\n    diff_state = nnx.DiffState(0,transfer_params)\n    \n    loss_grad = nnx.value_and_grad(compute_loss, argnums = diff_state, has_aux = True)\n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None,),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( optimizer, x, obs ):\n        (loss,mae), grads = loss_grad( optimizer.model, x, obs )        \n        optimizer.update( grads )\n        return optimizer, (loss,mae)\n\n    optimizer_transfer = nnx.Optimizer(model, optax.adamw(learning_rate = 1e-3), wrt = transfer_params)\n\n    optimizer, (losses,maes) = optimizer_scan( optimizer_transfer, x1, obs1 )\n\n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = optimizer.model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', optimizer.model.kernel.value )\n    print( 'b approximation:', optimizer.model.bias.value )"
        ],
        "link": "https://stackoverflow.com/questions/79580101/freezing-filtered-parameter-collections-with-flax-nnx"
    },
    {
        "title": "DIfference in variable values in jax non-jit runtime and jit transformed runtime",
        "question": "I have a deep learning mode which I am running in the jit transformed manner by:\nmy_function_checked = checkify.checkify(model.apply)\n    model_jitted = jax.jit(my_function_checked)\n    err, pred = model_jitted({\"params\": params}, batch, training=training, rng=rng)\n    err.throw()\nThe code is compiling fine, but now I want to debug the intermediate values after every few steps, save the arrays, and then compare them with pytorch tensors. For this, I need to repeatedly save the arrays. The easiest way to do this is to use any IDE's inbuilt debugger and evaluate the save expression after every few steps. But jax.jit transformed code doesn't allow external debuggers. But, I can do this after disabling the jit. Should I be expecting any discrepancies between the two runs? Can I assume that the values in jit and non-jit runs will remain same?",
        "answers": [
            "In general when comparing the same JAX operation with and without JIT, you should expect equivalence up to typical floating point rounding errors, but you should not expect bitwise equivalence, as the compiler may fuse operations in a way that leads to differing float error accumulation."
        ],
        "link": "https://stackoverflow.com/questions/79571227/difference-in-variable-values-in-jax-non-jit-runtime-and-jit-transformed-runtime"
    },
    {
        "title": "Reproducibility of JAX calculations",
        "question": "I am using JAX in running Reinforcement Learning (RL) & Multi-Agent Reinforcement Learning (MARL) calculations. I have noticed the following behaviour:\nIn RL, my results are always fully reproducible.\nIn MARL, where computations become significantly heavier, my results are reprodicible when running on CPU.\nHowever, when running MARL in GPU I encounter a different behaviour. I have noticed that repeating the same calculation within a script execution leads to identical results. However, executing the same script twice leads to different results. The latter problem is only mitigated when I use:\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_ops\"\nos.environ[\"JAX_DISABLE_MOST_FASTER_PATHS\"] = \"1\"\nUnfortunately, this measure significantly reduces the computation speed.\nAny idea about dealing with this issue?",
        "answers": [
            "On an accelerator like GPU, there will generally be a tradeoff between strict bit-wise reproducibility and speed of computation.\nWhy is this? Fundamentally, this is because of the fact that floating point arithmetic only approximates real arithmetic, and so the order in which operations are executed can change the results, and order of operations is a degree of freedom that the GPU can exploit to execute code faster.\nAs a simple example, consider summing the same array in different orders:\nIn [1]: import numpy as np\n\nIn [2]: rng = np.random.default_rng(0)\n\nIn [3]: x = rng.normal(size=10000).astype('float32')\n\nIn [4]: x.sum()\nOut[4]: np.float32(63.11888)\n\nIn [5]: x[::-1].sum()\nOut[5]: np.float32(63.118877)\nThe results differ slightly.\nThis is relevant to your question because of the way a GPU works: GPUs do fast vector operations by automatically running them in parallel. So, for example, to compute a sum, it might chunk the array across N cores, sum each chunk individually, and then accumulate the intermediate sums to get the final result.\nIf you only care mainly about speed, you can sacrifice reproducibility and accumulate those intermediate sums in the order they're ready, which might vary from run to run, and therefore produce slightly different results. If you care mainly about reproducibility, then you have to sacrifice some speed by ensuring that you accumulate those intermediate sums in exactly the same order every time, which may leave the process waiting for a slower chunk even if a faster chunk is already ready. This is a simplistic example but the same principal applies for any computation parallelized on a GPU.\nSo fundamentally speaking, there will always be a tradeoff between bitwise reproducibility and speed of computation. You've already discovered the primary flags for controlling this tradeoff (XLA_FLAGS=\"--xla_gpu_deterministic_ops\" and JAX_DISABLE_MOST_FASTER_PATHS=1 ). Your question seems to be \"can I somehow get both speed and strict bitwise reproducibility at once\": the answer to that question is No."
        ],
        "link": "https://stackoverflow.com/questions/79563698/reproducibility-of-jax-calculations"
    },
    {
        "title": "Flax nnx / jax: tree.map for layers of incongruent size",
        "question": "I am trying to figure out how to use nnx.split_rngs. Can somebody give a version of the code below that uses nnx.split_rngs with jax.tree.map to produce an arbitrary number of Linear layers with different out_features?\nimport jax\nfrom flax import nnx\nfrom functools import partial\n\nif __name__ == '__main__':\n\n    session_sizes = {\n        'a':2,\n        'b':3,\n        'c':4,\n        'd':5,\n        'e':6,\n    }\n    dz = 2\n\n    rngs = nnx.Rngs(0)\n    \n    my_linear = partial(\n        nnx.Linear,\n        use_bias = False,\n        in_features = dz,\n        rngs=rngs )\n    \n    def my_linear_wrapper(a):\n        return my_linear( out_features=a )\n\n    q_s = jax.tree.map(my_linear_wrapper, session_sizes)\n\n    for k in session_sizes.keys():\n        print(q_s[k].kernel)\nSo in this case, we would need a tree of layers that will take our 2 in_features into spaces of 2, ..., 6 out_features.\nThe function my_linear_wrapper is sort of a workaround for the original solution we had in mind, which is to map in very much the same fashion as we're doing, but instead use (something like) the @nnx.split_rngs function decorator.\nIs there a way to use nnx.split_rngs on my_linear in order to map over the rng argument to nnx.Linear?",
        "answers": [
            "split_rngs is mostly useful when you are going to pass the Rngs through a transform like vmap, here you want to produce variable sized Modules so the current solution is the way to go. Because of how partial works you can simplify this to:\ndin = 2\nrngs = nnx.Rngs(0)\n\nmy_linear = functools.partial(\n  nnx.Linear, din, use_bias=False, rngs=rngs\n)\n\nq_s = jax.tree.map(my_linear, session_sizes)\n\nfor k in session_sizes.keys():\n  print(q_s[k].kernel)"
        ],
        "link": "https://stackoverflow.com/questions/79551198/flax-nnx-jax-tree-map-for-layers-of-incongruent-size"
    },
    {
        "title": "Why is Jax treating floating point values as tracers rather than concretizing them when nesting jitted functions?",
        "question": "I am doing some physics simulations using jax, and this involves a function called the Hamiltonian defined as follows:\n# Constructing the Hamiltonian\n@partial(jit, static_argnames=['n', 'omega'])\ndef hamiltonian(n: int, omega: float):\n    \"\"\"Construct the Hamiltonian for the system.\"\"\"\n    H = omega *  create(n) @ annhilate(n)\n    return H \nand then a bigger function def solve_diff(n, omega, kappa, alpha0): that is defined as follows:\n@partial(jit, static_argnames=['n', 'omega'])\ndef solve_diff(n, omega, kappa, alpha0):\n    # Some functionality that uses kappa and alpha0\n    \n    H = hamiltonian(n, omega)\n\n    # returns an expectation value\nWhen I try to compute the gradient of this function using jax.grad\nn = 16   \nomega = 1.0   \nkappa = 0.1  \nalpha0 = 1.0 \n\n# Compute gradients with respect to omega, kappa, and alpha0\ngrad_population = grad(solve_diff, argnums=(1, 2, 3))\ngrads = grad_population(n, omega, kappa, alpha0)\n\nprint(f\"Gradient w.r.t. omega: {grads[0]}\")\nprint(f\"Gradient w.r.t. kappa: {grads[1]}\")\nprint(f\"Gradient w.r.t. alpha0: {grads[2]}\")\nit outputs the following error:\nValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jax._src.interpreters.ad.JVPTracer'>, Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n  primal = 1.0\n  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n    pval = (ShapedArray(float32[], weak_type=True), None)\n    recipe = LambdaBinding(). The error was:\nTypeError: unhashable type: 'JVPTracer'\nThough, running solve_diff(16,1.0,0.1,1.0) on its own works as expected.\nNow if I remove omega from the list of static variables for both the hamiltonian function and the solve_diff, the grad is output as expected.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls, both n and omega are constants and indeed should not change between function calls.",
        "answers": [
            "The fundamental issue is that you cannot differentiate with respect to a static variable, and if you try to do so you will get the error you observed.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls\nIn JAX, the term \"static\" does not have to do with whether the variable is changed between function calls. Rather, a static variable is a variable that does not participate in tracing, which is the mechanism used to compute transformations like vmap, grad, jit, etc. When you differentiate with respect to a variable, it is no longer static because it is participating in the autodiff transformation, and trying to treat it as static later in the computation will lead to an error.\nFor a discussion of transformations, tracing, and related concepts, I'd start with JAX Key Concepts: transformations."
        ],
        "link": "https://stackoverflow.com/questions/79550040/why-is-jax-treating-floating-point-values-as-tracers-rather-than-concretizing-th"
    },
    {
        "title": "General way to define JAX functions with non-differentiable arguments",
        "question": "For a particular JAX function func, one can define non-differentiable arguments by using the decorator @partial(jax.custom_jvp, nondiff_argnums=...). However, in order to make it work, one must also explicitly define the differentiation rules in a custom jvp function by using the decorator @func.defjvp. I'm wondering if there is a generic way to define non-differentiable arguments for any given func, without defining a custom jvp (or vjp) function? This will be useful when the differentiation rules are too complicated to write out.",
        "answers": [
            "In JAX's design, non-differentiated arguments are a property of the gradient transformation being used, not a property of the function being differentiated. custom_jvp is fundamentally about customizing the gradient behavior, and using it to mark non-differentiable arguments without actually customizing the gradient is not an intended use.\nThe way to ensure that arguments do not participate in an autodiff transformation is to specify the arguments you want to differentiate against when you call the jax.grad, jax.jacobian, or other autodiff transformation; e.g.\njax.grad(func, argnums=(0,))  # differentiate with respect to argument 0.\nRegardless of what func is, this will attempt to differentiate with respect to the 0th argument, and if that argument is either explicitly or implicitly not differentiable due to how func is defined, an error will be raised."
        ],
        "link": "https://stackoverflow.com/questions/79516990/general-way-to-define-jax-functions-with-non-differentiable-arguments"
    },
    {
        "title": "How can I apply member functions of a list of objects across slices of a JAX array using vmap?",
        "question": "I have a list of a objects, each of which has a function to be applied on a slice of a jax.numpy.array. There are n objects and n corresponding slices. How can I vectorise this using vmap?\nFor example, for the following code snippet:\nimport jax\nimport jax.numpy as jnp\n\nclass Obj:\n    def __init__(self, i):\n        self.i = i\n\n    def f1(self, x): return (x - self.i)\n\nx = jnp.arange(9).reshape(3, 3).astype(jnp.float32)\n\nfunctions_obj = [Obj(1).f1, Obj(2).f1, Obj(3).f1]\nhow would I apply the functions in functions_obj to slices of x?\nMore details, probably not relevant: My specific use-case is running the member functions of a lot of Reinforcement Learning Gym environment objects on slices of an actions array, but I believe my problem is more general and I formulated it as above. (P.S.: I know about AsyncVectorEnv by the way but that does not solve my problem as I am not trying to run the step function).",
        "answers": [
            "Use jax.lax.switch to select between the functions in the list and map over the desired axis of x at the same time:\ndef apply_func_obj(i, x_slice):\n    return jax.lax.switch(i, functions_obj, x_slice)\n\nindices = jnp.arange(len(functions_obj)) \n# Use vmap to apply the function element-wise\nresults = jax.vmap(apply_func_obj, in_axes=(0, 0))(indices, x)"
        ],
        "link": "https://stackoverflow.com/questions/79499056/how-can-i-apply-member-functions-of-a-list-of-objects-across-slices-of-a-jax-arr"
    },
    {
        "title": "Why does JAX's grad not always print inside the cost function?",
        "question": "I am new to JAX and trying to use it with PennyLane and optax to optimize a simple quantum circuit. However, I noticed that my print statement inside the cost function does not execute in every iteration. Specifically, it prints only once at the beginning and then stops appearing.\nThe quantum circuit itself does not make sense; I just wanted to simplify the example as much as possible. I believe the circuit is not actually relevant to the question, but it's included as an example.\nHere is my code:\nimport pennylane as qml\nimport jax\nimport jax.numpy as jnp\nimport optax\n\njax.config.update(\"jax_enable_x64\", True)\n\ndevice = qml.device(\"default.qubit\", wires=1)\n\n\n@qml.qnode(device, interface='jax')\ndef circuit(params):\n    qml.RX(params, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(params):\n    print('Evaluating')\n    return circuit(params)\n\n# Define optimizer\nparams = jnp.array(0.1)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\n\n# JIT the gradient function\ngrad = jax.jit(jax.grad(cost))\n\nfor epoch in range(5):\n    print(f'{epoch = }')\n    grad_value = grad(params)\n    updates, opt_state = opt.update(grad_value, opt_state)\n    params = optax.apply_updates(params, updates)\nExpected output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nEvaluating\nepoch = 3\nEvaluating\nepoch = 4\nEvaluating\nActual output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nepoch = 3\nepoch = 4\nQuestion:\nWhy is the print statement inside cost not executed after the first iteration? Is JAX caching the function call or optimizing it in a way that skips execution? How can I ensure that cost is evaluated in every iteration?",
        "answers": [
            "When working with JAX it is important to understand the difference between \"trace time\" and \"runtime\". For JIT compilation JAX does an abstract evaluation of the function when it is called first. This is used to \"trace\" the computational graph of the function and then create a fully compiled replacement, which is cached and then invoked on the next calls (\"runtime\") of the function. Now, Python's print statements are only evaluated at trace time and not at runtime, because the code of the function has been effectively replaced by a compiled version.\nFor the case of printing during runtime, JAX has a special jax.debug.print function, you can use:\ndef cost(params):\n    jax.debug.print('Evaluating')\n    return circuit(params)\nMore on the jax.debug utilities: https://docs.jax.dev/en/latest/debugging/index.html\nAnd JIT compilation: https://docs.jax.dev/en/latest/jit-compilation.html"
        ],
        "link": "https://stackoverflow.com/questions/79498911/why-does-jaxs-grad-not-always-print-inside-the-cost-function"
    },
    {
        "title": "Jax numpy extracting non-nan values gives NonConcreteBooleanIndexError",
        "question": "I have a jax 2d array with some nan-values\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\nand want to get an array which contains for each row only the non-nan values. The resulting array has thus the same number of rows, and either less columns or the same number but with nan values padded at the end. So in this case, the result should be\narray_2d = jnp.array([\n    [1,   2,      3],\n    [10  20,jnp.nan]\n    ])\nThe order (among non-nan values) should stay the same.\nTo make things easier, I know that each row has at most k (in this case 3) non-nan values. Getting the indices for the non-nan values is very easy, but ``moving them to the front'' is harder.\nI tried to work on a row-by-row basis; the following function works indeed:\n# we want to vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\nHowever, I can't vmap this. Even though I payed attention that the returned array always has the same size, the line ret_vals = row_vals[row_mask] makes problems, since this has a dynamic size. Does anyone know how to circumvent this? I believe that functions like `jnp.where' etc don't help either.\nHere is the full MWE:\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n# we want to get -- efficiently -- all non-nan values per row.\n# we know that each row has at most 3 non-nan values\n\n# we will vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\n\n# we now vmap\nnon_nan_vals = jax.vmap(get_non_nan_values)(array_2d) # this gives error: NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[5])\nNB: The array will be very large in practice and have many nan values, while k (the number of non-nan values) is on the order of 10 or 100.\nThank you very much!",
        "answers": [
            "By padding the array with a fill value at the end of each row first, you can rely on jnp.nonzero and its size and fill_value arguments, which define a fixed output size and fill value index, when the size requirement is not met. Here is a minimal example:\nimport jax.numpy as jnp\nimport jax\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n\n@jax.vmap\ndef get_non_nan_values(row_vals, size=3):\n    padded = jnp.pad(row_vals, (0, 1), constant_values=jnp.nan)\n    non_nan = jnp.nonzero(~jnp.isnan(padded), size=size, fill_value=-1)\n    return padded[non_nan]\n\nget_non_nan_values(array_2d)\nWhich returns:\nArray([[ 1.,  2.,  3.],\n       [10., 20., nan]], dtype=float32)\nI think this solution is a bit more compact and clearer in intend, however I have not checked the performance.\nI hope this helps!",
            "I think you can do what you want with this function, which rather than sorting the array (as I commented), sorts and masks the indices of the non-nan values:\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(1,))\ndef func(array, k=3):\n    m, n = array.shape[-2:]\n    indices = jnp.broadcast_to(jnp.arange(n)[None, :], (m, n))\n    sorted_masked_indices = jnp.sort(jnp.where(jnp.isnan(array), jnp.nan, indices))\n    array_rearranged = array[jnp.arange(m)[:, None], sorted_masked_indices.astype(int)]\n    return jnp.where(jnp.isnan(sorted_masked_indices), jnp.nan, array_rearranged)[:, :k]\nTest:\nimport numpy as np\nrng = np.random.default_rng(0)\nk = 3\n\na = rng.random((12, 6))\na[np.arange(12)[:, None], rng.integers(0, 6, (12, 6))] = np.nan\n\nprint(a)\nprint(func(a, k=k))\nGives:\n[[0.63696169        nan        nan 0.01652764 0.81327024        nan]\n [       nan 0.72949656        nan        nan 0.81585355        nan]\n [       nan 0.03358558        nan        nan        nan        nan]\n [0.29971189        nan        nan        nan        nan 0.64718951]\n [       nan        nan        nan 0.98083534        nan 0.65045928]\n [       nan        nan 0.13509651 0.72148834        nan        nan]\n [       nan 0.88948783 0.93404352 0.3577952         nan        nan]\n [       nan 0.33791123 0.391619   0.89027435        nan        nan]\n [       nan 0.83264415        nan        nan 0.87648423        nan]\n [0.33611706        nan        nan 0.79632427        nan 0.0520213 ]\n [       nan        nan 0.09075305 0.58033239        nan        nan]\n [       nan 0.94211311        nan        nan 0.62910815        nan]]\n[[0.6369617  0.01652764 0.8132702 ]\n [0.72949654 0.81585354        nan]\n [0.03358557        nan        nan]\n [0.29971188 0.6471895         nan]\n [0.9808353  0.6504593         nan]\n [0.1350965  0.72148836        nan]\n [0.88948786 0.9340435  0.3577952 ]\n [0.33791122 0.391619   0.89027435]\n [0.83264416 0.8764842         nan]\n [0.33611706 0.79632425 0.0520213 ]\n [0.09075305 0.5803324         nan]\n [0.9421131  0.62910813        nan]]",
            "With the stable=True option, argsort on a boolean array is guaranteed to preserve the relative order between True and False elements. So this should do the trick:\ndef get_non_nan_values(row_vals):\n    return row_vals[jnp.argsort(jnp.isnan(rowvals), stable=True)[:3]]\nHowever, for wide rows, sorting the entire row seems unnecessary when we already know there are only at most 3 non-nan values. So another simple approach using jax.lax.top_k:\ndef get_top_3_non_nan(row_vals):\n  return row_vals[jax.lax.top_k(~jnp.isnan(row_vals), 3)[1]]",
            "I would do this using vmap of argsort of isnan:\nimport jax\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n])\n\nresult = jax.vmap(lambda x: x[jnp.argsort(jnp.isnan(x))])(array_2d)\nprint(result)\n# [[ 1.  2.  3. nan nan]\n#  [10. 20. nan nan nan]]\nThis approach uses static shapes, and thus will be compatible with jit."
        ],
        "link": "https://stackoverflow.com/questions/79443943/jax-numpy-extracting-non-nan-values-gives-nonconcretebooleanindexerror"
    },
    {
        "title": "Problems when boolean indexing in Jax, getting NonConcreteBooleanIndexError",
        "question": "I'm currently trying to create a CustomProblem inheriting from the BaseProblem class in TensorNEAT which is a Jax based library. In trying to implement the evaluate function of this class, I'm using a boolean mask, but I have problems getting it to work. My code results in jax.errors.NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[n,n]) which I think is due to some of my arrays not having a definite shape. How do I circumvent this?\nConsider this example in np:\nimport numpy as np\n\nran_int = np.random.randint(1, 5, size=(2, 2))\nprint(ran_int)\n\nran_bool = np.random.randint(0,2, size=(2,2), dtype=bool)\nprint(ran_bool)\n\na = (ran_int[ran_bool]>0).astype(int)\nprint(a)\nIt could give an output like this:\n[[2 2]\n [3 4]]\n[[ True False]\n [ True  True]]\n[1 1 1] #Is 1D and has less elements than before boolean mask was applied!\nBut in Jax, the same way of thinking results in the NonConcreteBooleanIndexError error I got.\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #ERROR HAPPENS HERE\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #WOULD HAPPEN HERE TOO IF THIS PART WAS FIRST\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI was considering using jnp.where to solve the issue, but the resulting pairwise_labels and pairwise_predictions have a different shape than what I expect (namely (n, n)) as seen in the code below:\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) # shape (n, n)\n\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) # shape (n, n)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n ,n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI fear that the differing shapes of pairwise_predictions and pairwise_labels after using jnp.where will result in a different loss than if I had just used the boolean mask as I would in np. There is also the fact that I get another error that happens later in the pipeline with the output ValueError: max() iterable argument is empty from line 143 in the pipeline.py file of TensorNeat. This is curiously circumvented by changing pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold to pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold, which probably also results in some loss that is incorrect.\nBelow is some code that should be enough to setup a minimal running example that is similar to my setup:\nfrom tensorneat import algorithm, genome, common\nfrom tensorneat.pipeline import Pipeline\nfrom tensorneat.genome.gene.node import DefaultNode\nfrom tensorneat.genome.gene.conn import DefaultConn\nfrom tensorneat.genome.operations import mutation\nimport jax, jax.numpy as jnp\nfrom tensorneat.problem import BaseProblem\n\ndef binary_cross_entropy(prediction, target):\n    return -(target * jnp.log(prediction) + (1 - target) * jnp.log(1 - prediction))\n\n# Define the custom Problem\nclass CustomProblem(BaseProblem):\n\n    jitable = True  # necessary\n\n    def __init__(self, inputs, labels, threshold):\n        self.inputs = jnp.array(inputs) #nb! already has shape (n, 768)\n        self.labels = jnp.array(labels).reshape((-1,1)) #nb! has shape (n), must be transformed to have shape (n, 1) \n        self.threshold = threshold\n\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold #this is the thing I actually want\n        #pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold #weird fix to circumvent ValueError: max() iterable argument is empty when using jnp.where for pairwise_labels and pairwise_predictions\n        print(pairs_to_keep.shape)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape)\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\n\n    @property\n    def input_shape(self):\n        # the input shape that the act_func expects\n        return (self.inputs.shape[1],)\n\n    @property\n    def output_shape(self):\n        # the output shape that the act_func returns\n        return (1,)\n\n    def show(self, state, randkey, act_func, params, *args, **kwargs):\n        # showcase the performance of one individual\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(state, params, self.inputs)\n\n        loss = jnp.mean(jnp.square(predict - self.labels))\n\n        n_elements = 5\n        if n_elements > len(self.inputs):\n            n_elements = len(self.inputs)\n\n        msg = f\"Looking at {n_elements} first elements of input\\n\"\n        for i in range(n_elements):\n            msg += f\"for input i: {i}, target: {self.labels[i]}, predict: {predict[i]}\\n\"\n        msg += f\"total loss: {loss}\\n\"\n        print(msg)\n\nalgorithm = algorithm.NEAT(\n    pop_size=10,\n    survival_threshold=0.2,\n    min_species_size=2,\n    compatibility_threshold=3.0,  \n    species_elitism=2,  \n    genome=genome.DefaultGenome(\n        num_inputs=768,\n        num_outputs=1,\n        max_nodes=769,  # must at least be same as inputs and outputs\n        max_conns=768,  # must be 768 connections for the network to be fully connected\n        output_transform=common.ACT.sigmoid,\n        mutation=mutation.DefaultMutation(\n            # no allowing adding or deleting nodes\n            node_add=0.0,\n            node_delete=0.0,\n            # set mutation rates for edges to 0.5\n            conn_add=0.5,\n            conn_delete=0.5,\n        ),\n        node_gene=DefaultNode(),\n        conn_gene=DefaultConn(),\n    ),\n)\n\n\nINPUTS = jax.random.uniform(jax.random.PRNGKey(0), (100, 768)) #the input data x\nLABELS = jax.random.uniform(jax.random.PRNGKey(0), (100)) #the annotated labels y\n\nproblem = CustomProblem(INPUTS, LABELS, 0.25)\n\nprint(\"Setting up pipeline and running it\")\nprint(\"-----------------------------------------------------------------------\")\npipeline = Pipeline(\n    algorithm,\n    problem,\n    generation_limit=1,\n    fitness_target=1,\n    seed=42,\n)\n\nstate = pipeline.setup()\n# run until termination\nstate, best = pipeline.auto_run(state)\n# show results\npipeline.show(state, best)",
        "answers": [
            "The solution I got from the authors of TensorNEAT was to update the evaluate() function to use jnp.nan instead of -jnp.inf in the first jnp.where() calls used on pairwise_labels and pairwise_predictions. I also had to make the loss take into consideration the nan values that would be present in the loss after running the bce. The new evaluate() function that has the same behavior as boolean indexing is pasted below.\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold\n\n        #finding only the labels to keep\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, jnp.nan) #use jnp.nan here\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n\n        #finding only the predictions to keep\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, jnp.nan) #use jnp.nan here\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # loss with shape (len(labels), len(labels)), we need to reduce it to a scalar\n        loss = jnp.mean(loss, where=~jnp.isnan(loss)) #only use number values in loss\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss        \n        return -loss",
            "Yes, the mask operation makes the shape of the resulting array dependent on the content of the array. And jax only supports static shapes. The workaround you propose looks reasonable, with using the value -inf as a placeholder. The missing part is ignoring the zero entries in the mean. This you could achieve by a custom “masked” mean function along the lines of:\nfrom jax import numpy as jnp\nfrom jax import random\nimport jax\n\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (4, 4))\n\nkey, subkey = random.split(key)\nmask = random.bernoulli(key, 0.5, (4, 4))\n\n@jax.jit\ndef masked_mean(x, mask):\n    return jnp.sum(jnp.where(mask, x, 0), axis=0) / jnp.sum(mask, axis=0)\n\n\nmasked_mean(x, mask)\nI have not checked other parts of the code in detail, but e.g. the statement jnp.where(pairwise_labels > 0, True, False) has no effect. And with the masked mean you might not need the placeholder values at all.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79423352/problems-when-boolean-indexing-in-jax-getting-nonconcretebooleanindexerror"
    },
    {
        "title": "How to use jax.vmap with a tuple of flax TrainStates as input?",
        "question": "I am setting up a Deep MARL framework and I need to assess my actor policies. Ideally, this would entail using jax.vmap over a tuple of actor flax TrainStates. I have tried the following:\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.linen.initializers import constant, orthogonal\nfrom flax.training.train_state import TrainState\nimport optax\nimport distrax\n\nclass PGActor_1(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 4\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(128, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0))(x)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nclass PGActor_2(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 2\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nstate= jnp.zeros((1, 5))\n\nnetwork_1 = PGActor_1()\nnetwork_1_init_rng = jax.random.PRNGKey(42)\nparams_1 = network_1.init(network_1_init_rng, state)\n\nnetwork_2 = PGActor_2()\nnetwork_2_init_rng = jax.random.PRNGKey(42)\nparams_2 = network_2.init(network_2_init_rng, state)\n\ntx = optax.chain(\noptax.clip_by_global_norm(1),\noptax.adam(lr=1e-3)\n)\nactor_trainstates= (\n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_1),             \n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_2)\n )\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(actor_trainstates)\nbut I recieve the following error:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nDoes anybody have any idea how to make this work?\nThank you in advance.",
        "answers": [
            "This is quite similar to other questions (e.g. Jax - vmap over batch of dataclasses). The key point is that JAX transformations like vmap require data in a struct of arrays pattern, whereas you are using an array of structs pattern.\nTo work directly with an array of structs pattern in JAX, you can use Python's built-in map function – due to JAX's asynchronous dispatch, the resulting operations will be executed in parallel where possible:\npis = map(lambda x: x.apply_fn(x.params, state), actor_trainstates)\nHowever, this doesn't take advantage of the automatic vectorization done by vmap. In order to do this, you can convert your data from an array of structs to a struct of arrays, although this requires that all entries have the same structure.\nFor compatible cases, the solution would look something like this, however it errors for your data:\ntrain_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(train_states_soa)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-36-da904fa40b9c> in <cell line: 0>()\n----> 1 train_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\n\nValueError: Dict key mismatch; expected keys: ['Dense_0', 'Dense_1', 'Dense_2']\nThe problem is that your two train states do not have matching structure, and so they cannot be transformed into a single struct of arrays. You can see the difference in structure by inspecting the params:\nprint(actor_trainstates[0].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1', 'Dense_2'])\nprint(actor_trainstates[1].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1'])\nThere is no way to use vmap in a context where your inputs have different structure, so you'll either have to change the problem to ensure the same structure, or stick with the map approach."
        ],
        "link": "https://stackoverflow.com/questions/79405049/how-to-use-jax-vmap-with-a-tuple-of-flax-trainstates-as-input"
    },
    {
        "title": "Serialization in JAX",
        "question": "What is the recommended way to do serialization/deserialization in JAX?\nIn the context of reinforcement learning my starting point in terms of data might be e.g. match replays that have to be pre-processed to obtain tuples of JAX arrays. This is a process that I would like to do just once, save to disk, then wrap around that a data loading interface like grain.DataLoader.\nBut I have no idea how to do the actual serialization.\nRight now I'm doing\ndef save_jax(path, x: jnp.array):\n    y = np.array(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.array(x)\n    return y\nIt works. My hope is there won't be any copies in wrapping jnp->np or the other way around, and then hopefully this will be mmapped.\nIs this approach bad for performance? What is a better way?",
        "answers": [
            "Both Jax and Numpy support the Python buffer protocol, allowing for zero-copy data sharing between the different types. That being said when using jnp.array or np.array, the default is to copy (see linked docs). So right now you do an unnecessary copy of the data. So I would suggest to use np.asarray instead (and the Jax equivalent), which only copies when needed. So your code would look like:\ndef save_jax(path, x: jnp.array):\n    y = np.asarray(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.asarray(x)\n    return y\nAside from the copy the native Numpy format might not be the best choice of format in neither I/O performance nor associated meta data. For a lightweight alternative you might want to look for example into safetensors.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79387670/serialization-in-jax"
    },
    {
        "title": "How to do jittable masked get?",
        "question": "How do I do a jax get from a masked index?\nThe code below works without jit.\nx = jnp.arange(25).reshape((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n    [1,2],\n])\ncoords_mask = jnp.array([True, True, False, True])\n\n@jax.jit\ndef masked_gather(x, coords, coords_mask):\n    coords_masked = coords[coords_mask]\n    return x.at[coords_masked[:, 0], coords_masked[:, 1]].get()\n\nmasked_gather(x, coords, coords_mask)\nFails with NonConcreteBooleanIndexError.\nShould return Array([ 7, 13,  7], dtype=int32)",
        "answers": [
            "There is no way to execute this function in a JIT-compatible way, because JAX does not support compilation of programs with dynamic shapes. In your case, the size of the returned array depends on the number of True elements in coords_mask, and so the shape is dynamic by definition.\nSee JAX Sharp Bits: Dynamic Shapes for more information.\nDepending on what you are doing with the resulting value, there are a number of available approaches to work around this: for example, if the shape is truly unknown, you could return an array padded with zeros; it might look something like this:\n@jax.jit\ndef masked_gather_padded(x, coords, coords_mask, fill_value=0):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='fill', fill_value=fill_value)\n  return result[order]\n\nmasked_gather_padded(x, coords, coords_mask)\n# Array([ 7, 13,  7,  0], dtype=int32)\nAlternatively, if the number of True entries in the mask is known a priori, you could modify the function to accept a static size argument and use that to construct an appropriate output. It might look something like this:\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['size'])\ndef masked_gather_with_size(x, coords, coords_mask, *, size):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='drop')\n  return result[order[:size]]\n\nmasked_gather_with_size(x, coords, coords_mask, size=3)\n# Array([ 7, 13,  7], dtype=int32)\nThe best approach will depend on your application."
        ],
        "link": "https://stackoverflow.com/questions/79375141/how-to-do-jittable-masked-get"
    },
    {
        "title": "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?",
        "question": "Is it possible to use vmap for auto-batching if your function isn't jittable?\nI have a function that's not jittable:\ndef testfunc(model, x1, x2, x2_mask):\n    ( ... non-jittable stuff with masks ... )\nI'm trying to wrap it in vmap so I can benefit from auto-batching as explained here.\nSo I do:\ntestfunc_batched = jax.vmap(testfunc, in_axes=(None, 0, 0, 0))\nThe intention is that in batched mode, each of x1, x2, and x2_mask will have an additional outter dimension, the batching dimension. The model shouldn't be treated differently in batched mode hence the None. Let me know if the syntax isn't right.\nI create batches of size one just to test, schematically:\nx1s = x1.reshape(1, ...)\nx2s = x2.reshape(1, ...)\nx2_masks = x2_mask.reshape(1, ...)\n\ntestfunc_batched(model, x1s, x2s, x2_masks)\nThe last line fails with ConcretizationTypeError.\nI've recently learned that stuff with masks makes functions not jittable. But does that mean that I also can't use vmap? Or am I doing something wrong?\n(There is further context in How to JIT code involving masked arrays without NonConcreteBooleanIndexError?, but you don't have to read that question to understand this one.)",
        "answers": [
            "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?\nNo. In general, functions which are incompatible with jit will also be incompatible with vmap, because both jit and vmap use the same JAX tracing mechanism to transform the program."
        ],
        "link": "https://stackoverflow.com/questions/79374152/is-it-possible-to-use-jax-vmap-for-auto-batching-if-your-function-isnt-jittable"
    },
    {
        "title": "Count onto 2D JAX coordinates of another 2D array",
        "question": "I have\nx = jnp.zeros((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\nI would like to count onto x how many times each of the individual (x,y) coordinates appear in coords. In other words, obtain the output:\nArray([[0., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI've tried x.at[coords].add(1) and this gives me:\nArray([[0., 0., 0., 0., 0.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI understand what it's doing, but not how to make it do the thing I want.\nThere's this related question[1], but I haven't been able to use it to solve my problem.\n[1] Update JAX array based on values in another array",
        "answers": [
            "For multiple indices, you should pass a tuple of index arrays:\nx = x.at[coords[:, 0], coords[:, 1]].add(1)\nprint(x)\n[[0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]",
            "The generalized operation is basically computing a histogram, especially when the coordinate arrays are float values. So depending on the context the code is used, the following alternative might communicate the intent a bit more clearly:\nfrom jax import numpy as jnp\n\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\n\nbins = jnp.arange(5 + 1) - 0.5 \nx, _ = jnp.histogramdd(coords, bins=(bins, bins))\nIt will also handle if coordinates are out of bounds. But I presume under the hood, it does the same operation as at[...].add(1). So I would not expect any relevant difference in performance."
        ],
        "link": "https://stackoverflow.com/questions/79370053/count-onto-2d-jax-coordinates-of-another-2d-array"
    },
    {
        "title": "Return a different class based on an optional flag in the arguments without factory",
        "question": "I am implementing a series of classes in Equinox to enable taking derivatives with respect to the class parameters. Most of the time, the user will be instantiating class A and using the fn function to generate some data, the details of which are unimportant. However, in cases where we are interested in gradients, it is beneficial to represent param_c in terms of a sigmoid function to ensure that it remains clamped in the range (0,1). However, I don't want the user to notice a difference in how the class behaves if they do this. As such, I implement another class A_sigmoid that has param_c as a property and use A_abstract to ensure that both classes inherit the fn method, which will call param_c in its logic. While I could simply have the user instantiate an A_sigmoid object with a _param_c_sigmoid instead of param_c I don't want to force the user to have to make this distinction. Rather, I would want them to pass in the same kwargs dictionary no matter the class and have conversion happen behind the scenes. I also wanted to make it so that when making a new A one could simply pass an optional flag to direct the program to use the sigmoid version of the code. To do so, I implemented the following MWE:\nclass A_abstract(eqx.Module):\n    param_a: jax.Array\n    param_b: jax.Array\n    param_c: eqx.AbstractVar[jax.Array]\n    \n    def fn(self,*args,**kwargs):\n        pass\n\nclass A_sigmoid(A_abstract):\n    _param_c_sigmoid: jax.Array\n\n    @property\n    def param_c(self):\n        return 1 / (1 + jnp.exp(-self._param_c_sigmoid))\n\nclass A(A_abstract):\n    param_c: jax.Array\n\n    def __new__(cls, **kwargs):\n        sigmoid_flag = kwargs.pop('use_sigmoid_c',False)\n        if sigmoid_flag == True:\n            param_c = kwargs.pop('param_c')\n            _param_c_sigmoid = jnp.log(param_c / (1 - param_c))\n            kwargs['_param_c_sigmoid'] = _param_c_sigmoid\n            instance = A_sigmoid.__new__(A_sigmoid)\n            instance.__init__(**kwargs)\n            print(type(instance))\n            return instance\n        else:\n            return super(A,cls).__new__(cls)\n\nclassA = A(param_a = 1.,param_b = 2.,param_c = 0.5,use_sigmoid_c=True)\nprint(type(classA))\nThe code correctly says that instance has type A_sigmoid when print is called in the __new__ method. However, when I print type(classA), it is of type A and has no attribute param_c, though it does have a value for _param_c_sigmoid. Why is this the case? Am I missing something in my use of __new__ that is causing this error? While I know that in principle a factory would be the best way to do this, there are other classes of types B, C, etc. that don't have this need for a sigmoid implementation and that I would like to behave exactly the same way as A to enable them to be easily swapped. Thus, I don't want some custom method to instantiate A that would be different from calling the default constructor on the other classes.\nI am running this on a Jupyter notebook with the following package versions:\nPython           : 3.12.4\nIPython          : 8.30.0\nipykernel        : 6.29.5\njupyter_client   : 8.6.3\njupyter_core     : 5.7.2",
        "answers": [
            "If you were using a normal class, what you did is perfectly reasonable:\nclass A_abstract:\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A_sigmoid'>\nHowever, eqx.Module includes a bunch of metaclass logic that overrides how __new__ works, and this seems to collide with the __new__ overrides that you're making. Notice here the only difference is that A_abstract inherits from eqx.Module, and the result is A rather than A_sigmoid:\nimport equinox as eqx\n\nclass A_abstract(eqx.Module):\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A'>\nI dug-in for a few minutes to try and find the exact cause of this change, but wasn't able to pin it down.\nIf you're trying to do metaprogramming during class construction, you'll have to modify it to work within the construction-time metaprogramming that equinox is already doing."
        ],
        "link": "https://stackoverflow.com/questions/79359839/return-a-different-class-based-on-an-optional-flag-in-the-arguments-without-fact"
    },
    {
        "title": "Trying to install an older version of Jax",
        "question": "Trying to add a specific version of jax and jaxlib\npip install -U jaxlib==0.4.10          \nERROR: Ignored the following yanked versions: 0.4.32\nERROR: Could not find a version that satisfies the requirement jaxlib==0.4.10 (from versions: 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.24, 0.4.25, 0.4.26, 0.4.27, 0.4.28, 0.4.29, 0.4.30, 0.4.31, 0.4.33, 0.4.34, 0.4.35, 0.4.36, 0.4.38)\nERROR: No matching distribution found for jaxlib==0.4.10\nLooks like my old app needs jax to be '<=0.4.10'\nNot sure how to move forward",
        "answers": [
            "From the error message you're seeing, I suspect you're using Python 3.12. The first jaxlib release to support Python 3.12 was v0.4.17. If you want to use an older jaxlib version, you'll have to install an older version of Python.\njaxlib v0.4.10 supports Python v3.8-3.11; one way to see this is to look at the available wheel files on PyPI for this version: https://pypi.org/project/jaxlib/0.4.10/#files"
        ],
        "link": "https://stackoverflow.com/questions/79333553/trying-to-install-an-older-version-of-jax"
    },
    {
        "title": "How Can I Use GPU to Accelerate Image Augmentation?",
        "question": "When setting up image augmentation pipelines using keras.layers.Random* or other augmentation or processing methods, we often integrate these pipelines with a data loader, such as the tf.data API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.\nTo address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?",
        "answers": [
            "We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in keras. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.\nClassification\nLet’s take a classification task as an example. If we use the tf.data API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.\nimport numpy as np\nfrom keras import layers\n\na = np.ones((4, 224, 224, 3)).astype(np.float32)\nb = np.ones((4, 2)).astype(np.float32)\n\naugmentation_layers = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.2),\n    ]\n)\n\ndataset = tf.data.Dataset.from_tensor_slices((a, b))\ndataset = dataset.batch(3, drop_remainder=True)\ndataset = dataset.map(\n    lambda x, y: (augmentation_layers(x), y), \n    num_parallel_calls=tf.data.AUTOTUNE\n)\nx.shape, y.shape\n(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))\nBut for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.\ninputs = keras.Input(shape=(224, 224, 3))\nprocessed = augmentation_layers(inputs)\nbackbone = keras.applications.EfficientNetB0(\n    include_top=True, pooling='avg'\n)(processed)\noutput = keras.layers.Dense(10)(backbone)\nmodel = keras.Model(inputs, output)\nmodel.count_params() / 1e6\n5.340581\nHere, we set the augmentation pipeline right after keras.Input. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.\nSegmentation\nI'll use this dataset for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using keras-3, which might allow for multi-backend support.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # torch, jax\n\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nkeras.__version__ # 3.4.1\n# ref https://keras.io/examples/vision/oxford_pets_image_segmentation/\n# u-net model\ndef get_model(img_size, num_classes, classifier_activation):\n    ...\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(\n        num_classes, \n        3, \n        activation=classifier_activation, \n        padding=\"same\", \n        dtype='float32'\n    )(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\nimg_size = (224, 224)\nnum_classes = 1\nclassifier_activation = 'sigmoid'\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nLet's define the augmentation pipelines.\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\")\n]\n\ndef augment_data(images, masks):\n    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = tf.cast(combined[..., 3:], tf.int32)\n    return images_augmented, masks_augmented\nLet’s define the tf.data API to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    \n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = tf.cast(image, tf.int32)\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.\n        \n    return image\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask  = read_image(mask_list, mask=True)\n    return image, mask\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE) \n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n    # Augmenting on CPU\n    dataset = dataset.map(\n        augment_data, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nprint(\"Train Dataset:\", train_dataset)\nTrain Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))>\nNow, let's compile it and run it.\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nmodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n\n%%time\nepochs = 2\nmodel.fit(\n    train_dataset, \n    epochs=epochs, \n)\nEpoch 1/2\n318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087\nEpoch 2/2\n318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338\nCPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s\nWall time: 1min 48s\nNext, we will remove the augmentation layers from the dataloader.\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE)\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nTo offload augmentation to the GPU, we’ll create a custom model class, override the train_step, and use the augment_data method that we defined earlier. Here's how to structure it:\nclass ExtendedModel(keras.Model):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def call(self, inputs):\n        return self.model(inputs)\n\n    def save(\n        self, filepath, \n        overwrite=True, \n        include_optimizer=True, \n        save_format=None, \n        add_loss=None, \n    ):\n        # Overriding this method will allow us to use the `ModelCheckpoint`\n        self.model.save(\n            filepath=filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            include_optimizer=include_optimizer,\n        )\nNow that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nemodel = ExtendedModel(model)\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nemodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n%%time\nepochs = 2\nemodel.fit(\n    train_dataset, \n    epochs=epochs, \n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            filepath='model.{epoch:02d}-{loss:.3f}.keras',\n            monitor='loss',\n            mode='min',\n            save_best_only=True\n        )\n    ]\n)\nEpoch 1/2\n318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748\nEpoch 2/2\n318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585\nCPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s\nWall time: 1min 29s\nSo, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around 18.35% improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.\nAs shown in the ExtendedModel class above, we override the save method, allowing the callbacks.ModelCheckpoint to save the full model. Inference can then be performed as shown below.\nloaded_model = keras.saving.load_model(\n    \"/kaggle/working/model.02-0.0585.keras\"\n)\nx, y = next(iter(train_dataset))\noutput = loaded_model.predict(x)\n1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step\nUpdate\nIn order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the augment_data that is used in ExtendedModel use the following backend agnostic keras.ops functions.\ndef augment_data(images, masks):\n    combined = keras.ops.concatenate(\n        [images, keras.ops.cast(masks, 'float32')], axis=-1\n    )\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')\n    return images_augmented, masks_augmented\nAdditionally, to make the pipeline flexible for all backend, we can update the ExtendedModel as follows. Now, this code can run with tensorflow, jax, and torch backends.\nclass ExtendedModel(keras.Model):\n    ...\n\n    def train_step(self, *args, **kwargs):\n        if keras.backend.backend() == \"jax\":\n            return self._jax_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"tensorflow\":\n            return self._tensorflow_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"torch\":\n            return self._torch_train_step(*args, **kwargs)\n\n    def _jax_train_step(self, state, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step(state, (x, y))\n\n    def _tensorflow_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def _torch_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    ..."
        ],
        "link": "https://stackoverflow.com/questions/79327723/how-can-i-use-gpu-to-accelerate-image-augmentation"
    },
    {
        "title": "jax and flax not playing nicely with each other",
        "question": "I want to implement a neural network with multiple LSTM gates stacked one after the other.I set the hidden states to 0, as suggested here. When I try to run the code, I get\nJaxTransformError: Jax transforms and Flax models cannot be mixed. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.JaxTransformError)\nWhen I try to replace jax.lax.scan by flax.linen.scan, it gives another error. Not quite sure how to proceed or what's actually going wrong here. Code attached below. Thanks!\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom typing import Sequence\n\n\nclass LSTMModel(nn.Module):\nlstm_hidden_size: int\nnum_lstm_layers: int\nlinear_layer_sizes: Sequence[int]\nmean_aggregation: bool\n\ndef initialize_carry(self, batch_size, feature_size=1):\n    \"\"\"Initialize carry states with zeros for all LSTM layers.\"\"\"\n    return [\n        (\n            # Hidden state (h)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n            # Cell state (c)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n        )\n        for _ in range(self.num_lstm_layers)\n    ]\n\n@nn.compact\ndef __call__(self, x, carry=None):\n    if carry is None:\n        raise ValueError(\n            \"Carry must be initialized explicitly using `initialize_carry`.\"\n        )\n\n    # Expand 2D input to 3D (if necessary)\n    if x.ndim == 2:\n        # [batch_size, sequence_length] -> [batch_size, sequence_length, 1]\n        x = jnp.expand_dims(x, axis=-1)\n\n    # Process through LSTM layers\n    for i in range(self.num_lstm_layers):\n        lstm_cell = nn.LSTMCell(\n            features=self.lstm_hidden_size, name=f'lstm_cell_{i}')\n\n        def step_fn(carry, xt):\n            new_carry, yt = lstm_cell(carry, xt)\n            return new_carry, yt\n\n        # Use lax.scan to process the sequence\n        carry[i], outputs = jax.lax.scan(step_fn, carry[i], x)\n        x = outputs  # Update x for the next layer\n\n    # Aggregate outputs\n    if self.mean_aggregation:\n        x = jnp.mean(x, axis=1)  # Average over the sequence\n    else:\n        x = x[:, -1, :]  # Use the last output\n\n    # Pass through linear layers\n    for size in self.linear_layer_sizes:\n        x = nn.Dense(features=size)(x)\n        x = nn.elu(x)\n\n    # Final output layer\n    x = nn.Dense(features=1)(x)\n    return x\n\n\n# Model hyperparameters\nlstm_hidden_size = 64\nnum_lstm_layers = 2\nlinear_layer_sizes = [32, 16]\nmean_aggregation = False\n\n# Initialize model\nmodel = LSTMModel(\n    lstm_hidden_size=lstm_hidden_size,\n    num_lstm_layers=num_lstm_layers,\n    linear_layer_sizes=linear_layer_sizes,\n    mean_aggregation=mean_aggregation\n)\n\n# Dummy input: batch of sequences with 10 timesteps\nkey = jax.random.PRNGKey(0)\n# [batch_size, sequence_length, feature_size]\ndummy_input = jax.random.normal(key, (32, 10, 1))\n\n# Initialize carry states\ncarry = model.initialize_carry(\n    batch_size=dummy_input.shape[0], feature_size=dummy_input.shape[-1])\n\n# Initialize parameters\nparams = model.init(key, dummy_input, carry)\n\n# Apply the model\noutputs = model.apply(params, dummy_input, carry)\n\n# Should print: [batch_size, 1]\nprint(\"Model output shape:\", outputs.shape)",
        "answers": [
            "Consider using nn.RNN to simplify your code:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  name=f'lstm_cell_{i}'\n)\noutputs = lstm(x)\nRNN will handle the carries for you. If you really want to handle the carries yourself you could use return_carry and initial_carry:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  return_carry=True, \n  name=f'lstm_cell_{i}'\n)\ncarry[i], outputs = lstm(x, initial_carry=carry[i])"
        ],
        "link": "https://stackoverflow.com/questions/79266328/jax-and-flax-not-playing-nicely-with-each-other"
    },
    {
        "title": "Efficiently custom array creation routines in JAX",
        "question": "I'm still getting a handle of best practices in jax. My broad question is the following:\nWhat are best practices for the implementation of custom array creation routines in jax?\nFor instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):\nimport numpy as np\nimport jax.numpy as jnp\n\ndef ones_at_col(shape_mat, idx):\n    idxs = jnp.arange(shape_mat[1])[None,:]\n    mat = jnp.where(idx==idxs, 1, 0)\n    mat = jnp.repeat(mat, shape_mat[0], axis=0)\n    return mat\n\nshape_mat = (5,10)\n\nprint(ones_at_col(shape_mat, 5))\n\n%timeit np.zeros(shape_mat)\n\n%timeit jnp.zeros(shape_mat)\n\n%timeit ones_at_col(shape_mat, 5)\nThe output is\n[[0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]]\n127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nMy function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.\nBut then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?\nIs there something better I can do or am I pushing the limits of what is possible in jax?",
        "answers": [
            "The best way to do this is probably something like this:\nmat = jnp.zeros(shape_mat).at[:, 5].set(1)\nRegarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.\nIf you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):\n@partial(jax.jit, static_argnames=['shape_mat', 'idx'])\ndef ones_at_col(shape_mat, idx):\n  return jnp.zeros(shape_mat).at[:, idx].set(1)\nYou could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.\nAnother side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects."
        ],
        "link": "https://stackoverflow.com/questions/79256001/efficiently-custom-array-creation-routines-in-jax"
    },
    {
        "title": "How to handle PRNG splitting in a jax.vmap context?",
        "question": "I have a function which simulates a stochastic differential equation. Currently, without stochastic noise, my invokation of simulating the process up to time t looks like this (and, yeah, I need to use jax):\ndef evolve(u, t):\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\n\ndef simulate(x, t):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt), u)\nNow, the pain comes with the noise. I'm a C++-guy who only occasionally needs to use Python for research/scientific work. And I really don't understand how I need (or should) implement PRNG splitting here. I guess I would change evolve to\ndef evolve(u, t, key):\n    noise = jax.random.multivariate_normal(key, jax.numpy.zeros(d), covariance_matrix, shape = (n,))\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\nBut that will not work properly I guess. If I got it right, I need to use jax.random.split to split the key. Cause if I don't, I end up with correlated samples. But how and where do I need to split?\nAlso: I guess I would need to modify simulate to def simulate(x, t, key). But then, should simulate also return the modified key?\nAnd to make it even more complicated: I actually wrap simulate into a batch_simulate function which uses jax.vmap to process a whole batch of x's and t's. How do I pass the PRNG to that batch_simulate function, how do I pass it (and broadcast it) to jax.vmap and what should batch_forward return? At first glance, it seems to me that it would take a single PRNG and split it into many (due to the vmap). But what does the caller of batch_forward do then ...\nCompletely lost on this. Any help is highly appreciated!",
        "answers": [
            "If I understand your setup correctly, you should make both evolve and simulate accept a key, and within simulate, use fold_in to generate unique keys for the loop:\ndef evolve(u, t, key):\n    ...\n\ndef simulate(x, t, key):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt, jax.random.fold_in(key, i)), u)\nThen if you want to vmap over simulate, you can split the key and map over it:\nx_batch = ...  # your batched x inputs\nt_batch = ...  # your batched t inputs\nkey_batch = jax.random.split(key, x_batch.shape[0])\n\nbatch_result = jax.vmap(simulate)(x_batch, t_batch, key_batch)"
        ],
        "link": "https://stackoverflow.com/questions/79238188/how-to-handle-prng-splitting-in-a-jax-vmap-context"
    },
    {
        "title": "Hello World for jaxtyping?",
        "question": "I can't find any instructions or tutorials for getting started with jaxtyping. I tried the simplest possible program and it fails to parse. I'm on Python 3.11. I don't see anything on GitHub jaxtyping project about an upper bound (lower bound is Python 3.9) and it looks like it's actively maintained (last commit was 8 hours ago). What step am I missing?\njaxtyping==0.2.36\nnumpy==2.1.3\ntorch==2.5.1\ntypeguard==4.4.1\n(It seems like numpy is required for some reason even though I'm not using it)\nfrom typeguard import typechecked\nfrom jaxtyping import Float\nfrom torch import Tensor\n\n\n@typechecked\ndef matmul(a: Float[Tensor, \"m n\"], b: Float[Tensor, \"n p\"]) -> Float[Tensor, \"m p\"]:\n    \"\"\"\n    Matrix multiplication of two 2D arrays.\n    \"\"\"\n    raise NotImplementedError(\"This function is not implemented yet.\")\n(venv) dspyz@dspyz-desktop:~/helloworld$ python matmul.py \nTraceback (most recent call last):\n  File \"/home/dspyz/helloworld/matmul.py\", line 6, in <module>\n    @typechecked\n     ^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 221, in typechecked\n    retval = instrument(target)\n             ^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 72, in instrument\n    instrumentor.visit(module_ast)\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 598, in visit_Module\n    self.generic_visit(node)\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 498, in generic_visit\n    node = super().generic_visit(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 494, in generic_visit\n    value = self.visit(value)\n            ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 672, in visit_FunctionDef\n    with self._use_memo(node):\n  File \"/usr/lib/python3.11/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 556, in _use_memo\n    new_memo.return_annotation = self._convert_annotation(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 582, in _convert_annotation\n    new_annotation = cast(expr, AnnotationTransformer(self).visit(annotation))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in visit_Subscript\n    [self.visit(item) for item in node.slice.elts],\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in <listcomp>\n    [self.visit(item) for item in node.slice.elts],\n     ^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 474, in visit_Constant\n    expression = ast.parse(node.value, mode=\"eval\")\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n    return compile(source, filename, mode, flags,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<unknown>\", line 1\n    m p\n      ^\nSyntaxError: invalid syntax",
        "answers": [
            "(jaxtyping author here)\nSadly this is a known bug in typeguard v4. It's been around forever and hasn't been fixed. (At a technical level: typeguard v4 attempts to load and reparse the source code of your function, but it doesn't properly parse all type annotations.)\nI use typeguard==2.13.3 myself, which seems to be pretty robust.\nEDIT: removed some other suggested workarounds. These turned out not to, well, work. For now I just recommend pinning to that earlier version of typeguard.",
            "You are running into the issue reported here: https://github.com/patrick-kidger/jaxtyping/issues/80\nYou can work around this by installing typeguard version 3.0, but given how long this bug has remained open without any real fix, I suspect the best conclusion is that jaxtyping should no longer be considered compatible with typeguard."
        ],
        "link": "https://stackoverflow.com/questions/79201839/hello-world-for-jaxtyping"
    },
    {
        "title": "Why is JAX's jit compilation slower on the second run in my example?",
        "question": "I am new to using JAX, and I’m still getting familiar with how it works. From what I understand, when using Just-In-Time (JIT) compilation (jax.jit), the first execution of a function might be slower due to the compilation overhead, but subsequent executions should be faster. However, I am seeing the opposite behavior.\nIn the following code snippet:\nfrom icecream import ic\nimport jax\nfrom time import time\nimport numpy as np\n\n\n@jax.jit\ndef my_function(x, y):\n    return x @ y\n\n\nvectorized_function = jax.vmap(my_function, in_axes=(0, None))\n\nshape = (1_000_000, 1_000)\n\nx = np.ones(shape)\ny = np.ones(shape[1])\n\nstart = time()\nvectorized_function(x, y)\nt_1 = time() - start\n\nstart = time()\nvectorized_function(x, y)\nt_2 = time() - start\n\nprint(f'{t_1 = }\\n{t_2 = }')\nI get the following results:\nt_1 = 13.106784582138062\nt_2 = 15.664098024368286\nAs you can see, the second run (t_2) is actually slower than the first one (t_1), which seems counterintuitive to me. I expected the second run to be faster due to JAX’s JIT caching.\nHas anyone encountered a similar situation or have any insights into why this might be happening?\nPS: I know I could have done x @ y directly without invoking vmap, but this is an easy example just to test its behaviour. My actual code is more complex, and the difference in runtime is even bigger (around 8x slower). I hope this simple example works similar.",
        "answers": [
            "For general tips on running JAX microbenchmarks effectively, see FAQ: Benchmarking JAX code.\nI cannot reproduce the timings from your snippet, but in your more complicated case, I suspect you are getting fooled by JAX's Asynchronous dispatch, which means that the timing method you're using will not actually reflect the time taken by the underlying computation. To address this, you can wrap your results in jax.block_until_ready:\nstart = time()\nvectorized_function(x, y).block_until_ready()\nt_1 = time() - start"
        ],
        "link": "https://stackoverflow.com/questions/79192549/why-is-jaxs-jit-compilation-slower-on-the-second-run-in-my-example"
    },
    {
        "title": "Restoring flax model checkpoints using orbax throws ValueError",
        "question": "The following code blocks are being utlized to save the train state of the model during training and to restore the state back into memory.\nfrom flax.training import orbax_utils\nimport orbax.checkpoint\n\ndirectory_gen_path = \"checkpoints_loc\"\norbax_checkpointer_gen = orbax.checkpoint.PyTreeCheckpointer()\ngen_options = orbax.checkpoint.CheckpointManagerOptions(save_interval_steps=5, create=True)\ngen_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n    directory_gen_path, orbax_checkpointer_gen, gen_options\n)\n\ndef save_model_checkpoints(step_, generator_state, generator_batch_stats):\n\n    gen_ckpt = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    save_args_gen = orbax_utils.save_args_from_target(gen_ckpt)\n    gen_checkpoint_manager.save(step_, gen_ckpt, save_kwargs={\"save_args\": save_args_gen})\n\ndef load_model_checkpoints(generator_state, generator_batch_stats):\n    gen_target = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    latest_step = gen_checkpoint_manager.latest_step()\n    gen_ckpt = gen_checkpoint_manager.restore(latest_step, items=gen_target)\n    generator_state = gen_ckpt[\"model\"]\n    generator_batch_stats = gen_ckpt[\"batch_stats\"]\n\n    return generator_state, generator_batch_stats\nThe training of the model was done on a GPU and loading the state onto GPU device works fine, however, when trying to load the model to cpu, the following error is being thrown by the orbax checkpoint manager's restore method\nValueError: SingleDeviceSharding with Device=cuda:0 was not found in jax.local_devices().\nI'm not quite sure what could be the reason, any thoughts folks?\nUpdate: Updated to the latest version of orbax-checkpoint, 0.8.0 traceback changed to the following error\nValueError: sharding passed to deserialization should be specified, concrete and an instance of `jax.sharding.Sharding`. Got None",
        "answers": [
            "What version of orbax.checkpoint are you using?\nIt looks like this issue was fixed in https://github.com/google/orbax/issues/678 – you should update to the most recent version of orbax-checkpoint, and try running your code again. If that doesn't work, I'd suggest reporting the problem at https://github.com/google/orbax/issues/new"
        ],
        "link": "https://stackoverflow.com/questions/79162665/restoring-flax-model-checkpoints-using-orbax-throws-valueerror"
    },
    {
        "title": "Storing and jax.vmap() over Pytrees",
        "question": "I've ran into an issue with Jax that will make me rewrite an entire 20000-line application if I don't solve it.\nI have a non-ML application which relies on pytrees to store data, and the pytrees are deep - about 6-7 layers of data storage (class1 stores class2, and that stores an array of class3 etc.)\nI've used python lists to store pytrees and hoped to vmap over them, but turns out jax can't vmap over lists.\n(So one solution is to rewrite literally every single dataclass to be a structured array and work from there, possibly putting all 6-7 layers of data into one mega-array)\nIs there a way to avoid the rewrite? Is there a way to store pytree classes in a vmappable state so that everything works as before?\nI have my classes marked with flax.struct.dataclass if that helps.",
        "answers": [
            "jax.vmap is designed to work with a struct-of-arrays pattern, and it sounds like you have an array-of-structs pattern. From your description, it sounds like you have a sequence of nested structs that look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom flax.struct import dataclass\n\n@dataclass\nclass Params:\n  x: jax.Array\n  y: jax.Array\n\n\n@dataclass\nclass AllParams:\n  p: list[Params]\n\n\nparams_list = [AllParams([Params(4, 2), Params(4, 3)]),\n               AllParams([Params(3, 5), Params(2, 4)]),\n               AllParams([Params(3, 2), Params(6, 3)])]\nThen you have a function that you want to apply to each element of the list; something like this:\ndef some_func(params):\n  a, b = params.p\n  return a.x * b.y - b.x * a.y\n\n[some_func(params) for params in params_list]\n[4, 2, -3]\nBut as you found, if you try to do this with vmap, you get an error:\njax.vmap(some_func)(params_list)\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nThe issue is that vmap operates separately over each entry of the list or pytree you pass to it, not over the elements of the list.\nTo address this, you can often transform your data structure from an array-of-structs into a struct-of-arrays, and then apply vmap over this. For example:\nparams_array = jax.tree.map(lambda *vals: jnp.array(vals), *params_list)\nprint(params_array)\nAllParams(p=[\n  Params(x=Array([4, 3, 3], dtype=int32), y=Array([2, 5, 2], dtype=int32)),\n  Params(x=Array([4, 2, 6], dtype=int32), y=Array([3, 4, 3], dtype=int32))\n])\nNotice that rather than a list of structures, this is now a single structure with the batching pushed all the way down to the leaves. This is the \"struct-of-arrays\" pattern that vmap is designed to work with, and so vmap will work correctly:\njax.vmap(some_func)(params_array)\nArray([ 4,  2, -3], dtype=int32)\nNow, this assumes that every dataclass in your list has identical structure: if not, then vmap will not be applicable, because by design it must map over computations with identical structure."
        ],
        "link": "https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees"
    },
    {
        "title": "JIT: partial or with static argnums? Non hashable input, but hashable partial",
        "question": "I am a bit lost on what exactly going on and what option to choose. Let's go trough an example:\nimport jax\nfrom functools import partial\nfrom typing import List\n\ndef dummy(a: int, b: List[str]):\n    return a + 1\nAs b argument is mutable, jitting with static argnames will be failed:\nj_dummy = jax.jit(dummy, static_argnames=['b'])\nj_dummy(2, ['kek'])\nValueError: Non-hashable static arguments are not supported\nHowever, if we do partial: jp_dummy = jax.jit(partial(dummy, b=['kek'])), we aim the goal. Somehow, partial object is indeed has __hash__ method, so we can check it with hash(partial(dummy, b=['kek'])).\nSo, I am a bit lost here: how I should proceed in a bigger picture? Should I produce partial functions with whatever arguments and then jit them or should I try to maintain my arguments hashable? What are situations when one approach is better than other? Is there any drawbacks?",
        "answers": [
            "When you use static_argnames, the static values passed to the function become part of the cache key, so if the value changes the function is re-compiled:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, s):\n  return x * len(s)\n\nf_jit = jax.jit(f, static_argnames=['s'])\n\nprint(f_jit(2, \"abc\"))  # 6\nprint(f_jit(2, \"abcd\"))  # 8\nThis is why the static arguments must be hashable: their hash is used as the JIT cache key.\nOn the other hand, when you wrap a static argument via closure, its value does not affect the cache key, and so it need not be hashable. But since it's not part of the cache key, if the global value changes, it does not trigger a recompilation and so you may get unexpected results:\nf_closure = jax.jit(lambda x: f(x, s))\n\ns = \"abc\"\nprint(f_closure(2))  # 6\ns = \"abcd\"\nprint(f_closure(2))  # 6\nFor this reason, explicit static arguments can be safer. In your case, it may be best to change your list into a tuple, as tuples are hashable and can be used as explicit static arguments."
        ],
        "link": "https://stackoverflow.com/questions/79114391/jit-partial-or-with-static-argnums-non-hashable-input-but-hashable-partial"
    },
    {
        "title": "precision of JAX",
        "question": "I have a question regarding the precision of float in JAX. For the following code,\nimport numpy as np\nimport jax.numpy as jnp\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is:','%.60f' % np.arctan(10))\n\njnp.arctan(10) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\n\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is:','%.60f' % np.arctan(10+1e-7))\n\njnp.arctan(10+1e-7) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\njnp gave identical results for arctan(x) for a small change of input variable (1e-7), but np did not. My question is how to let jax.numpy get the right number for a small change of x?\nAny comments are appreciated.",
        "answers": [
            "JAX defaults to float32 computation, which has a relative precision of about 1E-7. This means that your two inputs are effectively identical:\n>>> np.float32(10) == np.float32(10 + 1E-7)\nTrue\nIf you want 64-bit precision like NumPy, you can enable it as discussed at JAX sharp bits: double precision, and then the results will match to 64-bit precision:\nimport jax\njax.config.update('jax_enable_x64', True)\n\nimport jax.numpy as jnp\nimport numpy as np\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is: ','%.60f' % np.arctan(10))\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is: ','%.60f' % np.arctan(10+1e-7))\njnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\nnp.arctan(10) is:  1.471127674303734700345103192375972867012023925781250000000000\njnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\nnp.arctan(10+1e-7) is:  1.471127675293833592107262120407540351152420043945312500000000\n(but please note that even the 64-bit precision used by Python and NumPy is only accurate to about one part in 10^16, so most of the digits in the representation you printed are inaccurate compared to the true arctan value)."
        ],
        "link": "https://stackoverflow.com/questions/79098013/precision-of-jax"
    },
    {
        "title": "jax register_pytree_node_class and register_dataclass returns non consistent datatype: list and tuple accordingly",
        "question": "I am writing custom class, which is basically a wrapper around list, with custom setitem method. I would like this class participate in jax.jit code, so during that I found a following problem: during jitting List field converted to tuple. However, this is case only when using\nregister_pytree_node_class When use register_dataclas , then List keep being list.\nI simplify example to highlight only this problem.\nimport jax\nfrom jax.tree_util import register_dataclass\nfrom jax.tree_util import register_pytree_node_class\nfrom functools import partial\nfrom dataclasses import dataclass\nfrom typing import List\n\n@partial(register_dataclass,\n         data_fields=['data'],\n         meta_fields=['shift'])\n@dataclass\nclass DecoratorFlatten:\n    data: List[int]\n    shift: int = 5\n\n@register_pytree_node_class\n@dataclass\nclass CustomFlatten:\n    data: List[int]\n    shift: int = 5\n\n    def tree_flatten(self):\n            children = self.data\n            aux_data = self.shift\n            return (children, aux_data)\n    \n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        obj = object.__new__(cls)\n        obj.data = children\n        setattr(obj, 'shift', aux_data)\n        return obj\nNow let's call a simple as this function over instances of this two class:\n@jax.jit\ndef get_value(a):\n    return a.data\ndf = DecoratorFlatten([0,1,2])\ncf = CustomFlatten([0,1,3])\nget_value(df), get_value(cf)\nIn first case we get list as output, but in second tuple. I thought maybe this is because of my implementation of the tree_flatten method, however:\ncf.tree_flatten()\nLeads to ([0, 1, 3], 5) as desirable.",
        "answers": [
            "In tree_unflatten, children is a tuple, and you are assigning this directly to obj.data. If you want it to be a list, you should use obj.data = list(children)."
        ],
        "link": "https://stackoverflow.com/questions/79093341/jax-register-pytree-node-class-and-register-dataclass-returns-non-consistent-dat"
    },
    {
        "title": "Batched matrix multiplication with JAX on GPU faster with larger matrices",
        "question": "I'm trying to perform batched matrix multiplication with JAX on GPU, and noticed that it is ~3x faster to multiply shapes (1000, 1000, 3, 35) @ (1000, 1000, 35, 1) than it is to multiply (1000, 1000, 3, 25) @ (1000, 1000, 25, 1) with f64 and ~5x with f32.\nWhat explains this difference, considering that on cpu neither JAX or NumPy show this behaviour, and on GPU CuPy doesn't show this behaviour?\nI'm running this with JAX: 0.4.32 on an NVIDIA RTX A5000 (and get similar results on a Tesla T4), code to reproduce:\nimport numpy as np\nimport cupy as cp\nfrom cupyx.profiler import benchmark\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng()\n\nx = np.arange(5, 55, 5)\nGPU timings:\ndtype = cp.float64\ntimings_cp = []\nfor i in range(5, 55, 5):\n    a = cp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n    b = cp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n    timings_cp.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ndtype = jnp.float64\ntimings_jax_gpu = []\nwith jax.default_device(jax.devices('gpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n        b = jnp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_gpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.gpu_times.mean() for i in timings_cp], label=\"CuPy\")\nplt.plot(x, [i.gpu_times.mean() for i in timings_jax_gpu], label=\"JAX GPU\")\nplt.legend()\nTimings with those specific shapes:\ndtype = jnp.float64\nwith jax.default_device(jax.devices('gpu')[0]):\n    a = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\n    func = jax.jit(lambda a, b: a@b)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\n\n    a = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\nGives\nf64:\n0.01453789699935913\n0.004859122595310211\n\nf32:\n\n0.005860503035545349\n0.001209742688536644\nCPU timings:\ntimings_np = []\nfor i in range(5, 55, 5):\n    a = rng.random((1000, 1000, 3, i))\n    b = rng.random((1000, 1000, i, 1))\n    timings_np.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ntimings_jax_cpu = []\nwith jax.default_device(jax.devices('cpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)))\n        b = jnp.array(rng.random((1000, 1000, i, 1)))\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_cpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.cpu_times.mean() for i in timings_np], label=\"NumPy\")\nplt.plot(x, [i.cpu_times.mean() for i in timings_jax_cpu], label=\"JAX CPU\")\nplt.legend()",
        "answers": [
            "The difference seems to come from the compiler emitting a kLoop fusion for smaller sizes, and a kInput fusion for larger sizes. You can read about the effect of these in this source comment: https://github.com/openxla/xla/blob/e6b6e61b29cc439350a6ad2f9d39535cb06011e5/xla/hlo/ir/hlo_instruction.h#L639-L656\nThe compiler likely uses some heuristic to choose between the two, and it appears that this heuristic is suboptimal at the boundary for your particular problem. You can see this by outputting the compiled HLO for your operation:\na = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\nHloModule jit__lambda_, is_scheduled=true, entry_computation_layout={(f64[1000,1000,3,25]{3,2,1,0}, f64[1000,1000,25,1]{3,2,1,0})->f64[1000,1000,3,1]{3,2,1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=\"a02cbfe0fda9d44e2bd23462363b6cc0\"}\n\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.7: f64[1000,1000,3,25], param_1.6: f64[1000,1000,25,1]) -> f64[1000,1000,3] {\n  %param_0.7 = f64[1000,1000,3,25]{3,2,1,0} parameter(0)\n  %param_1.6 = f64[1000,1000,25,1]{3,2,1,0} parameter(1)\n  %bitcast.28.5 = f64[1000,1000,25]{2,1,0} bitcast(f64[1000,1000,25,1]{3,2,1,0} %param_1.6)\n  %broadcast.2.5 = f64[1000,1000,3,25]{3,2,1,0} broadcast(f64[1000,1000,25]{2,1,0} %bitcast.28.5), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  %multiply.2.3 = f64[1000,1000,3,25]{3,2,1,0} multiply(f64[1000,1000,3,25]{3,2,1,0} %param_0.7, f64[1000,1000,3,25]{3,2,1,0} %broadcast.2.5)\n  %constant_4 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,25]{3,2,1,0} %multiply.2.3, f64[] %constant_4), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,25], Arg_1.2.0: f64[1000,1000,25,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,25,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,25]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %loop_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,25]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,25,1]{3,2,1,0} %Arg_1.2.0), kind=kLoop, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %loop_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\na = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.5: f64[1000,1000,3,35], param_1.2: f64[1000,1000,35,1]) -> f64[1000,1000,3] {\n  %param_0.5 = f64[1000,1000,3,35]{3,2,1,0} parameter(0)\n  %param_1.2 = f64[1000,1000,35,1]{3,2,1,0} parameter(1)\n  %bitcast.28.3 = f64[1000,1000,35]{2,1,0} bitcast(f64[1000,1000,35,1]{3,2,1,0} %param_1.2)\n  %broadcast.2.3 = f64[1000,1000,3,35]{3,2,1,0} broadcast(f64[1000,1000,35]{2,1,0} %bitcast.28.3), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  %multiply.2.1 = f64[1000,1000,3,35]{3,2,1,0} multiply(f64[1000,1000,3,35]{3,2,1,0} %param_0.5, f64[1000,1000,3,35]{3,2,1,0} %broadcast.2.3)\n  %constant_3 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,35]{3,2,1,0} %multiply.2.1, f64[] %constant_3), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,35], Arg_1.2.0: f64[1000,1000,35,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,35,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,35]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %input_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,35]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,35,1]{3,2,1,0} %Arg_1.2.0), kind=kInput, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %input_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\nHere's a script to observe this compiler decision with respect to size:\nfor size in range(10, 55, 5):\n  a = jnp.array(rng.random((1000, 1000, 3, size)), dtype=dtype)\n  b = jnp.array(rng.random((1000, 1000, size, 1)), dtype=dtype)\n  hlo_text = jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text()\n  print(f\"{size=} {'kLoop' in hlo_text=}\")\nsize=10 'kLoop' in hlo_text=True\nsize=15 'kLoop' in hlo_text=True\nsize=20 'kLoop' in hlo_text=True\nsize=25 'kLoop' in hlo_text=True\nsize=30 'kLoop' in hlo_text=True\nsize=35 'kLoop' in hlo_text=False\nsize=40 'kLoop' in hlo_text=False\nsize=45 'kLoop' in hlo_text=False\nsize=50 'kLoop' in hlo_text=False\nI don't have any suggestion beyond perhaps reporting this at https://github.com/openxla/xla; it may be that the compiler heuristic for choosing to emit kLoop vs. kInput needs some additional logic."
        ],
        "link": "https://stackoverflow.com/questions/79085795/batched-matrix-multiplication-with-jax-on-gpu-faster-with-larger-matrices"
    },
    {
        "title": "Computing gradient using JAX of a function that outputs a list of arrays",
        "question": "I have a function which returns a list of arrays, and I need to find its derivative with respect to a single parameter. For instance, let's say we have\ndef fun(x):\n...\nreturn [a,b,c]\nwhere a,b,c and d are multi-dimensional arrays (for example, 2 by 2 by 2 real arrays). Now I want to obtain [da/dx, db/dx, dc/dx]. By db/dx I mean I want to obtain derivative of each element in the a:222 array with respect to x, so da/dx, db/dx, dc/dx are all 222 arrays.\nThis is me using JAX differentiation for the first time, and most of the examples I find online are about functions that has scalar output.\nFrom my search, I understand one way to find this is basically get the gradient of each scalar in all these arrays one at a time (probably making it faster using vmap). Is there any other way that is faster? I think JAX.jacobian might do the trick, but I am having hard time finding its documentation to see what does the function does exactly. Any help is very much appreciated.\nNow, I have tried JAX.jacobian with simple examples, and it does give me the answer that I expect. This assures me a bit, but I would like to find official documentation or assurance from others that is the right way to do it, and it is doing what I expect it.",
        "answers": [
            "You can use jax.jacobian for what you describe. Here is an example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  a = jnp.full((2, 2), 2) * x\n  b = jnp.full((2, 2), 3) * x\n  c = jnp.full((2, 2), 4) * x\n  return [a, b, c]\n\nda_dx, db_dx, dc_dx = jax.jacobian(f)(1.0)\n\nprint(da_dx)\n# [[2. 2.]\n#  [2. 2.]]\n\nprint(db_dx)\n# [[3. 3.]\n#  [3. 3.]]\n\nprint(dc_dx)\n# [[4. 4.]\n#  [4. 4.]]\njax.jacobian is an alias of jax.jacrev, and you can find the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.jacrev.html"
        ],
        "link": "https://stackoverflow.com/questions/79025241/computing-gradient-using-jax-of-a-function-that-outputs-a-list-of-arrays"
    },
    {
        "title": "Modifying multiple dimensions of Jax array simultaneously",
        "question": "When using the jax_array.at[idx] function, I wish to be able to set values at both a set of specified rows and columns within the jax_array to another jax_array containing values in the same shape. For example, given a 5x5 jax array, I might want to set the values, jax_array.at[[0,3],:][:,[1,2]] to some 2x2 array of values. However, I am coming across an issue where the _IndexUpdateRef' object is not subscriptable. I understand the idea of the error (and I get a similar one when using 2 chained .at[]s), but I want to know if there is anyway to achieve the desired functionality within 1 line.",
        "answers": [
            "JAX follows the indexing semantics of NumPy, and NumPy's indexing semantics allow you to do this via broadcasted arrays of indices (this is discussed in Integer array indexing in the NumPy docs).\nSo for example, you could do something like this:\nimport jax.numpy as jnp\n\nx = jnp.zeros((4, 6), dtype=int)\ny = jnp.array([[1, 2],\n               [3, 4]])\ni = jnp.array([0, 3])\nj = jnp.array([1, 2])\n\n# reshape indices so they broadcast \ni = i[:, jnp.newaxis]\nj = j[jnp.newaxis, :]\n\nx = x.at[i, j].set(y)\nprint(x)\n[[0 1 2 0 0 0]\n [0 0 0 0 0 0]\n [0 0 0 0 0 0]\n [0 3 4 0 0 0]]\nHere the i index has shape (2, 1), and the j index has shape (1, 2), and via broadcasting rules they index a 2x2 noncontiguous subgrid of the array x, which you can then set to the contents of y in a single statement."
        ],
        "link": "https://stackoverflow.com/questions/78985089/modifying-multiple-dimensions-of-jax-array-simultaneously"
    },
    {
        "title": "Mapping Over Arrays of Functions in JAX",
        "question": "What is the most performant, idiomatic way of mapping over arrays of functions in JAX?\nContext: This GitHub issue shows a way to apply vmap to several functions using lax.switch. The example is reproduced below:\nfrom jax import lax, vmap\nimport jax.numpy as jnp\n\ndef func1(x):\n  return 2 * x\n\ndef func2(x):\n  return -2 * x\n\ndef func3(x):\n  return 0 * x\n\nfunctions = [func1, func2, func3]\nindex = jnp.arange(len(functions))\nx = jnp.ones((3, 5))\n\nvmap_functions = vmap(lambda i, x: lax.switch(i, functions, x))\nvmap_functions(index, x)\n# DeviceArray([[ 2.,  2.,  2.,  2.,  2.],\n#              [-2., -2., -2., -2., -2.],\n#              [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)\nMy specific questions are:\nIs this (currently) the most idiomatic way of mapping over arrays of functions in JAX?\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)",
        "answers": [
            "For the kind of operation you're doing, where the functions are applied over full axes of an array in a way that's known statically, you'll probably get the best performance via a simple Python loop:\ndef map_functions(functions: list[Callable[[Array], Array], x: Array) -> Array:\n  assert len(functions) == x.shape[0]\n  return jnp.array([f(row) for f, row in zip(functions, x)])\nThe method based on switch is designed for the more general case where the structure of the indices is not known statically.\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)\nvmap of switch is implemented via select, which will compute the output of each function for the full input array before selecting just the pieces needed to construct the output, so if the functions are expensive to compute, it may lead to longer runtimes."
        ],
        "link": "https://stackoverflow.com/questions/78980521/mapping-over-arrays-of-functions-in-jax"
    },
    {
        "title": "JAX TypeError: 'Device' object is not callable",
        "question": "I found a piece of JAX codes from few years ago.\nimport jax\nimport jax.random as rand\n\ndevice_cpu = None\n\ndef do_on_cpu(f):\n    global device_cpu\n    if device_cpu is None:\n        device_cpu = jax.devices('cpu')[0]\n\n    def inner(*args, **kwargs):\n        with jax.default_device(device_cpu):\n            return f(*args, **kwargs)\n    return inner\n\nseed2key = do_on_cpu(rand.PRNGKey)\nseed2key.__doc__ = '''Same as `jax.random.PRNGKey`, but always produces the result on CPU.'''\nand I call it with:\nkey = seed2key(42)\nBut it results in TypeError:\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 14\n---> 14 key = seed2key(42)\n\nFile ~/bert-tokenizer-cantonese/lib/seed2key.py:12, in do_on_cpu.<locals>.inner(*args, **kwargs)\n     11 def inner(*args, **kwargs):\n---> 12     with jax.default_device(device_cpu):\n     13         return f(*args, **kwargs)\n\nTypeError: 'Device' object is not callable\nI think the function has breaking changes after version upgrade.\nCurrent versions:\njax 0.4.31\njaxlib 0.4.31\n(latest version at the moment of writing)\nHow can I change the codes to avoid the error? Thanks.",
        "answers": [
            "This code works fine in all recent versions of JAX: jax.default_device is a configuration function designed to be used as a context manager.\nI can reproduce the error you're seeing if I add this to the top of your script:\njax.default_device = jax.devices('cpu')[0]  # wrong!\nI suspect you inadvertently executed something similar to this at some point earlier in your notebook session. Try restarting your notebook runtime and rerunning just your valid code."
        ],
        "link": "https://stackoverflow.com/questions/78951225/jax-typeerror-device-object-is-not-callable"
    },
    {
        "title": "Using Jax Jit on a method as decorator versus applying jit function directly",
        "question": "I guess most people familiar with jax have seen this example in the documentation and know that it does not work:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit  # <---- How to do this correctly?\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\nc.calc(3)  \n3 workarounds are mentioned, but it appears that applying jit as a function directly, rather than a decorator works fine as well. That is, JAX does not complain about not knowing how to deal with the CustomClass type of self:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  # No decorator here !\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\n6 # works fine!\nAlthough not documented (which it maybe should be?), this appears to function identical to marking self as static via @partial(jax.jit, static_argnums=0), in that changing self does nothing for subsequent calls, i.e.:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\n6\n6 # no update\nSo I originally assumed that decorators in general might just deal with self as a static parameter when applying them directly. Because the method might be saved to another variable with a specific instance (copy) of self. As a sanity check, I checked if non-jit decorators indeed do this as well, but this appears not to be the case, as the below non-jit \"decorated\" function happily deals with changes to self:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\n\ncustom = CustomClass(2, True)\ndecorated_calc = decorator(custom.calc)\nprint(decorated_calc(3))\ncustom.mul = False\nprint(decorated_calc(3))\n6\n3\nI saw some other questions about applying decorators directly as functions versus decorator style (e.g. here and here), and there it is mentioned there is a slight difference in the two versions, but this should almost never matter. I am left wondering what it is about the jit decorator that makes these versions behave so differently, in that JAX.jit cán deal with the self type if not in decorated style. If anyone has an answer, that would be much appreciated.",
        "answers": [
            "Decorators have nothing to do with static arguments: static arguments are a concept specific to jax.jit.\nBacking up, you should keep in mind that whenever jax.jit compiles a function, it caches the compilation artifact based on several quantites, including:\nthe ID of the function or callable being compiled\nthe static attributes of any non-static arguments, such as shape and dtype\nthe hash of any arguments marked static via static_argnums or static_argnames\nthe value of any global configurations that would affect outputs\nWith this in mind, let's examine this snippet:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\nthe reason that jitted_calc doesn't update when you update attributes of c is because nothing related to the cache key has changed: (1) the function ID is the same, (2) the shape and dtype of the argument is unchanged, (3) there are no static arguments, (4) no global configurations have changed. Thus the previous cached compilation artifact (with the previous value of mul) is executed again. This is the primary reason I didn't mention this strategy in the doc you linked to: it's rarely the behavior that users would want.\nThis approach of wrapping the bound method in JIT is incidentally similar to wrapping the method definition with @partial(jit, static_argnums=0), but the details are not the same: in the static_argnums version, self is marked as a static argument, and so its hash becomes part of the JIT cache. The default __hash__ method for a class is simply based on the ID of the instance, and so changing c.mul does not change the hash, and does not trigger re-compilation. You can see an example of how to rectify this under Strategy 2 in the doc you linked to: basically, define appropriate __hash__ and __eq__ methods for the class:\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def __hash__(self):\n    return hash((self.x, self.mul))\n\n  def __eq__(self, other):\n    return (isinstance(other, CustomClass) and\n            (self.x, self.mul) == (other.x, other.mul))\nIn your last example, you define this:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\nThis code does not use jax.jit at all. The fact that changes to c.mul lead to changes in outputs has nothing to do with decorator syntax, but rather has to do with the fact that there is no JIT cache in play here.\nI hope that's all clear!"
        ],
        "link": "https://stackoverflow.com/questions/78918066/using-jax-jit-on-a-method-as-decorator-versus-applying-jit-function-directly"
    },
    {
        "title": "Zero length error of non-zero length array",
        "question": "I'm writing environment for rl agent training.\nMy env.step method takes as action array with length 3\n    def scan(self, f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n\n    def step_env(\n        self,\n        key: chex.PRNGKey,\n        state: EnvState,\n        action: Union[int, float, chex.Array],\n        params: EnvParams,\n    ) -> Tuple[chex.Array, EnvState, jnp.ndarray, jnp.ndarray, Dict[Any, Any]]:\n        \n        c_action = jnp.clip(action,\n                          params.min_action, \n                          params.max_action)\n        \n        _, m1 = self.scan(self.Rx, 0, action[0])\n        _, m2 = self.scan(self.Rx, 0, action[1])\n        _, m3 = self.scan(self.Rx, 0, action[2])\nI vectorize the env.step using and then call it\nobsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(rng_step,\n                                                                                          env_state,\n                                                                                          action,\n                                                                                          env_params)\nBut I got error\nCell In[9], line 65, in PCJ1_0.scan(self, f, init, xs, length)\n     63 ys = []\n     64 print(xs)\n---> 65 for x in xs:\n     66     carry, y = f(carry, x)\n     67     ys.append(y)\n\n    [... skipping hidden 1 frame]\n\nFile ~/anaconda3/envs/jax/lib/python3.10/site-packages/jax/_src/lax/lax.py:1592, in _iter(tracer)\n   1590 def _iter(tracer):\n   1591   if tracer.ndim == 0:\n-> 1592     raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n   1593   else:\n   1594     n = int(tracer.shape[0])\n\nTypeError: iteration over a 0-d array\nHow is it possible? If I plot the action array in the scan function I got array with length 5 (I vectored env.step for 5 envs), the length!=0\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0",
        "answers": [
            "When you print your value, it gives this:\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0\nHere float32[] tells you that this is a tracer with dtype float32 and shape []: that is, your array is zero-dimensional within the context of the vmapped function.\nThe purpose of vmap is to efficiently map a function over an axis of an array, so that within the function evaluation the array has one less dimension than it does outside the vmapped context. You can see that this way:\n>>> import jax\n\n>>> def f(x):\n...  print(f\"{x.shape=}\")\n...  print(f\"{x=}\")\n...\n>>> x = jax.numpy.arange(4.0)\n\n>>> f(x)\nx.shape=(4,)\nx=Array([0., 1., 2., 3.], dtype=float32)\n\n>>> jax.vmap(f)(x)\nx.shape=()\nx=Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0., 1., 2., 3.], dtype=float32)\n  batch_dim = 0\nIf you're passing a 1D input into your function and you want to manipulate the full 1D array within your function (instead of evaluating the function element-by-element), then it sounds like you should remove the vmap."
        ],
        "link": "https://stackoverflow.com/questions/78838517/zero-length-error-of-non-zero-length-array"
    }
]