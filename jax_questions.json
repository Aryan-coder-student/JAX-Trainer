[
    {
        "title": "Subtlety in initializing attributes with methods in modules from the `equinox` `jax` library",
        "question": "I have the following code that defines an abstract class and its final subclasse. The two classes are both subclasses of the equinox.Module class, which registers class attributes as the leaves of a PyTree container.\n# === IMPORTS ===\nfrom abc import ABC, abstractmethod\nimport jax\nfrom jax.typing import ArrayLike\nimport jax.numpy as jnp\nimport equinox as eqx\nfrom quadax import quadgk\n\njax.config.update(\"jax_enable_x64\", True)\n\n\nclass MyClass(eqx.Module): # Works if I toggle to MyClass(ABC)\n\n    rtol = 1e-12\n    atol = 1e-12\n\n    param: ArrayLike\n\n    def __init__(self):\n        self.param = self._integral_moment(3) # Fails, but works if I toggle to something like \"self.param = self.func(1.)\"\n\n    @abstractmethod \n    def func(self, tau):\n        pass\n\n    def func_abs(self, tau):\n        return jnp.abs(self.func(tau))\n    \n    def _integral_moment(self, order): \n        return quadgk(self._integrand_moment, [0, jnp.inf], args=(order,), epsrel=self.rtol, epsabs=self.atol)[0]\n\n    def _integrand_moment(self, tau, order):\n        return self.func_abs(tau) * jnp.abs(tau)**order\n \n\nclass MySubClass(MyClass):\n\n    gamma: ArrayLike\n    kappa: ArrayLike\n    w0: ArrayLike\n\n    def __init__(self, gamma, kappa, w0):\n        self.gamma = jnp.asarray(gamma)\n        self.kappa = jnp.asarray(kappa) \n        self.w0 = jnp.asarray(w0)\n        super().__init__()\n\n    def func(self, tau):\n        return self.gamma * jnp.exp(-1j * self.w0 * tau) * jnp.exp(-self.kappa*jnp.abs(tau)/2)\n    \n\n# Test    \ntest = MySubClass(gamma=1., kappa=1., w0=1.)\ntest.param\nThis code produces the AttributeError message:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[21], line 52\n     48         return self.gamma * jnp.exp(-1j * self.w0 * tau) * jnp.exp(-self.kappa*jnp.abs(tau)/2)\n     51 # Test    \n---> 52 test = MySubClass(gamma=1., kappa=1., w0=1.)\n     53 test.param\n\n    [... skipping hidden 2 frame]\n\nCell In[21], line 45\n     43 self.kappa = jnp.asarray(kappa) \n     44 self.w0 = jnp.asarray(w0)\n---> 45 super().__init__()\n\nCell In[21], line 19\n     18 def __init__(self):\n---> 19     self.param = self._integral_moment(3)\n\n    [... skipping hidden 1 frame]\n\nCell In[21], line 29\n     28 def _integral_moment(self, order): \n---> 29     return quadgk(self._integrand_moment, [0, jnp.inf], args=(order,), epsrel=self.rtol, epsabs=self.atol)[0]\n...\n    659         and isinstance(out, types.MethodType)\n    660         and out.__self__ is self\n    661     ):\n\nAttributeError: 'MySubClass' object has no attribute 'param'\nThis error clearly comes from a restriction of the equinox.Module, since if I change the parent class to ABC, the code runs fine.\nFirst, I thought that maybe equinox did not allow me to use methods to initialize attributes. But if I use the func() method instead of the _integral_moment() method to initialize param, the code works fine.\nSo I just don't understand what is going on here. I thought it would be better to ask here before asking the developers at equinox.\nThis uses equinox version 0.13.1 with jax version 0.7.2.",
        "answers": [
            "The issue here is that when traced, eqx.Module attempts to access all the declared attributes of the Module, so the module cannot be traced before those attributes are created. Here's a simpler repro of the same problem:\nimport jax\nimport equinox as eqx\n\nclass MyClass(eqx.Module):\n    param: ArrayLike\n\n    def __init__(self):\n      self.param = jax.jit(self.func)()\n\n    def func(self):\n      return 4\n\nMyClass()  # AttributeError: 'MyClass' object has no attribute 'param'\nThe quadgk function traces its input, and since you call it before setting param, you get this error. With this issue in mind, you can fix your problem by setting the missing param to a placeholder value before you call a function that traces the object's methods:\nclass MyClass(eqx.Module):\n\n    ...\n\n    def __init__(self):\n        self.param = 0  # set to a placeholder to allow tracing\n        self.param = self._integral_moment(3)\n\n    ...",
            "To follow up on @jakevdp's answer, a completely equivalent but perhaps slightly more elegant way of systematically pre-empting this issue in equinox is to assign a value directly in the attribute definition:\nclass MyClass(eqx.Module):\n\n    ...\n\n    param: float = 0 # set to a placeholder to allow tracing\n    \n    def __init__(self):\n        self.param = self._integral_moment(3)\n\n    ...\nEDIT: Importantly, not that it is NOT allowed to initialize attributes as mutables or jax arrays at the class level in dataclasses like equinox modules, which raises a ValueError: Use default_factory. For the above code, all instances of the class will initially share the same instance object for the field, which is not desired behavior in a dataclass if the attribute can later be modified in some way. This is probably why the previous answer made that choice of initializing in init, which will always work."
        ],
        "link": "https://stackoverflow.com/questions/79787529/subtlety-in-initializing-attributes-with-methods-in-modules-from-the-equinox"
    },
    {
        "title": "Wrap `jax.lax.fori_loop` to systematically override `upper<=lower` tracing behavior",
        "question": "This is a follow-up to a previous question about the jax.lax.fori_loop function, with a little bit of a challenge for you at the end.\nAs described in the documentation, the fori_loop is never executed at runtime for the case upper<=lower. However, as has been pointed out several times, it is still traced. This can cause issues with out-of-bound indexing. I understand that the consensus is that this is intended behavior for fori_loop.\nNevertheless, in my use cases, the python-like behavior makes things much, much easier conceptually. So in my previous question, I came up with the following wrapper that overrides the default behavior when the indexing issue occurs:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI TO HANDLE THE CASE UPPER<=LOWER SEPARATELY\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((3,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThe above works fine. However, I noticed that I can't replace all my fori_loops with this wrapper. Specifically, it fails when used with nested loops. For example, the following modification of the function part_binom_conv() fails:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# # WRAPPER FOR FORI TO HANDLE THE CASE UPPER<=LOWER SEPARATELY\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = wrapped_fori(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0] #<--- This causes an error\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((3,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThe error is a TracerBoolConversionError which I think is related to the tracing the condition in my wrapper:\n---------------------------------------------------------------------------\nTracerBoolConversionError                 Traceback (most recent call last)\nCell In[4], line 55\n     53 Hks = jnp.zeros((3,2,2), dtype=complex)\n     54 U = jnp.eye(2, dtype=complex)\n---> 55 build(U, Hks)\n\n    [... skipping hidden 13 frame]\n\nCell In[4], line 43\n     41 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     42 Uks = Uks.at[0].set(U)\n---> 43 Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n     44 return Uks\n\nCell In[4], line 10\n      8     out = init_val\n      9 else:\n---> 10     out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n     11 return out\n\n    [... skipping hidden 12 frame]\n\nCell In[4], line 48\n     46 def update_Uks(k, val):\n...\n-> 1806   raise TracerBoolConversionError(arg)\n\nTracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function update_Uks at /var/folders/x0/28x522xx1vb2xl75tn781lqr0000gn/T/ipykernel_54810/1590930335.py:46 for fori_loop. This concrete value was not available in Python because it depends on the value of the argument k.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\nMy question is a little bit of a challenge. Is it possible to modify this wrapper for the fori_loop so that it doesn't trace the body when upper<=lower, and that it never causes an error in nested loops?\nI understand that this will not be implemented in jax, but I was wondering if it is something I could do in my code.",
        "answers": [
            "Is it possible to modify this wrapper for the fori_loop so that it doesn't trace the body when upper<=lower...\nNo, I don't believe that is possible.\nThe problematic case you point out occurs when the fori_loop start and endpoints are traced, in which case their concrete values are by definition unknown at trace-time. You cannot condition tracing behavior on values that are not known at trace time.\n... and that it never causes an error in nested loops?\nI don't think you need to worry about this. The reason your previous question ran into an error is because you were in a situation where the array shapes were related to the loop length, and so for loop length zero, indexing into the array failed. With dynamic loop endpoints, the array shapes cannot be related to the loop length, because shapes cannot be dynamic. So I don't think you'd ever run into an issue where tracing a zero-length dynamic/inner loop causes problems, unless your code had a bug such that it would error in all cases."
        ],
        "link": "https://stackoverflow.com/questions/79784971/wrap-jax-lax-fori-loop-to-systematically-override-upper-lower-tracing-behav"
    },
    {
        "title": "`jax.lax.fori_loop` with equal `lower` and `upper` should produce no iteration, but body still executed",
        "question": "I have a code that uses a bunch of jax.lax.fori_loop. The documentation of fori_loop says that \"setting upper <= lower will produce no iterations\". So I was naively expecting the loop to just return its init_val unchanged. But in my case, it seems like it does attempt to execute the body.\nThe code is as follows:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis returns the following error:\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 47\n     45 Hks = jnp.zeros((0,2,2), dtype=complex)\n     46 U = jnp.eye(2, dtype=complex)\n---> 47 build(U, Hks)\n\nCell In[10], line 35\n     33 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     34 Uks = Uks.at[0].set(U)\n---> 35 Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n     36 return Uks\n\n    [... skipping hidden 12 frame]\n\nCell In[10], line 40\n     38 def update_Uks(k, val):\n     39     Uks, Hks = val\n---> 40     Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n     41     return Uks, Hks\n\nCell In[10], line 12\n     11 def binom_conv(n, Aks, Bks):\n---> 12     return part_binom_conv(n, 0, n, Aks, Bks)\n...\n--> 930     raise IndexError(f\"index is out of bounds for axis {x_axis} with size 0\")\n    931   i = _normalize_index(i, x_shape[x_axis]) if normalize_indices else i\n    932   i_converted = lax.convert_element_type(i, index_dtype)\n\nIndexError: index is out of bounds for axis 0 with size 0\nI'm not sure I understand what is going on here. Shouldn't the fori_loop just return its init_val and not cause this error?",
        "answers": [
            "JAX code has two phases of execution: tracing and runtime (see JAX Key Concepts: tracing for a description of this). A fori_loop with upper <= lower will have no iterations at runtime, but the code is still traced. The error you're seeing is coming up during tracing, which is necessary even for an empty loop in order to determine the shape and type of the output. You could work around this by specially handling the zero-length case in your fori_loop body function.\nA similar issue with while_loop is discussed at https://github.com/jax-ml/jax/issues/3285.",
            "Following the insight by @Obaskly and @jakevdp, I went with a wrapper for the fori_loop:\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI [WORKS IN EXTERNAL LOOPS, BUT CAUSES ISSUE IN part_binom_conv()]\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis produces the correct behavior, and it seems to trace correctly under minimal testing.\nNote however that there is still an error if I replace the inner fori_loop in part_binom_conv by this wrapper. I think it causes an issue in tracing of nested loops.\nSorry for deleting and undeleting this answer a few times, this last point had me confused for a while."
        ],
        "link": "https://stackoverflow.com/questions/79783857/jax-lax-fori-loop-with-equal-lower-and-upper-should-produce-no-iteration"
    },
    {
        "title": "Sequential compilation times of a jax-jitted recursive function",
        "question": "I have a recursively defined function my_func that is jitted using jax.jit from the jax library. It is defined below:\n# Imports\n\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\nimport time\n\n\n# Constants and subroutines used in the core recursive routing below ...\n\nsx = jnp.asarray([[0,1.],[1.,0]], dtype=complex)\nsy = jnp.asarray([[0,-1j],[1j,0]], dtype=complex)\n\ndef conj_op(A):\n    return jnp.swapaxes(A, -1,-2).conj()\n\ndef commutator_herm(A, B):\n    comm = A @ B\n    comm = comm - conj_op(comm)\n    return comm\n\ndef H(t):\n    return jnp.cos(t) * sy\n\ndef X0(t):\n    return sx\n\n# Core recursive routine ...\n\n@partial(jax.jit, static_argnames=\"k\")\ndef my_func(t, k):\n    if k==0:\n        X_k = X0(t)\n        return X_k\n    else:\n        X_km1 = lambda t: my_func(t,k-1)\n        X_k = 1j * commutator_herm(H(t), X_km1(t)) + jax.jacfwd(X_km1, holomorphic=True)(t)\n        return X_k\nwith the relevant test:\n# Tests ...\n\nt = jnp.asarray(1, dtype=complex)\n\nseq_exec_times = []\n\nfor k in range(9,10): # or toggle to range(10) to compile sequentially\n    start = time.time()\n    my_func(t, k)\n    dur = time.time() - start\n    seq_exec_times.append(dur)\n\ntotal_seq_exec_time = sum(seq_exec_times)\n\nprint(\"Sequential execution times:\")\nprint([\"{:.3e} s\".format(x) for x in seq_exec_times])\nprint(\"Total execution time:\")\nprint(\"{:.3e} s\".format(total_seq_exec_time))\nIf I execute this function the first time only for k=9, then I get a quite long compilation time, which I figure is because tracing a recursive function like this one takes an effort that scales exponentially with recursion depth. The output is:\nSequential execution times:\n['6.306e+01 s']   # First execution time when calling directly with k=9\nTotal execution time:\n6.306e+01 s\nBut then I thought that in practice, I need to evaluate my_func for increasing values of k=0,1,2,3... anyway. And if the lower step has already been traced, then you only need to trace the next level of the tree, and that should be more efficient. And indeed, executing k=1,2,3...,8 before executing k=9 yields a slightly lower execution time the first time k=9 is evaluated:\nSequential execution times:\n['3.797e-03 s',\n'2.203e-02 s',\n'3.487e-02 s',\n'7.054e-02 s',\n'1.779e-01 s',\n'4.680e-01 s',\n'1.326e+00 s',\n'4.145e+00 s',\n'1.456e+01 s',\n'5.550e+01 s']    # First execution time of k=9 after calling k=0,1,2,3...,8 first\nTotal execution time:\n7.631e+01 s\nThat said, this still scales exponentially with recursion depth, and I was naively expecting the compilation of k=9 to be more efficient. If the lower levels k=1,2,3...,8 are already compiled, then I would naively expect the compilation at the next level k=9 to be relatively simple. I would think that you can simply trace the link between k=9 and k=8, and avoid going through the whole recursion tree again at the lower levels.\nBut it looks like I was wrong, and I'm curious to know why? And if I'm not wrong, how do I make this better?\nThis was run with jax - 0.4.33 on MacOS - 15.6.1.",
        "answers": [
            "In general, you should avoid a recursive coding style when using JAX code with JIT, autodiff, or other transformations.\nThere are three different things at play here that complicate the analysis of runtimes:\ntracing: this is the general process used in transforming JAX code, whether for jit or for autodiff like jacfwd . I believe the main reason you are seeing different timings depending on the sequence of executions is because of the trace cache: for each value of k, the function will be traced only once and subsequent calls will use the cached trace.\nautodiff: the jacfwd call in your function retraces the original function and generates a sequence of operations representing the forward-jacobian. I don't believe that there is any cache for this, so each time you call jacfwd the transformation will be recomputed from the cached trace.\ncompilation: I don't believe the that compilation pass currently makes use of previously-compiled units using the trace cache. Any control flow in JAX (loops, recursive calls, etc.) are effectively flattened before being passed to the compiler: in your case the number of operations looks to scale roughly as O[3^k]. Compilation cost is superlinear—and often roughly quadratic—in the number of operations, and so you'll find compilation will become very expensive as k gets larger.\nUnfortunately, there's not really any workaround for these issues. When using JAX, you should avoid deep Python control flow like for loops and recursion. You may be able to make progress by re-expressing your recursive function as an iterative function, using one of JAX's control flow operators like fori_loop to reduce the number of lines and cut down the compilation time."
        ],
        "link": "https://stackoverflow.com/questions/79769647/sequential-compilation-times-of-a-jax-jitted-recursive-function"
    },
    {
        "title": "Using select_n to define a piecewise function",
        "question": "I'm trying to use jax.lax.select_n to define a piecewise function, and I don't understand what it does. I thought that the behavior should be evaluating to evaluate each of the Boolean terms in the which argument, and then return the result of evaluating the function in cases corresponding to the first of the conditions to be true, but this doesn't seem to be what happens.\nFor example, I would like the expression\n(lambda i: jax.lax.select_n(\n    jnp.array([jnp.less(i, 10), jnp.less(i, 20), True]),\n    jnp.array([i, i+1, i+2]),\n))(15)\nto evaluate to 16, but instead it evaluates to the array [15,16,17]. How can I get that desired behavior?",
        "answers": [
            "What you're describing is the semantics of jnp.select , which you can use like this:\n>>> import jax\n\n>>> import jax.numpy as jnp\n\n>>> (lambda i: jnp.select(\n...    jnp.array([jnp.less(i, 10), jnp.less(i, 20), True]),\n...    jnp.array([i, i+1, i+2]),\n... ))(15)\nArray(16, dtype=int32)\nOn the other hand, the first argument of lax.select_n is not a sequence of masks, but rather an index used to select among the subsequent arguments; for example:\n>>> i = 1\n>>> jax.lax.select_n(i, jnp.array([15]), jnp.array([16]), jnp.array([17]))\nArray([16], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/79768929/using-select-n-to-define-a-piecewise-function"
    },
    {
        "title": "How to control hyperparameter within flax.nnx loss function using an optax.schedule?",
        "question": "from jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 )\n    \n    def f(x,m=2.234,b=-1.123):\n        return m*x+b\n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n\n    c = 0.9\n    learning_rate_schedule = optax.schedules.cosine_decay_schedule(\n        init_value=2e-1,\n        decay_steps = int(c*epochs),\n        alpha= 0.01,\n    )\n    ## how do I get the values from hyperparam_schedule into the loss function?\n    hyperparam_schedule = optax.schedules.linear_schedule(\n        init_value=12,\n        end_value=234,\n        transition_steps=int(c*epochs),\n    )\n    \n    params = nnx.Param\n    optimizer = nnx.Optimizer(\n        model,\n        tx=optax.adam(learning_rate_schedule),\n        wrt = params\n    )\n    \n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( carry, x, obs ):\n        def loss_function(model, inputs, obs):\n            prediction = model(inputs)\n            ## I want this to instead look like:\n            #prediction = model( my_hyperparameter, inputs )\n            error = obs - prediction\n            loss = jnp.mean(error ** 2)\n            mae = jnp.mean(jnp.abs(error ) )\n            return loss, mae\n\n        model, optimizer= carry\n        (loss,mae), grads = nnx.value_and_grad(loss_function,has_aux = True)( model, x, obs )\n        optimizer.update( model, grads )\n        return (model,optimizer), (loss,mae)\n        #return (model,optimizer), (loss,mae,foo)\n    (model,optimizer), (losses,maes) = optimizer_scan( (model, optimizer), x1, obs1 )\n    #(model,optimizer), (losses,maes,foos) = optimizer_scan( (model, optimizer), x1, obs1 )\n    \n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', model.kernel.value )\n    print( 'b approximation:', model.bias.value )\nI want to control an argument to my model's __call__() method with an optax schedule. This is a hyperparameter, and I want it to change slowly over training. I have marked the schedule that I have already defined for this purpose with a comment. I have also shown in the loss function what I want the forward pass call to look like.\nI see that there is an optax.schedules method to inject hyperparameters, but I couldn't get it to do something useful here.\nFor a practical example, let's fill the array foos with values from the hyperparameter schedule. See the commented out return in the optimizer_scan function and the commented out line where optimizer_scan could be called such that this array is filled.\nPS: This is updated for and tested on flax version 0.11",
        "answers": [
            "    def optimizer_scan( carry,  x, obs ):\n        def loss_function( model, inputs, obs ):\n            prediction = model( inputs )\n            error = obs - prediction\n            loss = jnp.mean( error ** 2 )\n            mae = jnp.mean( jnp.abs( error ) )\n            return loss, mae\n\n        model, optimizer= carry\n        foo=hyperparam_schedule( optimizer.step.value )\n        (loss,mae), grads = nnx.value_and_grad(loss_function,has_aux = True)( model, x, obs )\n        optimizer.update( model, grads )\n        return (model,optimizer), (loss,mae,foo)\n    (model,optimizer), (losses,maes,bar) = optimizer_scan( (model, optimizer), x1, obs1 )\nOK so the hyperparam_schedule is just a function. I had imagined that it needed to be embedded in the optimizer state somehow, but that is not the case.\nI have found that the optimizer state has a variable that indicates how many optimization iterations have occurred. In the snippet above, I show how we can use the optimizer.step attribute as the argument for the hyperparam_schedule call."
        ],
        "link": "https://stackoverflow.com/questions/79760266/how-to-control-hyperparameter-within-flax-nnx-loss-function-using-an-optax-sched"
    },
    {
        "title": "JAX scanning over a array with additional argument",
        "question": "I am using jax.lax.scan to propagate a system dynamics forward. To capture parameters for the dynamics, (dt and k), I use lambda, def, or carry (and carry them in the scan loop). The execution time is almost the same, probably because the dynamics is simple. What is the most jax friendly and idiomatic way of implementation for baking in parameters? I probably use more complex dynamics and call the propagation from outer functions, so would like to know what is happening here.\nimport jax\nimport jax.numpy as jnp\nimport time\n\nN = 100\n\ndef dynamics(x, u, dt, k):\n    vel = jnp.array([jnp.cos(x[2])*u[0], jnp.sin(x[2])*u[0], u[1]])\n    x_next = x + vel*dt - k*vel\n    return x_next\n\ndef one_step(state, u, dt, k):\n    x, kk = state\n    x_next = dynamics(x, u[:,kk], dt, k)\n    return (x_next,kk+1), x_next\n\n# uses lamda for scan\ndef inner_lambda(x0, u, dt, k):\n    one_step_ = lambda state, input: one_step(state, u, dt, k)\n    state0 = (x0, 0)\n    return jax.lax.scan(one_step_, state0, None, length = N)\n\n# defines a new inner function for scan\ndef inner_def(x0, u, dt, k):\n    def one_step_(state, input):\n        return one_step(state,u, dt, k)\n    return jax.lax.scan(one_step_, (x0, 0), None, length = N)    \n\ndef one_step_carry(state, input):\n    x, kk, u, dt, k = state\n    x_next = dynamics(x, u[:,kk], dt, k)\n    return (x_next,kk+1, u, dt, k), x_next\n\n# carry everything for scan    \ndef carry_all(x0, u, dt, k):\n    return jax.lax.scan(one_step_carry, (x0, 0, u, dt, k), None, length = N)\n\nx0 = jnp.array([2., 3., 4])\nkey = jax.random.PRNGKey(0)\nu = jax.random.uniform(key, shape=(2, N))\ndt = 0.01\nk = 1e-6\n\n# --- Benchmark helper ---\ndef benchmark(fn, name):\n    fn_jit = jax.jit(fn)\n    \n    # First call (includes compilation)\n    carry, x_list = fn_jit(x0, u, dt, k)\n    carry[0].block_until_ready()\n    \n    t0 = time.time()\n    for i in range(1000):\n        # Second call (cached execution)\n        carry, ys = fn_jit(x0, u, dt, k)\n        carry[0].block_until_ready()\n    t1 = time.time()\n    \n    print(f\"{name:10s} | cached run: {t1 - t0:.6f}s\")\n\n\n# --- Run benchmarks ---\nbenchmark(inner_lambda, \"lambda\")\nbenchmark(inner_def, \"def\")\nbenchmark(carry_all, \"carry_all\")\n\n# --- The result ---\nlambda     | cached run: 0.677526s\ndef        | cached run: 0.673526s\ncarry_all  | cached run: 0.686203s",
        "answers": [
            "All three versions here are idiomatic uses of JAX, and will lead to virtually identical executed code paths. From JAX's perspective, there is no reason to prefer one over the others.\nIn particular, there is absolutely zero difference between the lambda and the def versions: JAX does not distinguish between anonymous functions (defined with lambda) and named functions (defined with def), and treats them identically.\nYour third version differs only slightly in that u, dt, and k are passed to scan as carry variables rather than closed-over constants. This will lead to slightly different parameters in the scan lowering, but I don't believe the difference will have any effect on the resulting compiled function."
        ],
        "link": "https://stackoverflow.com/questions/79744979/jax-scanning-over-a-array-with-additional-argument"
    },
    {
        "title": "JAX, recompilation when using closure for a function",
        "question": "I have a jax code where I would like to scan over an array. In the body function of the scan, I have a pytree to store some parameters and functions that I want to apply during the scan. For the scan, I used lambda to bake in the object/pytree named params.\nDoes this trigger a new compilation when a new params is passed in the function example? If so, how can I avoid the recompilation?\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\n\nclass Params:\n    def __init__(self, x_array, a):    \n        self.x_array = x_array\n        self.a = a\n        return\n    \n    def one_step(self,state, input):\n        x = state\n        y = input\n        next_state = (self.x_array + x + jnp.ones(self.a))*y\n        return next_state\n\n    def _tree_flatten(self):\n        children = (self.x_array,)\n        aux_data = {'a':self.a}\n        return (children, aux_data)\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n        \ntree_util.register_pytree_node(Params,\n                               Params._tree_flatten,\n                               Params._tree_unflatten)\n\ndef scan_body(params, state, input):\n    x = state\n    y = input \n    x_new = params.one_step(x, y) \n    return x_new, [x_new]\n\n@jax.jit\ndef example(params):\n    body_fun = lambda state, input: scan_body(params, state, input)\n    init_state = jnp.array([0.,1.])\n    input_array = jnp.array([1.,2.,3.])\n    last_state, result_list = jax.lax.scan(body_fun, init_state, input_array)\n    return last_state, result_list\n\nif __name__ == \"__main__\":\n\n    params1 = Params(jnp.array([1.,2.]), 2)\n    last_state, result_list = example(params1)\n    print(last_state)\n\n    params2 = Params(jnp.array([3.,4.]), 2)\n    last_state, result_list = example(params2)\n    print(last_state)",
        "answers": [
            "Passing a new params object would only trigger recompilation if the static attributes of your params were to change. Since aux_data is static, changing the value of params.a will lead to re-compilation. Since children are dynamic, then changing the shape, dtype, or sharding of params.x will lead to recompilation, but changing the array values/contents will not.\nIn your example, in both calls params.x has the same shape, dtype, and sharding, and params.a has the same value, so there should not be any recompilation (if you're unsure, you could confirm this using the approach mentioned at https://stackoverflow.com/a/70127930/2937831).\nNote in particular that the lambda functions used in the method implementations cannot affect the JIT cache key because they are not referenced in the pytree flattening output."
        ],
        "link": "https://stackoverflow.com/questions/79744486/jax-recompilation-when-using-closure-for-a-function"
    },
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Is it expected that vmapping over different input sizes for the same function impacts the accuracy of the result?",
        "question": "I was suprised to see that depending on the size of an input matrix, which is vmapped over inside of a function, the output of the function changes slightly. That is, not only does the size of the output change (which is what I would expect from vmapping) but also the numerics changed slightly. (Note that this only occurs in float32 and only on the GPU)\nI wrote a minimally reproducible example to illustrate the behaviour:\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\ndef equinox_vmap(x, mlp):\n    out = eqx.filter_vmap(mlp.__call__)(x)\n    return out\n\nkey = jax.random.PRNGKey(0)\nkey, network_key = jax.random.split(key, 2)\nmlp = eqx.nn.MLP(2, 2, 10, 2, key=network_key)\n\nkey, key_x = jax.random.split(key, 2)\nx = jax.random.normal(key_x, (10000, 2))\n\nerror_eqx = equinox_vmap(x[:10], mlp) - equinox_vmap(x, mlp)[:10]\nprint(\"eqx error:\", error_eqx)\nWhen running this example I get the output:\neqx error: [[-1.4442205e-04  1.0999292e-04]\n [-5.9515238e-05 -9.1716647e-06]\n [ 1.4841557e-05  5.6132674e-05]\n [ 0.0000000e+00  0.0000000e+00]\n [-9.1642141e-06 -2.5466084e-05]\n [ 3.8832426e-05 -3.3110380e-05]\n [ 3.3825636e-05 -2.4946406e-05]\n [ 4.0918589e-05 -3.2216311e-05]\n [ 1.3601780e-04  8.7693334e-06]\n [ 0.0000000e+00  0.0000000e+00]]\nI understand that the numerics of float32 are not fully accurate and some error is to be expected. However, I was suprised that the result changes depending on how much of the input array is put into the function. I was expecting that the first row of the x array, i.e., x[0,:] would still be filled with the same values and therefore the first row in the output would be the same.\nFurther notes:\nI enabled the use of float64 (jax.config.update(\"jax_enable_x64\", False)) which completely removed this from occuring. I understand that this is a numerical problem, but I am a little bit confused how the vmapping interacts with the example.\nWhen I run the same example on the CPU (using jax.config.update(\"jax_platform_name\", \"cpu\")) this problem also disappears which I also find difficult to understand.\nQuestions:\nIs this to be expected?\nWhere does this \"inconsistency\" come from?\nWhy does it not occur on the CPU and only on the GPU?\nSetup:\nGPU: NVIDIA RTX 6000 Ada Generation 48 GB\nPython 3.11.11 with\nequinox                  0.13.0\njax                      0.7.0\njax-cuda12-pjrt          0.7.0\njax-cuda12-plugin        0.7.0\njaxlib                   0.7.0\njaxtyping                0.3.2\nml_dtypes                0.5.3\nnumpy                    2.3.2\nnvidia-cublas-cu12       12.9.1.4\nnvidia-cuda-cupti-cu12   12.9.79\nnvidia-cuda-nvcc-cu12    12.9.86\nnvidia-cuda-nvrtc-cu12   12.9.86\nnvidia-cuda-runtime-cu12 12.9.79\nnvidia-cudnn-cu12        9.11.0.98\nnvidia-cufft-cu12        11.4.1.4\nnvidia-cusolver-cu12     11.7.5.82\nnvidia-cusparse-cu12     12.5.10.65\nnvidia-nccl-cu12         2.27.6\nnvidia-nvjitlink-cu12    12.9.86\nnvidia-nvshmem-cu12      3.3.9\nopt_einsum               3.4.0\npip                      24.0\nscipy                    1.16.1\nsetuptools               65.5.0\ntyping_extensions        4.14.1\nwadler_lindig            0.1.7\nAny explanations are greatly appreachiated.",
        "answers": [
            "This is behaving as expected. This is not fundamentally about vmap; this is about floating point math. Whenever you're doing floating point operations, you will accumulate rounding errors, and when you do the \"same\" computation in two different ways, you will accumulate rounding errors differently (see Is floating-point math broken? for some discussion of this).\nRunning vmap over different batch sizes results in different sequences of operations, which in turn results in different rounding errors.\nAs for why this differs between CPU and GPU, it's all about how the floating point operations are sequenced. CPU is a serial architecture, so it's likely computing matrix products row-by-row with the same accumulation orders regardless of input size. GPU is a parallel architecture, and will generally distribute and accumulate results differently depending on the size of the inputs."
        ],
        "link": "https://stackoverflow.com/questions/79726091/is-it-expected-that-vmapping-over-different-input-sizes-for-the-same-function-im"
    }
]