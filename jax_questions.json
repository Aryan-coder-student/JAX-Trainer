[
    {
        "title": "JAX crashes with `CUDNN_STATUS_INTERNAL_ERROR` when using `joblib` or `multiprocessing`, but works in a single process",
        "question": "I am running into a FAILED_PRECONDITION: DNN library initialization failed error when trying to parallelize a JAX function using either Python's multiprocessing library or joblib.\nThe strange part is that my JAX installation is fundamentally working: if I run a simple JAX command in a single process, it correctly identifies and uses my NVIDIA GPUs. The crash only happens when I launch parallel workers.\nHere are my system and environment details:\nEnvironment:\nOS: Ubuntu 18.04 LTS\nPython: 3.10 (managed via Conda)\nGPU: 8 x NVIDIA Quadro RTX 8000\nNVIDIA Driver: 550.144.03\nCUDA Version (from driver): 12.4\nJAX Installation:\njax==0.4.26\njaxlib==0.4.26\njax-cuda12-plugin==0.4.26\nMinimal, Reproducible Example:\nThe following simplified script perfectly demonstrates the problem.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\n# Define a simple JAX function that will be executed on the GPU\ndef simple_worker(i):\n    \"\"\"A simple function that performs a JAX computation.\"\"\"\n    try:\n        # Create some data on the GPU and perform a computation\n        x = jnp.ones((100, 100))\n        y = jnp.dot(x, x)\n        # block_until_ready() ensures the computation is finished before returning\n        y.block_until_ready()\n        return i, \"Success\"\n    except Exception as e:\n        return i, f\"Failed with: {e}\"\n\nif __name__ == \"__main__\":\n\n    # --- Verification Step ---\n    print(\"--- Verifying JAX in the main process ---\")\n    try:\n        devices = jax.devices()\n        print(f\"JAX sees {len(devices)} devices: {devices}\")\n        if 'gpu' not in str(devices[0]).lower() and 'cuda' not in str(devices[0]).lower():\n            print(\"WARNING: JAX does not see the GPU in the main process!\")\n    except Exception as e:\n        print(f\"Error during JAX verification: {e}\")\n    print(\"-\" * 40)\n\n\n    # --- Test 1: Serial execution (This works perfectly) ---\n    print(\"\\n--- Running jobs serially (expected to work) ---\")\n    results_serial = []\n    for i in range(4):\n        results_serial.append(simple_worker(i))\n    print(f\"Serial results: {results_serial}\\n\")\n    print(\"-\" * 40)\n\n\n    # --- Test 2: Parallel execution with joblib (This crashes) ---\n    print(\"\\n--- Running jobs in parallel with joblib (expected to fail) ---\")\n    try:\n        # Also tried backend='threading' and backend='multiprocessing' with 'spawn' context\n        multiprocessing.set_start_method('spawn', force=True) \n        \n        results_parallel = Parallel(n_jobs=4)(\n            delayed(simple_worker)(i) for i in range(4)\n        )\n        print(f\"Parallel results: {results_parallel}\")\n    except Exception as e:\n        print(f\"Joblib Parallel failed with error: {e}\")\n    print(\"-\" * 40)\nWhat Happens:\nWhen I run this script, the verification and serial execution sections work perfectly, showing that my base JAX installation can use the GPU. However, the parallel section immediately crashes. Each worker process throws a series of errors that look like this before the main script crashes:\npython\nCopy\nE ... external/xla/xla/stream_executor/cuda/cuda_dnn.cc:536] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n...\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I Have Already Tried:\nVerifying Installation: As shown in the MRE, running JAX code in the main, single process works flawlessly. jax.devices() correctly lists all my GPUs.\nChanging Multiprocessing Start Method: I tried adding multiprocessing.set_start_method('spawn', force=True) at the beginning of my script. The crash still occurs with the same error.\nChanging joblib Backend: I tried Parallel(n_jobs=4, backend='threading'). This also results in the exact same CUDNN_STATUS_INTERNAL_ERROR crash.\nIt seems that any attempt by a child process or thread to initialize its own connection to the GPU resources fails, even though the main process can do so without any issues.\nIs there a known configuration issue with certain Linux/NVIDIA driver setups that prevents parallel workers from initializing CUDA contexts? What is the correct way to structure a parallel Python script using a library like joblib to distribute JAX workloads?",
        "answers": [
            "JAX is not compatible with multiprocessing (see https://github.com/jax-ml/jax/issues/3691#issuecomment-658270846).\nYou could try to work around this by using spawn rather than fork as suggested in https://github.com/jax-ml/jax/issues/3691#issuecomment-658270846, but even then, you appear to have only a single GPU device available, and if you try to run multiple JAX processes targeting this single device, they will run into issues because the first process will reserve most device memory.\nYou could potentially alleviate this if you have multiple devices, but in that case I suspect you'd get better performance by relying on JAX's compiler to shard your computations appropriately (see https://docs.jax.dev/en/latest/sharded-computation.html for more on this).\nOverall, the recommendation would be to not use JAX within multiprocessing or joblib."
        ],
        "link": "https://stackoverflow.com/questions/79816992/jax-crashes-with-cudnn-status-internal-error-when-using-joblib-or-multiproc"
    },
    {
        "title": "Gradient Error of Batch Norm That is Implemented from Scratch",
        "question": "I am trying to implement batch normalization from scratch. Here is my code.\npython\nCopy\nfrom functools import partial\nimport jax\n\n@jax.tree_util.register_pytree_node_class\nclass MyBN:\n    def __init__(self, feature_count: int, epsilon: float = 1e-5, momentum: float = 0.1, axis_name: str = 'batch'):\n        \n        self.scale = jax.numpy.ones((feature_count,))\n        self.shift = jax.numpy.zeros((feature_count,))\n        self.running_mean = jax.numpy.zeros((feature_count,))\n        self.running_var = jax.numpy.ones((feature_count,))\n        self.momentum = momentum\n        self.epsilon = epsilon\n        \n        self.axis_name = axis_name\n\n    def tree_flatten(self):\n        return (self.scale, self.shift), (self.running_mean, self.running_var, self.momentum, self.epsilon, self.axis_name)\n    \n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        scale, shift = children\n        running_mean, running_var, momentum, epsilon, axis_name = aux_data\n        obj = cls.__new__(cls)\n        obj.scale = scale\n        obj.shift = shift\n        obj.running_mean = running_mean\n        obj.running_var = running_var\n        obj.momentum = momentum\n        obj.epsilon = epsilon\n        obj.axis_name = axis_name\n        return obj\n    \n    def __repr__(self):\n        return f'{jax.tree_util.tree_leaves(self)}'\n    \n    def __call__(self, x):\n        axis_to_manipulate = tuple(range(x.ndim-1))\n        batch_mean = jax.numpy.mean(x, axis=axis_to_manipulate, keepdims=True)\n        batch_mean = jax.lax.pmean(batch_mean, axis_name=self.axis_name)\n        batch_var = jax.numpy.mean((x - batch_mean)**2, axis=axis_to_manipulate, keepdims=True)\n        batch_var = jax.lax.pmean(batch_var, axis_name=self.axis_name)\n        batch_var = jax.numpy.maximum(0.0, batch_var)\n        \n        batch_mean_squeezed = jax.numpy.squeeze(batch_mean, axis_to_manipulate)\n        batch_var_squeezed = jax.numpy.squeeze(batch_var, axis_to_manipulate)\n        running_mean_updated = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean_squeezed\n        running_var_updated = self.momentum * self.running_var + (1. - self.momentum) * batch_var_squeezed\n\n        x_normalized = (x - batch_mean) / jax.numpy.sqrt(batch_var + self.epsilon)\n\n        scale_with_match_dimensions = jax.numpy.expand_dims(self.scale, axis_to_manipulate)\n        shift_with_match_dimensions = jax.numpy.expand_dims(self.shift, axis_to_manipulate)\n        output = x_normalized * scale_with_match_dimensions + shift_with_match_dimensions\n\n        return output, (running_mean_updated, running_var_updated)\nI also made a batch norm function to test my code above.\npython\nCopy\nfeature_count = 5\nkey_input = jax.random.key(0)\nx_bhwc = jax.random.normal(key_input, (20, 30, 40, feature_count))\nbn = MyBN(feature_count=feature_count)\ndel key_input\n\n# manual batch norm\ndef calculate_batch_norm(x, scale, shift, running_mean, running_var, momentum, epsilon):\n    batch_mean = jax.numpy.mean(x, axis=(0,1,2), keepdims=True)\n    batch_var = jax.numpy.var(x, axis=(0,1,2), keepdims=True)\n\n    running_mean_updated = momentum * running_mean + (1 - momentum) * jax.numpy.squeeze(batch_mean)\n    running_var_updated = momentum * running_var + (1. - momentum) * jax.numpy.squeeze(batch_var)\n\n    x_normalized = (x - batch_mean) / jax.numpy.sqrt(batch_var + epsilon)\n\n    scale_with_match_dimensions = jax.numpy.expand_dims(scale, axis=(0,1,2))\n    shift_with_match_dimensions = jax.numpy.expand_dims(shift, axis=(0,1,2))\n    output = x_normalized * scale_with_match_dimensions + shift_with_match_dimensions\n\n    return output, (running_mean_updated, running_var_updated)\nTesting for vmap and jit gives the same or very close result but not for grad. Here is my test for grad.\npython\nCopy\n# jitted gradient of a loss function that uses the vmapped batch norm\n@jax.jit\n@jax.grad\ndef loss_bn(model, x):\n    normalized_x, _ = jax.vmap(model, out_axes=(0, None), axis_name='batch')(x)\n    loss = jax.numpy.mean(normalized_x**2)\n    return loss\n\n@jax.jit\n@partial(jax.grad, argnums=(0,1))\ndef loss_manual_bn(scale, shift, running_mean, running_var, momentum, epsilon, x):\n    normalized_x, _ = calculate_batch_norm(x, scale, shift, running_mean, running_var, momentum, epsilon)\n    loss = jax.numpy.mean(normalized_x**2)\n    return loss\n\ngrad_loss_vbn = loss_bn(bn, x_bhwc)\ngrad_loss_manual_bn = loss_manual_bn(bn.scale, bn.shift, bn.running_mean, bn.running_var, bn.momentum, bn.epsilon, x_bhwc)\nprint(grad_loss_vbn)\nprint(grad_loss_manual_bn)\nThe result is like this.\npython\nCopy\n[Array([0.39999574, 0.3999958 , 0.39999616, 0.3999952 , 0.3999954 ],      dtype=float32), Array([-3.5292942e-09, -3.7252903e-09, -1.1641532e-10,  3.0267984e-09,\n        6.0535967e-09], dtype=float32)]\n(Array([0.3999961 , 0.39999634, 0.39999685, 0.3999954 , 0.39999533],      dtype=float32), Array([-3.0895535e-09,  6.0535967e-09,  7.5669959e-10, -4.7730282e-09,\n        1.1059456e-08], dtype=float32))\nWhy are both gradient w.r.t. shift not similar? Which part of the code is wrong? I really appreciate for any help.",
        "answers": [
            "It looks like your two approaches are returning similar results up to expected floating point rounding differences. Floating point operations always accrue rounding errors (see Is floating-point math broken?), and if you compute the same value in two different ways, they will generally accrue rounding errors differently. Using your input, and a portion of your calculation as an example:\npython\nCopy\nprint(x_bhwc.mean())  # -0.00097422465\nprint(x_bhwc.mean((0, 1, 2)).mean())  # -0.00097423664\nThe relative difference here is about 10^-5, and the absolute difference is about 10^-9. The gradients you compute with respect to the scale show similar relative differences, and the gradients with respect to the shift have similar absolute differences: given how the scale and shift enter the computation (i.e., (scale * x + shift) ** 2), I think this is probably what would be expected from floating point rounding differences between the two approaches.\nSide-note: your tree flattening has a bug: since running_mean and running_var are arrays, they cannot be included in the aux_data, which must contain static, hashable contents. You should include them in children instead. That issue doesn't affect this gradient question, but it would cause other problems down the line."
        ],
        "link": "https://stackoverflow.com/questions/79805112/gradient-error-of-batch-norm-that-is-implemented-from-scratch"
    },
    {
        "title": "Does `jax` compilation save runtime memory by recognizing array elements that are duplicated by indexing",
        "question": "Consider the example code:\npython\nCopy\nfrom functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnums=(0,))\ndef my_function(n):\n\n    idx = jnp.tile(jnp.arange(n, dtype=int),(1,n)) # Contains duplicate indices\n    A = jnp.ones((n**2,), dtype=float)\n    B = jnp.ones((n,100,100), dtype=float)\n\n    return jnp.sum(A[...,None,None]*B[idx]) # Will data in B be duplicated in memory here?\n\nmy_function(5)\nWhen compiling through B[idx], will jax compilation recognize that there are duplicate indices and thereby avoid unnecessarily duplicating the data in B?\nI suspect probably not because it's value dependent in general, but just want to understand better.",
        "answers": [
            "No, I don't believe this is an optimization that the compiler does. I'm basing this on the fact that XLA's computational model requires all array shapes to be known at compile-time, and the values in idx are not known until runtime.\nIf you're not convinced and want to see for yourself what the compiler is doing with this code, you can use JAX's Ahead of time compilation APIs to peek at the compiled HLO produced by XLA for this code (note that the compiler will perform different optimizations on different hardware).\nFor example:\npython\nCopy\nprint(my_function.lower(5).compile().as_text())\nHloModule jit_my_function, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%region_0.1.clone (reduce_sum.0: f32[], reduce_sum.1: f32[]) -> f32[] {\n  %reduce_sum.0 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.1 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.2 = f32[] add(%reduce_sum.0, %reduce_sum.1), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%fused_computation () -> f32[1,25,100,100] {\n  %constant.2 = f32[] constant(1)\n  %broadcast_in_dim.0 = f32[5,100,100]{2,1,0} broadcast(%constant.2), dimensions={}, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=10 source_end_line=10 source_column=8 source_end_column=42}\n  %iota.5 = s32[1,1,5,5]{3,2,1,0} iota(), iota_dimension=3, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %bitcast.5 = s32[1,25]{1,0} bitcast(%iota.5), metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %constant.1 = s32[] constant(0)\n  %broadcast.4 = s32[1,25]{1,0} broadcast(%constant.1), dimensions={}\n  %lt.0 = pred[1,25]{1,0} compare(%bitcast.5, %broadcast.4), direction=LT, metadata={op_name=\"jit(my_function)/lt\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %constant.0 = s32[] constant(5)\n  %broadcast.1 = s32[1,25]{1,0} broadcast(%constant.0), dimensions={}\n  %add.0 = s32[1,25]{1,0} add(%bitcast.5, %broadcast.1), metadata={op_name=\"jit(my_function)/add\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %select_n.0 = s32[1,25]{1,0} select(%lt.0, %add.0, %bitcast.5), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %bitcast.4 = s32[25,1]{1,0} bitcast(%select_n.0), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %gather.2 = f32[25,1,100,100]{3,2,1,0} gather(%broadcast_in_dim.0, %bitcast.4), offset_dims={1,2,3}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,100,100}, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  ROOT %bitcast.3 = f32[1,25,100,100]{3,2,1,0} bitcast(%gather.2), metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n}\n\nENTRY %main.2 () -> f32[] {\n  %constant.7 = f32[] constant(0)\n  %gather_bitcast_fusion = f32[1,25,100,100]{3,2,1,0} fusion(), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}, backend_config={\"outer_dimension_partitions\":[\"1\",\"2\"]}\n  %reduce-window = f32[1,1,4,4]{3,2,1,0} reduce-window(%gather_bitcast_fusion, %constant.7), window={size=1x25x32x32 stride=1x25x32x32 pad=0_0x0_0x14_14x14_14}, to_apply=%region_0.1, backend_config={\"outer_dimension_partitions\":[\"1\",\"1\",\"2\"]}\n  ROOT %reduce_sum.7 = f32[] reduce(%reduce-window, %constant.7), dimensions={0,1,2,3}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\nReading this output takes some practice, but the relevant piece to answer your question is the line that starts with %gather.2 = f32[25,1,100,100]{3,2,1,0}: the gather primitive is XLA's version of indexing, and you see that it's explicitly constructing the full 25x100x100 array, and not removing the duplicated indices."
        ],
        "link": "https://stackoverflow.com/questions/79798175/does-jax-compilation-save-runtime-memory-by-recognizing-array-elements-that-ar"
    },
    {
        "title": "How to Make Batching Rule for Multiple Outputs",
        "question": "I am still exploring how to make batching rule correctly. Right now, my code of batching rule doesn't work as expected for multiple outputs. Here is my code.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.extend import core\nfrom jax.interpreters import batching\n\nmy_sum_p = core.Primitive('my_sum')\nmy_sum_p.multiple_results = True\n\ndef my_sum(x):\n    return my_sum_p.bind(x)\n\n# primal evaluation rule\ndef my_sum_impl(x):\n    return jnp.sum(x), jnp.ones((3,))\nmy_sum_p.def_impl(my_sum_impl)\n\n# batching rule\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (bd_x, bd_x)\n    return output, dims\nbatching.primitive_batchers[my_sum_p] = my_sum_batching_rule\n\n# example usage\nif __name__ == \"__main__\":\n    x = jnp.array([[1.0, 2.0, 3.0],\n                   [4.0, 5.0, 6.0]])\n    \n    vms = jax.vmap(my_sum)\n    print('my_sum:', vms(x))\n\n    def original_sum(x):\n        return jnp.sum(x), jnp.ones((3,))\n    vos = jax.vmap(original_sum)\n    print('original sum:', vos(x))\nThe output is like this\npython\nCopy\nmy_sum: [Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\noriginal sum: (Array([ 6., 15.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\nThe second output of my_sum is not batched like the one of original_sum.\nOn another case, when I change the vmap setup of my_sum such as follows\npython\nCopy\nvms = jax.vmap(my_sum, out_axes=(0, None))\nit raises an error\npython\nCopy\nTraceback (most recent call last):\n  File \"/home/yahya/Projects/neuralnet/neuralnet/nn/playground.py\", line 33, in <module>\n    print('my_sum:', vms(x))\n                     ~~~^^^\nValueError: vmap out_axes specification must be a tree prefix of the corresponding value, got specification (0, None) for value tree PyTreeDef([*, *]).\nMeanwhile the same vmap setup for the original_sum makes a correct output such as follows\npython\nCopy\noriginal_sum: (Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32))\nHow should my batching rule be? I really appreciate for any help",
        "answers": [
            "Your batching rule should look like this:\npython\nCopy\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (0, None)\n    return output, dims\nThe operative change here is that dims = (bd_x, bd_x) became dims = (0, None). The output batch dimensions must reflect the batching characteristics of the output, not the input: in this case, the first output is batched along the leading axis (bdim = 0), and the second output is unbatched (bdim = None).\nYou can test the correctness of this further by comparing the results when vmapping over different input and output axes; for example:\npython\nCopy\nprint(jax.vmap(original_sum, in_axes=(1,))(x))\nprint(jax.vmap(my_sum, in_axes=(1,))(x))\npython\nCopy\n(Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)]\npython\nCopy\nprint(jax.vmap(original_sum, in_axes=(1,), out_axes=(0, None))(x))\nprint(jax.vmap(my_sum, in_axes=(1,), out_axes=[0, None])(x))\npython\nCopy\n(Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\n(notice that out_axes is a tuple in the first case, because original_fun returns a tuple, and a list in the second case, because primitives with multiple_outputs=True return a list rather than a tuple)."
        ],
        "link": "https://stackoverflow.com/questions/79790782/how-to-make-batching-rule-for-multiple-outputs"
    },
    {
        "title": "How to Make Batching Rule Properly",
        "question": "I am trying to make a custom primitive and following this tutorial. I think I have followed the tutorial correctly, but the result shows the code doesn't work properly. Here is my code:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.extend import core\nfrom jax.interpreters import batching\n\nmy_sum_p = core.Primitive('my_sum')\n\ndef my_sum(x):\n    return my_sum_p.bind(x)\n\n# primal evaluation rule\ndef my_sum_impl(x):\n    return jnp.sum(x)\nmy_sum_p.def_impl(my_sum_impl)\n\n# batching rule\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    return my_sum(x), bd_x\nbatching.primitive_batchers[my_sum_p] = my_sum_batching_rule\n\n# example usage\nif __name__ == \"__main__\":\n    x = jnp.array([[1.0, 2.0, 3.0],\n                   [4.0, 5.0, 6.0]])\n    \n    vms = jax.vmap(my_sum)\n    print('my_sum:', vms(x))\n\n    def original_sum(x):\n        return jnp.sum(x)\n    vos = jax.vmap(original_sum)\n    print('original_sum:', vos(x))\nThe result is as follow:\npython\nCopy\nmy_sum: 21.0\noriginal_sum: [ 6. 15.]\nI expect that the result of my_sum is the same with the result of original_sum. Any help, I really appreciate. Thank you in advance.",
        "answers": [
            "A batched sum means that you're summing along all axes except the batch axis. You can encode this in your batching rule like this:\npython\nCopy\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    return jnp.sum(x, axis=axis), 0\nWith this substitution, vmap(my_sum)(x) and vmap(jnp.sum)(x) should have the same behavior for any valid x.\nNotice that the returned batch dimension is 0 regardless of what the input batch dimension is: this is because the batched sum is always one-dimensional with batched values along axis 0."
        ],
        "link": "https://stackoverflow.com/questions/79790324/how-to-make-batching-rule-properly"
    },
    {
        "title": "Subtlety in initializing attributes with methods in modules from the `equinox` `jax` library",
        "question": "I have the following code that defines an abstract class and its final subclasse. The two classes are both subclasses of the equinox.Module class, which registers class attributes as the leaves of a PyTree container.\npython\nCopy\n# === IMPORTS ===\nfrom abc import ABC, abstractmethod\nimport jax\nfrom jax.typing import ArrayLike\nimport jax.numpy as jnp\nimport equinox as eqx\nfrom quadax import quadgk\n\njax.config.update(\"jax_enable_x64\", True)\n\n\nclass MyClass(eqx.Module): # Works if I toggle to MyClass(ABC)\n\n    rtol = 1e-12\n    atol = 1e-12\n\n    param: ArrayLike\n\n    def __init__(self):\n        self.param = self._integral_moment(3) # Fails, but works if I toggle to something like \"self.param = self.func(1.)\"\n\n    @abstractmethod \n    def func(self, tau):\n        pass\n\n    def func_abs(self, tau):\n        return jnp.abs(self.func(tau))\n    \n    def _integral_moment(self, order): \n        return quadgk(self._integrand_moment, [0, jnp.inf], args=(order,), epsrel=self.rtol, epsabs=self.atol)[0]\n\n    def _integrand_moment(self, tau, order):\n        return self.func_abs(tau) * jnp.abs(tau)**order\n \n\nclass MySubClass(MyClass):\n\n    gamma: ArrayLike\n    kappa: ArrayLike\n    w0: ArrayLike\n\n    def __init__(self, gamma, kappa, w0):\n        self.gamma = jnp.asarray(gamma)\n        self.kappa = jnp.asarray(kappa) \n        self.w0 = jnp.asarray(w0)\n        super().__init__()\n\n    def func(self, tau):\n        return self.gamma * jnp.exp(-1j * self.w0 * tau) * jnp.exp(-self.kappa*jnp.abs(tau)/2)\n    \n\n# Test    \ntest = MySubClass(gamma=1., kappa=1., w0=1.)\ntest.param\nThis code produces the AttributeError message:\npython\nCopy\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[21], line 52\n     48         return self.gamma * jnp.exp(-1j * self.w0 * tau) * jnp.exp(-self.kappa*jnp.abs(tau)/2)\n     51 # Test    \n---> 52 test = MySubClass(gamma=1., kappa=1., w0=1.)\n     53 test.param\n\n    [... skipping hidden 2 frame]\n\nCell In[21], line 45\n     43 self.kappa = jnp.asarray(kappa) \n     44 self.w0 = jnp.asarray(w0)\n---> 45 super().__init__()\n\nCell In[21], line 19\n     18 def __init__(self):\n---> 19     self.param = self._integral_moment(3)\n\n    [... skipping hidden 1 frame]\n\nCell In[21], line 29\n     28 def _integral_moment(self, order): \n---> 29     return quadgk(self._integrand_moment, [0, jnp.inf], args=(order,), epsrel=self.rtol, epsabs=self.atol)[0]\n...\n    659         and isinstance(out, types.MethodType)\n    660         and out.__self__ is self\n    661     ):\n\nAttributeError: 'MySubClass' object has no attribute 'param'\nThis error clearly comes from a restriction of the equinox.Module, since if I change the parent class to ABC, the code runs fine.\nFirst, I thought that maybe equinox did not allow me to use methods to initialize attributes. But if I use the func() method instead of the _integral_moment() method to initialize param, the code works fine.\nSo I just don't understand what is going on here. I thought it would be better to ask here before asking the developers at equinox.\nThis uses equinox version 0.13.1 with jax version 0.7.2.",
        "answers": [
            "The issue here is that when traced, eqx.Module attempts to access all the declared attributes of the Module, so the module cannot be traced before those attributes are created. Here's a simpler repro of the same problem:\npython\nCopy\nimport jax\nimport equinox as eqx\n\nclass MyClass(eqx.Module):\n    param: ArrayLike\n\n    def __init__(self):\n      self.param = jax.jit(self.func)()\n\n    def func(self):\n      return 4\n\nMyClass()  # AttributeError: 'MyClass' object has no attribute 'param'\nThe quadgk function traces its input, and since you call it before setting param, you get this error. With this issue in mind, you can fix your problem by setting the missing param to a placeholder value before you call a function that traces the object's methods:\npython\nCopy\nclass MyClass(eqx.Module):\n\n    ...\n\n    def __init__(self):\n        self.param = 0  # set to a placeholder to allow tracing\n        self.param = self._integral_moment(3)\n\n    ...",
            "To follow up on @jakevdp's answer, a completely equivalent but perhaps slightly more elegant way of systematically pre-empting this issue in equinox is to assign a value directly in the attribute definition:\npython\nCopy\nclass MyClass(eqx.Module):\n\n    ...\n\n    param: float = 0 # set to a placeholder to allow tracing\n    \n    def __init__(self):\n        self.param = self._integral_moment(3)\n\n    ...\nEDIT: Importantly, not that it is NOT allowed to initialize attributes as mutables or jax arrays at the class level in dataclasses like equinox modules, which raises a ValueError: Use default_factory. For the above code, all instances of the class will initially share the same instance object for the field, which is not desired behavior in a dataclass if the attribute can later be modified in some way. This is probably why the previous answer made that choice of initializing in init, which will always work."
        ],
        "link": "https://stackoverflow.com/questions/79787529/subtlety-in-initializing-attributes-with-methods-in-modules-from-the-equinox"
    },
    {
        "title": "Wrap `jax.lax.fori_loop` to systematically override `upper<=lower` tracing behavior",
        "question": "This is a follow-up to a previous question about the jax.lax.fori_loop function, with a little bit of a challenge for you at the end.\nAs described in the documentation, the fori_loop is never executed at runtime for the case upper<=lower. However, as has been pointed out several times, it is still traced. This can cause issues with out-of-bound indexing. I understand that the consensus is that this is intended behavior for fori_loop.\nNevertheless, in my use cases, the python-like behavior makes things much, much easier conceptually. So in my previous question, I came up with the following wrapper that overrides the default behavior when the indexing issue occurs:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI TO HANDLE THE CASE UPPER<=LOWER SEPARATELY\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((3,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThe above works fine. However, I noticed that I can't replace all my fori_loops with this wrapper. Specifically, it fails when used with nested loops. For example, the following modification of the function part_binom_conv() fails:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# # WRAPPER FOR FORI TO HANDLE THE CASE UPPER<=LOWER SEPARATELY\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = wrapped_fori(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0] #<--- This causes an error\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((3,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThe error is a TracerBoolConversionError which I think is related to the tracing the condition in my wrapper:\npython\nCopy\n---------------------------------------------------------------------------\nTracerBoolConversionError                 Traceback (most recent call last)\nCell In[4], line 55\n     53 Hks = jnp.zeros((3,2,2), dtype=complex)\n     54 U = jnp.eye(2, dtype=complex)\n---> 55 build(U, Hks)\n\n    [... skipping hidden 13 frame]\n\nCell In[4], line 43\n     41 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     42 Uks = Uks.at[0].set(U)\n---> 43 Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n     44 return Uks\n\nCell In[4], line 10\n      8     out = init_val\n      9 else:\n---> 10     out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n     11 return out\n\n    [... skipping hidden 12 frame]\n\nCell In[4], line 48\n     46 def update_Uks(k, val):\n...\n-> 1806   raise TracerBoolConversionError(arg)\n\nTracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function update_Uks at /var/folders/x0/28x522xx1vb2xl75tn781lqr0000gn/T/ipykernel_54810/1590930335.py:46 for fori_loop. This concrete value was not available in Python because it depends on the value of the argument k.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\nMy question is a little bit of a challenge. Is it possible to modify this wrapper for the fori_loop so that it doesn't trace the body when upper<=lower, and that it never causes an error in nested loops?\nI understand that this will not be implemented in jax, but I was wondering if it is something I could do in my code.",
        "answers": [
            "Is it possible to modify this wrapper for the fori_loop so that it doesn't trace the body when upper<=lower...\nNo, I don't believe that is possible.\nThe problematic case you point out occurs when the fori_loop start and endpoints are traced, in which case their concrete values are by definition unknown at trace-time. You cannot condition tracing behavior on values that are not known at trace time.\n... and that it never causes an error in nested loops?\nI don't think you need to worry about this. The reason your previous question ran into an error is because you were in a situation where the array shapes were related to the loop length, and so for loop length zero, indexing into the array failed. With dynamic loop endpoints, the array shapes cannot be related to the loop length, because shapes cannot be dynamic. So I don't think you'd ever run into an issue where tracing a zero-length dynamic/inner loop causes problems, unless your code had a bug such that it would error in all cases."
        ],
        "link": "https://stackoverflow.com/questions/79784971/wrap-jax-lax-fori-loop-to-systematically-override-upper-lower-tracing-behav"
    },
    {
        "title": "`jax.lax.fori_loop` with equal `lower` and `upper` should produce no iteration, but body still executed",
        "question": "I have a code that uses a bunch of jax.lax.fori_loop. The documentation of fori_loop says that \"setting upper <= lower will produce no iterations\". So I was naively expecting the loop to just return its init_val unchanged. But in my case, it seems like it does attempt to execute the body.\nThe code is as follows:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis returns the following error:\npython\nCopy\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 47\n     45 Hks = jnp.zeros((0,2,2), dtype=complex)\n     46 U = jnp.eye(2, dtype=complex)\n---> 47 build(U, Hks)\n\nCell In[10], line 35\n     33 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     34 Uks = Uks.at[0].set(U)\n---> 35 Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n     36 return Uks\n\n    [... skipping hidden 12 frame]\n\nCell In[10], line 40\n     38 def update_Uks(k, val):\n     39     Uks, Hks = val\n---> 40     Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n     41     return Uks, Hks\n\nCell In[10], line 12\n     11 def binom_conv(n, Aks, Bks):\n---> 12     return part_binom_conv(n, 0, n, Aks, Bks)\n...\n--> 930     raise IndexError(f\"index is out of bounds for axis {x_axis} with size 0\")\n    931   i = _normalize_index(i, x_shape[x_axis]) if normalize_indices else i\n    932   i_converted = lax.convert_element_type(i, index_dtype)\n\nIndexError: index is out of bounds for axis 0 with size 0\nI'm not sure I understand what is going on here. Shouldn't the fori_loop just return its init_val and not cause this error?",
        "answers": [
            "JAX code has two phases of execution: tracing and runtime (see JAX Key Concepts: tracing for a description of this). A fori_loop with upper <= lower will have no iterations at runtime, but the code is still traced. The error you're seeing is coming up during tracing, which is necessary even for an empty loop in order to determine the shape and type of the output. You could work around this by specially handling the zero-length case in your fori_loop body function.\nA similar issue with while_loop is discussed at https://github.com/jax-ml/jax/issues/3285.",
            "Following the insight by @Obaskly and @jakevdp, I went with a wrapper for the fori_loop:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI [WORKS IN EXTERNAL LOOPS, BUT CAUSES ISSUE IN part_binom_conv()]\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis produces the correct behavior, and it seems to trace correctly under minimal testing.\nNote however that there is still an error if I replace the inner fori_loop in part_binom_conv by this wrapper. I think it causes an issue in tracing of nested loops.\nSorry for deleting and undeleting this answer a few times, this last point had me confused for a while."
        ],
        "link": "https://stackoverflow.com/questions/79783857/jax-lax-fori-loop-with-equal-lower-and-upper-should-produce-no-iteration"
    },
    {
        "title": "Sequential compilation times of a jax-jitted recursive function",
        "question": "I have a recursively defined function my_func that is jitted using jax.jit from the jax library. It is defined below:\npython\nCopy\n# Imports\n\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\nimport time\n\n\n# Constants and subroutines used in the core recursive routing below ...\n\nsx = jnp.asarray([[0,1.],[1.,0]], dtype=complex)\nsy = jnp.asarray([[0,-1j],[1j,0]], dtype=complex)\n\ndef conj_op(A):\n    return jnp.swapaxes(A, -1,-2).conj()\n\ndef commutator_herm(A, B):\n    comm = A @ B\n    comm = comm - conj_op(comm)\n    return comm\n\ndef H(t):\n    return jnp.cos(t) * sy\n\ndef X0(t):\n    return sx\n\n# Core recursive routine ...\n\n@partial(jax.jit, static_argnames=\"k\")\ndef my_func(t, k):\n    if k==0:\n        X_k = X0(t)\n        return X_k\n    else:\n        X_km1 = lambda t: my_func(t,k-1)\n        X_k = 1j * commutator_herm(H(t), X_km1(t)) + jax.jacfwd(X_km1, holomorphic=True)(t)\n        return X_k\nwith the relevant test:\npython\nCopy\n# Tests ...\n\nt = jnp.asarray(1, dtype=complex)\n\nseq_exec_times = []\n\nfor k in range(9,10): # or toggle to range(10) to compile sequentially\n    start = time.time()\n    my_func(t, k)\n    dur = time.time() - start\n    seq_exec_times.append(dur)\n\ntotal_seq_exec_time = sum(seq_exec_times)\n\nprint(\"Sequential execution times:\")\nprint([\"{:.3e} s\".format(x) for x in seq_exec_times])\nprint(\"Total execution time:\")\nprint(\"{:.3e} s\".format(total_seq_exec_time))\nIf I execute this function the first time only for k=9, then I get a quite long compilation time, which I figure is because tracing a recursive function like this one takes an effort that scales exponentially with recursion depth. The output is:\nSequential execution times:\n['6.306e+01 s']   # First execution time when calling directly with k=9\nTotal execution time:\n6.306e+01 s\nBut then I thought that in practice, I need to evaluate my_func for increasing values of k=0,1,2,3... anyway. And if the lower step has already been traced, then you only need to trace the next level of the tree, and that should be more efficient. And indeed, executing k=1,2,3...,8 before executing k=9 yields a slightly lower execution time the first time k=9 is evaluated:\nSequential execution times:\n['3.797e-03 s',\n'2.203e-02 s',\n'3.487e-02 s',\n'7.054e-02 s',\n'1.779e-01 s',\n'4.680e-01 s',\n'1.326e+00 s',\n'4.145e+00 s',\n'1.456e+01 s',\n'5.550e+01 s']    # First execution time of k=9 after calling k=0,1,2,3...,8 first\nTotal execution time:\n7.631e+01 s\nThat said, this still scales exponentially with recursion depth, and I was naively expecting the compilation of k=9 to be more efficient. If the lower levels k=1,2,3...,8 are already compiled, then I would naively expect the compilation at the next level k=9 to be relatively simple. I would think that you can simply trace the link between k=9 and k=8, and avoid going through the whole recursion tree again at the lower levels.\nBut it looks like I was wrong, and I'm curious to know why? And if I'm not wrong, how do I make this better?\nThis was run with jax - 0.4.33 on MacOS - 15.6.1.",
        "answers": [
            "In general, you should avoid a recursive coding style when using JAX code with JIT, autodiff, or other transformations.\nThere are three different things at play here that complicate the analysis of runtimes:\ntracing: this is the general process used in transforming JAX code, whether for jit or for autodiff like jacfwd . I believe the main reason you are seeing different timings depending on the sequence of executions is because of the trace cache: for each value of k, the function will be traced only once and subsequent calls will use the cached trace.\nautodiff: the jacfwd call in your function retraces the original function and generates a sequence of operations representing the forward-jacobian. I don't believe that there is any cache for this, so each time you call jacfwd the transformation will be recomputed from the cached trace.\ncompilation: I don't believe the that compilation pass currently makes use of previously-compiled units using the trace cache. Any control flow in JAX (loops, recursive calls, etc.) are effectively flattened before being passed to the compiler: in your case the number of operations looks to scale roughly as O[3^k]. Compilation cost is superlinear—and often roughly quadratic—in the number of operations, and so you'll find compilation will become very expensive as k gets larger.\nUnfortunately, there's not really any workaround for these issues. When using JAX, you should avoid deep Python control flow like for loops and recursion. You may be able to make progress by re-expressing your recursive function as an iterative function, using one of JAX's control flow operators like fori_loop to reduce the number of lines and cut down the compilation time."
        ],
        "link": "https://stackoverflow.com/questions/79769647/sequential-compilation-times-of-a-jax-jitted-recursive-function"
    }
]