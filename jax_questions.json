[
    {
        "title": "JAX crashes with `CUDNN_STATUS_INTERNAL_ERROR` when using `joblib` or `multiprocessing`, but works in a single process",
        "question": "I am running into a FAILED_PRECONDITION: DNN library initialization failed error when trying to parallelize a JAX function using either Python's multiprocessing library or joblib.\nThe strange part is that my JAX installation is fundamentally working: if I run a simple JAX command in a single process, it correctly identifies and uses my NVIDIA GPUs. The crash only happens when I launch parallel workers.\nHere are my system and environment details:\nEnvironment:\nOS: Ubuntu 18.04 LTS\nPython: 3.10 (managed via Conda)\nGPU: 8 x NVIDIA Quadro RTX 8000\nNVIDIA Driver: 550.144.03\nCUDA Version (from driver): 12.4\nJAX Installation:\njax==0.4.26\njaxlib==0.4.26\njax-cuda12-plugin==0.4.26\nMinimal, Reproducible Example:\nThe following simplified script perfectly demonstrates the problem.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\n# Define a simple JAX function that will be executed on the GPU\ndef simple_worker(i):\n    \"\"\"A simple function that performs a JAX computation.\"\"\"\n    try:\n        # Create some data on the GPU and perform a computation\n        x = jnp.ones((100, 100))\n        y = jnp.dot(x, x)\n        # block_until_ready() ensures the computation is finished before returning\n        y.block_until_ready()\n        return i, \"Success\"\n    except Exception as e:\n        return i, f\"Failed with: {e}\"\n\nif __name__ == \"__main__\":\n\n    # --- Verification Step ---\n    print(\"--- Verifying JAX in the main process ---\")\n    try:\n        devices = jax.devices()\n        print(f\"JAX sees {len(devices)} devices: {devices}\")\n        if 'gpu' not in str(devices[0]).lower() and 'cuda' not in str(devices[0]).lower():\n            print(\"WARNING: JAX does not see the GPU in the main process!\")\n    except Exception as e:\n        print(f\"Error during JAX verification: {e}\")\n    print(\"-\" * 40)\n\n\n    # --- Test 1: Serial execution (This works perfectly) ---\n    print(\"\\n--- Running jobs serially (expected to work) ---\")\n    results_serial = []\n    for i in range(4):\n        results_serial.append(simple_worker(i))\n    print(f\"Serial results: {results_serial}\\n\")\n    print(\"-\" * 40)\n\n\n    # --- Test 2: Parallel execution with joblib (This crashes) ---\n    print(\"\\n--- Running jobs in parallel with joblib (expected to fail) ---\")\n    try:\n        # Also tried backend='threading' and backend='multiprocessing' with 'spawn' context\n        multiprocessing.set_start_method('spawn', force=True) \n        \n        results_parallel = Parallel(n_jobs=4)(\n            delayed(simple_worker)(i) for i in range(4)\n        )\n        print(f\"Parallel results: {results_parallel}\")\n    except Exception as e:\n        print(f\"Joblib Parallel failed with error: {e}\")\n    print(\"-\" * 40)\nWhat Happens:\nWhen I run this script, the verification and serial execution sections work perfectly, showing that my base JAX installation can use the GPU. However, the parallel section immediately crashes. Each worker process throws a series of errors that look like this before the main script crashes:\npython\nCopy\nE ... external/xla/xla/stream_executor/cuda/cuda_dnn.cc:536] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n...\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I Have Already Tried:\nVerifying Installation: As shown in the MRE, running JAX code in the main, single process works flawlessly. jax.devices() correctly lists all my GPUs.\nChanging Multiprocessing Start Method: I tried adding multiprocessing.set_start_method('spawn', force=True) at the beginning of my script. The crash still occurs with the same error.\nChanging joblib Backend: I tried Parallel(n_jobs=4, backend='threading'). This also results in the exact same CUDNN_STATUS_INTERNAL_ERROR crash.\nIt seems that any attempt by a child process or thread to initialize its own connection to the GPU resources fails, even though the main process can do so without any issues.\nIs there a known configuration issue with certain Linux/NVIDIA driver setups that prevents parallel workers from initializing CUDA contexts? What is the correct way to structure a parallel Python script using a library like joblib to distribute JAX workloads?",
        "answers": [
            "JAX is not compatible with multiprocessing (see https://github.com/jax-ml/jax/issues/3691#issuecomment-658270846).\nYou could try to work around this by using spawn rather than fork as suggested in https://github.com/jax-ml/jax/issues/3691#issuecomment-658270846, but even then, you appear to have only a single GPU device available, and if you try to run multiple JAX processes targeting this single device, they will run into issues because the first process will reserve most device memory.\nYou could potentially alleviate this if you have multiple devices, but in that case I suspect you'd get better performance by relying on JAX's compiler to shard your computations appropriately (see https://docs.jax.dev/en/latest/sharded-computation.html for more on this).\nOverall, the recommendation would be to not use JAX within multiprocessing or joblib."
        ],
        "link": "https://stackoverflow.com/questions/79816992/jax-crashes-with-cudnn-status-internal-error-when-using-joblib-or-multiproc"
    },
    {
        "title": "Does `jax` compilation save runtime memory by recognizing array elements that are duplicated by indexing",
        "question": "Consider the example code:\npython\nCopy\nfrom functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnums=(0,))\ndef my_function(n):\n\n    idx = jnp.tile(jnp.arange(n, dtype=int),(1,n)) # Contains duplicate indices\n    A = jnp.ones((n**2,), dtype=float)\n    B = jnp.ones((n,100,100), dtype=float)\n\n    return jnp.sum(A[...,None,None]*B[idx]) # Will data in B be duplicated in memory here?\n\nmy_function(5)\nWhen compiling through B[idx], will jax compilation recognize that there are duplicate indices and thereby avoid unnecessarily duplicating the data in B?\nI suspect probably not because it's value dependent in general, but just want to understand better.",
        "answers": [
            "No, I don't believe this is an optimization that the compiler does. I'm basing this on the fact that XLA's computational model requires all array shapes to be known at compile-time, and the values in idx are not known until runtime.\nIf you're not convinced and want to see for yourself what the compiler is doing with this code, you can use JAX's Ahead of time compilation APIs to peek at the compiled HLO produced by XLA for this code (note that the compiler will perform different optimizations on different hardware).\nFor example:\npython\nCopy\nprint(my_function.lower(5).compile().as_text())\nHloModule jit_my_function, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%region_0.1.clone (reduce_sum.0: f32[], reduce_sum.1: f32[]) -> f32[] {\n  %reduce_sum.0 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.1 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.2 = f32[] add(%reduce_sum.0, %reduce_sum.1), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%fused_computation () -> f32[1,25,100,100] {\n  %constant.2 = f32[] constant(1)\n  %broadcast_in_dim.0 = f32[5,100,100]{2,1,0} broadcast(%constant.2), dimensions={}, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=10 source_end_line=10 source_column=8 source_end_column=42}\n  %iota.5 = s32[1,1,5,5]{3,2,1,0} iota(), iota_dimension=3, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %bitcast.5 = s32[1,25]{1,0} bitcast(%iota.5), metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %constant.1 = s32[] constant(0)\n  %broadcast.4 = s32[1,25]{1,0} broadcast(%constant.1), dimensions={}\n  %lt.0 = pred[1,25]{1,0} compare(%bitcast.5, %broadcast.4), direction=LT, metadata={op_name=\"jit(my_function)/lt\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %constant.0 = s32[] constant(5)\n  %broadcast.1 = s32[1,25]{1,0} broadcast(%constant.0), dimensions={}\n  %add.0 = s32[1,25]{1,0} add(%bitcast.5, %broadcast.1), metadata={op_name=\"jit(my_function)/add\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %select_n.0 = s32[1,25]{1,0} select(%lt.0, %add.0, %bitcast.5), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %bitcast.4 = s32[25,1]{1,0} bitcast(%select_n.0), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %gather.2 = f32[25,1,100,100]{3,2,1,0} gather(%broadcast_in_dim.0, %bitcast.4), offset_dims={1,2,3}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,100,100}, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  ROOT %bitcast.3 = f32[1,25,100,100]{3,2,1,0} bitcast(%gather.2), metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n}\n\nENTRY %main.2 () -> f32[] {\n  %constant.7 = f32[] constant(0)\n  %gather_bitcast_fusion = f32[1,25,100,100]{3,2,1,0} fusion(), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}, backend_config={\"outer_dimension_partitions\":[\"1\",\"2\"]}\n  %reduce-window = f32[1,1,4,4]{3,2,1,0} reduce-window(%gather_bitcast_fusion, %constant.7), window={size=1x25x32x32 stride=1x25x32x32 pad=0_0x0_0x14_14x14_14}, to_apply=%region_0.1, backend_config={\"outer_dimension_partitions\":[\"1\",\"1\",\"2\"]}\n  ROOT %reduce_sum.7 = f32[] reduce(%reduce-window, %constant.7), dimensions={0,1,2,3}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\nReading this output takes some practice, but the relevant piece to answer your question is the line that starts with %gather.2 = f32[25,1,100,100]{3,2,1,0}: the gather primitive is XLA's version of indexing, and you see that it's explicitly constructing the full 25x100x100 array, and not removing the duplicated indices."
        ],
        "link": "https://stackoverflow.com/questions/79798175/does-jax-compilation-save-runtime-memory-by-recognizing-array-elements-that-ar"
    },
    {
        "title": "How to Make Batching Rule for Multiple Outputs",
        "question": "I am still exploring how to make batching rule correctly. Right now, my code of batching rule doesn't work as expected for multiple outputs. Here is my code.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.extend import core\nfrom jax.interpreters import batching\n\nmy_sum_p = core.Primitive('my_sum')\nmy_sum_p.multiple_results = True\n\ndef my_sum(x):\n    return my_sum_p.bind(x)\n\n# primal evaluation rule\ndef my_sum_impl(x):\n    return jnp.sum(x), jnp.ones((3,))\nmy_sum_p.def_impl(my_sum_impl)\n\n# batching rule\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (bd_x, bd_x)\n    return output, dims\nbatching.primitive_batchers[my_sum_p] = my_sum_batching_rule\n\n# example usage\nif __name__ == \"__main__\":\n    x = jnp.array([[1.0, 2.0, 3.0],\n                   [4.0, 5.0, 6.0]])\n    \n    vms = jax.vmap(my_sum)\n    print('my_sum:', vms(x))\n\n    def original_sum(x):\n        return jnp.sum(x), jnp.ones((3,))\n    vos = jax.vmap(original_sum)\n    print('original sum:', vos(x))\nThe output is like this\npython\nCopy\nmy_sum: [Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\noriginal sum: (Array([ 6., 15.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\nThe second output of my_sum is not batched like the one of original_sum.\nOn another case, when I change the vmap setup of my_sum such as follows\npython\nCopy\nvms = jax.vmap(my_sum, out_axes=(0, None))\nit raises an error\npython\nCopy\nTraceback (most recent call last):\n  File \"/home/yahya/Projects/neuralnet/neuralnet/nn/playground.py\", line 33, in <module>\n    print('my_sum:', vms(x))\n                     ~~~^^^\nValueError: vmap out_axes specification must be a tree prefix of the corresponding value, got specification (0, None) for value tree PyTreeDef([*, *]).\nMeanwhile the same vmap setup for the original_sum makes a correct output such as follows\npython\nCopy\noriginal_sum: (Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32))\nHow should my batching rule be? I really appreciate for any help",
        "answers": [
            "Your batching rule should look like this:\npython\nCopy\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (0, None)\n    return output, dims\nThe operative change here is that dims = (bd_x, bd_x) became dims = (0, None). The output batch dimensions must reflect the batching characteristics of the output, not the input: in this case, the first output is batched along the leading axis (bdim = 0), and the second output is unbatched (bdim = None).\nYou can test the correctness of this further by comparing the results when vmapping over different input and output axes; for example:\npython\nCopy\nprint(jax.vmap(original_sum, in_axes=(1,))(x))\nprint(jax.vmap(my_sum, in_axes=(1,))(x))\npython\nCopy\n(Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)]\npython\nCopy\nprint(jax.vmap(original_sum, in_axes=(1,), out_axes=(0, None))(x))\nprint(jax.vmap(my_sum, in_axes=(1,), out_axes=[0, None])(x))\npython\nCopy\n(Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\n(notice that out_axes is a tuple in the first case, because original_fun returns a tuple, and a list in the second case, because primitives with multiple_outputs=True return a list rather than a tuple)."
        ],
        "link": "https://stackoverflow.com/questions/79790782/how-to-make-batching-rule-for-multiple-outputs"
    },
    {
        "title": "`jax.lax.fori_loop` with equal `lower` and `upper` should produce no iteration, but body still executed",
        "question": "I have a code that uses a bunch of jax.lax.fori_loop. The documentation of fori_loop says that \"setting upper <= lower will produce no iterations\". So I was naively expecting the loop to just return its init_val unchanged. But in my case, it seems like it does attempt to execute the body.\nThe code is as follows:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis returns the following error:\npython\nCopy\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 47\n     45 Hks = jnp.zeros((0,2,2), dtype=complex)\n     46 U = jnp.eye(2, dtype=complex)\n---> 47 build(U, Hks)\n\nCell In[10], line 35\n     33 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     34 Uks = Uks.at[0].set(U)\n---> 35 Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n     36 return Uks\n\n    [... skipping hidden 12 frame]\n\nCell In[10], line 40\n     38 def update_Uks(k, val):\n     39     Uks, Hks = val\n---> 40     Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n     41     return Uks, Hks\n\nCell In[10], line 12\n     11 def binom_conv(n, Aks, Bks):\n---> 12     return part_binom_conv(n, 0, n, Aks, Bks)\n...\n--> 930     raise IndexError(f\"index is out of bounds for axis {x_axis} with size 0\")\n    931   i = _normalize_index(i, x_shape[x_axis]) if normalize_indices else i\n    932   i_converted = lax.convert_element_type(i, index_dtype)\n\nIndexError: index is out of bounds for axis 0 with size 0\nI'm not sure I understand what is going on here. Shouldn't the fori_loop just return its init_val and not cause this error?",
        "answers": [
            "JAX code has two phases of execution: tracing and runtime (see JAX Key Concepts: tracing for a description of this). A fori_loop with upper <= lower will have no iterations at runtime, but the code is still traced. The error you're seeing is coming up during tracing, which is necessary even for an empty loop in order to determine the shape and type of the output. You could work around this by specially handling the zero-length case in your fori_loop body function.\nA similar issue with while_loop is discussed at https://github.com/jax-ml/jax/issues/3285.",
            "Following the insight by @Obaskly and @jakevdp, I went with a wrapper for the fori_loop:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI [WORKS IN EXTERNAL LOOPS, BUT CAUSES ISSUE IN part_binom_conv()]\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis produces the correct behavior, and it seems to trace correctly under minimal testing.\nNote however that there is still an error if I replace the inner fori_loop in part_binom_conv by this wrapper. I think it causes an issue in tracing of nested loops.\nSorry for deleting and undeleting this answer a few times, this last point had me confused for a while."
        ],
        "link": "https://stackoverflow.com/questions/79783857/jax-lax-fori-loop-with-equal-lower-and-upper-should-produce-no-iteration"
    },
    {
        "title": "Sequential compilation times of a jax-jitted recursive function",
        "question": "I have a recursively defined function my_func that is jitted using jax.jit from the jax library. It is defined below:\npython\nCopy\n# Imports\n\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\nimport time\n\n\n# Constants and subroutines used in the core recursive routing below ...\n\nsx = jnp.asarray([[0,1.],[1.,0]], dtype=complex)\nsy = jnp.asarray([[0,-1j],[1j,0]], dtype=complex)\n\ndef conj_op(A):\n    return jnp.swapaxes(A, -1,-2).conj()\n\ndef commutator_herm(A, B):\n    comm = A @ B\n    comm = comm - conj_op(comm)\n    return comm\n\ndef H(t):\n    return jnp.cos(t) * sy\n\ndef X0(t):\n    return sx\n\n# Core recursive routine ...\n\n@partial(jax.jit, static_argnames=\"k\")\ndef my_func(t, k):\n    if k==0:\n        X_k = X0(t)\n        return X_k\n    else:\n        X_km1 = lambda t: my_func(t,k-1)\n        X_k = 1j * commutator_herm(H(t), X_km1(t)) + jax.jacfwd(X_km1, holomorphic=True)(t)\n        return X_k\nwith the relevant test:\npython\nCopy\n# Tests ...\n\nt = jnp.asarray(1, dtype=complex)\n\nseq_exec_times = []\n\nfor k in range(9,10): # or toggle to range(10) to compile sequentially\n    start = time.time()\n    my_func(t, k)\n    dur = time.time() - start\n    seq_exec_times.append(dur)\n\ntotal_seq_exec_time = sum(seq_exec_times)\n\nprint(\"Sequential execution times:\")\nprint([\"{:.3e} s\".format(x) for x in seq_exec_times])\nprint(\"Total execution time:\")\nprint(\"{:.3e} s\".format(total_seq_exec_time))\nIf I execute this function the first time only for k=9, then I get a quite long compilation time, which I figure is because tracing a recursive function like this one takes an effort that scales exponentially with recursion depth. The output is:\nSequential execution times:\n['6.306e+01 s']   # First execution time when calling directly with k=9\nTotal execution time:\n6.306e+01 s\nBut then I thought that in practice, I need to evaluate my_func for increasing values of k=0,1,2,3... anyway. And if the lower step has already been traced, then you only need to trace the next level of the tree, and that should be more efficient. And indeed, executing k=1,2,3...,8 before executing k=9 yields a slightly lower execution time the first time k=9 is evaluated:\nSequential execution times:\n['3.797e-03 s',\n'2.203e-02 s',\n'3.487e-02 s',\n'7.054e-02 s',\n'1.779e-01 s',\n'4.680e-01 s',\n'1.326e+00 s',\n'4.145e+00 s',\n'1.456e+01 s',\n'5.550e+01 s']    # First execution time of k=9 after calling k=0,1,2,3...,8 first\nTotal execution time:\n7.631e+01 s\nThat said, this still scales exponentially with recursion depth, and I was naively expecting the compilation of k=9 to be more efficient. If the lower levels k=1,2,3...,8 are already compiled, then I would naively expect the compilation at the next level k=9 to be relatively simple. I would think that you can simply trace the link between k=9 and k=8, and avoid going through the whole recursion tree again at the lower levels.\nBut it looks like I was wrong, and I'm curious to know why? And if I'm not wrong, how do I make this better?\nThis was run with jax - 0.4.33 on MacOS - 15.6.1.",
        "answers": [
            "In general, you should avoid a recursive coding style when using JAX code with JIT, autodiff, or other transformations.\nThere are three different things at play here that complicate the analysis of runtimes:\ntracing: this is the general process used in transforming JAX code, whether for jit or for autodiff like jacfwd . I believe the main reason you are seeing different timings depending on the sequence of executions is because of the trace cache: for each value of k, the function will be traced only once and subsequent calls will use the cached trace.\nautodiff: the jacfwd call in your function retraces the original function and generates a sequence of operations representing the forward-jacobian. I don't believe that there is any cache for this, so each time you call jacfwd the transformation will be recomputed from the cached trace.\ncompilation: I don't believe the that compilation pass currently makes use of previously-compiled units using the trace cache. Any control flow in JAX (loops, recursive calls, etc.) are effectively flattened before being passed to the compiler: in your case the number of operations looks to scale roughly as O[3^k]. Compilation cost is superlinear—and often roughly quadratic—in the number of operations, and so you'll find compilation will become very expensive as k gets larger.\nUnfortunately, there's not really any workaround for these issues. When using JAX, you should avoid deep Python control flow like for loops and recursion. You may be able to make progress by re-expressing your recursive function as an iterative function, using one of JAX's control flow operators like fori_loop to reduce the number of lines and cut down the compilation time."
        ],
        "link": "https://stackoverflow.com/questions/79769647/sequential-compilation-times-of-a-jax-jitted-recursive-function"
    },
    {
        "title": "JAX, recompilation when using closure for a function",
        "question": "I have a jax code where I would like to scan over an array. In the body function of the scan, I have a pytree to store some parameters and functions that I want to apply during the scan. For the scan, I used lambda to bake in the object/pytree named params.\nDoes this trigger a new compilation when a new params is passed in the function example? If so, how can I avoid the recompilation?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\n\nclass Params:\n    def __init__(self, x_array, a):    \n        self.x_array = x_array\n        self.a = a\n        return\n    \n    def one_step(self,state, input):\n        x = state\n        y = input\n        next_state = (self.x_array + x + jnp.ones(self.a))*y\n        return next_state\n\n    def _tree_flatten(self):\n        children = (self.x_array,)\n        aux_data = {'a':self.a}\n        return (children, aux_data)\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n        \ntree_util.register_pytree_node(Params,\n                               Params._tree_flatten,\n                               Params._tree_unflatten)\n\ndef scan_body(params, state, input):\n    x = state\n    y = input \n    x_new = params.one_step(x, y) \n    return x_new, [x_new]\n\n@jax.jit\ndef example(params):\n    body_fun = lambda state, input: scan_body(params, state, input)\n    init_state = jnp.array([0.,1.])\n    input_array = jnp.array([1.,2.,3.])\n    last_state, result_list = jax.lax.scan(body_fun, init_state, input_array)\n    return last_state, result_list\n\nif __name__ == \"__main__\":\n\n    params1 = Params(jnp.array([1.,2.]), 2)\n    last_state, result_list = example(params1)\n    print(last_state)\n\n    params2 = Params(jnp.array([3.,4.]), 2)\n    last_state, result_list = example(params2)\n    print(last_state)",
        "answers": [
            "Passing a new params object would only trigger recompilation if the static attributes of your params were to change. Since aux_data is static, changing the value of params.a will lead to re-compilation. Since children are dynamic, then changing the shape, dtype, or sharding of params.x will lead to recompilation, but changing the array values/contents will not.\nIn your example, in both calls params.x has the same shape, dtype, and sharding, and params.a has the same value, so there should not be any recompilation (if you're unsure, you could confirm this using the approach mentioned at https://stackoverflow.com/a/70127930/2937831).\nNote in particular that the lambda functions used in the method implementations cannot affect the JIT cache key because they are not referenced in the pytree flattening output."
        ],
        "link": "https://stackoverflow.com/questions/79744486/jax-recompilation-when-using-closure-for-a-function"
    },
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\npython\nCopy\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\npython\nCopy\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\npython\nCopy\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Are JAX operations already vectorized?",
        "question": "In the documentation, JAX provides vectorization. However, aren't JAX operations already vectorized? For example, to add two vectors, I thought that the element-wise additions were vectorized internally already.\nMy guess is that vectorization is useful when: it's hard for us to add a dimension for broadcasting, so we resort to a more explicit vectorization.\nEDIT: for example, instead of vectorizing convolution2d with different kernels, I simply stack the kernels, copy and stack the channel, then perform the convolution2d with this stack of kernels.",
        "answers": [
            "I have also raised a similar question here: https://github.com/jax-ml/jax/issues/26212 By now I think there is no universal answer to this and it will remain a matter of taste to a certain degree. However in some cases there is a clearer answer:\nSome operations in JAX are not natively vectorized, such as e.g. jnp.histogram or jnp.bincount, in this case you can use vmap to get a \"batched\" version of that function (for example search for \"batched_histogram\" here http://axeldonath.com/jax-diffusion-models-pydata-boston-2025/). This is really convenient and avoids loops to improve readability as well as performance.\nvmap works over PyTrees. Some libraries (most notably equinox) use this to avoid the need for handling a batch axis in models completely and just finally vmap over the whole parameter tree by convention. This frees developers from thinking about the batch axis at all, but when working with equinox you have to stick to that convention. It also only works if operations are independent across different batches. It does not work for operations such as a \"batch norm\" (see also https://docs.kidger.site/equinox/examples/stateful/)\nIn some cases one introduces a local(!) extra dimension to an array to avoid writing a Python loop and optionally reduce after. This can often be implemented more shortly and with clearer intent using vmap (basically what you said).\nAs broadcasting and batch axes are universally accepted convention in deep learning I mostly stick with them. But I rely on vmap whenever there is no native vectorization, whenever I work with libraries that rely on vmap by convention, or whenever I need to vectorize operations along non-conventional axes (basically everything except batch axis).",
            "It's true that some JAX operations are automatically vectorized. However, not all functions are batch-aware, and even functions which are batch-aware may not have the intrinsic batching semantics you need in your code.\nAs a simple example, consider a function which multiplies two 1D vectors:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(x: jax.Array, y: jax.Array) -> jax.Array:\n  return x @ y\n\nx = jnp.ones(10)\ny = jnp.ones(10)\nresult = f(x, y)\nprint(result)  # 10.0\n10.0\nNow say you'd like to execute this over a batch of x values: due to the batching semantics of the @ operator, this just works:\npython\nCopy\nx_batch = jnp.arange(50).reshape(5, 10)\nresult = f(x_batch, y)\nprint(result)\n[ 45. 145. 245. 345. 445.]\nOn the other hand, if you want to execute this over a batch of y vectors, it won't work out so easily:\npython\nCopy\ny_batch = jnp.arange(50).reshape(5, 10)\nf(x, y_batch)\nTypeError: dot_general requires contracting dimensions to have the same shape, got (10,) and (5,).\nWithout a tool like vmap, you'd have to either re-define the function to work appropriately for batched inputs, or modify the inputs based on what you know of the function's implementation (e.g. pass f(x, y.T) in this case). However, this type of approach can be error-prone and brittle, especially as the function increases in complexity.\nWith jax.vmap, you can automatically generate an appropriately batched version of the function, which saves you the headache of re-implementing things:\npython\nCopy\nf_batched = jax.vmap(f, in_axes=(None, 0))  # x unmapped, y mapped over axis 0\nresult = f_batched(x, y_batch)\nprint(result)\n[ 45. 145. 245. 345. 445.]\nMoreover, this vmap solution will work correctly for essentially any function composed of transformable JAX operations.\nSo even though some JAX operations support implicit batching, the explicit batch transformation provided by vmap can be quite useful in practice."
        ],
        "link": "https://stackoverflow.com/questions/79718029/are-jax-operations-already-vectorized"
    },
    {
        "title": "Does vmap correctly split the RNG keys?",
        "question": "In the following code, when I remove the vmap, I have the right randomized behavior. However, with vmap, I don't anymore. Isn't this supposed to be one of the features of nnx.vmap?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\n@nnx.vmap(\n    in_axes=(None, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((4, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")",
        "answers": [
            "To make dropout work together with vmap in flax, we need to use split_rngs and StateAxes :\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\nbs = 4\n\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=bs, only='dropout')\n@nnx.vmap(\n    in_axes=(state_axes, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\nmodel.train()\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\nOutput with jax: 0.7.0.dev20250704, flax: 0.10.6\nOutput batch:\n[[0.         0.1736668  1.6533196  0.         0.        ]\n [0.         0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.",
            "I'm not sure nnx.vmap and nnx.split_rngs are necessary in vfdev's answer. Also, having a train kwarg is unnecessary in most situations since NNX models can dynamically jump between train=True, train=False with .train() and .eval()\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    x = self.linear(x)\n    x = self.dropout(x)\n    return x\n\nkey = jax.random.PRNGKey(42)\n\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n@nnx.jit\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  return model(x)\n\nbs = 4\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Enable training. This works because Dropout layers have a .deterministic property\n# that can be modified.\nmodel.train()\n\noutput_batch = batched_apply(model, batch_input)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\noutput:\nModel initialized successfully.\nDropout Rate: 0.5\n------------------------------\nInput batch shape: (4, 10)\nInput batch:\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n------------------------------\nRunning the batched model in training mode (dropout is active)...\nOutput batch shape: (4, 5)\n\nOutput batch:\n[[0.         0.1736668  0.         0.         0.        ]\n [0.         0.         1.6533196  1.0752656  0.        ]\n [0.         0.         0.         0.         0.7218913 ]\n [0.09358063 0.         0.         1.0752656  0.        ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.\nand if instead you do model.eval()\nOutput batch:\n[[0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]]\n------------------------------\n❌ Verification failed: All outputs were the same."
        ],
        "link": "https://stackoverflow.com/questions/79698307/does-vmap-correctly-split-the-rng-keys"
    },
    {
        "title": "Configuration options varying between jax installs?",
        "question": "I have a laptop I do work on for a program that includes jax, the program ends up getting run here on a small scale to test it, then it is sent off to a server for batch processing.\nIn the program I have set these flags for jax:\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\n(as well as others but these are the relevant ones)\nThis runs fine on my laptop (using sharding to CPU parallelise), but when running on the server on GPU, I get an error message:\nAttributeError: Unrecognized config option: jax_captured_constants_report_frames\n(and the same for jax_captured_constants_warn_bytes if that were to run first)\nWhy is there this discrepancy? Can I use these flags some other way that is generalised between different jax installs?\npip list | grep jax, on laptop:\njax                       0.6.2\njaxlib                    0.6.2\njaxtyping                 0.3.2\non server:\njax                       0.6.0\njax-cuda12-pjrt           0.6.0\njax-cuda12-plugin         0.6.0\njaxlib                    0.6.0\njaxtyping                 0.3.2\nEDIT: As a side note, what is the scope of jax flags? I have a jax initialisation function to set os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=\" + str(cpu_count()) before the rest of the code runs, if I set jax.config.update(..., ...) options in here, will they hold in files called after it that also import jax? Or do I have to set them again? Is there a function to check the current value of these flags?",
        "answers": [
            "The jax_captured_constants_report_frames and jax_captured_constants_warn_bytes configurations were added in JAX version 0.6.1 (Relevant PR: https://github.com/jax-ml/jax/pull/28157) If you want to use them on your server, you'll have to update JAX to v0.6.1 or later."
        ],
        "link": "https://stackoverflow.com/questions/79693916/configuration-options-varying-between-jax-installs"
    },
    {
        "title": "Jax vmapping while loop [closed]",
        "question": "Closed. This question needs debugging details. It is not currently accepting answers.\nEdit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\nClosed 5 months ago.\nImprove this question\nI have a function that has jax.lax.while_loop. Now, I want to vmap it. However, vmap makes the execution time very slow compared to the original one.\nI understand that in the case of lax.cond, it is transformed into select, which evaluates all branches and thus may decrease the computational speed.\nIs a similar thing happening here? If so, what is the best practice to do do xx while y is true with vmap?",
        "answers": [
            "A while_loop under vmap becomes a single while_loop over a batched body_fun and cond_fun, meaning effectively that every loop in the batch executes for the same number of iterations. If different batches lead to vastly different iteration times, this can result in extra computation compared to executing individual while_loops in sequence."
        ],
        "link": "https://stackoverflow.com/questions/79660448/jax-vmapping-while-loop"
    },
    {
        "title": "Looking for an efficent JAX function to reconstruct an image from patches",
        "question": "I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.\nI am trying to reconstruct the original images from the patches. Here is vanilla python code that works:\npython\nCopy\nkernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)\n\ndef fwd(x):\n    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')\n\n    # return channels last\n    return jnp.transpose(xcv, [0,2,3,1])\n\npatches = fwd(bfrc)\n\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\n\n# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT\n# H_PATCHES == IMG_WIDTH // PATCH_WIDTH\n\nreconstructed = np.zeros(EXPECTED_IMG_SHAPE)\n\nfor vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):\n    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):\n        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):\n            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):\n                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):\n                    row = vpatch * PATCH_HEIGHT + prow\n                    col = hpatch * PATCH_WIDTH + pcol\n                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]\n\n# This assert passes\nassert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0\nOf course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?",
        "answers": [
            "I'm not sure what happened here:\npython\nCopy\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\nbut I assume it's some kind of mistake.\nAssuming bfrc has shape of (batch, channels, height, width), and\npython\nCopy\nV_PATCHES = IMG_HEIGHT // PATCH_HEIGHT\nH_PATCHES = IMG_WIDTH // PATCH_WIDTH\nthen patch_reshaped_pn_c_h_w will have the shape of (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH).\nKeeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.\npython\nCopy\nV, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape\n\nH_total = V * PH\nW_total = H * PW\n\npatches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)\n\nreconstructed = patches.reshape(V, H, PH, PW, C)\nreconstructed = reconstructed.transpose(0, 2, 1, 3, 4)\nreconstructed = reconstructed.reshape(H_total, W_total, C)\nreconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)\nYou can additionally wrap it into @jax.jit, which should be slightly faster."
        ],
        "link": "https://stackoverflow.com/questions/79647350/looking-for-an-efficent-jax-function-to-reconstruct-an-image-from-patches"
    },
    {
        "title": "Would using lists rather than jax.numpy arrays lead to more accurate numerical transformations?",
        "question": "I am doing a project with RNNs using jax and flax and I have noticed some behavior that I do not really understand.\nMy code is basically an optimization loop where the user provides the initial parameters for the system they want to optimize. This system is divided onto several time steps. He feeds the initial input into the first time step of the the system, gets a certain output, feeds this output into a RNN which returns the parameters for the following time step and so on. Then it is optimized using adam (particularly using optax).\nNow the user inputs his initial parameters as a dict and then there is a function called prepare_parameters_from_dict that basically converts this dict into a list of lists (or a list of jnp arrays for that matter).\nMy question/observation is when I make this function return a list of jnp.arrays instead of a list of lists, the property I am optimizing is an order of magnitude worse!\nFor example, using a list of lists outputs 0.9997 and a list of jnp.arrays outputs 0.998 (the closer to one the better).\nNoting: the RNN output a list of jnp.arrays (it is using flax linnen) and everything in the code remains the same.\nHere are said function:\nOutputing list of lists:\npython\nCopy\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(flat_params)\n        shapes.append(len(flat_params))\n    return res, shapes\nUsing list of jnp.arrays:\npython\nCopy\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(jnp.array(flat_params))\n        shapes.append(jnp.array(flat_params).shape[0])\n    return res, shapes\nand this is an example of the users input initial params:\npython\nCopy\ninitial_params = {\n    \"param1\": {\n        \"gamma\": 0.1,\n        \"delta\": -3 * jnp.pi / 2,\n    }\n}\nThe rest of the code remains exactly the same for both.\nAfter optimization if for example there were five time steps, this is how the final optimized params for each time step would look like:\nusing list of jnp.arrays:\npython\nCopy\n[[Array([ 0.1       , -4.71238898], dtype=float64)],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nusing list of lists:\npython\nCopy\n[[ [0.1       , -4.71238898] ]],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nWould such a difference in behavior be due to how jax handles grad and jit and others with lists compared to jnp.arrays or am I missing something?",
        "answers": [
            "The main operative difference between these two cases is that Python floats are treated as weakly-typed, meaning that the list version of your code could result in operations being performed at a lower precision. For example:\npython\nCopy\nIn [1]: import jax\n\nIn [2]: import jax.numpy as jnp\n\nIn [3]: jax.config.update('jax_enable_x64', True)\n\nIn [4]: list_values = [0.1, -4.71238898]\n\nIn [5]: array_values = jax.numpy.array(list_values)\n\nIn [6]: x = jax.numpy.float32(1.0)\n\nIn [7]: x + list_values[1]\nOut[7]: Array(-3.712389, dtype=float32)\n\nIn [8]: x + array_values[1]\nOut[8]: Array(-3.71238898, dtype=float64)\nNotice that the array version leads to higher-precision computations in this case. If I had to guess what the main difference is in your two runs, I'd guess something to do with the precision implied by strict vs weak types."
        ],
        "link": "https://stackoverflow.com/questions/79634990/would-using-lists-rather-than-jax-numpy-arrays-lead-to-more-accurate-numerical-t"
    },
    {
        "title": "JAX Point Cloud Processing: Slow index_points_3d operation causing extreme XLA fusion loops in backpropagation",
        "question": "I'm trying to use JAX for implementing point cloud processing. However, I found that training becomes extremely slow due to my implementation of the following index_points_3d operation, which performs selection of features based on 3D indices.\nHere's my current implementation:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    features_expanded = features[..., None, :]\n    idx_expanded = indices[..., None]\n    return jnp.take_along_axis(features_expanded, idx_expanded, axis=1)\nWhen I traced the profiler, I found that this operation triggers extreme repetitions of loop_dynamic_update_slice_fusion, loop_add_fusion, input_reduce_fusion, and loop_select_fusion in the backpropagation stage as in following.\nThe forward pass is not a problem since the learning went fast when I stopped the gradient of the output features.\nI've tried different implementations such as using vmap on the batch dimension, but failed to achieve any performance gains.\nI'm not deeply familiar with JAX's low-level operations, so I'm unsure if this is a fundamental limitation of JAX/XLA or if there's a more efficient approach. Any help or guidance on optimizing this operation would be greatly appreciated!",
        "answers": [
            "Thanks to jakevdp's comment, I got a significant speedup using one-hot matrix multiplication. I changed to the following code:\npython\nCopy\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    B, N, C = features.shape\n    _, S, K = indices.shape\n    one_hot = jax.nn.one_hot(indices, num_classes=N, dtype=features.dtype)\n    return jnp.einsum('bskn,bnc->bskc', one_hot, features)"
        ],
        "link": "https://stackoverflow.com/questions/79631678/jax-point-cloud-processing-slow-index-points-3d-operation-causing-extreme-xla-f"
    },
    {
        "title": "Why is array manipulation in JAX much slower?",
        "question": "I'm working on converting a transformation-heavy numerical pipeline from NumPy to JAX to take advantage of JIT acceleration. However, I’ve found that some basic operations like broadcast_to and moveaxis are significantly slower in JAX—even without JIT—compared to NumPy, and even for large batch sizes like 3,000,000 where I would expect JAX to be much quicker.\npython\nCopy\n### Benchmark: moveaxis + broadcast_to ###\nNumPy: moveaxis + broadcast_to → 0.000116 s\nJAX: moveaxis + broadcast_to → 0.204249 s\nJAX JIT: moveaxis + broadcast_to → 0.054713 s\n\n### Benchmark: broadcast_to only ###\nNumPy: broadcast_to → 0.000059 s\nJAX: broadcast_to → 0.062167 s\nJAX JIT: broadcast_to → 0.057625 s\nAm I doing something wrong? Are there better ways of performing these kind of manipulations?\nHere's a minimal benchmark ChatGPT generated, comparing broadcast_to and moveaxis in NumPy, JAX, and JAX with JIT:\npython\nCopy\nimport timeit\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit\n\n# Base transformation matrix\nM_np = np.array([[1, 0, 0, 0.5],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1]])\n\nM_jax = jnp.array(M_np)\n\n# Batch size\nn = 1_000_000\n\nprint(\"### Benchmark: moveaxis + broadcast_to ###\")\n\n# NumPy\nt_numpy = timeit.timeit(\n    lambda: np.moveaxis(np.broadcast_to(M_np[:, :, None], (4, 4, n)), 2, 0),\n    number=10\n)\nprint(f\"NumPy: moveaxis + broadcast_to → {t_numpy:.6f} s\")\n\n# JAX\nt_jax = timeit.timeit(\n    lambda: jnp.moveaxis(jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)), 2, 0).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: moveaxis + broadcast_to → {t_jax:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_and_move_jax(M):\n    return jnp.moveaxis(jnp.broadcast_to(M[:, :, None], (4, 4, n)), 2, 0)\n\n# Warm-up\nbroadcast_and_move_jax(M_jax).block_until_ready()\n\nt_jit = timeit.timeit(\n    lambda: broadcast_and_move_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: moveaxis + broadcast_to → {t_jit:.6f} s\")\n\nprint(\"\\n### Benchmark: broadcast_to only ###\")\n\n# NumPy\nt_numpy_b = timeit.timeit(\n    lambda: np.broadcast_to(M_np[:, :, None], (4, 4, n)),\n    number=10\n)\nprint(f\"NumPy: broadcast_to → {t_numpy_b:.6f} s\")\n\n# JAX\nt_jax_b = timeit.timeit(\n    lambda: jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: broadcast_to → {t_jax_b:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_only_jax(M):\n    return jnp.broadcast_to(M[:, :, None], (4, 4, n))\n\nbroadcast_only_jax(M_jax).block_until_ready()\n\nt_jit_b = timeit.timeit(\n    lambda: broadcast_only_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: broadcast_to → {t_jit_b:.6f} s\")",
        "answers": [
            "There are a couple things happening here that come from the different execution models of NumPy and JAX.\nFirst, NumPy operations like broadcasting, transposing, reshaping, slicing, etc. typically return views of the original buffer. In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies. I suspect this is the largest contribution to the timing difference here.\nSecond, NumPy tends to have very fast dispatch time for individual operations. JAX has much slower dispatch time for individual operations, and this can become important when the operation itself is very cheap (like \"return a view of the array with different strides/shape\")\nYou might wonder given these points how JAX could ever be faster than NumPy. The key is JIT compilation of sequences of operations: within JIT-compiled code, sequences of operations are fused so that the output of each individual operation need not be allocated (or indeed, need not even exist at all as a buffer of intermediate values). Additionally, for JIT compiled sequences of operations the dispatch overhead is paid only once for the whole program. Compare this to NumPy where there's no way to fuse operations or to avoid paying the dispatch cost of each and every operation.\nSo in microbenchmarks like this, you can expect JAX to be slower than NumPy. But for real-world sequences of operations wrapped in JIT, you should often find that JAX is faster, even when executing on CPU.\nThis type of question comes up enough that there's a section devoted to it in JAX's FAQ: FAQ: is JAX faster than NumPy?\nAnswering the followup question:\nIs the statement \"In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies\", within a jitted environment?\nThis question is not really well-formulated, because in a jitted environment, array objects do not necessarily correspond to buffers of values. Let's make this more concrete with a simple example:\npython\nCopy\nimport jax\n\n@jax.jit\ndef f(x):\n  y = x[::2]\n  return y.sum()\nYou might ask: in this program, is y a copy or a view of x? The answer is neither, because y is never explicitly created. Instead, JIT fuses the slice and the sum into a single operation: the array x is the input, and the array y.sum() is the output, and the intermediate array y is never actually created.\nYou can see this by printing the compiled HLO for this function:\npython\nCopy\nx = jax.numpy.arange(10)\nprint(f.lower(x).compile().as_text())\npython\nCopy\nHloModule jit_f, is_scheduled=true, entry_computation_layout={(s32[10]{0})->s32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.9 (Arg_0.10: s32[], Arg_1.11: s32[]) -> s32[] {\n  %Arg_0.10 = s32[] parameter(0), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  %Arg_1.11 = s32[] parameter(1), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  ROOT %add.12 = s32[] add(s32[] %Arg_0.10, s32[] %Arg_1.11), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\n%fused_computation (param_0.2: s32[10]) -> s32[] {\n  %param_0.2 = s32[10]{0} parameter(0)\n  %iota.0 = s32[5]{0} iota(), iota_dimension=0, metadata={op_name=\"jit(f)/jit(main)/iota\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.1 = s32[] constant(2)\n  %broadcast.0 = s32[5]{0} broadcast(s32[] %constant.1), dimensions={}\n  %multiply.0 = s32[5]{0} multiply(s32[5]{0} %iota.0, s32[5]{0} %broadcast.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %bitcast.1 = s32[5,1]{1,0} bitcast(s32[5]{0} %multiply.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %gather.0 = s32[5]{0} gather(s32[10]{0} %param_0.2, s32[5,1]{1,0} %bitcast.1), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}, indices_are_sorted=true, metadata={op_name=\"jit(f)/jit(main)/gather\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.0 = s32[] constant(0)\n  ROOT %reduce.0 = s32[] reduce(s32[5]{0} %gather.0, s32[] %constant.0), dimensions={0}, to_apply=%region_0.9, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\nENTRY %main.14 (Arg_0.1: s32[10]) -> s32[] {\n  %Arg_0.1 = s32[10]{0} parameter(0), metadata={op_name=\"x\"}\n  ROOT %gather_reduce_fusion = s32[] fusion(s32[10]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\nThe output is complicated, but the main thing to look at here is the ENTRY %main section, which is the \"main\" program generated by compilation. It consists of two steps: %Arg0.1 identifies the input argument, and ROOT %gather_reduce_fusion is essentially a single compiled kernel that sums every second element of the input. No intermediate arrays are generated. The blocks above this (e.g. the %fused_computation (param_0.2: s32[10]) -> s32[] definition) give you information about what operations are done within this kernel, but represent a single fused operation.\nNotice that the sliced array represented by y in the Python code never actually appears in the main function block, so questions about its memory layout cannot be answered except by saying \"y doesn't exist in the compiled program\".",
            "According to the Jax Docs (emphasis mine):\nif you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead"
        ],
        "link": "https://stackoverflow.com/questions/79615872/why-is-array-manipulation-in-jax-much-slower"
    },
    {
        "title": "DIfference in variable values in jax non-jit runtime and jit transformed runtime",
        "question": "I have a deep learning mode which I am running in the jit transformed manner by:\npython\nCopy\nmy_function_checked = checkify.checkify(model.apply)\n    model_jitted = jax.jit(my_function_checked)\n    err, pred = model_jitted({\"params\": params}, batch, training=training, rng=rng)\n    err.throw()\nThe code is compiling fine, but now I want to debug the intermediate values after every few steps, save the arrays, and then compare them with pytorch tensors. For this, I need to repeatedly save the arrays. The easiest way to do this is to use any IDE's inbuilt debugger and evaluate the save expression after every few steps. But jax.jit transformed code doesn't allow external debuggers. But, I can do this after disabling the jit. Should I be expecting any discrepancies between the two runs? Can I assume that the values in jit and non-jit runs will remain same?",
        "answers": [
            "In general when comparing the same JAX operation with and without JIT, you should expect equivalence up to typical floating point rounding errors, but you should not expect bitwise equivalence, as the compiler may fuse operations in a way that leads to differing float error accumulation."
        ],
        "link": "https://stackoverflow.com/questions/79571227/difference-in-variable-values-in-jax-non-jit-runtime-and-jit-transformed-runtime"
    },
    {
        "title": "Why is Jax treating floating point values as tracers rather than concretizing them when nesting jitted functions?",
        "question": "I am doing some physics simulations using jax, and this involves a function called the Hamiltonian defined as follows:\npython\nCopy\n# Constructing the Hamiltonian\n@partial(jit, static_argnames=['n', 'omega'])\ndef hamiltonian(n: int, omega: float):\n    \"\"\"Construct the Hamiltonian for the system.\"\"\"\n    H = omega *  create(n) @ annhilate(n)\n    return H \nand then a bigger function def solve_diff(n, omega, kappa, alpha0): that is defined as follows:\npython\nCopy\n@partial(jit, static_argnames=['n', 'omega'])\ndef solve_diff(n, omega, kappa, alpha0):\n    # Some functionality that uses kappa and alpha0\n    \n    H = hamiltonian(n, omega)\n\n    # returns an expectation value\nWhen I try to compute the gradient of this function using jax.grad\npython\nCopy\nn = 16   \nomega = 1.0   \nkappa = 0.1  \nalpha0 = 1.0 \n\n# Compute gradients with respect to omega, kappa, and alpha0\ngrad_population = grad(solve_diff, argnums=(1, 2, 3))\ngrads = grad_population(n, omega, kappa, alpha0)\n\nprint(f\"Gradient w.r.t. omega: {grads[0]}\")\nprint(f\"Gradient w.r.t. kappa: {grads[1]}\")\nprint(f\"Gradient w.r.t. alpha0: {grads[2]}\")\nit outputs the following error:\npython\nCopy\nValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jax._src.interpreters.ad.JVPTracer'>, Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n  primal = 1.0\n  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n    pval = (ShapedArray(float32[], weak_type=True), None)\n    recipe = LambdaBinding(). The error was:\nTypeError: unhashable type: 'JVPTracer'\nThough, running solve_diff(16,1.0,0.1,1.0) on its own works as expected.\nNow if I remove omega from the list of static variables for both the hamiltonian function and the solve_diff, the grad is output as expected.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls, both n and omega are constants and indeed should not change between function calls.",
        "answers": [
            "The fundamental issue is that you cannot differentiate with respect to a static variable, and if you try to do so you will get the error you observed.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls\nIn JAX, the term \"static\" does not have to do with whether the variable is changed between function calls. Rather, a static variable is a variable that does not participate in tracing, which is the mechanism used to compute transformations like vmap, grad, jit, etc. When you differentiate with respect to a variable, it is no longer static because it is participating in the autodiff transformation, and trying to treat it as static later in the computation will lead to an error.\nFor a discussion of transformations, tracing, and related concepts, I'd start with JAX Key Concepts: transformations."
        ],
        "link": "https://stackoverflow.com/questions/79550040/why-is-jax-treating-floating-point-values-as-tracers-rather-than-concretizing-th"
    },
    {
        "title": "How to make a custom pytree node works with grad in JAX",
        "question": "I created a custom pytree node following this tutorial. My custom pytree node works for many operations and transformations, except grad.\nHere is my code.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\n\nclass MyLinear:\n    def __init__(self, w, b):\n        self.w = jnp.array(w)\n        self.b = jnp.array(b)\n\n    def __call__(self, x):\n        return jnp.dot(x, self.w) + self.b\n\n    def tree_flatten(self):\n        return (self.w, self.b), ()\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n\ntree_util.register_pytree_node(MyLinear, MyLinear.tree_flatten, MyLinear.tree_unflatten)\nThen I tested this code\npython\nCopy\ninputs = jnp.ones((2,2))\ninput = inputs[0]\nmylinear = MyLinear([1.0, 1.0], 1.)\nprint(tree_util.tree_map(lambda x: x+1, input)) # [2. 2.] works as expected\nprint(jax.vmap(mylinear)(inputs))   # [3. 3.] works as expected\nprint(jax.jit(jax.vmap(mylinear))(inputs))  # [3. 3.] works as expected\ndef loss_fn(model, x):\n    out = jax.vmap(model)(x)\n    return jnp.sum(out ** 2)\nprint(loss_fn(mylinear, inputs))    # 18.0 works as expected\nprint(jax.grad(loss_fn)(mylinear, inputs))  # <__main__.MyLinear object at 0x10e317890> doesn't work as expected\nIt seems grad doesn't recognize object of MyLinear as a flattenable tree like object of tuple, list, or dict. What should the code be so that objects of this class recognizable by all jax transformations? Thank you for the help.",
        "answers": [
            "I think there is a misunderstanding here. From what I can tell the code works exactly as intended. JAX operates on \"structs of arrays\". So your MyLinear class works as a data container for arrays, as well as their gradients. When applying jax.grad() to a PyTree, JAX will return the same PyTree, but containing the gradients of the corresponding nodes. So you can access the individual gradient like so:\npython\nCopy\ninputs = jnp.ones((2,2))\nmylinear = MyLinear([1.0, 1.0], 1.)\n\ndef loss_fn(model, x):\n    out = jax.vmap(model)(x)\n    return jnp.sum(out ** 2)\n\ngrads = jax.grad(loss_fn)(mylinear, inputs)\nprint(grads.w)\nprint(grads.b)\nI hope this clarifies the behavior!"
        ],
        "link": "https://stackoverflow.com/questions/79522105/how-to-make-a-custom-pytree-node-works-with-grad-in-jax"
    },
    {
        "title": "How can I apply member functions of a list of objects across slices of a JAX array using vmap?",
        "question": "I have a list of a objects, each of which has a function to be applied on a slice of a jax.numpy.array. There are n objects and n corresponding slices. How can I vectorise this using vmap?\nFor example, for the following code snippet:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nclass Obj:\n    def __init__(self, i):\n        self.i = i\n\n    def f1(self, x): return (x - self.i)\n\nx = jnp.arange(9).reshape(3, 3).astype(jnp.float32)\n\nfunctions_obj = [Obj(1).f1, Obj(2).f1, Obj(3).f1]\nhow would I apply the functions in functions_obj to slices of x?\nMore details, probably not relevant: My specific use-case is running the member functions of a lot of Reinforcement Learning Gym environment objects on slices of an actions array, but I believe my problem is more general and I formulated it as above. (P.S.: I know about AsyncVectorEnv by the way but that does not solve my problem as I am not trying to run the step function).",
        "answers": [
            "Use jax.lax.switch to select between the functions in the list and map over the desired axis of x at the same time:\npython\nCopy\ndef apply_func_obj(i, x_slice):\n    return jax.lax.switch(i, functions_obj, x_slice)\n\nindices = jnp.arange(len(functions_obj)) \n# Use vmap to apply the function element-wise\nresults = jax.vmap(apply_func_obj, in_axes=(0, 0))(indices, x)"
        ],
        "link": "https://stackoverflow.com/questions/79499056/how-can-i-apply-member-functions-of-a-list-of-objects-across-slices-of-a-jax-arr"
    },
    {
        "title": "Jax numpy extracting non-nan values gives NonConcreteBooleanIndexError",
        "question": "I have a jax 2d array with some nan-values\npython\nCopy\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\nand want to get an array which contains for each row only the non-nan values. The resulting array has thus the same number of rows, and either less columns or the same number but with nan values padded at the end. So in this case, the result should be\npython\nCopy\narray_2d = jnp.array([\n    [1,   2,      3],\n    [10  20,jnp.nan]\n    ])\nThe order (among non-nan values) should stay the same.\nTo make things easier, I know that each row has at most k (in this case 3) non-nan values. Getting the indices for the non-nan values is very easy, but ``moving them to the front'' is harder.\nI tried to work on a row-by-row basis; the following function works indeed:\npython\nCopy\n# we want to vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\nHowever, I can't vmap this. Even though I payed attention that the returned array always has the same size, the line ret_vals = row_vals[row_mask] makes problems, since this has a dynamic size. Does anyone know how to circumvent this? I believe that functions like `jnp.where' etc don't help either.\nHere is the full MWE:\npython\nCopy\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n# we want to get -- efficiently -- all non-nan values per row.\n# we know that each row has at most 3 non-nan values\n\n# we will vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\n\n# we now vmap\nnon_nan_vals = jax.vmap(get_non_nan_values)(array_2d) # this gives error: NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[5])\nNB: The array will be very large in practice and have many nan values, while k (the number of non-nan values) is on the order of 10 or 100.\nThank you very much!",
        "answers": [
            "By padding the array with a fill value at the end of each row first, you can rely on jnp.nonzero and its size and fill_value arguments, which define a fixed output size and fill value index, when the size requirement is not met. Here is a minimal example:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n\n@jax.vmap\ndef get_non_nan_values(row_vals, size=3):\n    padded = jnp.pad(row_vals, (0, 1), constant_values=jnp.nan)\n    non_nan = jnp.nonzero(~jnp.isnan(padded), size=size, fill_value=-1)\n    return padded[non_nan]\n\nget_non_nan_values(array_2d)\nWhich returns:\npython\nCopy\nArray([[ 1.,  2.,  3.],\n       [10., 20., nan]], dtype=float32)\nI think this solution is a bit more compact and clearer in intend, however I have not checked the performance.\nI hope this helps!",
            "I think you can do what you want with this function, which rather than sorting the array (as I commented), sorts and masks the indices of the non-nan values:\npython\nCopy\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(1,))\ndef func(array, k=3):\n    m, n = array.shape[-2:]\n    indices = jnp.broadcast_to(jnp.arange(n)[None, :], (m, n))\n    sorted_masked_indices = jnp.sort(jnp.where(jnp.isnan(array), jnp.nan, indices))\n    array_rearranged = array[jnp.arange(m)[:, None], sorted_masked_indices.astype(int)]\n    return jnp.where(jnp.isnan(sorted_masked_indices), jnp.nan, array_rearranged)[:, :k]\nTest:\npython\nCopy\nimport numpy as np\nrng = np.random.default_rng(0)\nk = 3\n\na = rng.random((12, 6))\na[np.arange(12)[:, None], rng.integers(0, 6, (12, 6))] = np.nan\n\nprint(a)\nprint(func(a, k=k))\nGives:\npython\nCopy\n[[0.63696169        nan        nan 0.01652764 0.81327024        nan]\n [       nan 0.72949656        nan        nan 0.81585355        nan]\n [       nan 0.03358558        nan        nan        nan        nan]\n [0.29971189        nan        nan        nan        nan 0.64718951]\n [       nan        nan        nan 0.98083534        nan 0.65045928]\n [       nan        nan 0.13509651 0.72148834        nan        nan]\n [       nan 0.88948783 0.93404352 0.3577952         nan        nan]\n [       nan 0.33791123 0.391619   0.89027435        nan        nan]\n [       nan 0.83264415        nan        nan 0.87648423        nan]\n [0.33611706        nan        nan 0.79632427        nan 0.0520213 ]\n [       nan        nan 0.09075305 0.58033239        nan        nan]\n [       nan 0.94211311        nan        nan 0.62910815        nan]]\n[[0.6369617  0.01652764 0.8132702 ]\n [0.72949654 0.81585354        nan]\n [0.03358557        nan        nan]\n [0.29971188 0.6471895         nan]\n [0.9808353  0.6504593         nan]\n [0.1350965  0.72148836        nan]\n [0.88948786 0.9340435  0.3577952 ]\n [0.33791122 0.391619   0.89027435]\n [0.83264416 0.8764842         nan]\n [0.33611706 0.79632425 0.0520213 ]\n [0.09075305 0.5803324         nan]\n [0.9421131  0.62910813        nan]]",
            "With the stable=True option, argsort on a boolean array is guaranteed to preserve the relative order between True and False elements. So this should do the trick:\npython\nCopy\ndef get_non_nan_values(row_vals):\n    return row_vals[jnp.argsort(jnp.isnan(rowvals), stable=True)[:3]]\nHowever, for wide rows, sorting the entire row seems unnecessary when we already know there are only at most 3 non-nan values. So another simple approach using jax.lax.top_k:\npython\nCopy\ndef get_top_3_non_nan(row_vals):\n  return row_vals[jax.lax.top_k(~jnp.isnan(row_vals), 3)[1]]",
            "I would do this using vmap of argsort of isnan:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n])\n\nresult = jax.vmap(lambda x: x[jnp.argsort(jnp.isnan(x))])(array_2d)\nprint(result)\n# [[ 1.  2.  3. nan nan]\n#  [10. 20. nan nan nan]]\nThis approach uses static shapes, and thus will be compatible with jit."
        ],
        "link": "https://stackoverflow.com/questions/79443943/jax-numpy-extracting-non-nan-values-gives-nonconcretebooleanindexerror"
    },
    {
        "title": "Problems when boolean indexing in Jax, getting NonConcreteBooleanIndexError",
        "question": "I'm currently trying to create a CustomProblem inheriting from the BaseProblem class in TensorNEAT which is a Jax based library. In trying to implement the evaluate function of this class, I'm using a boolean mask, but I have problems getting it to work. My code results in jax.errors.NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[n,n]) which I think is due to some of my arrays not having a definite shape. How do I circumvent this?\nConsider this example in np:\npython\nCopy\nimport numpy as np\n\nran_int = np.random.randint(1, 5, size=(2, 2))\nprint(ran_int)\n\nran_bool = np.random.randint(0,2, size=(2,2), dtype=bool)\nprint(ran_bool)\n\na = (ran_int[ran_bool]>0).astype(int)\nprint(a)\nIt could give an output like this:\npython\nCopy\n[[2 2]\n [3 4]]\n[[ True False]\n [ True  True]]\n[1 1 1] #Is 1D and has less elements than before boolean mask was applied!\nBut in Jax, the same way of thinking results in the NonConcreteBooleanIndexError error I got.\npython\nCopy\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #ERROR HAPPENS HERE\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #WOULD HAPPEN HERE TOO IF THIS PART WAS FIRST\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI was considering using jnp.where to solve the issue, but the resulting pairwise_labels and pairwise_predictions have a different shape than what I expect (namely (n, n)) as seen in the code below:\npython\nCopy\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) # shape (n, n)\n\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) # shape (n, n)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n ,n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI fear that the differing shapes of pairwise_predictions and pairwise_labels after using jnp.where will result in a different loss than if I had just used the boolean mask as I would in np. There is also the fact that I get another error that happens later in the pipeline with the output ValueError: max() iterable argument is empty from line 143 in the pipeline.py file of TensorNeat. This is curiously circumvented by changing pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold to pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold, which probably also results in some loss that is incorrect.\nBelow is some code that should be enough to setup a minimal running example that is similar to my setup:\npython\nCopy\nfrom tensorneat import algorithm, genome, common\nfrom tensorneat.pipeline import Pipeline\nfrom tensorneat.genome.gene.node import DefaultNode\nfrom tensorneat.genome.gene.conn import DefaultConn\nfrom tensorneat.genome.operations import mutation\nimport jax, jax.numpy as jnp\nfrom tensorneat.problem import BaseProblem\n\ndef binary_cross_entropy(prediction, target):\n    return -(target * jnp.log(prediction) + (1 - target) * jnp.log(1 - prediction))\n\n# Define the custom Problem\nclass CustomProblem(BaseProblem):\n\n    jitable = True  # necessary\n\n    def __init__(self, inputs, labels, threshold):\n        self.inputs = jnp.array(inputs) #nb! already has shape (n, 768)\n        self.labels = jnp.array(labels).reshape((-1,1)) #nb! has shape (n), must be transformed to have shape (n, 1) \n        self.threshold = threshold\n\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold #this is the thing I actually want\n        #pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold #weird fix to circumvent ValueError: max() iterable argument is empty when using jnp.where for pairwise_labels and pairwise_predictions\n        print(pairs_to_keep.shape)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape)\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\n\n    @property\n    def input_shape(self):\n        # the input shape that the act_func expects\n        return (self.inputs.shape[1],)\n\n    @property\n    def output_shape(self):\n        # the output shape that the act_func returns\n        return (1,)\n\n    def show(self, state, randkey, act_func, params, *args, **kwargs):\n        # showcase the performance of one individual\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(state, params, self.inputs)\n\n        loss = jnp.mean(jnp.square(predict - self.labels))\n\n        n_elements = 5\n        if n_elements > len(self.inputs):\n            n_elements = len(self.inputs)\n\n        msg = f\"Looking at {n_elements} first elements of input\\n\"\n        for i in range(n_elements):\n            msg += f\"for input i: {i}, target: {self.labels[i]}, predict: {predict[i]}\\n\"\n        msg += f\"total loss: {loss}\\n\"\n        print(msg)\n\nalgorithm = algorithm.NEAT(\n    pop_size=10,\n    survival_threshold=0.2,\n    min_species_size=2,\n    compatibility_threshold=3.0,  \n    species_elitism=2,  \n    genome=genome.DefaultGenome(\n        num_inputs=768,\n        num_outputs=1,\n        max_nodes=769,  # must at least be same as inputs and outputs\n        max_conns=768,  # must be 768 connections for the network to be fully connected\n        output_transform=common.ACT.sigmoid,\n        mutation=mutation.DefaultMutation(\n            # no allowing adding or deleting nodes\n            node_add=0.0,\n            node_delete=0.0,\n            # set mutation rates for edges to 0.5\n            conn_add=0.5,\n            conn_delete=0.5,\n        ),\n        node_gene=DefaultNode(),\n        conn_gene=DefaultConn(),\n    ),\n)\n\n\nINPUTS = jax.random.uniform(jax.random.PRNGKey(0), (100, 768)) #the input data x\nLABELS = jax.random.uniform(jax.random.PRNGKey(0), (100)) #the annotated labels y\n\nproblem = CustomProblem(INPUTS, LABELS, 0.25)\n\nprint(\"Setting up pipeline and running it\")\nprint(\"-----------------------------------------------------------------------\")\npipeline = Pipeline(\n    algorithm,\n    problem,\n    generation_limit=1,\n    fitness_target=1,\n    seed=42,\n)\n\nstate = pipeline.setup()\n# run until termination\nstate, best = pipeline.auto_run(state)\n# show results\npipeline.show(state, best)",
        "answers": [
            "The solution I got from the authors of TensorNEAT was to update the evaluate() function to use jnp.nan instead of -jnp.inf in the first jnp.where() calls used on pairwise_labels and pairwise_predictions. I also had to make the loss take into consideration the nan values that would be present in the loss after running the bce. The new evaluate() function that has the same behavior as boolean indexing is pasted below.\npython\nCopy\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold\n\n        #finding only the labels to keep\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, jnp.nan) #use jnp.nan here\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n\n        #finding only the predictions to keep\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, jnp.nan) #use jnp.nan here\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # loss with shape (len(labels), len(labels)), we need to reduce it to a scalar\n        loss = jnp.mean(loss, where=~jnp.isnan(loss)) #only use number values in loss\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss        \n        return -loss",
            "Yes, the mask operation makes the shape of the resulting array dependent on the content of the array. And jax only supports static shapes. The workaround you propose looks reasonable, with using the value -inf as a placeholder. The missing part is ignoring the zero entries in the mean. This you could achieve by a custom “masked” mean function along the lines of:\nfrom jax import numpy as jnp\nfrom jax import random\nimport jax\n\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (4, 4))\n\nkey, subkey = random.split(key)\nmask = random.bernoulli(key, 0.5, (4, 4))\n\n@jax.jit\ndef masked_mean(x, mask):\n    return jnp.sum(jnp.where(mask, x, 0), axis=0) / jnp.sum(mask, axis=0)\n\n\nmasked_mean(x, mask)\nI have not checked other parts of the code in detail, but e.g. the statement jnp.where(pairwise_labels > 0, True, False) has no effect. And with the masked mean you might not need the placeholder values at all.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79423352/problems-when-boolean-indexing-in-jax-getting-nonconcretebooleanindexerror"
    },
    {
        "title": "How to use jax.vmap with a tuple of flax TrainStates as input?",
        "question": "I am setting up a Deep MARL framework and I need to assess my actor policies. Ideally, this would entail using jax.vmap over a tuple of actor flax TrainStates. I have tried the following:\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.linen.initializers import constant, orthogonal\nfrom flax.training.train_state import TrainState\nimport optax\nimport distrax\n\nclass PGActor_1(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 4\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(128, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0))(x)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nclass PGActor_2(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 2\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nstate= jnp.zeros((1, 5))\n\nnetwork_1 = PGActor_1()\nnetwork_1_init_rng = jax.random.PRNGKey(42)\nparams_1 = network_1.init(network_1_init_rng, state)\n\nnetwork_2 = PGActor_2()\nnetwork_2_init_rng = jax.random.PRNGKey(42)\nparams_2 = network_2.init(network_2_init_rng, state)\n\ntx = optax.chain(\noptax.clip_by_global_norm(1),\noptax.adam(lr=1e-3)\n)\nactor_trainstates= (\n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_1),             \n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_2)\n )\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(actor_trainstates)\nbut I recieve the following error:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nDoes anybody have any idea how to make this work?\nThank you in advance.",
        "answers": [
            "This is quite similar to other questions (e.g. Jax - vmap over batch of dataclasses). The key point is that JAX transformations like vmap require data in a struct of arrays pattern, whereas you are using an array of structs pattern.\nTo work directly with an array of structs pattern in JAX, you can use Python's built-in map function – due to JAX's asynchronous dispatch, the resulting operations will be executed in parallel where possible:\npython\nCopy\npis = map(lambda x: x.apply_fn(x.params, state), actor_trainstates)\nHowever, this doesn't take advantage of the automatic vectorization done by vmap. In order to do this, you can convert your data from an array of structs to a struct of arrays, although this requires that all entries have the same structure.\nFor compatible cases, the solution would look something like this, however it errors for your data:\npython\nCopy\ntrain_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(train_states_soa)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-36-da904fa40b9c> in <cell line: 0>()\n----> 1 train_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\n\nValueError: Dict key mismatch; expected keys: ['Dense_0', 'Dense_1', 'Dense_2']\nThe problem is that your two train states do not have matching structure, and so they cannot be transformed into a single struct of arrays. You can see the difference in structure by inspecting the params:\npython\nCopy\nprint(actor_trainstates[0].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1', 'Dense_2'])\nprint(actor_trainstates[1].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1'])\nThere is no way to use vmap in a context where your inputs have different structure, so you'll either have to change the problem to ensure the same structure, or stick with the map approach."
        ],
        "link": "https://stackoverflow.com/questions/79405049/how-to-use-jax-vmap-with-a-tuple-of-flax-trainstates-as-input"
    },
    {
        "title": "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?",
        "question": "Is it possible to use vmap for auto-batching if your function isn't jittable?\nI have a function that's not jittable:\ndef testfunc(model, x1, x2, x2_mask):\n    ( ... non-jittable stuff with masks ... )\nI'm trying to wrap it in vmap so I can benefit from auto-batching as explained here.\nSo I do:\ntestfunc_batched = jax.vmap(testfunc, in_axes=(None, 0, 0, 0))\nThe intention is that in batched mode, each of x1, x2, and x2_mask will have an additional outter dimension, the batching dimension. The model shouldn't be treated differently in batched mode hence the None. Let me know if the syntax isn't right.\nI create batches of size one just to test, schematically:\nx1s = x1.reshape(1, ...)\nx2s = x2.reshape(1, ...)\nx2_masks = x2_mask.reshape(1, ...)\n\ntestfunc_batched(model, x1s, x2s, x2_masks)\nThe last line fails with ConcretizationTypeError.\nI've recently learned that stuff with masks makes functions not jittable. But does that mean that I also can't use vmap? Or am I doing something wrong?\n(There is further context in How to JIT code involving masked arrays without NonConcreteBooleanIndexError?, but you don't have to read that question to understand this one.)",
        "answers": [
            "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?\nNo. In general, functions which are incompatible with jit will also be incompatible with vmap, because both jit and vmap use the same JAX tracing mechanism to transform the program."
        ],
        "link": "https://stackoverflow.com/questions/79374152/is-it-possible-to-use-jax-vmap-for-auto-batching-if-your-function-isnt-jittable"
    },
    {
        "title": "Count onto 2D JAX coordinates of another 2D array",
        "question": "I have\nx = jnp.zeros((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\nI would like to count onto x how many times each of the individual (x,y) coordinates appear in coords. In other words, obtain the output:\nArray([[0., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI've tried x.at[coords].add(1) and this gives me:\nArray([[0., 0., 0., 0., 0.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI understand what it's doing, but not how to make it do the thing I want.\nThere's this related question[1], but I haven't been able to use it to solve my problem.\n[1] Update JAX array based on values in another array",
        "answers": [
            "For multiple indices, you should pass a tuple of index arrays:\npython\nCopy\nx = x.at[coords[:, 0], coords[:, 1]].add(1)\nprint(x)\n[[0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]",
            "The generalized operation is basically computing a histogram, especially when the coordinate arrays are float values. So depending on the context the code is used, the following alternative might communicate the intent a bit more clearly:\npython\nCopy\nfrom jax import numpy as jnp\n\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\n\nbins = jnp.arange(5 + 1) - 0.5 \nx, _ = jnp.histogramdd(coords, bins=(bins, bins))\nIt will also handle if coordinates are out of bounds. But I presume under the hood, it does the same operation as at[...].add(1). So I would not expect any relevant difference in performance."
        ],
        "link": "https://stackoverflow.com/questions/79370053/count-onto-2d-jax-coordinates-of-another-2d-array"
    },
    {
        "title": "How Can I Use GPU to Accelerate Image Augmentation?",
        "question": "When setting up image augmentation pipelines using keras.layers.Random* or other augmentation or processing methods, we often integrate these pipelines with a data loader, such as the tf.data API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.\nTo address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?",
        "answers": [
            "We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in keras. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.\nClassification\nLet’s take a classification task as an example. If we use the tf.data API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.\npython\nCopy\nimport numpy as np\nfrom keras import layers\n\na = np.ones((4, 224, 224, 3)).astype(np.float32)\nb = np.ones((4, 2)).astype(np.float32)\n\naugmentation_layers = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.2),\n    ]\n)\n\ndataset = tf.data.Dataset.from_tensor_slices((a, b))\ndataset = dataset.batch(3, drop_remainder=True)\ndataset = dataset.map(\n    lambda x, y: (augmentation_layers(x), y), \n    num_parallel_calls=tf.data.AUTOTUNE\n)\nx.shape, y.shape\n(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))\nBut for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.\npython\nCopy\ninputs = keras.Input(shape=(224, 224, 3))\nprocessed = augmentation_layers(inputs)\nbackbone = keras.applications.EfficientNetB0(\n    include_top=True, pooling='avg'\n)(processed)\noutput = keras.layers.Dense(10)(backbone)\nmodel = keras.Model(inputs, output)\nmodel.count_params() / 1e6\n5.340581\nHere, we set the augmentation pipeline right after keras.Input. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.\nSegmentation\nI'll use this dataset for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using keras-3, which might allow for multi-backend support.\npython\nCopy\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # torch, jax\n\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nkeras.__version__ # 3.4.1\npython\nCopy\n# ref https://keras.io/examples/vision/oxford_pets_image_segmentation/\n# u-net model\ndef get_model(img_size, num_classes, classifier_activation):\n    ...\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(\n        num_classes, \n        3, \n        activation=classifier_activation, \n        padding=\"same\", \n        dtype='float32'\n    )(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\nimg_size = (224, 224)\nnum_classes = 1\nclassifier_activation = 'sigmoid'\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nLet's define the augmentation pipelines.\npython\nCopy\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\")\n]\n\ndef augment_data(images, masks):\n    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = tf.cast(combined[..., 3:], tf.int32)\n    return images_augmented, masks_augmented\nLet’s define the tf.data API to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.\npython\nCopy\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    \n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = tf.cast(image, tf.int32)\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.\n        \n    return image\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask  = read_image(mask_list, mask=True)\n    return image, mask\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE) \n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n    # Augmenting on CPU\n    dataset = dataset.map(\n        augment_data, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\npython\nCopy\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nprint(\"Train Dataset:\", train_dataset)\nTrain Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))>\nNow, let's compile it and run it.\npython\nCopy\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nmodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n\n%%time\nepochs = 2\nmodel.fit(\n    train_dataset, \n    epochs=epochs, \n)\nEpoch 1/2\n318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087\nEpoch 2/2\n318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338\nCPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s\nWall time: 1min 48s\nNext, we will remove the augmentation layers from the dataloader.\npython\nCopy\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE)\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nTo offload augmentation to the GPU, we’ll create a custom model class, override the train_step, and use the augment_data method that we defined earlier. Here's how to structure it:\npython\nCopy\nclass ExtendedModel(keras.Model):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def call(self, inputs):\n        return self.model(inputs)\n\n    def save(\n        self, filepath, \n        overwrite=True, \n        include_optimizer=True, \n        save_format=None, \n        add_loss=None, \n    ):\n        # Overriding this method will allow us to use the `ModelCheckpoint`\n        self.model.save(\n            filepath=filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            include_optimizer=include_optimizer,\n        )\nNow that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.\npython\nCopy\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nemodel = ExtendedModel(model)\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nemodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\npython\nCopy\n%%time\nepochs = 2\nemodel.fit(\n    train_dataset, \n    epochs=epochs, \n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            filepath='model.{epoch:02d}-{loss:.3f}.keras',\n            monitor='loss',\n            mode='min',\n            save_best_only=True\n        )\n    ]\n)\nEpoch 1/2\n318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748\nEpoch 2/2\n318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585\nCPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s\nWall time: 1min 29s\nSo, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around 18.35% improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.\nAs shown in the ExtendedModel class above, we override the save method, allowing the callbacks.ModelCheckpoint to save the full model. Inference can then be performed as shown below.\npython\nCopy\nloaded_model = keras.saving.load_model(\n    \"/kaggle/working/model.02-0.0585.keras\"\n)\nx, y = next(iter(train_dataset))\noutput = loaded_model.predict(x)\n1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step\nUpdate\nIn order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the augment_data that is used in ExtendedModel use the following backend agnostic keras.ops functions.\npython\nCopy\ndef augment_data(images, masks):\n    combined = keras.ops.concatenate(\n        [images, keras.ops.cast(masks, 'float32')], axis=-1\n    )\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')\n    return images_augmented, masks_augmented\nAdditionally, to make the pipeline flexible for all backend, we can update the ExtendedModel as follows. Now, this code can run with tensorflow, jax, and torch backends.\npython\nCopy\nclass ExtendedModel(keras.Model):\n    ...\n\n    def train_step(self, *args, **kwargs):\n        if keras.backend.backend() == \"jax\":\n            return self._jax_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"tensorflow\":\n            return self._tensorflow_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"torch\":\n            return self._torch_train_step(*args, **kwargs)\n\n    def _jax_train_step(self, state, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step(state, (x, y))\n\n    def _tensorflow_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def _torch_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    ..."
        ],
        "link": "https://stackoverflow.com/questions/79327723/how-can-i-use-gpu-to-accelerate-image-augmentation"
    },
    {
        "title": "Efficiently custom array creation routines in JAX",
        "question": "I'm still getting a handle of best practices in jax. My broad question is the following:\nWhat are best practices for the implementation of custom array creation routines in jax?\nFor instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):\npython\nCopy\nimport numpy as np\nimport jax.numpy as jnp\n\ndef ones_at_col(shape_mat, idx):\n    idxs = jnp.arange(shape_mat[1])[None,:]\n    mat = jnp.where(idx==idxs, 1, 0)\n    mat = jnp.repeat(mat, shape_mat[0], axis=0)\n    return mat\n\nshape_mat = (5,10)\n\nprint(ones_at_col(shape_mat, 5))\n\n%timeit np.zeros(shape_mat)\n\n%timeit jnp.zeros(shape_mat)\n\n%timeit ones_at_col(shape_mat, 5)\nThe output is\npython\nCopy\n[[0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]]\n127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nMy function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.\nBut then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?\nIs there something better I can do or am I pushing the limits of what is possible in jax?",
        "answers": [
            "The best way to do this is probably something like this:\npython\nCopy\nmat = jnp.zeros(shape_mat).at[:, 5].set(1)\nRegarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.\nIf you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):\npython\nCopy\n@partial(jax.jit, static_argnames=['shape_mat', 'idx'])\ndef ones_at_col(shape_mat, idx):\n  return jnp.zeros(shape_mat).at[:, idx].set(1)\nYou could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.\nAnother side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects."
        ],
        "link": "https://stackoverflow.com/questions/79256001/efficiently-custom-array-creation-routines-in-jax"
    },
    {
        "title": "Hello World for jaxtyping?",
        "question": "I can't find any instructions or tutorials for getting started with jaxtyping. I tried the simplest possible program and it fails to parse. I'm on Python 3.11. I don't see anything on GitHub jaxtyping project about an upper bound (lower bound is Python 3.9) and it looks like it's actively maintained (last commit was 8 hours ago). What step am I missing?\npython\nCopy\njaxtyping==0.2.36\nnumpy==2.1.3\ntorch==2.5.1\ntypeguard==4.4.1\n(It seems like numpy is required for some reason even though I'm not using it)\npython\nCopy\nfrom typeguard import typechecked\nfrom jaxtyping import Float\nfrom torch import Tensor\n\n\n@typechecked\ndef matmul(a: Float[Tensor, \"m n\"], b: Float[Tensor, \"n p\"]) -> Float[Tensor, \"m p\"]:\n    \"\"\"\n    Matrix multiplication of two 2D arrays.\n    \"\"\"\n    raise NotImplementedError(\"This function is not implemented yet.\")\npython\nCopy\n(venv) dspyz@dspyz-desktop:~/helloworld$ python matmul.py \nTraceback (most recent call last):\n  File \"/home/dspyz/helloworld/matmul.py\", line 6, in <module>\n    @typechecked\n     ^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 221, in typechecked\n    retval = instrument(target)\n             ^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 72, in instrument\n    instrumentor.visit(module_ast)\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 598, in visit_Module\n    self.generic_visit(node)\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 498, in generic_visit\n    node = super().generic_visit(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 494, in generic_visit\n    value = self.visit(value)\n            ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 672, in visit_FunctionDef\n    with self._use_memo(node):\n  File \"/usr/lib/python3.11/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 556, in _use_memo\n    new_memo.return_annotation = self._convert_annotation(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 582, in _convert_annotation\n    new_annotation = cast(expr, AnnotationTransformer(self).visit(annotation))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in visit_Subscript\n    [self.visit(item) for item in node.slice.elts],\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in <listcomp>\n    [self.visit(item) for item in node.slice.elts],\n     ^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 474, in visit_Constant\n    expression = ast.parse(node.value, mode=\"eval\")\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n    return compile(source, filename, mode, flags,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<unknown>\", line 1\n    m p\n      ^\nSyntaxError: invalid syntax",
        "answers": [
            "(jaxtyping author here)\nSadly this is a known bug in typeguard v4. It's been around forever and hasn't been fixed. (At a technical level: typeguard v4 attempts to load and reparse the source code of your function, but it doesn't properly parse all type annotations.)\nI use typeguard==2.13.3 myself, which seems to be pretty robust.\nEDIT: removed some other suggested workarounds. These turned out not to, well, work. For now I just recommend pinning to that earlier version of typeguard.",
            "You are running into the issue reported here: https://github.com/patrick-kidger/jaxtyping/issues/80\nYou can work around this by installing typeguard version 3.0, but given how long this bug has remained open without any real fix, I suspect the best conclusion is that jaxtyping should no longer be considered compatible with typeguard."
        ],
        "link": "https://stackoverflow.com/questions/79201839/hello-world-for-jaxtyping"
    },
    {
        "title": "Restoring flax model checkpoints using orbax throws ValueError",
        "question": "The following code blocks are being utlized to save the train state of the model during training and to restore the state back into memory.\npython\nCopy\nfrom flax.training import orbax_utils\nimport orbax.checkpoint\n\ndirectory_gen_path = \"checkpoints_loc\"\norbax_checkpointer_gen = orbax.checkpoint.PyTreeCheckpointer()\ngen_options = orbax.checkpoint.CheckpointManagerOptions(save_interval_steps=5, create=True)\ngen_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n    directory_gen_path, orbax_checkpointer_gen, gen_options\n)\n\ndef save_model_checkpoints(step_, generator_state, generator_batch_stats):\n\n    gen_ckpt = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    save_args_gen = orbax_utils.save_args_from_target(gen_ckpt)\n    gen_checkpoint_manager.save(step_, gen_ckpt, save_kwargs={\"save_args\": save_args_gen})\n\ndef load_model_checkpoints(generator_state, generator_batch_stats):\n    gen_target = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    latest_step = gen_checkpoint_manager.latest_step()\n    gen_ckpt = gen_checkpoint_manager.restore(latest_step, items=gen_target)\n    generator_state = gen_ckpt[\"model\"]\n    generator_batch_stats = gen_ckpt[\"batch_stats\"]\n\n    return generator_state, generator_batch_stats\nThe training of the model was done on a GPU and loading the state onto GPU device works fine, however, when trying to load the model to cpu, the following error is being thrown by the orbax checkpoint manager's restore method\npython\nCopy\nValueError: SingleDeviceSharding with Device=cuda:0 was not found in jax.local_devices().\nI'm not quite sure what could be the reason, any thoughts folks?\nUpdate: Updated to the latest version of orbax-checkpoint, 0.8.0 traceback changed to the following error\npython\nCopy\nValueError: sharding passed to deserialization should be specified, concrete and an instance of `jax.sharding.Sharding`. Got None",
        "answers": [
            "What version of orbax.checkpoint are you using?\nIt looks like this issue was fixed in https://github.com/google/orbax/issues/678 – you should update to the most recent version of orbax-checkpoint, and try running your code again. If that doesn't work, I'd suggest reporting the problem at https://github.com/google/orbax/issues/new"
        ],
        "link": "https://stackoverflow.com/questions/79162665/restoring-flax-model-checkpoints-using-orbax-throws-valueerror"
    },
    {
        "title": "Storing and jax.vmap() over Pytrees",
        "question": "I've ran into an issue with Jax that will make me rewrite an entire 20000-line application if I don't solve it.\nI have a non-ML application which relies on pytrees to store data, and the pytrees are deep - about 6-7 layers of data storage (class1 stores class2, and that stores an array of class3 etc.)\nI've used python lists to store pytrees and hoped to vmap over them, but turns out jax can't vmap over lists.\n(So one solution is to rewrite literally every single dataclass to be a structured array and work from there, possibly putting all 6-7 layers of data into one mega-array)\nIs there a way to avoid the rewrite? Is there a way to store pytree classes in a vmappable state so that everything works as before?\nI have my classes marked with flax.struct.dataclass if that helps.",
        "answers": [
            "jax.vmap is designed to work with a struct-of-arrays pattern, and it sounds like you have an array-of-structs pattern. From your description, it sounds like you have a sequence of nested structs that look something like this:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom flax.struct import dataclass\n\n@dataclass\nclass Params:\n  x: jax.Array\n  y: jax.Array\n\n\n@dataclass\nclass AllParams:\n  p: list[Params]\n\n\nparams_list = [AllParams([Params(4, 2), Params(4, 3)]),\n               AllParams([Params(3, 5), Params(2, 4)]),\n               AllParams([Params(3, 2), Params(6, 3)])]\nThen you have a function that you want to apply to each element of the list; something like this:\npython\nCopy\ndef some_func(params):\n  a, b = params.p\n  return a.x * b.y - b.x * a.y\n\n[some_func(params) for params in params_list]\n[4, 2, -3]\nBut as you found, if you try to do this with vmap, you get an error:\npython\nCopy\njax.vmap(some_func)(params_list)\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nThe issue is that vmap operates separately over each entry of the list or pytree you pass to it, not over the elements of the list.\nTo address this, you can often transform your data structure from an array-of-structs into a struct-of-arrays, and then apply vmap over this. For example:\npython\nCopy\nparams_array = jax.tree.map(lambda *vals: jnp.array(vals), *params_list)\nprint(params_array)\nAllParams(p=[\n  Params(x=Array([4, 3, 3], dtype=int32), y=Array([2, 5, 2], dtype=int32)),\n  Params(x=Array([4, 2, 6], dtype=int32), y=Array([3, 4, 3], dtype=int32))\n])\nNotice that rather than a list of structures, this is now a single structure with the batching pushed all the way down to the leaves. This is the \"struct-of-arrays\" pattern that vmap is designed to work with, and so vmap will work correctly:\npython\nCopy\njax.vmap(some_func)(params_array)\nArray([ 4,  2, -3], dtype=int32)\nNow, this assumes that every dataclass in your list has identical structure: if not, then vmap will not be applicable, because by design it must map over computations with identical structure."
        ],
        "link": "https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees"
    },
    {
        "title": "precision of JAX",
        "question": "I have a question regarding the precision of float in JAX. For the following code,\npython\nCopy\nimport numpy as np\nimport jax.numpy as jnp\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is:','%.60f' % np.arctan(10))\n\njnp.arctan(10) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\n\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is:','%.60f' % np.arctan(10+1e-7))\n\njnp.arctan(10+1e-7) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\njnp gave identical results for arctan(x) for a small change of input variable (1e-7), but np did not. My question is how to let jax.numpy get the right number for a small change of x?\nAny comments are appreciated.",
        "answers": [
            "JAX defaults to float32 computation, which has a relative precision of about 1E-7. This means that your two inputs are effectively identical:\npython\nCopy\n>>> np.float32(10) == np.float32(10 + 1E-7)\nTrue\nIf you want 64-bit precision like NumPy, you can enable it as discussed at JAX sharp bits: double precision, and then the results will match to 64-bit precision:\npython\nCopy\nimport jax\njax.config.update('jax_enable_x64', True)\n\nimport jax.numpy as jnp\nimport numpy as np\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is: ','%.60f' % np.arctan(10))\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is: ','%.60f' % np.arctan(10+1e-7))\npython\nCopy\njnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\nnp.arctan(10) is:  1.471127674303734700345103192375972867012023925781250000000000\njnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\nnp.arctan(10+1e-7) is:  1.471127675293833592107262120407540351152420043945312500000000\n(but please note that even the 64-bit precision used by Python and NumPy is only accurate to about one part in 10^16, so most of the digits in the representation you printed are inaccurate compared to the true arctan value)."
        ],
        "link": "https://stackoverflow.com/questions/79098013/precision-of-jax"
    },
    {
        "title": "jax register_pytree_node_class and register_dataclass returns non consistent datatype: list and tuple accordingly",
        "question": "I am writing custom class, which is basically a wrapper around list, with custom setitem method. I would like this class participate in jax.jit code, so during that I found a following problem: during jitting List field converted to tuple. However, this is case only when using\nregister_pytree_node_class When use register_dataclas , then List keep being list.\nI simplify example to highlight only this problem.\npython\nCopy\nimport jax\nfrom jax.tree_util import register_dataclass\nfrom jax.tree_util import register_pytree_node_class\nfrom functools import partial\nfrom dataclasses import dataclass\nfrom typing import List\n\n@partial(register_dataclass,\n         data_fields=['data'],\n         meta_fields=['shift'])\n@dataclass\nclass DecoratorFlatten:\n    data: List[int]\n    shift: int = 5\n\n@register_pytree_node_class\n@dataclass\nclass CustomFlatten:\n    data: List[int]\n    shift: int = 5\n\n    def tree_flatten(self):\n            children = self.data\n            aux_data = self.shift\n            return (children, aux_data)\n    \n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        obj = object.__new__(cls)\n        obj.data = children\n        setattr(obj, 'shift', aux_data)\n        return obj\nNow let's call a simple as this function over instances of this two class:\npython\nCopy\n@jax.jit\ndef get_value(a):\n    return a.data\npython\nCopy\ndf = DecoratorFlatten([0,1,2])\ncf = CustomFlatten([0,1,3])\nget_value(df), get_value(cf)\nIn first case we get list as output, but in second tuple. I thought maybe this is because of my implementation of the tree_flatten method, however:\npython\nCopy\ncf.tree_flatten()\nLeads to ([0, 1, 3], 5) as desirable.",
        "answers": [
            "In tree_unflatten, children is a tuple, and you are assigning this directly to obj.data. If you want it to be a list, you should use obj.data = list(children)."
        ],
        "link": "https://stackoverflow.com/questions/79093341/jax-register-pytree-node-class-and-register-dataclass-returns-non-consistent-dat"
    },
    {
        "title": "Batched matrix multiplication with JAX on GPU faster with larger matrices",
        "question": "I'm trying to perform batched matrix multiplication with JAX on GPU, and noticed that it is ~3x faster to multiply shapes (1000, 1000, 3, 35) @ (1000, 1000, 35, 1) than it is to multiply (1000, 1000, 3, 25) @ (1000, 1000, 25, 1) with f64 and ~5x with f32.\nWhat explains this difference, considering that on cpu neither JAX or NumPy show this behaviour, and on GPU CuPy doesn't show this behaviour?\nI'm running this with JAX: 0.4.32 on an NVIDIA RTX A5000 (and get similar results on a Tesla T4), code to reproduce:\npython\nCopy\nimport numpy as np\nimport cupy as cp\nfrom cupyx.profiler import benchmark\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng()\n\nx = np.arange(5, 55, 5)\nGPU timings:\npython\nCopy\ndtype = cp.float64\ntimings_cp = []\nfor i in range(5, 55, 5):\n    a = cp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n    b = cp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n    timings_cp.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ndtype = jnp.float64\ntimings_jax_gpu = []\nwith jax.default_device(jax.devices('gpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n        b = jnp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_gpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.gpu_times.mean() for i in timings_cp], label=\"CuPy\")\nplt.plot(x, [i.gpu_times.mean() for i in timings_jax_gpu], label=\"JAX GPU\")\nplt.legend()\nTimings with those specific shapes:\npython\nCopy\ndtype = jnp.float64\nwith jax.default_device(jax.devices('gpu')[0]):\n    a = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\n    func = jax.jit(lambda a, b: a@b)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\n\n    a = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\nGives\npython\nCopy\nf64:\n0.01453789699935913\n0.004859122595310211\n\nf32:\n\n0.005860503035545349\n0.001209742688536644\nCPU timings:\npython\nCopy\ntimings_np = []\nfor i in range(5, 55, 5):\n    a = rng.random((1000, 1000, 3, i))\n    b = rng.random((1000, 1000, i, 1))\n    timings_np.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ntimings_jax_cpu = []\nwith jax.default_device(jax.devices('cpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)))\n        b = jnp.array(rng.random((1000, 1000, i, 1)))\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_cpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.cpu_times.mean() for i in timings_np], label=\"NumPy\")\nplt.plot(x, [i.cpu_times.mean() for i in timings_jax_cpu], label=\"JAX CPU\")\nplt.legend()",
        "answers": [
            "The difference seems to come from the compiler emitting a kLoop fusion for smaller sizes, and a kInput fusion for larger sizes. You can read about the effect of these in this source comment: https://github.com/openxla/xla/blob/e6b6e61b29cc439350a6ad2f9d39535cb06011e5/xla/hlo/ir/hlo_instruction.h#L639-L656\nThe compiler likely uses some heuristic to choose between the two, and it appears that this heuristic is suboptimal at the boundary for your particular problem. You can see this by outputting the compiled HLO for your operation:\npython\nCopy\na = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\npython\nCopy\nHloModule jit__lambda_, is_scheduled=true, entry_computation_layout={(f64[1000,1000,3,25]{3,2,1,0}, f64[1000,1000,25,1]{3,2,1,0})->f64[1000,1000,3,1]{3,2,1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=\"a02cbfe0fda9d44e2bd23462363b6cc0\"}\n\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.7: f64[1000,1000,3,25], param_1.6: f64[1000,1000,25,1]) -> f64[1000,1000,3] {\n  %param_0.7 = f64[1000,1000,3,25]{3,2,1,0} parameter(0)\n  %param_1.6 = f64[1000,1000,25,1]{3,2,1,0} parameter(1)\n  %bitcast.28.5 = f64[1000,1000,25]{2,1,0} bitcast(f64[1000,1000,25,1]{3,2,1,0} %param_1.6)\n  %broadcast.2.5 = f64[1000,1000,3,25]{3,2,1,0} broadcast(f64[1000,1000,25]{2,1,0} %bitcast.28.5), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  %multiply.2.3 = f64[1000,1000,3,25]{3,2,1,0} multiply(f64[1000,1000,3,25]{3,2,1,0} %param_0.7, f64[1000,1000,3,25]{3,2,1,0} %broadcast.2.5)\n  %constant_4 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,25]{3,2,1,0} %multiply.2.3, f64[] %constant_4), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,25], Arg_1.2.0: f64[1000,1000,25,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,25,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,25]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %loop_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,25]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,25,1]{3,2,1,0} %Arg_1.2.0), kind=kLoop, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %loop_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\npython\nCopy\na = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\npython\nCopy\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.5: f64[1000,1000,3,35], param_1.2: f64[1000,1000,35,1]) -> f64[1000,1000,3] {\n  %param_0.5 = f64[1000,1000,3,35]{3,2,1,0} parameter(0)\n  %param_1.2 = f64[1000,1000,35,1]{3,2,1,0} parameter(1)\n  %bitcast.28.3 = f64[1000,1000,35]{2,1,0} bitcast(f64[1000,1000,35,1]{3,2,1,0} %param_1.2)\n  %broadcast.2.3 = f64[1000,1000,3,35]{3,2,1,0} broadcast(f64[1000,1000,35]{2,1,0} %bitcast.28.3), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  %multiply.2.1 = f64[1000,1000,3,35]{3,2,1,0} multiply(f64[1000,1000,3,35]{3,2,1,0} %param_0.5, f64[1000,1000,3,35]{3,2,1,0} %broadcast.2.3)\n  %constant_3 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,35]{3,2,1,0} %multiply.2.1, f64[] %constant_3), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,35], Arg_1.2.0: f64[1000,1000,35,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,35,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,35]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %input_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,35]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,35,1]{3,2,1,0} %Arg_1.2.0), kind=kInput, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %input_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\nHere's a script to observe this compiler decision with respect to size:\npython\nCopy\nfor size in range(10, 55, 5):\n  a = jnp.array(rng.random((1000, 1000, 3, size)), dtype=dtype)\n  b = jnp.array(rng.random((1000, 1000, size, 1)), dtype=dtype)\n  hlo_text = jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text()\n  print(f\"{size=} {'kLoop' in hlo_text=}\")\npython\nCopy\nsize=10 'kLoop' in hlo_text=True\nsize=15 'kLoop' in hlo_text=True\nsize=20 'kLoop' in hlo_text=True\nsize=25 'kLoop' in hlo_text=True\nsize=30 'kLoop' in hlo_text=True\nsize=35 'kLoop' in hlo_text=False\nsize=40 'kLoop' in hlo_text=False\nsize=45 'kLoop' in hlo_text=False\nsize=50 'kLoop' in hlo_text=False\nI don't have any suggestion beyond perhaps reporting this at https://github.com/openxla/xla; it may be that the compiler heuristic for choosing to emit kLoop vs. kInput needs some additional logic."
        ],
        "link": "https://stackoverflow.com/questions/79085795/batched-matrix-multiplication-with-jax-on-gpu-faster-with-larger-matrices"
    },
    {
        "title": "Mapping Over Arrays of Functions in JAX",
        "question": "What is the most performant, idiomatic way of mapping over arrays of functions in JAX?\nContext: This GitHub issue shows a way to apply vmap to several functions using lax.switch. The example is reproduced below:\npython\nCopy\nfrom jax import lax, vmap\nimport jax.numpy as jnp\n\ndef func1(x):\n  return 2 * x\n\ndef func2(x):\n  return -2 * x\n\ndef func3(x):\n  return 0 * x\n\nfunctions = [func1, func2, func3]\nindex = jnp.arange(len(functions))\nx = jnp.ones((3, 5))\n\nvmap_functions = vmap(lambda i, x: lax.switch(i, functions, x))\nvmap_functions(index, x)\n# DeviceArray([[ 2.,  2.,  2.,  2.,  2.],\n#              [-2., -2., -2., -2., -2.],\n#              [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)\nMy specific questions are:\nIs this (currently) the most idiomatic way of mapping over arrays of functions in JAX?\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)",
        "answers": [
            "For the kind of operation you're doing, where the functions are applied over full axes of an array in a way that's known statically, you'll probably get the best performance via a simple Python loop:\npython\nCopy\ndef map_functions(functions: list[Callable[[Array], Array], x: Array) -> Array:\n  assert len(functions) == x.shape[0]\n  return jnp.array([f(row) for f, row in zip(functions, x)])\nThe method based on switch is designed for the more general case where the structure of the indices is not known statically.\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)\nvmap of switch is implemented via select, which will compute the output of each function for the full input array before selecting just the pieces needed to construct the output, so if the functions are expensive to compute, it may lead to longer runtimes."
        ],
        "link": "https://stackoverflow.com/questions/78980521/mapping-over-arrays-of-functions-in-jax"
    },
    {
        "title": "JAX TypeError: 'Device' object is not callable",
        "question": "I found a piece of JAX codes from few years ago.\npython\nCopy\nimport jax\nimport jax.random as rand\n\ndevice_cpu = None\n\ndef do_on_cpu(f):\n    global device_cpu\n    if device_cpu is None:\n        device_cpu = jax.devices('cpu')[0]\n\n    def inner(*args, **kwargs):\n        with jax.default_device(device_cpu):\n            return f(*args, **kwargs)\n    return inner\n\nseed2key = do_on_cpu(rand.PRNGKey)\nseed2key.__doc__ = '''Same as `jax.random.PRNGKey`, but always produces the result on CPU.'''\nand I call it with:\npython\nCopy\nkey = seed2key(42)\nBut it results in TypeError:\npython\nCopy\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 14\n---> 14 key = seed2key(42)\n\nFile ~/bert-tokenizer-cantonese/lib/seed2key.py:12, in do_on_cpu.<locals>.inner(*args, **kwargs)\n     11 def inner(*args, **kwargs):\n---> 12     with jax.default_device(device_cpu):\n     13         return f(*args, **kwargs)\n\nTypeError: 'Device' object is not callable\nI think the function has breaking changes after version upgrade.\nCurrent versions:\njax 0.4.31\njaxlib 0.4.31\n(latest version at the moment of writing)\nHow can I change the codes to avoid the error? Thanks.",
        "answers": [
            "This code works fine in all recent versions of JAX: jax.default_device is a configuration function designed to be used as a context manager.\nI can reproduce the error you're seeing if I add this to the top of your script:\npython\nCopy\njax.default_device = jax.devices('cpu')[0]  # wrong!\nI suspect you inadvertently executed something similar to this at some point earlier in your notebook session. Try restarting your notebook runtime and rerunning just your valid code."
        ],
        "link": "https://stackoverflow.com/questions/78951225/jax-typeerror-device-object-is-not-callable"
    },
    {
        "title": "Jax jitting of kd-tree code taking an intractably long amount of time",
        "question": "I've written myself into a corner with the following situation:\nI'm running an optimiser which requires smooth gradients to work, and I'm using Jax for automatic differentiation. Since this code is Jax jitted, this means that anything connected to it has to be Jax jit traceable.\nI need to interpolate a function to use with the optimiser, but can't use the Scipy library as it isn't compatable with Jax (there's a jax.scipy.interpolate.RegularGridInterpolator implementation, but this isn't smooth - it only supports linear and nearest neighbour interpolation).\nThis means that I'm having to write my own Jax-compatible smooth interpolator, which I'm basing off the Scipy RBFInterpolator code. The implementation of this is very nice - it uses a kd-tree to find the nearest neighbours of a queried point in space, and then uses these to construct a local interpolation. This means that I also need to write a Jax-compatable kd-tree class (the Scipy one also isn't compatible with Jax), which I've done.\nThe problem comes with jit-compiling the kd-tree code. I've written it in the 'standard way', using objects for the tree nodes with left and right node fields for the children. At the leaf nodes, these fields have None values to signify the absense of children.\nThe code runs and is functionally correct, however jit-compiling it takes a long time: 72 seconds for a tree of 64 coordinates, 131 seconds for 343 coordinates, ... and my intended dataset has over 14 million points. I think internally Jax is tracing every single possible path through the tree, which is why it's taking so long. The results are that it's blazingly quick: 0.0075s for kd-tree 10-point retrieval vs 0.4s for a brute force search over all of the points (for 343 points). These are the kind of speeds I'm hoping to obtain for use in the optimiser (without jitting it will be too slow). However it doesn't seem possible if the compilation times are going to continue to grow as experienced.\nI thought that the problem might lie in the structure of the tree, with lots of different objects to be stored, so have also implemented a kd-tree search algorithm where the tree is represented by a set of Jax-numpy arrays (e.g. coord, value, left and right; where each index corresponds to a point in the tree) and iteration rather than recursion is used to do the tree search (this was a challenge but it works!). However, converting this to work with jit (changing if-statements for jax.lax.cond) is going to be complicated, and before I start I was wondering if it's going to be worth it - surely I'll have the same problem: Jax will trace all branches of the tree until the 'null terminators' (-1 values in the left and right arrays) are reached, and it will still take a very long time to compile. I've been investigating structures like jax.lax.while_loop, in case they might help?\n(I've also written a hybrid of the two approaches, with an array-based tree and a recursion-based algorithm. In this case the tracing goes into an infinite loop, I think because of the fact that the null-terminator is -1 rather than None. But the arrays should be known statically (they don't change after construction, and belong to an object which is marked as a static input), so maybe the solution lies in this and I'm doing something wrong.)\nI was wondering if I'm doing anything which is obviously wrong (or if my understanding is wrong), and if there is anything I can do to speed it up? Is it just to be expected that the compile time would be so high when there are so many code paths to trace? I don't suppose I could even build the jitted function only once and then save it?\nI'm concerned that the only solution may be to rewrite the optimiser code so that it doesn't use Jax (e.g. if I hard-code the derivatives, and rewrite some of the code so that it operates on arrays directly instead of being vectorised across the inputs).\nThe code is available here: https://github.com/FluffyCodeMonster/jax_kd_tree\nAll three varieties described are given: the node-based tree with recursion, the array-based tree with iteration, and the array-based tree with recursion. The former works, but is very slow to jit compile as the number of points in the tree increases; the second also works, but is not written in a jit-able way yet. The last is written to be jitted, but can't jit compile as it gets into an infinite recursion.\nI really need to get this working urgently so that I can obtain the optimisation results.",
        "answers": [
            "All python-level control flow, including if statements, for and while loops, and recursion, is traced in full and flattened into a linear set of commands that is then sent to the compiler. If you are attempting a tree traversal via Python-level control flow, you're going to end up with very large programs that take a very long time to compile. This issue is discussed broadly at JAX sharp bits: control flow.\nIf you want to traverse a KD tree under JIT without the long compilation, you'll have to use an iterative approach with XLA control-flow operators such as jax.lax.fori_loop and jax.lax.while_loop.\nAlternatively, you might think about instead using jax.pure_callback in order to run neighbors queries using scipy on the host. There is some discussion of this at Exploring pure_callback. It's not super efficient—each call will incur some host synchronization and data movement overhead—but it can be a pretty effective solution for things like this, particularly if you're running on CPU."
        ],
        "link": "https://stackoverflow.com/questions/78791013/jax-jitting-of-kd-tree-code-taking-an-intractably-long-amount-of-time"
    },
    {
        "title": "Taking derivatives with multiple inputs in JAX",
        "question": "I am trying to take first and second derivatives of functions in JAX however, my ways of doing that give me the wrong number or zeros. I have an array with two columns for each variable and two rows for each input\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\nrng = rng = jax.random.PRNGKey(1234)\narray = jax.random.normal(rng, (2,2))\nTwo test functions\npython\nCopy\ndef F1(arr):\n    return 1/arr\n\ndef F2(arr):\n    return jnp.array([arr[0]**2 + arr[1]**3])\nand two methods of taking first and second derivatives, one using jax.grad()\npython\nCopy\ndef dF_m1(arr, F):\n    return jax.grad(lambda arr: F(arr)[0])(arr)\n\ndef ddF_m1(arr, F, dF):\n    return jax.grad(lambda arr: dF(arr, F)[0])(arr)\nand another using jax.jacobian()\npython\nCopy\ndef dF_m2(arr, F):\n    jac = jax.jacobian(lambda arr: F(arr))(arr)\n    return jnp.diag(jac)\n\ndef ddF_m2(arr, F, dF):\n    hess = jax.jacobian(lambda arr: dF(arr, F))(arr)\n    return jnp.diag(hess)\nComputing the first and second derivative (and error) of each function using both methods gives the following\npython\nCopy\nexact_dF1  = (-1/array**2)\nexact_ddF1 = (2/array**3)\n\nprint(\"Function 1 using all grad()\")\ndF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)\nddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)\nprint(dF1_m1  - exact_dF1,\"\\n\")\nprint(ddF1_m1 - exact_ddF1,\"\\n\")\n\nprint(\"Function 1 using all jacobian()\")\ndF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)\nddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)\nprint(dF1_m2  - exact_dF1,\"\\n\")\nprint(ddF1_m2 - exact_ddF1,\"\\n\")\nOutput\npython\nCopy\nFunction 1 using all grad()\n[[ 0.         48.43877   ]\n [ 0.          0.62903005]] \n\n[[  0.        674.248    ]\n [  0.          0.9977852]] \n\nFunction 1 using all jacobian()\n[[0. 0.]\n [0. 0.]] \n\n[[0. 0.]\n [0. 0.]] \nand\npython\nCopy\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\nprint(\"Function 2 using all grad()\")\ndF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)\nddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)\nprint(dF2_m1  - exact_dF2,\"\\n\")\nprint(ddF2_m1 - exact_ddF2,\"\\n\")\n\nprint(\"Function 2 using all jacobian()\")\ndF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)\nddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)\nprint(dF2_m2  - exact_dF2,\"\\n\")\nprint(ddF2_m2 - exact_ddF2,\"\\n\")\nOutput\npython\nCopy\nFunction 2 using all grad()\n[[0. 0.]\n [0. 0.]] \n\n[[0.         0.86209416]\n [0.         7.5651155 ]] \n\nFunction 2 using all jacobian()\n[[ 0.         -0.10149619]\n [ 0.         -6.925739  ]] \n\n[[0.        2.8620942]\n [0.        9.565115 ]] \nI would prefer only to use jax.grad() for something like F1 but it seems right now that only jax.jacobian is working. The whole reason for this is that I need to calculate higher-order derivatives of a neural network with respect to its inputs. Thank you for any help.",
        "answers": [
            "Assuming exact_* is what you're attempting to compute, you're going about it in the wrong way. Your indexing within the differentiated functions (i.e. ...[0]) is removing some of the elements that you're trying to compute.\nWhat exact_dF1 and exact_ddF1 are computing is element-wise first and second derivatives for 2D inputs. You can compute this using either grad or jacobian by applying vmap twice (once for each input dimension). For example:\npython\nCopy\nexact_dF1  = (-1/array**2)\ngrad_dF1 = jax.vmap(jax.vmap(jax.grad(F1)))(array)\njac_dF1 = jax.vmap(jax.vmap(jax.jacobian(F1)))(array)\nprint(jnp.allclose(exact_dF1, grad_dF1))  # True\nprint(jnp.allclose(exact_dF1, jac_dF1))  # True\n\nexact_ddF1 = (2/array**3)\ngrad_ddF1 = jax.vmap(jax.vmap(jax.grad(jax.grad(F1))))(array)\njac_ddF1 = jax.vmap(jax.vmap(jax.jacobian(jax.jacobian(F1))))(array)\nprint(jnp.allclose(exact_ddF1, grad_ddF1))  # True\nprint(jnp.allclose(exact_ddF1, jac_ddF1))  # True\nWhat exact_dF2 and exact_ddF2 are computing is a row-wise jacobian and hessian of a 2D->1D mapping. By its nature, this is difficult to compute using jax.grad, which is meant for functions with scalar output, but you can compute it using the jacobian this way:\npython\nCopy\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\njac_dF2 = jax.vmap(jax.jacobian(lambda a: F2(a)[0]))(array)\njac_ddF2_full = jax.vmap(jax.jacobian(jax.jacobian(lambda a: F2(a)[0])))(array)\njac_ddF2 = jax.vmap(jnp.diagonal)(jac_ddF2_full)\nprint(jnp.allclose(exact_dF2, jac_dF2))  # True\nprint(jnp.allclose(exact_ddF2, jac_ddF2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax"
    },
    {
        "title": "Jax vmap with lax scan having different sequence length in batch dimension",
        "question": "I have this following code , where my sim_timestep is in batch I am not able to run this since the lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) ) requires the concrete array , but since I have vmapped the state_predictor function the sim_timestep is being as a tracedArray . Any help would be greatly appreciated . Thanks all\npython\nCopy\nfrom jax import random\nfrom jax import lax\nimport jax\nimport jax.numpy as jnp\nimport pdb\n\n\ndef fwd_dynamics(x_u, xs):\n    x0,uk =  x_u\n    Delta_T = 0.001\n    lwb = 1.2\n    psi0=x0[2][0]\n    v0= x0[3][0]\n    vdot0 = uk[0][0]\n    delta0 = uk[1][0]\n    thetadot0 = uk[2][0]\n        \n    xdot= jnp.asarray([[v0*jnp.cos(psi0) ],\n        [v0*jnp.sin(psi0)] ,\n        [v0*jnp.tan(delta0)/(lwb)],\n        [vdot0],\n        [thetadot0]])\n    x_next = x0 + xdot*Delta_T\n    return (x_next,uk), x_next  # (\"carryover\", \"accumulated\")\n\n\ndef state_predictor( xk,uk ,sim_timestep):\n    (x_next,_), _ = lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) )\n    return x_next\n\nlow = 0  # Adjust minimum value as needed\nhigh = 100  # Adjust maximum value as needed\nkey = jax.random.PRNGKey(44)\n\nsim_time = jax.random.randint(key, shape=(10, 1), minval=low, maxval=high)\n\nxk = jax.random.uniform(key, shape=(10,5, 1))\nuk = jax.random.uniform(key, shape=(10,2, 1))\n\nstate_predictor_vmap = jax.jit(jax.vmap(state_predictor,in_axes= 0 ,out_axes=0 ))\nx_next = state_predictor_vmap( xk,uk ,sim_time)\nprint(x_next.shape)\nI tried to solve it by above code , hoping to get alternative way to achieve the same functionality.",
        "answers": [
            "What you're asking to do is impossible: scan lengths must be static, and vmapped values are non-static by definition.\nWhat you can do instead is replace your scan with a fori_loop or a while_loop, and then the loop boundary does not need to be static. For example, if you implement your function this way and leave the rest of your code unchanged, it should work:\npython\nCopy\ndef state_predictor(xk, uk, sim_timestep):\n  body_fun = lambda i, x_u: fwd_dynamics(x_u, i)[0]\n  x_next, _ = lax.fori_loop(0, sim_timestep[0], body_fun, (xk, uk))\n  return x_next"
        ],
        "link": "https://stackoverflow.com/questions/78713478/jax-vmap-with-lax-scan-having-different-sequence-length-in-batch-dimension"
    },
    {
        "title": "Simplest equivalent implementation of numpy.ma.notmasked_edges() for use in JAX",
        "question": "I have a square numpy.ndarray and a numpy boolean mask of the same shape. I want to find the first element in each row of the array that is not masked.\nMy code currently relies on numpy.ma.notmasked_edges(), which does exactly what I need. However, I now need to migrate my code to JAX, which has not implemented numpy.ma within jax.numpy.\nWhat would be the simplest way to find the index of the first unmasked element in each row, calling only numpy functions that have been implemented in JAX (which exclude numpy.ma)?\nThe code I'm trying to reproduce is something like:\npython\nCopy\nimport numpy as np\nmy_array = np.random.rand(5,5)\nmask = (my_array < 0.5)\nmy_masked_array = np.ma.masked_array(my_array, mask=mask)\nnp.ma.notmasked_edges(my_masked_array, axis=1)[0]\nI'm sure there are many ways to do this, but I'm looking for the least unwieldy way.",
        "answers": [
            "Here's a JAX implementation of nonmasked_edges, which takes a boolean mask and returns the same indices returned by the numpy.ma function:\npython\nCopy\nimport jax.numpy as jnp\n\ndef notmasked_edges(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  alltrue = mask.all(axis=axis).ravel()\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind)[~alltrue] for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel()[~alltrue])\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel()[~alltrue])\n  \n  return [tuple(first), tuple(last)]\nThis will not be compatible with JIT, because the size of the output arrays depend on the values of the mask (rows which have no unmasked value are left out).\nIf you want a JIT-compatible version, you can remove the [~alltrue] indexing, and the first/last index will be returned for rows that have no unmasked value:\npython\nCopy\ndef notmasked_edges_v2(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind) for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel())\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel())\n\n  return [tuple(first), tuple(last)]\nHere's an example:\npython\nCopy\nimport numpy as np\nmask = np.array([[True, False, False, True],\n                 [False, False, True, True],\n                 [True, True, True, True]])\n\narr = np.ma.masked_array(np.ones_like(mask), mask=mask)\nprint(np.ma.notmasked_edges(arr, axis=1))\n# [(array([0, 1]), array([1, 0])), (array([0, 1]), array([2, 1]))]\n\nprint(notmasked_edges(mask, axis=1))\n# [(Array([0, 1], dtype=int32), Array([1, 0], dtype=int32)),\n#  (Array([0, 1], dtype=int32), Array([2, 1], dtype=int32))]\n\nprint(notmasked_edges_v2(mask, axis=1))\n# [(Array([0, 1, 2], dtype=int32), Array([1, 0, 0], dtype=int32)),\n#  (Array([0, 1, 2], dtype=int32), Array([2, 1, 3], dtype=int32))]"
        ],
        "link": "https://stackoverflow.com/questions/78660344/simplest-equivalent-implementation-of-numpy-ma-notmasked-edges-for-use-in-jax"
    },
    {
        "title": "Using JAX ndarray.at apply(ufunc) with arguments",
        "question": "Can arguments be passed to a jax.numpy.ufunc within a jax.numpy.ndarray.at call?\nThe following is an attempt to replicate jax.numpy.ndarray.at[...].add(...)\npython\nCopy\nimport jax.numpy as jnp\n\ndef myadd(a,b=1):\n    return a+b\n\numyadd = jnp.frompyfunc(myadd,2,1,identity=0)\n\nx = jnp.arange(4)\n\n# call jnp.add(x,x)\nx.at[:].add(x)\n# [0 2 4 6]\n\n# call umyadd.at\numyadd.at(x, np.arange(x.size), x, inplace=False)\n# [0 2 4 6]\n\n# Default b=1 (can b be passed here?)\nx.at[:].apply(umyadd)\n# [1 2 3 4]",
        "answers": [
            "arr.at[...].apply() only accepts unary functions that map a scalar to a scalar. So you could pass b via closure, as long as it's a scalar; for example:\npython\nCopy\nx.at[:].apply(lambda a: umyadd(a, 2))\n# [2, 3, 4, 5]\nBut there is no way to pass b=jnp.arange(4) within apply(), because then the applied function no longer maps a scalar to a scalar."
        ],
        "link": "https://stackoverflow.com/questions/78642505/using-jax-ndarray-at-applyufunc-with-arguments"
    },
    {
        "title": "how to log activation values using jax",
        "question": "I am following a jax tutorial that trains mnist using mlp network. I am trying to add an additional code that saves the activation patterns at every layer except the last. Here is the modified code:\npython\nCopy\nfrom collections import defaultdict\n# this is my activation pattern logger\nclass ActivationLogger:\n    def __init__(self, epoch):\n       self.reset(epoch)\n\n    def __call__(self, layer, activations):\n        D = activations.shape[0]\n        for i in range(D):\n            self.activations[(layer, i)].append(\n                    jax.lax.stop_gradient(activations[i]))\n\n    def reset(self, epoch):\n        self.epoch = epoch\n        self.activations = defaultdict(list)\n\nactivation_logger = ActivationLogger(epoch=1)\n\n...\n\ndef predict(params, image):\n    # per-example predictions\n    activations = image\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        activation_logger(l+1, activations) # <- this was added\n\n    final_w, final_b = params[-1]\n    logits = jnp.dot(final_w, activations) + final_b\n    return logits - logsumexp(logits)\n\nbatched_predict = jax.vmap(\n        predict, \n        in_axes=(None, 0), \n        out_axes=0)\n\n@jax.jit\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\nWhen I run my training code, I keep getting the following error message:\nTracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[].\nThis BatchTracer with object id 7541955024 was created on line:\n  /var/folders/km/3nj8tmq56s16dsgc9_63530r0000gn/T/ipykernel_73750/3494160804.py:12:20 (ActivationLogger.__call__)\nAny suggestions on how to fix this?",
        "answers": [
            "Python functions in JAX code are executed at trace-time, not runtime, and so as written you're not logging concrete runtime values, but rather their abstract trace-time representations.\nIf you want to log runtime values, the best tool is probably jax.debug.callback; for info on using this, I'd suggest starting with External Callbacks in JAX.\nUsing it in your case would look something like this:\npython\nCopy\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        jax.debug.callback(activation_logger, l+1, activations)\nFor more background on JAX's execution model, and why your function didn't work as expected when executed directly a trace-time, a good place to start is How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78611094/how-to-log-activation-values-using-jax"
    },
    {
        "title": "jax complaining about static start/stop/step",
        "question": "Here is a very simple computation in jax which errors out with complaints about static indices:\npython\nCopy\ndef get_slice(ar, k, I):\n  return ar[i:i+k]\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2,3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\npython\nCopy\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-32-6c60650ce6b7> in <cell line: 1>()\n----> 1 vec_get_slice(arr, 2, jnp.arange(3))\n\n    [... skipping hidden 3 frame]\n\n4 frames\n<ipython-input-29-9528369725c2> in get_slice(ar, k, i)\n      1 def get_slice(ar, k, i):\n----> 2   return ar[i:i+k]\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/array.py in __getitem__(self, idx)\n    346           return out\n    347 \n--> 348     return lax_numpy._rewriting_take(self, idx)\n    349 \n    350   def __iter__(self):\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _rewriting_take(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4602 \n   4603   treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape)\n-> 4604   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n   4605                  unique_indices, mode, fill_value)\n   4606 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4611             unique_indices, mode, fill_value):\n   4612   idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n-> 4613   indexer = _index_to_gather(shape(arr), idx)  # shared with _scatter_update\n   4614   y = arr\n   4615 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _index_to_gather(x_shape, idx, normalize_indices)\n   4854                \"dynamic_update_slice (JAX does not support dynamically sized \"\n   4855                \"arrays within JIT compiled functions).\")\n-> 4856         raise IndexError(msg)\n   4857 \n   4858       start, step, slice_size = _preprocess_slice(i, x_shape[x_axis])\n\nHorrible error output below. I am obviously missing something simple, but what?\n\n\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0, 1, 2], dtype=int32)\n  batch_dim = 0, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([2, 3, 4], dtype=int32)\n  batch_dim = 0, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).",
        "answers": [
            "Indices passed to slices in JAX must be static. Values that are mapped over in vmap are not static: because you're mapping over the start indices, your indices are not static and you see this error.\nThere is good news though: the size of your subarray is controlled by k, which is unmapped in your code and therefore static; it's only the location of the slice (given by I) that is dynamic. This is exactly the situation that jax.lax.dynamic_slicewas designed for, and so you can rewrite your code like this:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef get_slice(ar, k, I):\n  return jax.lax.dynamic_slice(ar, (I,), (k,))\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2, 3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n# Array([[1, 2],\n#        [2, 3],\n#        [3, 4]], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78588301/jax-complaining-about-static-start-stop-step"
    },
    {
        "title": "TypeError: unhashable type: 'ArrayImpl' when trying to use Equinox module with jax.lax.scan",
        "question": "I'm new to Equinox and JAX but wanted to use them to simulate a dynamical system.\nBut when I pass my system model as an Equinox module to jax.lax.scan I get the unhashable type error in the title. I understand that jax expects the function argument to be a pure function but I thought an Equinox Module would emulate that.\nHere is a test script to reproduce the error\npython\nCopy\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\n\n\nclass EqxModel(eqx.Module):\n    A: jax.Array\n    B: jax.Array\n    C: jax.Array\n    D: jax.Array\n\n    def __call__(self, states, inputs):\n        x = states.reshape(-1, 1)\n        u = inputs.reshape(-1, 1)\n        x_next = self.A @ x + self.B @ u\n        y = self.C @ x + self.D @ u\n        return x_next.reshape(-1), y.reshape(-1)\n\n\ndef simulate(model, inputs, x0):\n    xk = x0\n    outputs = []\n    for uk in inputs:\n        xk, yk = model(xk, uk)\n        outputs.append(yk)\n    outputs = jnp.stack(outputs)\n    return xk, outputs\n\n\nA = jnp.array([[0.7, 1.0], [0.0, 1.0]])\nB = jnp.array([[0.0], [1.0]])\nC = jnp.array([[0.3, 0.0]])\nD = jnp.array([[0.0]])\nmodel = EqxModel(A, B, C, D)\n\n# Test simulation\ninputs = jnp.array([[0.0], [1.0], [1.0], [1.0]])\nx0 = jnp.zeros(2)\nxk, outputs = simulate(model, inputs, x0)\nassert jnp.allclose(xk, jnp.array([2.7, 3.0]))\nassert jnp.allclose(outputs, jnp.array([[0.0], [0.0], [0.0], [0.3]]))\n\n# This raises TypeError\nxk, outputs = jax.lax.scan(model, x0, inputs)\nWhat is unhashable type: 'ArrayImpl' referring to? Is it the arrays A, B, C, and D? In this model, these matrices are parameters and therefore should be static for the duration of the simulation.\nI just found this issue thread that might be related:\nlax.scan for equinox Modules",
        "answers": [
            "Owen Lockwood (lockwo) has provided an explanation and answer in this issue thread, which I will re-iterate below.\nI believe your issue is happening because jax tries to hash the function you are scanning over, but it can't hash the arrays that are in the module. There are probably a number of things that you could do to solve this, the simplest being to just curry the model, e.g. xk, outputs = jax.lax.scan(lambda carry, y: model(carry, y), x0, inputs) works fine\nOr, re-written in terms of the variable names I am using:\npython\nCopy\nxk, outputs = jax.lax.scan(lambda xk, uk: model(xk, uk), x0, inputs)"
        ],
        "link": "https://stackoverflow.com/questions/78583009/typeerror-unhashable-type-arrayimpl-when-trying-to-use-equinox-module-with-j"
    },
    {
        "title": "Multiplying chains of matrices in JAX",
        "question": "Suppose I have a vector of parameters p which parameterizes a set of matrices A_1(p), A_2(p),...,A_N(p). I have a computation in which for some list of indices q of length M, I have to compute A_{q_M} * ... * A_{q_2} * A_{q_1} * v for several different q s. Each q has a different length, but crucially doesn't change! What changes, and what I wish to take gradients against is p.\nI'm trying to figure out how to convert this to performant JAX. One way to do it is to have some large matrix Q which contains all the different qs on each row, padded out with identity matrices such that each multiplication chain is the same length, and then scan over a function that switch es between N different functions doing matrix-vector multiplications by A_n(p).\nHowever -- I don't particularly like the idea of this padding. Also, since Q here is fixed, is there potentially a smarter way to do this? The distribution of lengths of q s has a very long tail, so Q will be dominated by padding.\nEDIT: Here's a (edit 2: functional) minimal example\npython\nCopy\nsigma0 = jnp.eye(2)\nsigmax = jnp.array([[0, 1], [1, 0]])\nsigmay = jnp.array([[0, -1j], [1j, 0]])\nsigmaz = jnp.array([[1, 0], [0, -1]])\nsigma = jnp.array([sigmax, sigmay, sigmaz])\n\ndef gates_func(params):\n    theta = params[\"theta\"]\n    epsilon = params[\"epsilon\"]\n\n    n = jnp.array([jnp.cos(theta), 0, jnp.sin(theta)])\n    omega = jnp.pi / 2 * (1 + epsilon)\n    X90 = expm(-1j * omega * jnp.einsum(\"i,ijk->jk\", n, sigma) / 2)\n\n    return {\n        \"Z90\": expm(-1j * jnp.pi / 2 * sigmaz / 2),\n        \"X90\": X90\n    }\n\ndef multiply_out(params):\n    gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n\n    gates = gates_func(params)\n    out = jnp.zeros(len(gate_lists)) \n    \n    for i, gate_list in enumerate(gate_lists):\n        init = jnp.array([1.0,0.0], dtype=jnp.complex128)\n        for g in gate_list:\n            init = gates[g] @ init\n        out = out.at[i].set(jnp.abs(init[0]))\n\n    return out\n\nparams = dict(theta=-0.0, epsilon=0.001)\nmultiply_out(params)",
        "answers": [
            "The main issue here is that JAX does not support string inputs. But you can use NumPy to manipulate string arrays and turn them into integer categorical arrays that can then be used by jax.jit and jax.vmap. The solution might look something like this:\npython\nCopy\nimport numpy as np\n\ndef gates_func_int(params, gate_list_vals):\n  g = gates_func(params)\n  identity = jnp.eye(*list(g.values())[0].shape)\n  return jnp.stack([g.get(val, identity) for val in gate_list_vals])\n\n@jax.jit\ndef multiply_out_2(params):\n  # compile-time pre-processing\n  gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n  max_size = max(map(len, gate_lists))\n  gate_array = np.array([gates + [''] * (max_size - len(gates))\n                        for gates in gate_lists])\n  gate_list_vals, gate_list_ints = np.unique(gate_array, return_inverse=True)\n  gate_list_ints = gate_list_ints.reshape(gate_array.shape)\n\n  # runtime computation\n  gates = gates_func_int(params, gate_list_vals)[gate_list_ints]\n  initial = jnp.array([[1.0],[0.0]], dtype=jnp.complex128)\n  return jax.vmap(lambda g: jnp.abs(jnp.linalg.multi_dot([*g, initial]))[0])(gates).ravel()\n\nmultiply_out_2(params)"
        ],
        "link": "https://stackoverflow.com/questions/78562406/multiplying-chains-of-matrices-in-jax"
    },
    {
        "title": "Why JAX is considering same list as different data structure depending on appending a new array inside function?",
        "question": "I am very new to JAX. Please excuse me if this something obvious or I am making some stupid mistake. I am trying to implement a function which does the following. All these functions will be called from other JIT-ed function. So, removing JIT may not be possible.\nget_elements function takes a JAX array( call it state (1D)). Looks at each element in it and calls a function get_condition.\nget_condition returns a tuple depending on the element at the given position of state. The tuple may be (1,0),(0,1) or (0,0)\nHere I want to call update_state only if the tuple received from get_conn is (0,1) or (1,0). In that case update_state_vec will get called and add a new vector of same length as state will get appended to the list.\nBut, I couldn't make jax.lax.cond work here. So, I tried to call update_state for each case, but I want the list to remain unchanged if the codition is (0,0).\nIn update_state_vec, no_update_state should return the same array\nthat it receives withourt appending anything\nHere, is the entire code:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import lax\nimport copy\nfrom copy import deepcopy\n\nimport numpy as np\n\n\ndef get_condition(state, x, y):\n   L = (jnp.sqrt(len(jnp.asarray(state)))).astype(int)\n   state = jnp.reshape(state, (L,L), order=\"F\")\n   s1 = state[x, y]\n\n   branches = [lambda : (0,1), lambda : (1,0), lambda : (0,0)]\n   conditions = jnp.array([s1==2, s1==4, True])\n   result = lax.switch(jnp.argmax(conditions), branches)\n   return tuple(x for x in result)\n\n\n\n\ndef update_state_vec(state, x, y, condition, list_scattered_states):\n   L = (jnp.sqrt(len(state))).astype(int)   \n   def update_state_4(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(4)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n   def update_state_2(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(2)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n\n   def no_update_state (list_scattered_states):\n       #state1 = jnp.ravel(state, order=\"F\")\n       #list_scattered_states.append(jnp.ravel(state, order=\"F\"))\n       #This doesn't work---------------------------------\n       return list_scattered_states\n\n\n\n   conditions = jnp.array([condition == (1, 0), condition == (0, 1), condition == (0, 0)])\n   print(conditions)\n   branches = [update_state_4, update_state_2,no_update_state]\n\n   return(lax.switch(jnp.argmax(conditions), branches, operand=list_scattered_states))\n           \n\n\ndef get_elements(state):\n\n   L = (jnp.sqrt(len(state))).astype(int)\n   list_scattered_states = []\n   for x in range(L):\n       for y in range(L):\n           condition=get_condition(state, x, y)\n           print(condition)\n           list_scattered_states = update_state_vec(state, x, y, condition, list_scattered_states)\n\n\n   return list_scattered_states\nWe can take an example input as follows,\npython\nCopy\narr=jnp.asarray([2., 1., 3., 4., 1., 2., 3., 4., 4., 1., 2., 3., 4., 2., 1., 3.])\nget_elements(arr)\nI get an error message as below:\npython\nCopy\n    print(conditions)\n 41 branches = [update_state_4, update_state_2,no_update_state]\n ---> 43 return(lax.switch(jnp.argmax(conditions), branches, \n operand=list_scattered_states))\n TypeError: branch 0 and 2 outputs must have same type structure, got PyTreeDef([*]) \n and PyTreeDef([]).\nSo, the error is coming from the face that no_update_state is returning something that doesn't match with return type of update_state_4 or update_state_2. I am quite clueless at this point. Any help will be much appreciated.",
        "answers": [
            "The root of the issue here is that under transformations like jit, vmap, switch, etc. JAX requires the shape of outputs to be known statically, i.e. at compile time (see JAX sharp bits: dynamic shapes). In your case, the functions you are passing to switch return outputs of different shapes, and since jnp.argmax(conditions) is not known at compile time, there's no way for the compiler to know what memory to allocate for the result of this function.\nSince you're not JIT-compiling or otherwise transforming your code, the easiest way to address this would be to replace the lax.switch statement with this:\npython\nCopy\n  if condition == (1, 0):\n    list_scattered_states = update_state_4(list_scattered_states)\n  elif condition == (0, 1):\n    list_scattered_states = update_state_2(list_scattered_states)\n  return list_scattered_states\nIf you do want your function to be compatible with jit or other JAX transformations, you'll have to re-write the logic so that the size of list_scattered_states remains constant, e.g. by padding it to the expected size from the beginning."
        ],
        "link": "https://stackoverflow.com/questions/78512663/why-jax-is-considering-same-list-as-different-data-structure-depending-on-append"
    },
    {
        "title": "Why is custom pytree 'aux_data' traced after jax.jit() for jnp.array but not for np.array?",
        "question": "I am trying to understand how pytrees work and registered my own class as a pytree. I noticed that if the aux_data in the pytree is a jax.numpy.ndarray the auxilliary data is subsequently traced and returned as a Traced<ShapedArray(...)>.... However, if the aux_data is a numpy.ndarray (i.e. not JAX array), then it is not traced and returns an array from a jit tranformed function.\nNow, I am aware of the tracing that happens during the jax.jit() transformation, but I do not understand why, on the level of pytrees, this results in the behaviour described above.\nHere is an example to reproduce this behaviour (multiplying both the aux_data and the tree leaves by two, which may be a problem in itself after JIT transformation...?). I have used the custom pytree implementations of accepted libraries (equinox and simple_pytree) for comparison, and they all give the same result, so that I am very sure that this is not a bug but a feature that I am trying to understand.\npython\nCopy\nimport jax\nfrom jax.tree_util import tree_structure, tree_leaves\nimport numpy as np\n\ndef get_pytree_impl(base):\n    if base == \"equinox\":\n        import equinox as eqx\n        Module = eqx.Module\n        static_field = eqx.static_field\n    elif base == \"simple_pytree\":\n        from simple_pytree import Pytree, static_field\n        Module = Pytree\n    elif base == \"dataclasses\":\n        from dataclasses import dataclass, field\n        @dataclass\n        class Module():\n            pass\n        static_field = field\n    \n    class PytreeImpl(Module):\n        x: jax.numpy.ndarray\n        y: jax.numpy.ndarray = static_field()\n\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n\n    if base == 'dataclasses':\n        from jax.tree_util import register_pytree_node\n        \n        def flatten(ptree):\n            return ((ptree.x,), ptree.y)\n        \n        def unflatten(aux_data, children):\n            return PytreeImpl(*children, aux_data)\n\n        register_pytree_node(PytreeImpl, flatten, unflatten)\n        \n    return PytreeImpl\n\ndef times_two(ptree):\n    return type(ptree)(ptree.x*2, ptree.y*2)\n\ntimes_two_jitted = jax.jit(times_two)\n\nbases = ['dataclasses', 'equinox', 'simple_pytree']\nfor base in bases:\n    print(\"========  \" + base + \"  ========\")\n    for lib_name, array_lib in zip(['jnp', 'np'], [jax.numpy, np]):\n        print(\"====  \" + lib_name)\n        PytreeImpl = get_pytree_impl(base)\n        x = jax.numpy.array([1,2])\n        y = array_lib.array([3,4])\n        input_tree = PytreeImpl(x, y)\n        for tag, pytree in zip([\"input\", \"no_jit\", \"jit\"],[input_tree, times_two(input_tree), times_two_jitted(input_tree)]):\n            print(f' {tag}:')\n            print(f'\\t Structure: {tree_structure(pytree)}')\n            print(f'\\t Leaves: {tree_leaves(pytree)}')\nThis produces the follwing, where dataclasses is my naive custom implementation of a pytree:\npython\nCopy\n========  dataclasses  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  equinox  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([3, 4], dtype=int32),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([6, 8], dtype=int32),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>,)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([3, 4]),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  simple_pytree  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([3, 4], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([6, 8], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>, '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([3, 4]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\nI ran this example using Python 3.12.1 with equinox 0.11.4 jax 0.4.28 jaxlib 0.4.28 simple-pytree 0.1.5",
        "answers": [
            "From the JAX docs:\nWhen defining unflattening functions, in general children should contain all the dynamic elements of the data structure (arrays, dynamic scalars, and pytrees), while aux_data should contain all the static elements that will be rolled into the treedef structure.\naux_data in a pytree flattening must contain static elements, and static elements must be hashable and immutable. Neither np.ndarray nor jax.Array satisfy this, so they should not be included in aux_data. If you do include such values in aux_data, you'll get unsupported, poorly-defined behavior.\nWith that background: the answer to your question of why you're seeing the results you're seeing is that you are defining your pytrees incorrectly. If you define aux_data to only contain static (i.e. hashable and immutable) attributes, you will no longer see this behavior."
        ],
        "link": "https://stackoverflow.com/questions/78485445/why-is-custom-pytree-aux-data-traced-after-jax-jit-for-jnp-array-but-not-for"
    },
    {
        "title": "how to vmap over multiple Dense instances in flax model? trying to avoid looping over a list of Dense instances",
        "question": "from jax import random,vmap\nfrom jax import numpy as jnp\nimport pprint\n\ndef f(s,layers,do,dx):\n    x = jnp.zeros((do,dx))\n    for i,layer in enumerate(layers):\n        x=x.at[i].set( layer( s[i] ) )\n    return x\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layers = [ nn.Dense( self.dx, use_bias=False )\n                        for _ in range(self.do) ]\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None,None))(s,self.layers,self.do,self.dx)\n        return x\n\nif __name__ == '__main__':\n    seed = 123\n    key = random.PRNGKey( seed )\n    key,subkey = random.split( key )\n    outer_batches = 4\n    s_observations = 5 # AKA the inner batch\n    x_features = 2\n    s_features = 3\n    s_shape = (outer_batches,s_observations, s_features)\n    s = random.uniform( subkey, s_shape )\n\n    key,subkey = random.split( key )    \n    model = net(x_features,s_observations)\n    p = model.init( subkey, s )\n    x = model.apply( p, s )    \n\n    params = p['params']\n    pkernels = jnp.array([params[key]['kernel'] for key in params.keys()])\n    x_=jnp.zeros((outer_batches,s_observations,x_features))\n    \n    g = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\n    \n    x_=g(s,pkernels)\n    print('s shape:',s.shape)\n    print('p shape:',pkernels.shape)\n    print('x shape:',x.shape)\n    print('x_ shape:',x_.shape)\n    print('sum of difference:',jnp.sum(x-x_))\nHi. I need some \"batch-specific\" parameters in my model. Here, there is an \"inner batch\" of length do such that there is a flax.linen.Dense instance for each element in that batch. The outer batch just passes multiple data instances into those layers. I accomplish this by creating a list of flax.linen.Dense instances in the setup method. Then in the __call__ method, I iterate over those layers to fill up an array. This iteration is encapsulated in a function f, and that function is wrapped in jax.vmap.\nI have also included some equivalent logic written as matrix multiplication (see the function g) to make it explicit what operation I was hoping to capture with this class.\nI would like to replace the for-loop in the __call__ method with a call to jax.vmap. I ofc get an error when I pass a list to vmap, and I ofc get an error when I try to put multiple Dense instances in a jax array. Is there an alternative to using a list to contain my multiple Dense instances? A constraint is that I should be able to create an arbitrary number of Dense instances at the time of model initialization.",
        "answers": [
            "vmap can be used to map a single function over batches of data. You are attempting to use it to map multiple functions over batches of data, which it cannot do.\nUpdated answer based on updated question:\nSince each layer is identical aside from the parameters fit to the input data, it sounds like what you want is to map a single dense layer against a batch of data. It might look something like this:\npython\nCopy\nkeys = vmap(random.fold_in, in_axes=(None, 0))(subkey, jnp.arange(s_observations))\nmodel = nn.Dense(x_features, use_bias=False)\np = vmap(model.init, in_axes=(0, 1))(keys, s)\nx = vmap(model.apply, in_axes=(0, 1), out_axes=1)(p, s)\n\npkernels = p['params']['kernel']\ng = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\nx_=g(s,pkernels)\n\nprint('sum of difference:',jnp.sum(x-x_))\n# sum of difference: 0.0\nPrevious answer\nIn general, the fix would be to define a single parameterized layer that you can pass to vmap. In the example you gave, every layer is identical, and so to achieve the result you're looking for you could write something like this:\npython\nCopy\ndef f(s,layer,dx):\n  return layer(s)\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layer = nn.Dense( self.dx, use_bias=False )\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None))(s,self.layer,self.dx)\n        return x\nIf you had different parameterization per layer, then you could achieve this within vmap by passing those parameters to vmap as well."
        ],
        "link": "https://stackoverflow.com/questions/78385261/how-to-vmap-over-multiple-dense-instances-in-flax-model-trying-to-avoid-looping"
    },
    {
        "title": "acme error - AttributeError: module 'jax' has no attribute 'linear_util'",
        "question": "I am using acme framework to run some experiments, and I installed acme based on documentation. However, I have attribute error that raised likely from JAX, HAIKU, and when I looked into github issue, there was no solution given at this time. Can anyone take a look what package dependecy caused this issue?\nmy venv spec:\nhere is my venv spec\npython\nCopy\ndm-acme                      0.4.0\ndm-control                   0.0.364896371\ndm-env                       1.6\ndm-haiku                     0.0.10\ndm-launchpad                 0.5.0\ndm-reverb                    0.7.0\ndm-tree                      0.1.8\nacme                         2.10.0\ndm-acme                      0.4.0\njax                          0.4.26\njaxlib                       0.4.26+cuda12.cudnn89\npython -V                    Python 3.9.5\nerror details:\nFile \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in from acme.agents.jax import dqn File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/init.py\", line 18, in from acme.agents.jax.dqn.actor import behavior_policy File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in from acme.agents.jax import actor_core as actor_core_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in from acme.jax import networks as networks_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/init.py\", line 18, in from acme.jax.networks.atari import AtariTorso File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/atari.py\", line 29, in from acme.jax.networks import base File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/base.py\", line 24, in import haiku as hk File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/init.py\", line 20, in from haiku import experimental File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/experimental/init.py\", line 34, in from haiku._src.dot import abstract_to_dot File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/_src/dot.py\", line 163, in @jax.linear_util.transformation File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 54, in getattr raise AttributeError(f\"module {module!r} has no attribute {name!r}\") AttributeError: module 'jax' has no attribute 'linear_util'\nseems it raised from haiku and JAX, how this can be fixed? any quick thoughts?\nupdated attempt\nbased on @jakevdp suggestion, I reinstalled jax, jaxlib, but now I am getting this error again:\npython\nCopy\nTraceback (most recent call last):\n  File \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in <module>\n    from acme.agents.jax import dqn\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/__init__.py\", line 18, in <module>\n    from acme.agents.jax.dqn.actor import behavior_policy\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in <module>\n    from acme.agents.jax import actor_core as actor_core_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in <module>\n    from acme.jax import networks as networks_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/__init__.py\", line 45, in <module>\n    from acme.jax.networks.multiplexers import CriticMultiplexer\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/multiplexers.py\", line 20, in <module>\n    from acme.jax import utils\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/utils.py\", line 190, in <module>\n    devices: Optional[Sequence[jax.xla.Device]] = None,\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 53, in getattr\n    raise AttributeError(f\"module {module!r} has no attribute {name!r}\")\nAttributeError: module 'jax' has no attribute 'xla'\nhere is my pip freeze list on this public gist: acme pip list\nI looked into this github issue: jax xla attribute issue\n@jakevdp, any updated comment or possible workaround for this jax.xla issue? thanks",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt sounds like you have too new a JAX version for the framework code you are using. I'd try installing an older version; e.g.\npython\nCopy\npip install --upgrade \"jax[cuda12_pip]<0.4.24\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSee JAX installation for more installation options.\nIf you're hoping to update the framework code for compatibility with more recent JAX versions, you might find replacements for previous functionality in jax.extend.linear_util."
        ],
        "link": "https://stackoverflow.com/questions/78372618/acme-error-attributeerror-module-jax-has-no-attribute-linear-util"
    },
    {
        "title": "AttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'",
        "question": "I'm trying to run a model written in jax, https://github.com/lindermanlab/S5. However, I ran into some error that says\npython\nCopy\n   Traceback (most recent call last):\n  File \"/Path/run_train.py\", line 101, in <module>\n    train(parser.parse_args())\n  File \"/Path/train.py\", line 144, in train\n    state = create_train_state(model_cls,\n  File \"/Path/train_helpers.py\", line 135, in create_train_state\n    params = variables[\"params\"].unfreeze()\nAttributeError: 'dict' object has no attribute 'unfreeze'\nI tried to replicate this error by\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\n\nmodel = nn.Dense(features=3)\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1, 2)))\nparams_unfrozen = flax.traverse_util.unfreeze(params)\nAnd the error reads:\npython\nCopy\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'\nI'm using:\npython\nCopy\nflax 0.7.4\njax 0.4.13\njaxlib 0.4.13+cuda12.cudnn89\nI think this is an issue relating to the version of flax, but does anyone know what exactly is going on? Any help is appreciated. Let me know if you need any further information",
        "answers": [
            "unfreeze is a method of Flax's FrozenDict class: (See FrozenDict.unfreeze). It appears that you have passed a Python dict where a FrozenDict is expected.\nTo fix this, you should ensure that variables['params'] is a FrozenDict, not a dict.\nRegarding the error in your attempted replication: flax.traverse_util does not define an unfreeze function, but this seems unrelated to the original problem."
        ],
        "link": "https://stackoverflow.com/questions/78256559/attributeerror-module-flax-traverse-util-has-no-attribute-unfreeze"
    },
    {
        "title": "what are the numbers in the operation names when profiling an application",
        "question": "What are the numbers in \"fusion_2\", \"fusion_4\"? Where do they come from? Thank you!",
        "answers": [
            "These numbers exist to de-duplicate the names of generated HLO operations. The first fusion operation created by the compiler is called fusion, the next is fusion_2, then fusion_3, and so on.\nNote that the order of creation does not necessarily match the order of execution."
        ],
        "link": "https://stackoverflow.com/questions/78236312/what-are-the-numbers-in-the-operation-names-when-profiling-an-application"
    },
    {
        "title": "Numpyro AR(1) mean switching model sampling incongrouencies",
        "question": "I'm trying to estimate an AR(1) process y with a switching mean according to a latent state S =0,1 that evolves as a markov process with fixed transition probabilities (as in here). In short, it takes the form:\npython\nCopy\ny_t - mu_{0/1} = phi * (y_{t-1} - mu_{0/1})+ epsilon_t\nwhere mu_0 would be used if state_t = 0 and mu_1 if state_t =1. I'm using jax/numpyro with DiscreteHMCGibbs (although normal NUTS with latent state enumeration yields the same result) but I can't seem to have the sampler work properly. From all diagnostics I run, it seems that all hyperparameters are stuck at initialization value, and summary returns accordingly with all std==0. Here below I have a MWE that reproduces my problem. Is there an obvious mistake I am making in the implementation?\nMWE:\npython\nCopy\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import MCMC, NUTS,DiscreteHMCGibbs\nfrom jax import random, pure_callback\nimport jax\nimport numpy as np\n\ndef generate_synthetic_data(T=100, mu=[0, 5], phi=0.5, sigma=1.0, p=np.array([[0.95, 0.05], [0.1, 0.9]])):\n    states = np.zeros(T, dtype=np.int32)\n    y = np.zeros(T)\n    current_state = np.random.choice([0, 1], p=[0.5, 0.5])\n    states[0] = current_state\n    y[0] = np.random.normal(mu[current_state], sigma)\n\n    for t in range(1, T):\n        current_state = np.random.choice([0, 1], p=p[current_state,:])\n        states[t] = current_state\n        y[t] = np.random.normal(mu[current_state] + phi * (y[t-1] - mu[current_state]), sigma)\n\n    return y, states\n\n\ndef mean_switching_AR1_model(y):\n    T = len(y)\n    phi = numpyro.sample('phi', dist.Normal(0, 1))\n    sigma = numpyro.sample('sigma', dist.Exponential(1))\n    \n    \n    with numpyro.plate('state_plate', 2):\n        mu = numpyro.sample('mu', dist.Normal(0, 5))\n        p = numpyro.sample('p', dist.Dirichlet(jnp.ones(2)))\n\n    probs_init = numpyro.sample('probs_init', dist.Dirichlet(jnp.ones(2)))\n    s_0 = numpyro.sample('s_0', dist.Categorical(probs_init))\n\n    def transition_fn(carry, y_t):\n        prev_state = carry\n        state_probs = p[prev_state]\n        state = numpyro.sample('state', dist.Categorical(state_probs))\n\n        mu_state = mu[state]\n        y_mean = mu_state + phi * (y_t - mu_state)\n        y_next = numpyro.sample('y_next', dist.Normal(y_mean, sigma), obs=y_t)\n        return state, (state, y_next)\n\n    _ , (signal, y)=scan(transition_fn, s_0, y[:-1], length=T-1)\n    return (signal, y)\n\n# Synthetic data generation\nT = 1000\nmu_true = [0, 3]\nphi_true = 0.5\nsigma_true = 0.25\ntransition_matrix_true = np.array([[0.95, 0.05], [0.1, 0.9]])\ny, states_true = generate_synthetic_data(T, mu=mu_true, phi=phi_true, sigma=sigma_true, p=transition_matrix_true)\n\n\nrng_key = random.PRNGKey(0)\nnuts_kernel = NUTS(mean_switching_AR1_model)\ngibbs_kernel = DiscreteHMCGibbs(nuts_kernel, modified=True)\n\n# Run MCMC\nmcmc = MCMC(gibbs_kernel, num_samples=1000, num_warmup=1000)\nmcmc.run(rng_key, y=y)\nmcmc.print_summary()",
        "answers": [
            "So it turns out there was indeed a pretty obvious mistake in the sense that I was not correctly carrying down y_{t-1} as part of the state variables. The following corrected transition functions yield the intended result without problems.\npython\nCopy\ndef transition_fn(carry, y_curr):\n    prev_state, y_prev = carry\n    state_probs = p[prev_state]\n    state = numpyro.sample('state', dist.Categorical(state_probs))\n\n    mu_state = mu[state]\n    y_mean = mu_state + phi * (y_prev - mu_state)\n    y_curr = numpyro.sample('y_curr', dist.Normal(y_mean, sigma), obs=y_curr)\n    return (state, y_curr), (state, y_curr)\n\n_, (signal, y) = scan(transition_fn, (s_0, y[0]), y[1:], length=T-1)"
        ],
        "link": "https://stackoverflow.com/questions/78209454/numpyro-ar1-mean-switching-model-sampling-incongrouencies"
    },
    {
        "title": "\"The truth value of an array with more than one element is ambiguous\" when trying to train a new JAX+Equinox model a second time",
        "question": "TL;DR: I create a new instance of my equinox.Module model and fit it using Optax. Everything works fine. When I create a new instance of the same model and try to fit it from scratch, using the same code, same initial values, same everything, I get:\npython\nCopy\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n...somewhere deep in Optax code. My code doesn't compare any arrays. The error message doesn't show where exactly the comparison happens. What's wrong?\nCode\npython\nCopy\n# 1. Import dependencies.\nimport jax; jax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as np, jax.random as rnd, equinox as eqx\nimport optax\n\n# 2. Define loss function. I'm fairly confident this is correct.\ndef npdf(x, var):\n    return np.exp(-0.5 * x**2 / var) / np.sqrt(2 * np.pi * var)\n\ndef mixpdf(x, ps, vars):\n    return ps.dot(npdf(x, vars))\n\ndef loss(model, series):\n    weights, condvars = model(series)\n    return -jax.vmap(\n        lambda x, vars: np.log(mixpdf(x, weights, vars))\n    )(series[1:], condvars[:-1]).mean()\n\n# 3. Define recurrent neural network.\nclass RNNCell(eqx.Module):\n    bias: np.ndarray\n    Wx: np.ndarray\n    Wh: np.ndarray\n    def __init__(self, ncomp: int, n_in: int=1, *, key: np.ndarray):\n        k1, k2, k3 = rnd.split(key, 3)\n        self.bias = rnd.uniform(k1, (ncomp, ))\n        self.Wx = rnd.uniform(k2, (ncomp, n_in))\n        self.Wh = 0.9 * rnd.uniform(k3, (ncomp, ))\n\n    def __call__(self, vars_prev, obs):\n        vars_new = self.bias + self.Wx @ obs + self.Wh * vars_prev\n        return vars_new, vars_new\n\nclass RNN(eqx.Module):\n    cell: RNNCell\n    logits: np.ndarray\n    vars0: np.ndarray = eqx.field(static=True)\n\n    def __init__(self, vars0: np.ndarray, n_in=1, *, key: np.ndarray):\n        self.vars0 = np.array(vars0)\n        K = len(self.vars0)\n        self.cell = RNNCell(K, n_in, key=key)\n        self.logits = np.zeros(K)\n\n    def __call__(self, series: np.ndarray):\n        _, hist = jax.lax.scan(self.cell.__call__, self.vars0, series**2)\n        return jax.nn.softmax(self.logits), abs(hist)\n\n    def condvar(self, series):\n        weights, variances = self(series)\n        return variances @ weights\n\n    def predict(self, series: np.ndarray):\n        return self.condvar(series).flatten()[-1]\n\n# 4. Training/fitting code.\ndef fit(model, logret, nepochs: int, optimizer, loss):\n    loss_and_grad = eqx.filter_value_and_grad(loss)\n    \n    @eqx.filter_jit\n    def make_step(model, opt_state):\n        loss_val, grads = loss_and_grad(model, logret)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss_val, model, opt_state\n\n    opt_state = optimizer.init(model)\n    for epoch in range(nepochs):\n        loss_val, model, opt_state = make_step(model, opt_state)\n    print(\"Works!\")\n    return model\n\ndef experiment():\n    series = rnd.normal(rnd.PRNGKey(8), (100, 1))\n    model = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\n    return fit(model, series, 100, optax.adam(0.01), loss)\n\n# 5. Run the exact same code twice.\nexperiment() # 1st call, works\nexperiment() # 2nd call, error\nError message\npython\nCopy\n> python my_RNN.py\nWorks!\nTraceback (most recent call last):\n  File \"/Users/forcebru/test/my_RNN.py\", line 75, in <module>\n    experiment() # 2nd call, error\n    ^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 72, in experiment\n    return fit(model, series, 100, optax.adam(0.01), loss)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 65, in fit\n    loss_val, model, opt_state = make_step(model, opt_state)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 206, in __call__\n    return self._call(False, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_module.py\", line 935, in __call__\n    return self.__func__(self.__self__, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 200, in _call\n    out = self._cached(dynamic_donate, dynamic_nodonate, static)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 495, in common_infer_params\n    jaxpr, consts, out_shardings, out_layouts_flat, attrs_tracked = _pjit_jaxpr(\n                                                                    ^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1150, in _pjit_jaxpr\n    jaxpr, final_consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n                                                   ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 350, in memoized_fun\n    ans = call(fun, *args)\n          ^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1089, in _create_pjit_jaxpr\n    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/profiler.py\", line 336, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2314, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2336, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 49, in fun_wrapped\n    out = fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 59, in make_step\n    updates, opt_state = optimizer.update(grads, opt_state)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/combine.py\", line 59, in update_fn\n    updates, new_s = fn(updates, s, params, **extra_args)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/base.py\", line 337, in update\n    return tx.update(updates, state, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/transform.py\", line 369, in update_fn\n    mu_hat = bias_correction(mu, b1, count_inc)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 491, in common_infer_params\n    canonicalized_in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 4, in __eq__\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 745, in __bool__\n    check_bool_conversion(self)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 662, in check_bool_conversion\n    raise ValueError(\"The truth value of an array with more than one element is \"\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nProblem\nThe error message says File \"<string>\", line 4, in __eq__, which doesn't help.\nIt refers to the line mu_hat = bias_correction(mu, b1, count_inc) in Optax code, but as far as I understand, it doesn't compare any arrays.\nIt also refers to JAX code that's supposedly responsible for JIT compilation, but this seems outside my control.\nIs there a bug in my model definition (RNNCell or RNN)? Did I implement the training loop wrong? I basically copied it straight from Equinox docs, so it should be fine. Why does it work when I call experiment() the first time, but not the second?",
        "answers": [
            "It appears this is a bug in equinox. The function _process_in_axis_resources is decorated in functools.lru_cache, meaning that all inputs are checked for equality with arguments from the previous call. On the second run, this triggers a call to equinox.Module.__eq__, which raises the error. You can see this problem by doing the equality check directly:\npython\nCopy\nmodel = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel2 = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel == model2\n# ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nI would suggest reporting this bug at https://github.com/patrick-kidger/equinox/issues\nYou could probably work around this issue by not storing a numpy array (vars0) as a static attribute. I suspect that equinox assumes that all static attributes are hashable, and numpy arrays are not.\nEdit: I just checked, and changing this:\npython\nCopy\nvars0: np.ndarray = eqx.field(static=True)\nto this:\npython\nCopy\nvars0: np.ndarray\nresolves the issue.\nEdit 2: Indeed it looks like static fields in equinox must be hashable, so this is not an equinox bug but rather a usage error (see the discussion at https://github.com/patrick-kidger/equinox/issues/154#issuecomment-1561735995). You might try storing vars0 as a tuple (which is hashable) rather than an array (which isn't)."
        ],
        "link": "https://stackoverflow.com/questions/78074623/the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous-when-tryin"
    },
    {
        "title": "Using Orbax to checkpoint flax `TrainState` with new `CheckpointManager` API",
        "question": "Context\nThe Flax docs describe how to checkpoint a flax.training.train_state.TrainState with orbax. In a nutshell, you set up a orbax.checkpoint.CheckpointManager which keeps track of checkpoints. Next, you use the CheckpointManager to save the state to disk. Summarising the code snippets from the Flax docs:\npython\nCopy\nimport orbax\n\n# <-- Code building an empty and a full chkpt. -->.\nabstract_chkpt = ...\nchkpt = ...\n\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\nsave_args = orbax_utils.save_args_from_target(ckpt)\n\noptions = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_manager = orbax.checkpoint.CheckpointManager(\n    '/tmp/flax_ckpt/orbax/managed', orbax_checkpointer, options)\n\n# Save and restore a checkpoint.\ncheckpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args})\ncheckpoint_manager.restore(1, items=abstract_ckpt)\nThe notebook provided by the Flax docs does what I want: periodically track TrainState, which can then be restored. However, when executing the code provided by the Flax docs warn that this orbax checkpoint API is deprecated:\nWARNING:absl:Configured CheckpointManager using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\nThe link indicated by the error message gives some pointers how to use the new orbax.checkpoint.CheckpointManager.\nQuestion\nHow do I save and restore a Flax TrainState with the new orbax.checkpoint.CheckpointManager API?\nHere is my failed attempt (based on the Orbax migration instructions) at saving and restoring a trivial flax.training.train_state.TrainState:\npython\nCopy\nimport orbax.checkpoint as obc\nfrom flax.training.train_state import TrainState\n\nabstract_ckpt = TrainState(step=0, apply_fn=lambda _: None, params={}, tx={}, opt_state={})\nckpt = abstract_ckpt.replace(step=1)\n\n# Set up the checkpointer.\noptions = obc.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_dir = obc.test_utils.create_empty('/tmp/checkpoint_manager')\ncheckpoint_manager = obc.CheckpointManager(checkpoint_dir, options=options)\nsave_args = obc.args.StandardSave(abstract_ckpt)\n\n# Do actual checkpointing.\ncheckpoint_manager.save(1, ckpt, args=save_args)\n\n# Restore checkpoint.\nrestore_args = obc.args.StandardRestore(abstract_ckpt)\nrestored_ckpt = checkpoint_manager.restore(1, args=restore_args)\n\n# Verify if it is correctly restored.\nassert ckpt.step == restored_ckpt.step  # AssertionError\nMy guess would be that the problem relates to save_args, but I haven't managed to pinpoint the problem and figure out a fix. Any suggestions how to correctly restore the checkpoint using the new CheckpointManager API?",
        "answers": [
            "You created save_args = ocp.args.StandardSave(abstract_ckpt) instead of save_args = ocp.args.StandardSave(ckpt), so you're just saving the wrong thing.\nAlso note that checkpoint_dir = ocp.test_utils.create_empty('/tmp/checkpoint_manager') is a bit unnecessary - it's just a test utility for deleting a directory if it already exists - makes running our colabs a bit easier. Probably you shouldn't need to use it in real life, as the create option in CheckpointManager will create the directory for you."
        ],
        "link": "https://stackoverflow.com/questions/78033458/using-orbax-to-checkpoint-flax-trainstate-with-new-checkpointmanager-api"
    },
    {
        "title": "How to return last index with jnp.where in jit function",
        "question": "Say I have two arrays:\npython\nCopy\nz = jnp.array([[5.55751118],\n              [5.18212974],\n              [4.35981727],\n              [3.4559711 ],\n              [3.35750248],\n              [2.65199945],\n              [2.02298999],\n              [1.59444971],\n              [0.80865185],\n              [0.77579791]])\n\nz1 = jnp.array([[ 1.58559484],\n               [ 3.79094097],\n               [-0.52712522],\n               [-1.0178286 ],\n               [-3.51076985],\n               [ 1.30108161],\n               [-1.29824303],\n               [-0.19209007],\n               [ 0.37451138],\n               [-2.33619987]])\nI would like to start at the first row in array z and find where in the second matrix a second value is within a threshold of this value.\nexample without @jit: I would like to return the last index of array z1. Value should be -3.51x\npython\nCopy\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nWhen using JIT (as needed in a larger scale model)\npython\nCopy\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nthis error is produced:\npython\nCopy\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThe error occurred while tracing the function test at /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:9 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:bool[10,1] = ge b c\n    from line /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:11:10 (test)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
        "answers": [
            "The problem is that jnp.where returns a dynamically-sized array, and JAX transformations like jit are not compatible with dynamically-sized arrays (See JAX Sharp Bits: Dynamic Shapes). You can pass a size argument to jnp.where to make the result statically sized. Since we don't know how many elements will be returned, we can choose the maximum possible number of returned elements, which is idx.shape[0]. Since the result will be padded with zeros, the maximum index will give what you're looking for:\npython\nCopy\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx, size=idx.shape[0])[0].max()]\n    return val\ntest()"
        ],
        "link": "https://stackoverflow.com/questions/78030406/how-to-return-last-index-with-jnp-where-in-jit-function"
    },
    {
        "title": "Does jax save the jaxpr of jit compiled functions?",
        "question": "Consider the following example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n    \nprint(test(jnp.ones(8,)))\nprint(test(jnp.ones(3,)))\nThe output is\npython\nCopy\n1\n-1\nHowever, I thought that on the first call jax compiles a function to use in subsequent calls. Shouldn't this then give the output 1 and 1, because jax traces through an if and does not use a conditional here? In the jaxpr of the first call is no conditional:\npython\nCopy\n{ lambda ; a:f32[8]. let\n    b:i32[] = pjit[name=test jaxpr={ lambda ; c:f32[8]. let  in (1,) }] a\n  in (b,) }\nSo how exactly does this work under the hood. Is the jaxpr unique for every call. Does jax only reuse jaxprs if the shape matches? Does jax recompile functions if the shape is different?",
        "answers": [
            "JAX does cache the jaxpr and compiled artifact for each compatible call of the function. This compatibility is determined via the cache key, which contains the shape and dtype of array arguments, as well as the hash of any static arguments and some additional information such as global flags that may affect the computation. Any time something in the cache key changes, it results in a new tracing & compilation of the function. You can see this by printing the _cache_size() of the compiled function. For example:\npython\nCopy\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n\nx8 = jnp.ones(8)\nx3 = jnp.ones(3)\n\nprint(test._cache_size())  # no calls yet, so no cache\n# 0\n\ntest(x8)\nprint(test._cache_size())  # first call caches the jaxpr\n# 1\n\ntest(x8)\nprint(test._cache_size())  # repeated call, so size doesn't change\n# 1\n\ntest(x3)\nprint(test._cache_size())  # new call, so size increases\n# 2\n\ntest(x8)\nprint(test._cache_size())  # repeated call -> size doesn't change\n# 2\nBy keeping track of these static attributes, jit-compiled functions can change their output based on static attributes, but still avoid recompilation for compatible inputs."
        ],
        "link": "https://stackoverflow.com/questions/78011718/does-jax-save-the-jaxpr-of-jit-compiled-functions"
    },
    {
        "title": "Jax Implementation of function similar to Torch's 'Scatter'",
        "question": "For graph learning purposes, I am trying to implement a global sum batching function, that takes as inputs batched graph representations 'x' of size (n x d) and a corresponding vector of batches (n x 1). I then want to compute the sum over all graph representations for each batch. Here is a graphical representation: torch's scatter function\nThis is my current attempt:\npython\nCopy\ndef global_sum_pool(x, batch):\n    graph_reps = []\n    i = 0\n    n = jnp.max(batch)\n    while True:\n        ind = jnp.where(batch == i, True, False).reshape(-1, 1)\n        ind = jnp.tile(ind, x.shape[1])\n        x_ind = jnp.where(ind == True, x, 0.0)\n        graph_reps.append(jnp.sum(x_ind, axis=0))\n        if i == n:\n            break\n        i += 1\n    return jnp.array(graph_reps)\nI get the following exception on the line if i == n:\npython\nCopy\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function make_step at /venvs/jax_env/lib/python3.11/site-packages/equinox/_jit.py:37 for jit. \nI understand this is due to the fact that at compile time, Jax does not a priori know the max value of the 'batch' array and hence cannot allocate memory. Does anyone know a workaround or different implementation?",
        "answers": [
            "Rather than implementing this via a for loop, you should use JAX's built-in scatter operator. The most convenient interface for this is the Array.at syntax. If I understand your goal correctly, it might look something like this:\npython\nCopy\nimport jax.numpy as jnp\nimport numpy as np\n\n# Generate some data\nnum_batches = 4\nn = 10\nd = 3\nx = np.random.randn(n, d)\nind = np.random.randint(low=0, high=num_batches, size=(n,))\n\n#Compute the result with jax.lax.scatter\nresult = jnp.zeros((num_batches, d)).at[ind].add(x)\nprint(result.shape)\n# (4, 3)"
        ],
        "link": "https://stackoverflow.com/questions/77932146/jax-implementation-of-function-similar-to-torchs-scatter"
    },
    {
        "title": "JAX `vjp` does not recognize cotangent argument with `custom_vjp`",
        "question": "I have a JAX function cart_deriv() which takes another function f and returns the Cartesian derivative of f, implemented as follows:\npython\nCopy\n@partial(custom_vjp, nondiff_argnums=0)\ndef cart_deriv(f: Callable[..., float],\n               l: int,\n               R: Array\n               ) -> Array:\n\n    df = lambda R: f(l, jnp.dot(R, R))\n\n    for i in range(l):\n        df = jacrev(df)\n\n    return df(R)\n\n\ndef cart_deriv_fwd(f, l, primal):\n\n    primal_out = cart_deriv(f, l, primal)\n    residual = cart_deriv(f, l+1, primal)  ## just a test\n\n    return primal_out, residual\n\n\ndef cart_deriv_bwd(f, residual, cotangent):\n\n    cotangent_out = jnp.ones(3)  ## just a test\n\n    return (None, cotangent_out)\n\n\ncart_deriv.defvjp(cart_deriv_fwd, cart_deriv_bwd)\n\n\n\nif __name__ == \"__main__\":\n\n    def test_func(l, r2):\n        return l + r2\n\n    primal_out, f_vjp = vjp(cart_deriv, \n                            jax.tree_util.Partial(test_func),\n                            2,\n                            jnp.array([1., 2., 3.])\n                            )\n\n    cotangent = jnp.ones((3, 3))\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[1].shape)\nHowever this code produces the error:\npython\nCopy\nTypeError: cart_deriv_bwd() missing 1 required positional argument: 'cotangent'\nI have checked that the syntax agrees with that in the documentation. I'm wondering why the argument cotangent is not recognized by vjp, and how to fix this error?",
        "answers": [
            "The issue is that nondiff_argnums is expected to be a sequence:\npython\nCopy\n@partial(custom_vjp, nondiff_argnums=(0,))\nWith this properly defined, it's better to avoid wrapping the function in Partial, and just pass it as a static argument by closing over it in the vjp call:\npython\nCopy\nprimal_out, f_vjp = vjp(partial(cart_deriv, test_func),\n                        2,\n                        jnp.array([1., 2., 3.])\n                        )\ncotangent_out = f_vjp(jnp.ones((3, 3)))\n\nprint(*cotangent_out)\n# (b'',) [1. 1. 1.]"
        ],
        "link": "https://stackoverflow.com/questions/77924142/jax-vjp-does-not-recognize-cotangent-argument-with-custom-vjp"
    },
    {
        "title": "Vectorizing power of `jax.grad`",
        "question": "I'm trying to vectorize the following \"power-of-grad\" function so that it accepts multiple orders: (see here)\npython\nCopy\ndef grad_pow(f, order, argnum):\n\n    for i in jnp.arange(order):\n        f = grad(f, argnums=argnum)\n\n    return f\nThis function produces the following error after applying vmap on the argument order:\npython\nCopy\njax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nIt arose in the jnp.arange argument 'stop'\nI have tried writing a static version of grad_pow using jax.lax.cond and jax.lax.scan, following the logic here:\npython\nCopy\ndef static_grad_pow(f, order, argnum):\n\n    order_max = 3  ## maximum order\n\n    def grad_pow(f, i):\n        return cond(i <= order, grad(f, argnum), f), None\n\n    return scan(grad_pow, f, jnp.arange(order_max+1))[0]\n\n\nif __name__ == \"__main__\":\n\n    test_func = lambda x: jnp.exp(-2*x)\n    test_func_grad_pow = static_grad_pow(jax.tree_util.Partial(test_func), 1, 0)\n    print(test_func_grad_pow(1.))\nNevertheless, this solution still produces an error:\npython\nCopy\n    return cond(i <= order, grad(f, argnum), f), None\nTypeError: differentiating with respect to argnums=0 requires at least 1 positional arguments to be passed by the caller, but got only 0 positional arguments.\nJust wondering how this issue can be resolved?",
        "answers": [
            "The fundamental issue with your question is that a vmapped function cannot return a function, it can only return arrays. All other details aside, that precludes any possibility of writing a valid function that does what you intend.\nThere are alternatives: for example, rather than attempting to create a function that will return a function, you could instead create a function that accepts arguments and applies that function to those arguments.\nIn that case, you'll run into another issue: if n is traced, there is no way to apply grad n times. JAX transformations like grad are evaluated at trace-time, and traced values like n are not available until runtime. One way to work around this is to pre-define all the functions you're interested in, and to use lax.switch to choose between them at runtime. The result would look something like this:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=[0], static_argnames=['argnum', 'max_order'])\ndef apply_multi_grad(f, order, *args, argnum=0, max_order=10):\n  funcs = [f]\n  for i in range(max_order):\n    funcs.append(jax.grad(funcs[-1], argnum))\n  return jax.lax.switch(order, funcs, *args)\n\n\norder = jnp.arange(3)\nx = jnp.ones(3)\nf = jnp.sin\n\nprint(jax.vmap(apply_multi_grad, in_axes=(None, 0, 0))(f, order, x))\n# [ 0.84147096  0.5403023  -0.84147096]\n\n# Compare by doing it manually:\nprint(jnp.array([f(x[0]), jax.grad(f)(x[1]), jax.grad(jax.grad(f))(x[2])]))\n# [ 0.84147096  0.5403023  -0.84147096]"
        ],
        "link": "https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad"
    },
    {
        "title": "JAX `grad` error for function with `jax.lax.switch` and compound boolean conditions",
        "question": "I have encountered a scenario where applying jax.grad to a function with jax.lax.switch and compound boolean conditions yields jax.errors.TracerBoolConversionError. A minimal program to reproduce this behavior is the following:\npython\nCopy\nfrom jax.lax import switch\nimport jax.numpy as jnp\nfrom jax import grad\n\nfunc_0 = lambda x: jnp.where(0. < x < 1., x, 0.)\nfunc_1 = lambda x: jnp.where(0. < x < 1., x, 1.)\n\nfunc_list = [func_0, func_1]\n\nfunc = lambda index, x: switch(index, func_list, x)\n\ndf = grad(func, argnums=1)(1, 2.)\nprint(df)\nThe error is the following:\npython\nCopy\nTraceback (most recent call last):\n  File \"***/grad_test.py\", line 12, in <module>\n    df = grad(func, argnums=1)(1, 0.5)\n  File \"***/grad_test.py\", line 10, in <lambda>\n    func = lambda index, x: switch(index, func_list, x)\n  File \"***/grad_test.py\", line 5, in <lambda>\n    func_0 = lambda x: jnp.where(0 < x < 1., x, 0.)\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function <lambda> at ***/grad_test.py:5 for switch. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\nHowever, if the boolean condition is changed to a single condition (for example, x < 1), then no error occurs. I'm wondering if this could be a bug, or otherwise, how the original program should be changed.",
        "answers": [
            "You cannot use chained inequalities with JAX or NumPy arrays. Instead of 0 < x < 1, you should write (0 < x) & (x < 1) (note that due to operator precedence, the parentheses are not optional here)."
        ],
        "link": "https://stackoverflow.com/questions/77900299/jax-grad-error-for-function-with-jax-lax-switch-and-compound-boolean-conditi"
    },
    {
        "title": "Unexpected behavior of JAX `vmap` for multiple arguments",
        "question": "I have found that vmap in JAX does not behave as expected when applied to multiple arguments. For example, consider the function below:\npython\nCopy\ndef f1(x, y, z):\n    f = x[:, None, None] * z[None, None, :] + y[None, :, None]\n    return f\nFor x = jnp.arange(7), y = jnp.arange(5), z = jnp.arange(3), the output of this function has shape (7, 5, 3). However, for the vmap version below:\npython\nCopy\n@partial(vmap, in_axes=(None, 0, 0), out_axes=(1, 2))\ndef f2(x, y, z):\n    f = x*z + y\n    return f\nIt outputs this error:\npython\nCopy\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 5: axis 0 of argument y of type int32[5];\n  * one axis had size 3: axis 0 of argument z of type int32[3]\nCould someone kindly explain what's behind this error?",
        "answers": [
            "The semantics of vmap are that it does a single batching operation along one or more arrays. When you specify in_axes=(None, 0, 0), the meaning is \"map simultaneously along the leading dimension of y and z\": the error you're seeing is telling you that the leading dimensions of y and z have different sizes, and so they are not compatible for batching.\nYour function f1 essentially uses broadcasting to encode three batching operations, so to replicate that logic with vmap you'll need three applications of vmap. You can express that as follows:\npython\nCopy\n@partial(vmap, in_axes=(0, None, None))\n@partial(vmap, in_axes=(None, 0, None))\n@partial(vmap, in_axes=(None, None, 0))\ndef f2(x, y, z):\n    f = x*z + y\n    return f"
        ],
        "link": "https://stackoverflow.com/questions/77886057/unexpected-behavior-of-jax-vmap-for-multiple-arguments"
    },
    {
        "title": "Why do I get different values from jnp.round and np.round?",
        "question": "I'm writing tests for some jax code and using np.testing.assert_array...-type functions and came across this difference in values that I didn't expect:\npython\nCopy\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\na = jnp.array([-0.78073686, -0.7908204 ,  2.174842])\nb = np.array(a, dtype='float32')\nassert_array_equal(a, b)\n\nprint(a.round(2), a.dtype)\nprint(b.round(2), b.dtype)\nOutput:\npython\nCopy\n[-0.78       -0.78999996  2.1699998 ] float32\n[-0.78 -0.79  2.17] float32\nTest:\npython\nCopy\nassert_array_equal(a.round(2), b.round(2))\nOutput:\nAssertionError: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 2.3841858e-07\nMax relative difference: 1.0987031e-07\n x: array([-0.78, -0.79,  2.17], dtype=float32)\n y: array([-0.78, -0.79,  2.17], dtype=float32)\nFootnote:\nI get exactly the same results if I define b as follows, so it's not a problem with the conversion of the array from jax to numpy:\npython\nCopy\nb = np.array([-0.78073686, -0.7908204 ,  2.174842], dtype='float32')",
        "answers": [
            "This is an example of a general property of floating point computations: two different ways of expressing the same computation will not always produce bitwise-equivalent outputs (see e.g. Is floating point math broken?).\nJAX and NumPy use identical implementations for x.round(2); essentially it is round_to_int(x * 100) / 100 (compare the JAX implementation and the NumPy implementation).\nThe difference is that JAX jit-compiles jnp.round by default. When you disable compilation and perform these operations in sequence, the results are identical:\npython\nCopy\nimport jax\nwith jax.disable_jit():\n  assert_array_equal(a.round(2), b.round(2))  # passes!\nBut JAX's JIT optimizes the implementation by fusing some operations – this leads to faster computation but in general you should not expect the result to be bitwise-equivalent to the unoptimized version.\nTo address this, whenever you are comparing floating point values, you should avoid exact equality checks in favor of checks that take this floating point roundoff error into account. For example:\npython\nCopy\nnp.testing.assert_allclose(a.round(2), b.round(2), rtol=1E-6)  # passes!"
        ],
        "link": "https://stackoverflow.com/questions/77868226/why-do-i-get-different-values-from-jnp-round-and-np-round"
    },
    {
        "title": "Custom JVP and VJP for higher order functions in JAX",
        "question": "I find custom automatic differentiation capabilities (JVP, VJP) very useful in JAX, but am having a hard time applying it to higher order functions. A minimal example of this sort is as follows: given a higher order function:\npython\nCopy\ndef parent_func(x):\n    def child_func(y):\n        return x**2 * y\n    return child_func\nI would like to define custom gradients of child_func with respect to x and y. What would be the correct syntax to achieve this?",
        "answers": [
            "Gradients in JAX are defined with respect to a function’s explicit inputs. Your child_func does not take x as an explicit input, so you cannot directly differentiate child_func with respect to x. However, you could do so indirectly by calling it from another function that takes x. For example:\npython\nCopy\ndef func_to_differentiate(x, y):\n  child_func = parent_func(x)\n  return child_func(y)\n\njax.grad(func_to_differentiate, argnums=0)(1.0, 1.0)  # 2.0\nThen if you wish, you could define standard custom derivative rules for func_to_differentiate."
        ],
        "link": "https://stackoverflow.com/questions/77859418/custom-jvp-and-vjp-for-higher-order-functions-in-jax"
    },
    {
        "title": "Occurence of NaN in softmax & JIT issues",
        "question": "I'm trying to implement the Transformer architecture from scratch using Jax. I find three issues while training:\njax.disable_jit() does not remove implicit jit compilations.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\npython\nCopy\nclass SelfAttention(eqx.Module):\n    def __call__(self, query, key, value, mask):\n        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n        scaled_dot_prod = mask + scaled_dot_prod\n        return (jax.nn.softmax(scaled_dot_prod) @ value)\n\ndef create_mask(arr):\n    return jnp.where(arr == 0, np.NINF, 0)\n\ndef loss(model, X, y, X_mask, y_mask, labels):\n    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n    y_pred = jnp.where(labels==0, 0, jnp.take(y_pred, labels, axis=-1))\n    count = jnp.count_nonzero(y_pred)\n    return -jnp.sum(y_pred)/count\n\nwith jax.disable_jit():\n    for e in range(EPOCHS):\n        total_loss = 0\n        num_batches = 0\n        total_tokens = 0\n        for i, (Xbt, ybt, labelbt) in enumerate(dataloader(Xtr, ytr, SEQ_LEN)):\n            total_tokens += len([token for seq in labelbt for token in list(filter(lambda x: x!=0, seq))])\n            Xbt, ybt, labelbt = [jnp.array(x) for x in (Xbt, ybt, labelbt)]\n            Xmask, ymask = [create_mask(x) for x in (Xbt, ybt)]\n\n            model, opt_state, batch_loss = step(model, opt_state, Xbt, ybt, Xmask, ymask, labelbt)\n            total_loss += batch_loss\n            num_batches += 1\n\n            if num_batches % 20 == 0:\n                print(f\"Batches trained: {num_batches} | Avg. Batch loss: {total_loss/num_batches}\")\n\n        epoch_loss = total_loss / num_batches\n        print(f\"Epoch {e} | loss: {epoch_loss}\")\nError:\npython\nCopy\ndef _softmax_deprecated(\n    478     x: ArrayLike,\n    479     axis: Optional[Union[int, tuple[int, ...]]] = -1,\n    480     where: Optional[ArrayLike] = None,\n    481     initial: Optional[ArrayLike] = None) -> Array:\n    482   x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n--> 483   unnormalized = jnp.exp(x - lax.stop_gradient(x_max))\n    484   result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)\n    485   if where is not None:\n\nFloatingPointError: invalid value (nan) encountered in jit(sub)\nThe above problem is encountered only after crossing 200 batches of training. I haven't checked that skipping the specific batch where the error occurs. Maybe I sohuld inspect if some specific inputs are the resason to this error.\nBut I can't find the answer to the above 3 questions :(",
        "answers": [
            "To answer your questions:\njax.disable_jit() does not remove implicit jit compilations.\nIf this is true, it is a bug and you should report it on the JAX issue tracker. It's unclear from your question what makes you believe this is the case.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nBecause _softmax_deprecated is the old default algorithm, that will someday be deprecated but the deprecation has not happened yet. See https://github.com/google/jax/pull/15677 for details. To use the newer algorithm, you can set the jax_softmax_custom_jvp=True configuration.\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nYou didn't include enough code to reproduce your issue (next time, try to add a minimal reproducible example to allow others to answer your question without guesswork). But it would be worth setting jax_softmax_custom_jvp=True to see if that addresses your issue. The pull request linked above has details."
        ],
        "link": "https://stackoverflow.com/questions/77677455/occurence-of-nan-in-softmax-jit-issues"
    },
    {
        "title": "Why does installing JAX with Docker create such a large image?",
        "question": "I am trying to pip install JAX using Docker and I found that using it just blows up the size of Docker image. The size of image currently is 4.82 GB.\nI made sure to bypass caching while installing packages by doing --no-cache-dir. While that did reduce the size, the size is still unreasonable huge.\nHere is my Dockerfile -\nFROM ubuntu:22.04\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    libosmesa6-dev \\\n    sudo \\\n    wget \\\n    curl \\\n    unzip \\\n    gcc \\\n    g++\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\nRUN mkdir -p ~/miniconda3\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nRUN bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nRUN rm -rf ~/miniconda3/miniconda.sh\nRUN ~/miniconda3/bin/conda init bash\nRUN conda init\n\nRUN pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nThis is how I built it -\ndocker build -t tbd_jax .\nWhen I do docker images, I get this -\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\ntbd_jax      latest    812292e2264e   7 minutes ago    4.82GB\nAfter doing docker history --no-trunc tbd_jax:latest -\nSIZE      COMMENT\nsha256:812292e2264e4340b7715956824055d7409f9546f8dfa54ccad1da056febf300   8 minutes ago    RUN |1 PATH=/root/miniconda3/bin:/root/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin /bin/sh -c pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # buildkit   3.54GB    buildkit.dockerfile.v0\nIs there something I can do to reduce the size? I am a bit of a Docker and Linux newbie so pardon my slowness.",
        "answers": [
            "Note that jax[cuda12_pip] installs all the cuda drivers listed here:\npython\nCopy\n'cuda12_pip': [\n  ...\n  \"nvidia-cublas-cu12>=12.2.5.6\",\n  \"nvidia-cuda-cupti-cu12>=12.2.142\",\n  \"nvidia-cuda-nvcc-cu12>=12.2.140\",\n  \"nvidia-cuda-runtime-cu12>=12.2.140\",\n  \"nvidia-cudnn-cu12>=8.9\",\n  \"nvidia-cufft-cu12>=11.0.8.103\",\n  \"nvidia-cusolver-cu12>=11.5.2\",\n  \"nvidia-cusparse-cu12>=12.1.2.141\",\n  \"nvidia-nccl-cu12>=2.18.3\",\nThese nvidia driver packages are quite large: for example the nvidia_cublas_cu12 wheel is over 400MB, and nvidia-cudnn-cu12 is over 700MB. You may be able to do better by setting up your docker image with system-native CUDA & CUDNN drivers, installed via apt. You can find a description of the requirements here. You can also use NVIDIA's pre-defined GPU containers, as mentioned here."
        ],
        "link": "https://stackoverflow.com/questions/77669055/why-does-installing-jax-with-docker-create-such-a-large-image"
    },
    {
        "title": "Parallelize inference of ensemble",
        "question": "I used the this tutorial from JAX to create an ensemble of networks. Currently I compute the loss of each network in a for-loop which I would like to avoid:\npython\nCopy\nfor params in ensemble_params:\n    loss = mse_loss(params, inputs=x, targets=y)\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\nHere ensemble_params is a list of pytrees (lists of tuples holding JAX parameter arrays). The parameter structure of each network is the same.\nI tried to get rid of the for-loop by applying jax.vmap:\npython\nCopy\nensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\nHowever, I keep getting the following error message which I do not understand.\npython\nCopy\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (8 of them) had size 3, e.g. axis 0 of argument params[0][0][0] of type float32[3,2];\n  * some axes (8 of them) had size 4, e.g. axis 0 of argument params[0][1][0] of type float32[4,3]\nHere is a minimal reproducible example:\npython\nCopy\nimport jax\nfrom jax import Array\nfrom jax import random\nimport jax.numpy as jnp\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits\n\nbatched_predict = jax.vmap(predict, in_axes=(None, 0))\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\n\nif __name__ == \"__main__\":\n\n    num_models = 4\n    dim_in = 2\n    dim_out = 4\n    layer_dims = [dim_in, 3, dim_out]\n    batch_size = 2\n\n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble_params = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    key_x, key_y = random.split(key)\n    x = random.normal(key=key_x, shape=(batch_size, dim_in))\n    y = random.normal(key=key_y, shape=(batch_size, dim_out))\n\n    for params in ensemble_params:\n        loss = mse_loss(params, inputs=x, targets=y)\n        print(f\"{loss = }\")\n\n    ensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\n    losses = ensemble_loss(ensemble_params, x, y)\n    print(f\"{losses = }\")  # Same losses expected as above.",
        "answers": [
            "The main issue here is that vmap maps over arrays, not over lists.\nYou are passing a list of parameter objects, expecting vmap to map over the elements of that list. However, the semantics of vmap are that it maps over the first axis of each tree leaf in the argument, and the leaves in your argument differ in their leading axis.\nTo fix this, instead of passing a list of parameter objects containing unbatched arrays, you need to pass a single parameter object containing batched arrays; in other words you need a struct-of-arrays pattern rather than a list-of-structs pattern.\nIn your case, you can create your batched ensemble parameters this way:\npython\nCopy\nensemble_params = jax.tree_map(lambda *args: jnp.stack(args), *ensemble_params)\nIf you pass this to the ensemble_loss function, you get the expected output:\npython\nCopy\nlosses = Array([3.762451 , 4.39846  , 4.1425314, 6.045669 ], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77581033/parallelize-inference-of-ensemble"
    },
    {
        "title": "Reduce list of lists in JAX",
        "question": "I have a list holding many lists of the same structure (Usually, there are much more than two sub-lists inside the list, the example shows two lists for the sake of simplicity). I would like to create the sum or product over all sub-lists so that the resulting list has the same structure as one of the sub-lists. So far I tried the following using the tree_reduce method but I get errors that I don't understand.\nI could need some guidance on how to use tree_reduce() in such a case.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nlist_2 = [\n    [jnp.asarray([7]), jnp.asarray([8, 9])],\n    [jnp.asarray([10]), jnp.asarray([11, 12])],\n]\n    \nlist_of_lists = [list_1, list_2]\n   \nreduced = jax.tree_util.tree_reduce(lambda x, y: x + y, list_of_lists, 0, is_leaf=True)\n    \n# Expected\n# reduced = [\n#     [jnp.asarray([8]), jnp.asarray([10, 12])],\n#     [jnp.asarray([14]), jnp.asarray([16, 18])],\n# ]",
        "answers": [
            "You can do this with tree_map of a sum over the splatted list:\npython\nCopy\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *list_of_lists)\nprint(reduced)\npython\nCopy\n[[Array([8], dtype=int32), Array([10, 12], dtype=int32)],\n [Array([14], dtype=int32), Array([16, 18], dtype=int32)]]"
        ],
        "link": "https://stackoverflow.com/questions/77548225/reduce-list-of-lists-in-jax"
    },
    {
        "title": "Jax vmap limit memory",
        "question": "I'm wondering if there is a good way to limit the memory usage for Jax's VMAP function? Equivalently, to vmap in batches at a time if that makes sense?\nIn my specific use case, I have a set of images and I'd like to calculate the affinity between each pair of images; so ~order((num_imgs)^2 * (img shape)) bytes of memory used all at once if I'm understanding vmap correctly (which gets huge since in my real example I have 10,000 100x100 images).\nA basic example is:\npython\nCopy\ndef affininty_matrix_ex(n_arrays=10, img_size=5, key=jax.random.PRNGKey(0), gamma=jnp.array([0.5])):\n    arr_of_imgs = jax.random.normal(jax.random.PRNGKey(0), (n_arrays, img_size, img_size))\n    arr_of_indices = jnp.arange(n_arrays)\n    inds_1, inds_2 = zip(*combinations(arr_of_indices, 2))\n    v_cPA = jax.vmap(calcPairAffinity2, (0, 0, None, None), 0)\n    affinities = v_cPA(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma)\n    print()\n    print(jax.make_jaxpr(v_cPA)(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma))\n    \n    affinities = affinities.reshape(-1)\n    \n    arr = jnp.zeros((n_arrays, n_arrays), dtype=jnp.float16)\n    arr = arr.at[jnp.triu_indices(arr.shape[0], k=1)].set(affinities)\n    arr = arr + arr.T\n    arr = arr + jnp.identity(n_arrays, dtype=jnp.float16)\n    \n    return arr\n\n\ndef calcPairAffinity2(ind1, ind2, imgs, gamma):\n    #Returns a jnp array of 1 float, jnp.sum adds all elements together\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\nI suppose I could just say something like \"only feed into vmap X pairs at a time, and loop through n_chunks = n_arrays/X, appending each groups results to a list\" but that doesn't seem to be ideal. My understanding is vmap does not like generators, not sure if that would be an alternative way around the issue.",
        "answers": [
            "Edit, Aug 13 2024\nAs of JAX version 0.4.31, what you're asking for is possible using the batch_size argument of lax.map. For an iterable of size N, this will perform a scan with N // batch_size steps, and within each step will vmap the function over the batch. lax.map has less flexible semantics than jax.vmap, but for the simplest cases they look relatively similar. Here's an example using your calcPairAffinity function:\nFor example\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef calcPairAffinity(ind1, ind2, imgs, gamma=0.5):\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\n\nimgs = jax.random.normal(jax.random.key(0), (100, 5, 5))\ninds = jnp.arange(imgs.shape[0])\ninds1, inds2 = map(jnp.ravel, jnp.meshgrid(inds, inds))\n\ndef f(inds):\n  return calcPairAffinity(*inds, imgs, 0.5)\n\n\nresult_vmap = jax.vmap(f)((inds1, inds2))\nresult_batched = jax.lax.map(f, (inds1, inds2), batch_size=1000)\nassert jnp.allclose(result_vmap, result_batched)\nOriginal answer\nThis is a frequent request, but unfortunately there's not yet (as of JAX version 0.4.20) any built-in utility to do chunked/batched vmap (xmap does have some functionality along these lines, but is experimental/incomplete and I wouldn't recommend relying on it).\nAdding chunking to vmap is tracked in https://github.com/google/jax/issues/11319, and there's some code there that does a limited version of what you have in mind. Hopefully something like what you describe will be possible with JAX's built-in vmap soon. In the meantime, you might think about applying vmap to chunks manually in the way you describe in your question."
        ],
        "link": "https://stackoverflow.com/questions/77527847/jax-vmap-limit-memory"
    },
    {
        "title": "How to implement nested for loops with branches efficiently in JAX",
        "question": "I am wanting to reimplement a function in jax that loops over a 2d array and modifies the output array at an index that is not necessarily the same as the current iterating index based on conditions. Currently I am implementing this via repeated use of jnp.where for the conditions separately, but the function is ~4x slower than the numba implementation on cpu, on gpu it is ~10x faster - which I suspect is due to the fact that I am iterating over the whole array again for every condition.\nThe numba implementation is as follows:\npython\nCopy\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numba as nb\n\nrng = np.random.default_rng()\n\n\n@nb.njit\ndef raytrace_np(ir, dx, dy):\n    assert ir.ndim == 2\n    n, m = ir.shape\n    assert ir.shape == dx.shape == dy.shape\n    output = np.zeros_like(ir)\n\n    for i in range(ir.shape[0]):\n        for j in range(ir.shape[1]):\n            dx_ij = dx[i, j]\n            dy_ij = dy[i, j]\n            \n            dxf_ij = np.floor(dx_ij)\n            dyf_ij = np.floor(dy_ij)\n\n            ir_ij = ir[i, j]\n            index0 = i + int(dyf_ij)\n            index1 = j + int(dxf_ij)\n\n            if 0 <= index0 <= n - 1 and 0 <= index1 <= m - 1:\n                output[index0, index1] += (\n                    ir_ij * (1 - (dx_ij - dxf_ij)) * (1 - (dy_ij - dyf_ij))\n                )\n            if 0 <= index0 <= n - 1 and 0 <= index1 + 1 <= m - 1:\n                output[index0, index1 + 1] += (\n                    ir_ij * (dx_ij - dxf_ij) * (1 - (dy_ij - dyf_ij))\n                )\n            if 0 <= index0 + 1 <= n - 1 and 0 <= index1 <= m - 1:\n                output[index0 + 1, index1] += (\n                    ir_ij * (1 - (dx_ij - dxf_ij)) * (dy_ij - dyf_ij)\n                )\n            if 0 <= index0 + 1 <= n - 1 and 0 <= index1 + 1 <= m - 1:\n                output[index0 + 1, index1 + 1] += (\n                    ir_ij * (dx_ij - dxf_ij) * (dy_ij - dyf_ij)\n                )\n    return output\nand my current jax reimplementation is:\npython\nCopy\n@jax.jit\ndef raytrace_jax(ir, dx, dy):\n    assert ir.ndim == 2\n    n, m = ir.shape\n    assert ir.shape == dx.shape == dy.shape\n\n    output = jnp.zeros_like(ir)\n\n    dxfloor = jnp.floor(dx)\n    dyfloor = jnp.floor(dy)\n    \n    dxfloor_int = dxfloor.astype(jnp.int64)\n    dyfloor_int = dyfloor.astype(jnp.int64)\n    \n    meshyfloor = dyfloor_int + jnp.arange(n)[:, None]\n    meshxfloor = dxfloor_int + jnp.arange(m)[None]\n\n    validx = (meshxfloor >= 0) & (meshxfloor <= m - 1)\n    validy = (meshyfloor >= 0) & (meshyfloor <= n - 1)\n    validx2 = (meshxfloor + 1 >= 0) & (meshxfloor + 1 <= m - 1)\n    validy2 = (meshyfloor + 1 >= 0) & (meshyfloor + 1 <= n - 1)\n\n    validxy = validx & validy\n    validx2y = validx2 & validy\n    validxy2 = validx & validy2\n    validx2y2 = validx2 & validy2\n    \n    dx_dxfloor = dx - dxfloor\n    dy_dyfloor = dy - dyfloor\n\n    output = output.at[\n        jnp.where(validxy, meshyfloor, 0), jnp.where(validxy, meshxfloor, 0)\n    ].add(\n        jnp.where(validxy, ir * (1 - dx_dxfloor) * (1 - dy_dyfloor), 0)\n    )\n    output = output.at[\n        jnp.where(validx2y, meshyfloor, 0),\n        jnp.where(validx2y, meshxfloor + 1, 0),\n    ].add(jnp.where(validx2y, ir * dx_dxfloor * (1 - dy_dyfloor), 0))\n    output = output.at[\n        jnp.where(validxy2, meshyfloor + 1, 0),\n        jnp.where(validxy2, meshxfloor, 0),\n    ].add(jnp.where(validxy2, ir * (1 - dx_dxfloor) * dy_dyfloor, 0))\n    output = output.at[\n        jnp.where(validx2y2, meshyfloor + 1, 0),\n        jnp.where(validx2y2, meshxfloor + 1, 0),\n    ].add(jnp.where(validx2y2, ir * dx_dxfloor * dy_dyfloor, 0))\n    return output\nTest and timings:\npython\nCopy\nshape = 2000, 2000\nir = rng.random(shape)\ndx = (rng.random(shape) - 0.5) * 5\ndy = (rng.random(shape) - 0.5) * 5\n\n_raytrace_np = raytrace_np(ir, dx, dy)\n_raytrace_jax = raytrace_jax(ir, dx, dy).block_until_ready()\n\nassert np.allclose(_raytrace_np, _raytrace_jax)\n\n%timeit raytrace_np(ir, dx, dy)\n%timeit raytrace_jax(ir, dx, dy).block_until_ready()\nOutput:\npython\nCopy\n14.3 ms ± 84.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n62.9 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nSo is there a way to implement this algorithm in jax with performance more comparable to the numba implementation?",
        "answers": [
            "The way you implemented it in JAX is pretty close to what I'd recommend. Yes, it's 3x slower than a custom Numba implementation on CPU, but I think for an operation like this, that is to be expected.\nThe operation you defined applies specific logic to each individual entry of the array – that is precisely the computational regime that Numba is designed for, and precisely the kind of computation that CPUs were designed for: it's not surprising that with Numba on CPU your computation is very fast.\nI suspect the reason you used Numba rather than NumPy here is that NumPy is not designed for this sort of algorithm: it is an array-oriented language, not an array-element-oriented language. JAX/XLA is more similar to NumPy than to Numba: it is an array-oriented language; it encodes operations across whole arrays at once, rather than choosing a different computation per-element.\nThe benefit of this array-oriented computing model becomes really apparent when you move away from CPU and run the code on an accelerator like a GPU or TPU: this hardware is specifically designed for vectorized array operations, which is why you found that the same, array-oriented code was 10x faster on GPU."
        ],
        "link": "https://stackoverflow.com/questions/77459386/how-to-implement-nested-for-loops-with-branches-efficiently-in-jax"
    },
    {
        "title": "JAX @jit for nested class method",
        "question": "I am trying to use @jit with nested function, having a problem. I have a class One that take in another class Plant with a method func. I would like to call this method jitted func from One. I think that I followed the FAQ of JAX, \"How to use jit with methods?\" section. https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods However, I encountered an error saying that TypeError: One.__init__() got multiple values for argument 'plant'. Would anyone tell me how to solve this?\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np\nfrom functools import partial\nfrom jax import tree_util\n\nclass One:\n    def __init__(self, plant,x):\n        self.plant = plant\n        self.x = x\n    \n    @jit\n    def call_plant_func(self,y):\n        out = self.plant.func(y) + self.x\n        return out\n    \n    def _tree_flatten(self):\n        children = (self.x,)  # arrays / dynamic values\n        aux_data = {'plant':self.plant}  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        import pdb; pdb.set_trace();\n        return cls(*children, **aux_data)\n        \ntree_util.register_pytree_node(One,\n                               One._tree_flatten,\n                               One._tree_unflatten)    \n    \nclass Plant:\n    def __init__(self, z,kk):\n        self.z =z\n    \n    @jit\n    def func(self,y):\n        y = y + self.z\n        return y\n    \n    def _tree_flatten(self):\n        children = (self.z,)  # arrays / dynamic values\n        aux_data = None # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, children):\n        return cls(*children)\n   \ntree_util.register_pytree_node(Plant,\n                               Plant._tree_flatten,\n                               Plant._tree_unflatten)\n\nplant = Plant(5,2)\none = One(plant,2)\nprint(one.call_plant_func(10))\nThe last line gives me an error described above.",
        "answers": [
            "You have issues in the tree_flatten and tree_unflatten code in both classes.\nOne._tree_flatten treats plant as static data, but it is not: it is a pytree that has non-static elements.\nOne._tree_unflatten instantiates One with arguments in the wrong order, leading to the error you're seeing\nPlant.__init__ does nothing with the kk argument.\nPlant._tree_unflatten is missing the aux_data argument, and fails to pass the kk argument to Plant.__init__\nWith these issues fixed, your code executes without error:\npython\nCopy\nclass One:\n    def __init__(self, plant,x):\n        self.plant = plant\n        self.x = x\n    \n    @jit\n    def call_plant_func(self,y):\n        out = self.plant.func(y) + self.x\n        return out\n    \n    def _tree_flatten(self):\n        children = (self.plant, self.x)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n        \ntree_util.register_pytree_node(One,\n                               One._tree_flatten,\n                               One._tree_unflatten)    \n    \nclass Plant:\n    def __init__(self, z, kk):\n        self.kk = kk\n        self.z =z\n    \n    @jit\n    def func(self, y):\n        y = y + self.z\n        return y\n    \n    def _tree_flatten(self):\n        children = (self.z, self.kk)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n   \ntree_util.register_pytree_node(Plant,\n                               Plant._tree_flatten,\n                               Plant._tree_unflatten)\n\nplant = Plant(5,2)\none = One(plant,2)\nprint(one.call_plant_func(10))"
        ],
        "link": "https://stackoverflow.com/questions/77439217/jax-jit-for-nested-class-method"
    },
    {
        "title": "Convert for loop to jax.lax.scan",
        "question": "How does one convert the following (to accelerate compiling)? The for loop version works with jax.jit,\npython\nCopy\nimport functools\nimport jax\nimport jax.numpy as jnp\n\n@functools.partial(jax.jit, static_argnums=0)\ndef func(n):\n\n    p = 1\n    x = jnp.arange(8)\n    y = jnp.zeros((n,))\n\n    for idx in range(n):\n        y = y.at[idx].set(jnp.sum(x[::p]))\n        p = 2*p\n\n    return y\n\nfunc(2)\n# >> Array([28., 12.], dtype=float32)\nbut will return static start/stop/step errors when using scan\npython\nCopy\nimport numpy as np\n\ndef body(p, xi):\n\n    y = jnp.sum(x[::p])\n\n    p = 2*p\n\n    return p, y\n\nx = jnp.arange(8)\n\njax.lax.scan(body, 1, np.arange(2))\n# >> IndexError: Array slice indices must have static start/stop/step ...",
        "answers": [
            "The issue here is that within scan, the p variable represents a dynamic value, meaning that x[::p] is a dynamically-sized array, so the operation is not allowed in JAX transformations (see JAX sharp bits: dynamic shapes).\nOften in such cases it's possible to replace approaches using dynamically-shaped intermediates with other approaches that compute the same thing using only use static arrays; in this case one thing you might do is replace this problematic line:\npython\nCopy\njnp.sum(x[::p])\nwith this, which does the same sum using only statically-sized arrays:\npython\nCopy\njnp.sum(x, where=jnp.arange(len(x)) % p == 0)\nUsing this idea, here's a version of your original function that uses scan:\npython\nCopy\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnums=0)\ndef func_scan(n):\n    p = 1\n    x = jnp.arange(8)\n    y = jnp.zeros((n,))\n\n    def body(carry, _):\n      idx, y, p = carry\n      y = y.at[idx].set(jnp.sum(x, where=jnp.arange(len(x)) % p == 0))\n      return (idx + 1, y, 2 * p), None\n\n    (i, y, p), _ = jax.lax.scan(body, (0, y, p), xs=None, length=n)\n    return y\n\nfunc_scan(2)\n# Array([28., 12.], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77364000/convert-for-loop-to-jax-lax-scan"
    },
    {
        "title": "how to use jit with control flow and arrays without slowing down",
        "question": "import jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n\narray1 = np.random.normal(size=(1000,1000))\ndef first():\n  for i in range(1000):\n    for j in range(1000):\n      if array1[i,j] >= 0:\n        array1[i,j] = 1\n      else:\n        array1[i,j] = -1\n# %timeit first()\n\nfrom functools import partial\nkey = jax.random.PRNGKey(seed=0)\narray2 = jax.random.normal(key, shape=(1000,1000))\n\n@partial(jit, static_argnums=(0,1,2))\ndef f( i,j):\n  r = jax.lax.cond(array2[i,j] >= 0, lambda x: 1, lambda x: -1, None)\n  # if array2[i,j] >= 0:\n  # # if i == j:\n  #   array2.at[i,j].set(1)\n  # else: array2.at[i,j].set(-1)\n  array2.at[i,j].set(r)\n\n# f_jit = jit(f, static_argnums=(0,1))\ndef second():\n  for i in range(1000):\n    for j in range(1000):\n      # jax.lax.cond(array2[i,j]>=0, lambda x: True, lambda x: False, None)\n      f(i,j)\n%timeit second()\nI have two functions: first and second. I want second to run as fast as (or faster) first does. first function is a function using numpy. second function uses jax. What is the best way to implement first function using jax in this case? jax.lax.cond significantly slows down the process I think.\nI left the comments on purpose to show what I've tried.",
        "answers": [
            "The reason first runs relatively quickly is because it does 1,000,000 numpy array operations, and numpy has been optimized for fast per-operation dispatch.\nThe reason second runs relatively slowly is because it does 1,000,000 JAX array operations, and JAX has not been optimized for fast per-operation dispatch.\nFor some general background on this, see JAX FAQ: Is JAX faster than NumPy?.\nBut if you're asking about the fastest way to accomplish what you're doing, in both NumPy and JAX the answer would be to avoid writing loops. Here is the equivalent, making the computation pure rather than in-place for ease of comparison (your original second function actually does nothing, because array.at[i].set() does not operate in-place):\ndef first_fast(array):\n  return np.where(array >= 0, 1, 0)\n\ndef second_fast(array):\n  return jnp.where(array >= 0, 1, 0)\nIn general, if you find yourself writing loops over array values in NumPy or in JAX, you can expect that the resulting code will be slow. In both NumPy and JAX, there's almost always a better way to compute the result using built-in vectorized operations.\nIf you're interested in further benchmarks between JAX and NumPy, be sure to read FAQ: Benchmarking JAX Code to ensure that you're comparing the right things."
        ],
        "link": "https://stackoverflow.com/questions/77333949/how-to-use-jit-with-control-flow-and-arrays-without-slowing-down"
    },
    {
        "title": "jax PRNG key error with tfp normal distribution",
        "question": "I am trying to run the following piece of Pyhon code from the Ubuntu WSL from Windows\npython\nCopy\nimport tensorflow_probability as tfp; tfp = tfp.substrates.jax\ntfd = tfp.distributions\ndist = tfd.Normal(loc=0., scale=3.)\ndist.cdf(1.)\ndist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\ndist.prob([0, 1.5])\ndist.sample([3])\nI am getting the following error\npython\nCopy\n> AttributeError                            Traceback (most recent call\n> last) Cell In[45], line 4\n>       2 tfd = tfp.distributions\n>       3 dist = tfd.Normal(loc=0., scale=3.)\n> ----> 4 dist.cdf(1.)\n>       5 dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\n>       6 dist.prob([0, 1.5])\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1429,\n> in Distribution.cdf(self, value, name, **kwargs)    1411 def cdf(self,\n> value, name='cdf', **kwargs):    1412   \"\"\"Cumulative distribution\n> function.    1413     1414   Given random variable `X`, the cumulative\n> distribution function `cdf` is:    (...)    1427       values of type\n> `self.dtype`.    1428   \"\"\"\n> -> 1429   return self._call_cdf(value, name, **kwargs)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1405,\n> in Distribution._call_cdf(self, value, name, **kwargs)    1403 with\n> self._name_and_control_scope(name, value, kwargs):    1404   if\n> hasattr(self, '_cdf'):\n> -> 1405     return self._cdf(value, **kwargs)    1406   if hasattr(self, '_log_cdf'):    1407     return\n> tf.exp(self._log_cdf(value, **kwargs))\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/normal.py:195,\n> in Normal._cdf(self, x)\n>     194 def _cdf(self, x):\n> --> 195   return special_math.ndtr(self._z(x))\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/internal/special_math.py:136,\n> in ndtr(x, name)\n>     132 if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n>     133   raise TypeError(\n>     134       \"x.dtype=%s is not handled, see docstring for supported types.\"\n>     135       % x.dtype)\n> --> 136 return _ndtr(x)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/internal/special_math.py:141,\n> in _ndtr(x)\n>     139 def _ndtr(x):\n>     140   \"\"\"Implements ndtr core logic.\"\"\"\n> --> 141   half_sqrt_2 = tf.constant(\n>     142       0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n>     143   half = tf.constant(0.5, x.dtype)\n>     144   one = tf.constant(1., x.dtype)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:117,\n> in _constant(value, dtype, shape, name)\n>     116 def _constant(value, dtype=None, shape=None, name='Const'):  # pylint: disable=unused-argument\n> --> 117   x = convert_to_tensor(value, dtype=dtype)\n>     118   if shape is None:\n>     119     return x\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:167,\n> in _convert_to_tensor(value, dtype, dtype_hint, name)\n>     164     pass\n>     166 if ret is None:\n> --> 167   ret = conversion_func(value, dtype=dtype)\n>     168 return ret\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:222,\n> in _default_convert_to_tensor(value, dtype)\n>     218 \"\"\"Default tensor conversion function for array, bool, int, float, and complex.\"\"\"\n>     219 if JAX_MODE:\n>     220   # TODO(b/223267515): We shouldn't need to specialize here.\n>     221   if hasattr(value, 'dtype') and jax.dtypes.issubdtype(\n> --> 222       value.dtype, jax.dtypes.prng_key\n>     223   ):\n>     224     return value\n>     225   if isinstance(value, (list, tuple)) and value:\n> \n> AttributeError: module 'jax.dtypes' has no attribute 'prng_key'\nThe installed packages are\npython\nCopy\n> Package                  Version             \n------------------------ --------------------\nabsl-py                  2.0.0               \nAmbit-Stochastics        1.0.6               \nanyio                    3.6.2               \nargon2-cffi              21.3.0              \nargon2-cffi-bindings     21.2.0              \nasttokens                2.2.1               \nattrs                    19.3.0              \nAutomat                  0.8.0               \nBabel                    2.11.0              \nbackcall                 0.2.0               \nbeautifulsoup4           4.11.2              \nbleach                   6.0.0               \nblinker                  1.4                 \ncertifi                  2019.11.28          \ncffi                     1.15.1              \nchardet                  3.0.4               \ncharset-normalizer       3.0.1               \nClick                    7.0                 \ncloud-init               23.2.1              \ncloudpickle              2.2.1               \ncolorama                 0.4.3               \ncomm                     0.1.2               \ncommand-not-found        0.3                 \nconfigobj                5.0.6               \nconstantly               15.1.0              \ncryptography             2.8                 \ncycler                   0.11.0              \ndbus-python              1.2.16              \ndebugpy                  1.6.6               \ndecorator                5.1.1               \ndefusedxml               0.7.1               \ndistlib                  0.3.6               \ndistro                   1.4.0               \ndistro-info              0.23ubuntu1         \ndm-tree                  0.1.8               \nentrypoints              0.3                 \nenv                      0.1.0               \nexecuting                1.2.0               \nfastjsonschema           2.16.2              \nfilelock                 3.9.0               \ngast                     0.5.4               \nhttplib2                 0.14.0              \nhyperlink                19.0.0              \nidna                     2.8                 \nimportlib-metadata       6.0.0               \nimportlib-resources      5.10.2              \nincremental              16.10.1             \nipykernel                6.20.2              \nipython                  8.9.0               \nipython-genutils         0.2.0               \nipywidgets               8.1.1               \njax                      0.4.13              \njaxlib                   0.4.13              \njedi                     0.18.2              \nJinja2                   3.1.2               \njson5                    0.9.11              \njsonpatch                1.22                \njsonpointer              2.0                 \njsonschema               4.17.3              \njupyter                  1.0.0               \njupyter-client           8.0.2               \njupyter-console          6.6.3               \njupyter-core             5.2.0               \njupyter-events           0.6.3               \njupyter-server           2.2.0               \njupyter-server-terminals 0.4.4               \njupyterlab               3.5.3               \njupyterlab-pygments      0.2.2               \njupyterlab-server        2.19.0              \njupyterlab-widgets       3.0.9               \nkeyring                  18.0.1              \nkiwisolver               1.3.2               \nlanguage-selector        0.1                 \nlaunchpadlib             1.10.13             \nlazr.restfulclient       0.14.2              \nlazr.uri                 1.0.3               \nMarkupSafe               2.1.2               \nmatplotlib               3.4.3               \nmatplotlib-inline        0.1.6               \nmistune                  2.0.4               \nml-dtypes                0.2.0               \nmore-itertools           4.2.0               \nnbclassic                0.5.1               \nnbclient                 0.7.2               \nnbconvert                7.2.9               \nnbformat                 5.7.3               \nnest-asyncio             1.5.6               \nnetifaces                0.10.4              \nnotebook                 6.5.2               \nnotebook-shim            0.2.2               \nnumpy                    1.21.3              \noauthlib                 3.1.0               \nopt-einsum               3.3.0               \npackaging                23.0                \npandas                   1.3.4               \npandocfilters            1.5.0               \nparso                    0.8.3               \npexpect                  4.6.0               \npickleshare              0.7.5               \nPillow                   8.4.0               \npip                      20.0.2              \npkgutil-resolve-name     1.3.10              \nplatformdirs             2.6.2               \nprometheus-client        0.16.0              \nprompt-toolkit           3.0.36              \npsutil                   5.9.4               \nptyprocess               0.7.0               \npure-eval                0.2.2               \npyasn1                   0.4.2               \npyasn1-modules           0.2.1               \npycparser                2.21                \nPygments                 2.14.0              \nPyGObject                3.36.0              \nPyHamcrest               1.9.0               \nPyJWT                    1.7.1               \npymacaroons              0.13.0              \nPyNaCl                   1.3.0               \npyOpenSSL                19.0.0              \npyparsing                3.0.4               \npyrsistent               0.15.5              \npyserial                 3.4                 \npython-apt               2.0.1+ubuntu0.20.4.1\npython-dateutil          2.8.2               \npython-debian            0.1.36+ubuntu1.1    \npython-json-logger       2.0.4               \npytz                     2021.3              \nPyYAML                   5.3.1               \npyzmq                    25.0.0              \nqtconsole                5.4.4               \nQtPy                     2.4.0               \nrequests                 2.28.2              \nrequests-unixsocket      0.2.0               \nrfc3339-validator        0.1.4               \nrfc3986-validator        0.1.1               \nscipy                    1.10.1              \nSecretStorage            2.3.1               \nSend2Trash               1.8.0               \nservice-identity         18.1.0              \nsetuptools               45.2.0              \nsimplejson               3.16.0              \nsix                      1.14.0              \nsniffio                  1.3.0               \nsos                      4.4                 \nsoupsieve                2.3.2.post1         \nssh-import-id            5.10                \nstack-data               0.6.2               \nsystemd-python           234                 \nterminado                0.17.1              \ntfp-nightly              0.22.0.dev20231002  \ntinycss2                 1.2.1               \ntomli                    2.0.1               \ntornado                  6.2                 \ntraitlets                5.9.0               \nTwisted                  18.9.0              \ntyping-extensions        4.5.0               \nubuntu-advantage-tools   8001                \nufw                      0.36                \nunattended-upgrades      0.1                 \nurllib3                  1.25.8              \nvirtualenv               20.17.1             \nwadllib                  1.3.3               \nwcwidth                  0.2.6               \nwebencodings             0.5.1               \nwebsocket-client         1.5.0               \nwheel                    0.34.2              \nwidgetsnbextension       4.0.9               \nzipp                     1.0.0               \nzope.interface           4.7.1     \nIf I change the Normal distribution to the Gamma distribution, I no longer get an error. Any ideas why this might be? The code snippet is taken directly from the tensorflow website. thanks!",
        "answers": [
            "jax.dtypes.prng_key was added in JAX version 0.4.14. You should update JAX to a newer version (0.4.14 or later), or if that is not possible, downgrade tensorflow_probability to an older version (0.21.0 or older should be sufficient for this issue)."
        ],
        "link": "https://stackoverflow.com/questions/77221932/jax-prng-key-error-with-tfp-normal-distribution"
    },
    {
        "title": "Does equinox (jax) do no batch dim broadcasting and expects you to use vmap instead?",
        "question": "https://docs.kidger.site/equinox/api/nn/mlp/#equinox.nn.MLP\nThe only way I was able to use MLP is like this\npython\nCopy\nimport jax\nimport equinox as eqx\nimport numpy as np\n\n\njax.vmap(eqx.nn.MLP(in_size=12, out_size=4, width_size=6, depth=5, key=key))(np.random.randn(5, 12)\nIs this the intended usage? It differs a bit from other frameworks then. But maybe safer.",
        "answers": [
            "Yup, this is intended!\nEvery layer in eqx.nn acts on a single batch element, and you can apply them to batches by calling jax.vmap, exactly as you're doing.\nSee also this FAQ entry: https://docs.kidger.site/equinox/faq/#how-do-i-input-higher-order-tensors-eg-with-batch-dimensions-into-my-model\nI hope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/77090293/does-equinox-jax-do-no-batch-dim-broadcasting-and-expects-you-to-use-vmap-inst"
    },
    {
        "title": "How to select between different function based on a value of a parameter in flax?",
        "question": "I am iterating through each head and applying either f1 or f2 function depending on the value of the parameter self.alpha.\nI only want to evaluate either function f1 or f2 not both and then select output of one based on conditional.\n        def f1 (x):\n            print('f1')\n            return x/x.shape[2]\n        def f2 (x):\n            print('f2')\n            temp = nn.relu(x)\n            return temp/(jnp.sum(temp,axis=-1,keepdims=True) + 1e-5)\n        \n        def choose_attention(alpha, x):\n            return jax.lax.cond(alpha[0, 0, 0,0],f2,f1,operand=x)\n        \n        results = []\n        func = [f1,f2]\n        for i in range(self.alpha.shape[1]):\n            print(i)\n            alpha_i = self.alpha[:, i:i+1, :, :]\n            x_i = attn_weights[:, i:i+1, :, :]\n            result_i = jax.lax.switch(self.alpha[0,0,0,0].astype(int),func,x_i)\n            results.append(result_i)\n\n        final_result = jnp.concatenate(results, axis=1)\nMy print statements read like 0 f1 f2 1 2 3 4 5 6 7 8 9 10 11",
        "answers": [
            "jax.lax.switch does what you want: it chooses between two different functions based on a runtime value. Your use of print statements is misleading you: Python print runs at trace-time rather than runtime, and all code will be traced even if it is not eventually executed.\nFor some background on how to think about the execution model of JAX programs, I would suggest How to think in JAX.\nSide note: for better performance, I would also suggest avoiding using Python for loops to loop through array values, and instead express your algorithm using either Numpy-style explicit vectorization, or using jax.vmap to automatically vectorize your code."
        ],
        "link": "https://stackoverflow.com/questions/77083595/how-to-select-between-different-function-based-on-a-value-of-a-parameter-in-flax"
    },
    {
        "title": "jax.errors.UnexpectedTracerError only when using jax.debug.breakpoint()",
        "question": "My jax code runs fine but when I try to insert a breakpoint with jax.debug.breakpoint I get the error: jax.errors.UnexpectedTracerError.\nI would expect this error to show up also without setting a breakpoint.\nIs this intended behavior or is something weird happening? When using jax_checking_leaks none of the reported tracers seem to actually be leaked.",
        "answers": [
            "There is currently a bug in jax.debug.breakpoint that can lead to spurious tracer leaks in some situations: see https://github.com/google/jax/issues/16732.\nThere's not any easy workaround at the moment, unfortunately, but hopefully the issue will be addressed soon."
        ],
        "link": "https://stackoverflow.com/questions/77067644/jax-errors-unexpectedtracererror-only-when-using-jax-debug-breakpoint"
    },
    {
        "title": "Reason to return updated stack along with top element from pop() method",
        "question": "This:\npython\nCopy\n  def pop(self) -> tuple[Any, Stack]:\n    \"\"\"Pops from the stack, returning an (elem, updated stack) pair.\"\"\"\nWhat is the reason behind returning updated stack along with top element from pop() method?",
        "answers": [
            "From the docstring of the Stack class: \"A bounded functional stack implementation.\"\nA functional implementation generally means that pure functions are used. The pop function implements two pure function properties:\nThe output does not change when called multiple times with the same input.\nThe function does not mutate the stack, ie it does not introduce side effects.\nThis is why it must return a (mutated) copy of the input Stack instead of modifying it."
        ],
        "link": "https://stackoverflow.com/questions/77057836/reason-to-return-updated-stack-along-with-top-element-from-pop-method"
    },
    {
        "title": "JAX update breaks working code of linear algebra solver",
        "question": "I am dealing with an issue related to an update of jax. I have a library which is supposed to solve a system of linear equations using the bicgstab algorithm.\nThe solver is implemented as follows:\npython\nCopy\ndef bicgstabsolver(A, b, eps):\n  '''Returns the loop initialization and iteration functions.'''\n\n  def init(z, b, x0):\n    '''Forms the args that will be used to update stuff.'''\n\n    x = x0\n    r = b - A(x, z)\n    rstrich = r\n    v = vecfield.zeros(b.shape)\n    p = vecfield.zeros(b.shape)\n    alpha = 1\n    rho = 1\n    omega = 1\n\n    term_err = eps * vecfield.norm(b)\n\n    return x, r, rstrich, v, p, alpha, rho, omega, term_err\n    \n  @jax.jit\n  def iter(x, r, rstrich, v, p, alpha, rho, omega, z):\n    '''Run the iteration loop `n` times.'''\n    rhoold = rho\n    rho = vecfield.dot(vecfield.conj(rstrich),r)\n    beta = (rho / rhoold) * (alpha / omega)\n    p = r + beta * (p - omega * v)\n    v = A(p,z)\n    alpha = rho / vecfield.dot(vecfield.conj(rstrich),v)\n    h = x + alpha * p\n    s = r - alpha * v\n    t = A(s,z)\n    omega = vecfield.dot(vecfield.conj(t),s) / vecfield.dot(vecfield.conj(t),t)\n    x = h + omega * s\n    r = s - omega * t\n    err = vecfield.norm(r)\n    return x, r, rstrich, v, p, alpha, rho, omega, err\n\n  return init, iter\nThe implementation of the VecField class:\npython\nCopy\nimport jax.numpy as np\nfrom typing import Any, NamedTuple\n\n\nclass VecField(NamedTuple):\n  '''Represents a 3-tuple of arrays.'''\n  x: Any\n  y: Any\n  z: Any\n\n  @property\n  def shape(self):\n    assert self.x.shape == self.y.shape == self.z.shape\n    return self.x.shape\n\n  @property\n  def dtype(self):\n    assert self.x.dtype == self.y.dtype == self.z.dtype\n    return self.x.dtype\n\n  def as_array(self):\n    return VecField(*(np.array(a) for a in self))\n\n  def __add__(x, y):\n    return VecField(*(a + b for a, b in zip(x, y)))\n\n  def __sub__(x, y):\n    return VecField(*(a - b for a, b in zip(x, y)))\n\n  def __mul__(x, y):\n    return VecField(*(a * b for a, b in zip(x, y)))\n\n  def __rmul__(y, x):\n    return VecField(*(x * b for b in y))\n\n\ndef zeros(shape):\n  return VecField(*(np.zeros(shape, np.complex128) for _ in range(3)))\n\ndef ones(shape):\n  return VecField(*(np.ones(shape, np.complex128) for _ in range(3)))\n\n# TODO: Check if this hack is still necessary to obtain good performance.\ndef dot(x, y):\n  z = VecField(*(a * b for a, b in zip(x, y)))\n  return sum(np.sum(np.real(c)) + 1j * np.sum(np.imag(c)) for c in z)\n\n\ndef norm(x):\n  return np.sqrt(sum(np.square(np.linalg.norm(a)) for a in x))\n\n\ndef conj(x):\n  return VecField(*(np.conj(a) for a in x))\n\n\ndef real(x):\n  return VecField(*(np.real(a) for a in x))\n\n\ndef from_tuple(x):\n  return VecField(*(np.reshape(a, (1, 1) + a.shape) for a in x))\n\n\ndef to_tuple(x):\n  return tuple(np.reshape(a, a.shape[2:]) for a in x)\nThe code is running perfectly fine using jax and jaxlib version 0.3.10. However, if I update jax to 0.4.13 it stops working with a cryptic error:\npython\nCopy\nFile \"***\", line 66, in iter\n    p = r + beta * (p - omega * v)\n  File \"***/python3.8/site-packages/jax/_src/numpy/array_methods.py\", line 791, in op\n    return getattr(self.aval, f\"_{name}\")(self, *args)\n  File \"***/python3.8/site-packages/jax/_src/numpy/array_methods.py\", line 260, in deferring_binary_op\n    raise TypeError(f\"unsupported operand type(s) for {opchar}: \"\njax._src.traceback_util.UnfilteredStackTrace: TypeError: unsupported operand type(s) for *: 'DynamicJaxprTracer' and 'VecField'\nI have no clue so far how to migrate this code to be compatible with the newer version of jax. Probably I'm missing something very obvious. Any help would be greatly appreciated!",
        "answers": [
            "It looks like JAX's array __mul__ methods are raising a TypeError on unsupported input rather than returning NotImplemented, which means that omega * v is not correctly dispatching to v.__rmul__().\nThis is a bug in JAX: I would suggest reporting this in a new issue at http://github.com/google/jax/issues/\nIn the meantime, you should be able to work around this by making sure that every time you operate between a VecField by a JAX array, the VecField appears on the left of the operand; e.g. change this:\npython\nCopy\np = r + beta * (p - omega * v)\nto this:\npython\nCopy\np = (p - v * omega) * beta + r\nEdit: it looks like the bug was introduced in https://github.com/google/jax/pull/11234 (meaning it's present in all JAX versions 0.3.14 and newer) and only affects subtypes of builtin collections (which includes NamedTuple).\nEdit 2: this has been fixed in https://github.com/google/jax/pull/17406, which should be part of a future JAX 0.4.16 release."
        ],
        "link": "https://stackoverflow.com/questions/77024323/jax-update-breaks-working-code-of-linear-algebra-solver"
    },
    {
        "title": "How to extract chunks of a 2D numpy array that has been flattened",
        "question": "I would like to know the best way of extacting chunks of elements from a 2D numpy array that has been flattened. See example python code below which hopefully explains what I want to do a little better.\npython\nCopy\nimport numpy as np\n\nnx = 5\nnz = 7\nnumGPs = nx*nz\n\nGPs_matrix = np.arange(numGPs).reshape((nx,nz), order='F')\nav = np.zeros_like(GPs_matrix)\nav[1:-1,1:-1] = (GPs_matrix[1:-1,2:] + GPs_matrix[1:-1,:-2] + GPs_matrix[2:,1:-1] + GPs_matrix[:-2,1:-1])/4\n# How to do the above if GPs is flattened as per below?\nGPs_flat = GPs_matrix.reshape(-1, order='F')\n# One (very clunky) way is to do the following\ncor = GPs_matrix[1:-1,1:-1].reshape(-1, order='F')\nbtm = GPs_matrix[1:-1,:-2].reshape(-1, order='F')\ntop = GPs_matrix[1:-1,2:].reshape(-1, order='F')\nlft = GPs_matrix[:-2,1:-1].reshape(-1, order='F')\nrgt = GPs_matrix[2:,1:-1].reshape(-1, order='F')\nav_flat = np.zeros_like(GPs_flat)\nav_flat[cor] = (GPs_flat[top] + GPs_flat[btm] + GPs_flat[rgt] + GPs_flat[lft])/4\n# Check\nprint(av.reshape(-1, order='F'))\nprint(av_flat)\n# Is there a better way?\n# I see np.r_() may be useful but I don't know how to best use it. I assume the below attempt can be improved upon\nsl_cor = np.r_[nx+1:2*nx-1,2*nx+1:3*nx-1,3*nx+1:4*nx-1,4*nx+1:5*nx-1,5*nx+1:6*nx-1] # Should match cor above\n# Check\nprint(sl_cor)\nprint(cor)\n# The below also works but I would like to avoid using loops if possible\nprint(np.r_[*[np.arange(1,nx-1)+nx*j for j in range(1,nz-1)]])\nEssentially, I am trying to solve the possion equation in two spatial dimensions. It is convenient to set up the problem in the form of a 2D array as the location of the elements in the array corresponds to the position of the grid points in the cartesian mesh. Ultimately I will be using an iterate solver in jax to solve the linear system (e.g. bicgstab) which requires a linear operator function as input. Therefore, the function needs to return a vector and loops are not efficient.",
        "answers": [
            "First off, it looks like what you're attempting to compute is a convolution. In general I'd avoid doing a convolution by hand, and instead use something like scipy.signal.convolve2d. Here's the equivalent for your case:\npython\nCopy\nfrom scipy.signal import convolve2d\nkernel = np.array(([[0, 1, 0],\n                    [1, 0, 1],\n                    [0, 1, 0]])) / 4\nav = np.zeros_like(GPs_matrix)\nav[1:-1, 1:-1] = convolve2d(GPs_matrix, kernel, mode='valid').astype(av.dtype)\nIn the flattened case, your best approach is probably going to be to reshape the 1D input, perform the 2D convolution, and then flatten the output. For example:\npython\nCopy\ndef eval_1D(vec):\n  mat = vec.reshape(nx, nz, order='F')\n  kernel = np.array(([[0, 1, 0],\n                      [1, 0, 1],\n                      [0, 1, 0]])) / 4\n  av = np.zeros_like(mat)\n  av[1:-1, 1:-1] = convolve2d(mat, kernel, mode='valid').astype(av.dtype)\n  return av.ravel(order='F')\n\nprint(eval_1D(GPs_matrix.ravel(order='F')))\n# [ 0  0  0  0  0  0  6  7  8  0  0 11 12 13  0  0 16 17\n#  18  0  0 21 22 23  0  0 26 27 28  0  0  0  0  0  0]"
        ],
        "link": "https://stackoverflow.com/questions/76952780/how-to-extract-chunks-of-a-2d-numpy-array-that-has-been-flattened"
    },
    {
        "title": "Migration from haiku: Alternative to Haiku's PRNGSequence?",
        "question": "I am writing a Markov chain Monte Carlo simulation in JAX which involves a large series of sampling steps. I currently rely on haiku's PRNGSequence to do the pseudo random number generator key bookkeeping:\npython\nCopy\nimport haiku as hk\n\ndef step(key, context):\n  key_seq = hk.PRNGSequence(key)\n  x1 = sampler(next(key_seq), context_1)\n  ...\n  xn = other_sampler(next(key_seq), context_n)\nQuestion:\nSince Haiku has been discontinued, I am looking for an alternative to PRNGSequence.\nI find the standard JAX approach:\npython\nCopy\ndef step(key, context):\n  key, subkey = jax.random.split(key)\n  x1 = sampler(subkey, context_1)\n  ...\n  key, subkey = jax.random.split(key)\n  xn = other_sampler(subkey, context_n)\nunsatisfactory on two accounts:\nVery error prone: It is easy to slip up and re-use a key. This is especially problematic in MCMC simulations, which are sensitive to these biases and very difficult to debug.\nIt is quite bulky: I need to roughly double the size of my code to split keys.\nAny suggestions how to mitigate these problems?\nThanks!\nHylke",
        "answers": [
            "If all you need is a simple class that locally handles splitting keys for you, why not define it yourself? You could create a suitable one in a few lines – for example:\npython\nCopy\nimport jax\n\nclass PRNGSequence:\n  def __init__(self, key):\n    self._key = key\n  def __next__(self):\n    self._key, key = jax.random.split(self._key)\n    return key\n\ndef step(key):\n  key_seq = PRNGSequence(key)\n  print(jax.random.uniform(next(key_seq)))\n  print(jax.random.uniform(next(key_seq)))\n\nstep(jax.random.PRNGKey(0))\n# 0.10536897\n# 0.2787192\nAs always, though, you have to be careful about this kind of hidden state when you're using JAX transformations like jit: see JAX Sharp Bits: Pure Functions for information on this.",
            "I recently came across this nice one-liner alternative:\npython\nCopy\nimport itertools\n\nkey_seq = map(jax.random.key, itertools.count())\nkey = next(key_seq)\nwhich uses the infinite iterator count."
        ],
        "link": "https://stackoverflow.com/questions/76912173/migration-from-haiku-alternative-to-haikus-prngsequence"
    },
    {
        "title": "Is it possible to obtain values from jax traced arrays with dynamicjaxprtrace level larger than 1 using any of the callback functions?",
        "question": "So I have a program that have multiple functions with its own jax calls and here is the main function:\npython\nCopy\n@partial(jax.jit, static_argnames=(\"numberOfVoxels\",))    \ndef process_valid_voxels(numberOfVoxels, voxelPositions, voxelLikelihoods, ps, t, M, tmp):\n   \n    func = lambda tmp_val: process_voxel(tmp_val, voxelPositions, voxelLikelihoods, ps, t, M, tmp)\n    ys, likelihoods = jax.vmap(func)(jnp.arange(numberOfVoxels))\n   \n    return ys, likelihoods\nThis is the output of ys and likelihoods:\npython\nCopy\n(Pdb) ys\nTraced<ShapedArray(int32[3700,3,1])>with<DynamicJaxprTrace(level=3/0)>`\nlikelihoods\nTraced<ShapedArray(float32[3700,7,1])>with<DynamicJaxprTrace(level=3/0)>\nI want to get values from traced arrays ys, likelihoods so that I can modify them. I have tried using the io_callback function:\npython\nCopy\ndef callback1(x):\n    return jax.experimental.io_callback(process_voxel, x , x)\na = callback1(jnp.arange(numberOfVoxels))\nbut the output is the same except for the shape of the array:\npython\nCopy\nTraced<ShapedArray(int32[3700])>with<DynamicJaxprTrace(level=3/0)>",
        "answers": [
            "This is similar to one of JAX's FAQs: How can I convert a tracer to a numpy array?. That answer mentions callbacks, which you use above, but I think you have the wrong mental model of what io_callback does.\nWhen you run transformed JAX code, there are essentially two stages of execution:\nTracing happens within the Python runtime, using abstract representations of the arrays (tracers) to extract the sequence of operations implied by your code. During tracing in most cases, array values are not available by design.\nExecution happens within the XLA runtime once tracing has encoded the sequence of operations to be run. Array values are available during XLA execution, but this is not a Python runtime, and so Python debugging, printing, etc. is not available.\nYour question amounts to \"How can I access array values from stage 2 by inserting breakpoints into the runtime during stage 1\" The answer is: you can't!\nBut you can use callbacks and jax debugging tools to encode an instruction to tell XLA to pause execution and pass its values to some callback function during stage 2 execution and let you interact with array values from within Python.\nOne way to do so might look like this:\npython\nCopy\n@partial(jax.jit, static_argnames=(\"numberOfVoxels\",))    \ndef process_valid_voxels(numberOfVoxels, voxelPositions, voxelLikelihoods, ps, t, M, tmp):\n    func = lambda tmp_val: process_voxel(tmp_val, voxelPositions, voxelLikelihoods, ps, t, M, tmp)\n    ys, likelihoods = jax.vmap(func)(jnp.arange(numberOfVoxels))\n    jax.debug.breakpoint()\n    return ys, likelihoods\nThis will tell XLA during stage 2 to pause execution, call back to a Python-side debugging tool, and let you interact with the values there.\nFor more intuition on the mental model of JAX program execution, you may find How to think in JAX useful."
        ],
        "link": "https://stackoverflow.com/questions/76827852/is-it-possible-to-obtain-values-from-jax-traced-arrays-with-dynamicjaxprtrace-le"
    },
    {
        "title": "Computing the gradient of a batched function using JAX",
        "question": "I would need to compute the gradient of a batched function using JAX. The following is a minimal example of what I would like to do:\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.expand_dims(jnp.linspace(-1, 1, 20), axis=1)\n\nu = lambda x: jnp.sin(jnp.pi * x)\nux = jax.vmap(jax.grad(u))\n\nplt.plot(x, u(x))\nplt.plot(x, ux(x))  # Use vx instead of ux\nplt.show()\nI have tried a variety of ways of making this work using vmap, but I don't seem to be able to get the code to run without removing the batch dimension in the input x. I have seen some workarounds using the Jacobian but this doesn't seem natural as the given is a scalar function of a single variable.\nIn the end u will be a neural network (implemented in Flax) that I need to differentiate with respect to the input (not the parameters of the network), so I cannot remove the batch dimension.",
        "answers": [
            "To ensure the kernel (u) returns a scalar value, so that jax.grad makes sense, the batched dimension also needs to be mapped over.\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.expand_dims(jnp.linspace(-1, 1, 20), axis=1)\n\nu = lambda x: jnp.sin(jnp.pi * x)\nux = jax.vmap(jax.vmap(jax.grad(u)))\n# ux = lambda x : jax.lax.map(jax.vmap(jax.grad(u)), x) # sequential version\n# ux = lambda x : jax.vmap(jax.grad(u))(x.reshape(-1)).reshape(x.shape) # flattened map version\n\nplt.plot(x, u(x))\nplt.plot(x, ux(x))  # Use vx instead of ux\nplt.show()\nWhich composition of maps to use depends on what's happening in the batched dimension."
        ],
        "link": "https://stackoverflow.com/questions/76791201/computing-the-gradient-of-a-batched-function-using-jax"
    },
    {
        "title": "How to apply constraints to optimisation in jax.scipy.optimize.minimize",
        "question": "I am trying to minimize the function so that the value of x that minimizes it remains between 0 and 1. One possible walk around is using sigmoid transformation. Is there any other solution associated with jax in such cases? Here is a minimal example:\nimport jax\nfrom jax.scipy.optimize import minimize\nimport jax.numpy as jnp\nrng = jax.random.PRNGKey(0)\n\ndef gen_num(shape):\n  return jax.random.uniform(rng,shape)\n\nshape = (10,) \nmu = gen_num(shape)*2+3\nlog_sigma = gen_num(shape)*2\nc = gen_num(shape)\nx1 = gen_num((10,10))*4 +5\n\ndef kl_loss(x1, mu, log_sigma, c):\n    return -(log_sigma* c[:, jnp.newaxis]).mean() + jnp.log(jnp.std(x1))\n\nkl_value = lambda x: kl_loss(x1, mu, log_sigma, x)\nres = minimize(kl_value, c, method='BFGS', tol=1e-5)\nans = res.x",
        "answers": [
            "jax.scipy.optimize.minimize is quite limited. The JAX team recommends jaxopt instead; it has information on constrained optimization here.\nIf you can edit your question here to expand your example snippet into a full minimal reproducible example, I could add an example of how to adapt your code for use with jaxopt.",
            "The only supported method in Jax is \"BFGS\". As you might see it doens't support hard constraint. However passing the constraint in the objective function can do the trick. For instance you can artificially increase the gradient outside [0, 1]:\npython\nCopy\ndef objective_function(x: jnp.array, mu: float, log_sigma: float, x1: jnp.array):\n  \"\"\"Objective function.\"\"\"\n  kl_value = kl_loss(x1, mu, log_sigma, x)\n  constraint = 10 * (1 - jnp.where(0 <= x <= 1)) * (x - 0.5)**2\n  return k1_value + constraint\n\nres = minimize(kl_value, c, args=(mu, log_sigma, x1), method='BFGS', tol=1e-5, )\nIn this case the gradient outside the domain is proportional to 10*(x-0.5) which may be sufficient to maintain the value in the domain while having no impact inside the domain.\nFinally you can try to fit the factor \"10\" to have a stable training and an efficient constraint.\nThis trick worked for me when I tried to implement a SVM in jax with jax.scipy.optimize.minimize."
        ],
        "link": "https://stackoverflow.com/questions/76695661/how-to-apply-constraints-to-optimisation-in-jax-scipy-optimize-minimize"
    },
    {
        "title": "How to make a function a valid jax type?",
        "question": "When I pass an object created using the following function function into a jax.lax.scan function:\npython\nCopy\ndef logdensity_create(model, centeredness = None, varname = None):\n    if centeredness is not None:\n        model = reparam(model, config={varname: LocScaleReparam(centered= centeredness)})\n          \n    init_params, potential_fn_gen, *_ = initialize_model(jax.random.PRNGKey(0),model,dynamic_args=True)\n    logdensity = lambda position: -potential_fn_gen()(position)\n    initial_position = init_params.z\n    return (logdensity, initial_position)\nI get the following error (on passing the logdensity to an iterative function created using jax.lax.scan):\nTypeError: Value .logdensity_create.. at 0x13fca7d80> with type  is not a valid JAX type\nHow can I resolve this error?",
        "answers": [
            "I would probably do this via jax.tree_util.Partial, which wraps callables in a PyTree for compatibility with jit and other transformations:\npython\nCopy\nlogdensity = jax.tree_util.Partial(lambda position: -potential_fn_gen()(position))"
        ],
        "link": "https://stackoverflow.com/questions/76655153/how-to-make-a-function-a-valid-jax-type"
    },
    {
        "title": "What are the tradeoffs between jax.lax.map and jax.vmap?",
        "question": "This Github issue hints that there are tradeoffs in performance / memory / compilation time when choosing between jax.lax.map and jax.vmap. What are the specific details of these tradeoffs with respect to both GPUs and CPUs?",
        "answers": [
            "The main difference is that jax.vmap is a vectorizing transformation, while lax.map is an iterative transformation. Let's look at an example.\nExample function: vector_dot\nSuppose you have implemented a simple function that takes 1D vectors as inputs. For simplicity let's make it a simple dot product, but one that asserts the inputs are one-dimensional:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef vector_dot(x, y):\n  assert x.ndim == y.ndim == 1, \"vector inputs required\"\n  return jnp.dot(x, y)\nWe can create some random 1D vectors to test this:\npython\nCopy\nrng = np.random.default_rng(8675309)\nx = rng.uniform(size=50)\ny = rng.uniform(size=50)\n\nprint(vector_dot(x, y))\n# 14.919376\nTo see what JAX is doing with this function under the hood, we can print the jaxpr, which is JAX's intermediate-level representation of a function:\npython\nCopy\nprint(jax.make_jaxpr(vector_dot)(x, y))\n# { lambda ; a:f32[50] b:f32[50]. let\n#     c:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] a b\n#   in (c,) }\nThis shows that JAX lowers this code to a single call to dot_general, the primitive for generalized dot products in JAX and XLA.\nIterating over vector_dot\nNow, suppose you have a 2D input, and you'd like to apply this function to each row. There are several ways you could imagine doing this: three examples are using a Python for loop, using jax.vmap, or using jax.lax.map:\npython\nCopy\ndef batched_dot_for_loop(x_batched, y):\n  return jnp.array([vector_dot(x, y) for x in x_batched])\n\ndef batched_dot_lax_map(x_batched, y):\n  return jax.lax.map(lambda x: vector_dot(x, y), x_batched)\n\nbatched_dot_vmap = jax.vmap(vector_dot, in_axes=(0, None))\nApplying these three functions to a batched input yields the same results, to within floating point precision:\npython\nCopy\nx_batched = rng.uniform(size=(4, 50))\n\nprint(batched_dot_for_loop(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_lax_map(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_vmap(x_batched, y))\n# [11.964927  12.485697  13.683528  12.9286175]\nBut if we look at the jaxpr for each, we can see that the three approaches lead to very different computational characteristics.\nThe for loop solution looks like this:\npython\nCopy\nprint(jax.make_jaxpr(batched_dot_for_loop)(x_batched, y))\npython\nCopy\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[1,50] = slice[\n      limit_indices=(1, 50)\n      start_indices=(0, 0)\n      strides=(1, 1)\n    ] a\n    d:f32[50] = squeeze[dimensions=(0,)] c\n    e:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d b\n    f:f32[1,50] = slice[\n      limit_indices=(2, 50)\n      start_indices=(1, 0)\n      strides=(1, 1)\n    ] a\n    g:f32[50] = squeeze[dimensions=(0,)] f\n    h:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] g b\n    i:f32[1,50] = slice[\n      limit_indices=(3, 50)\n      start_indices=(2, 0)\n      strides=(1, 1)\n    ] a\n    j:f32[50] = squeeze[dimensions=(0,)] i\n    k:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] j b\n    l:f32[1,50] = slice[\n      limit_indices=(4, 50)\n      start_indices=(3, 0)\n      strides=(1, 1)\n    ] a\n    m:f32[50] = squeeze[dimensions=(0,)] l\n    n:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] m b\n    o:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e\n    p:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] h\n    q:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] k\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] n\n    s:f32[4] = concatenate[dimension=0] o p q r\n  in (s,) }\nThe key feature is that the iterations in the for loop are unrolled into a single long program.\nThe lax.map version looks like this:\npython\nCopy\nprint(jax.make_jaxpr(batched_dot_lax_map)(x_batched, y))\npython\nCopy\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = scan[\n      jaxpr={ lambda ; d:f32[50] e:f32[50]. let\n          f:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] e d\n        in (f,) }\n      length=4\n      linear=(False, False)\n      num_carry=0\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] b a\n  in (c,) }\nThe key feature is that it is loaded into a scan primitive, which is XLA's native static loop operation.\nThe vmap version looks like this:\npython\nCopy\nprint(jax.make_jaxpr(batched_dot_vmap)(x_batched, y))\npython\nCopy\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n  in (c,) }\nThe key feature here is that the vmap transformation is able to recognize that a batched 1D dot product is equivalent to a 2D dot product, so the result is a single extremely efficient native operation.\nPerformance considerations\nThese three approaches can have very different performance characteristics. The details will depend on the specifics of the original function (here vector_dot) but in broad strokes, we can consider three aspects:\nCompilation Cost\nIf you JIT-compile your program, you'll find:\nThe for-loop based solution will have compilation times that grow super-linearly with the number of iterations. This is due to the unrolling seen in the jaxpr above.\nThe lax.map and jax.vmap solutions will have fast compilation time, which under normal circumstances will not grow with the size of the batch dimension.\nRuntime\nIn terms of runtime:\nThe for loop solution can be very fast, because XLA can often fuse operations between the unrolled iterations. This is the flip side of the long compilation times.\nThe lax.map solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.\nThe jax.vmap solution will generally be the fastest, especially on accelerators like GPU or TPU, because it can make use of native batching parallelism on the device.\nMemory Cost\nThe for loop and lax.map solutions generally have good memory performance, because they execute sequentially and don't require storage of large intermediate results.\nThe main downside of the jax.vmap solution is that it can cause memory to blow up because the entire problem must fit into memory at once. This is not an issue with the simple vector_dot function used here, but can be for more complicated functions.\nBenchmarks\nYou can see these general principles at play when benchmarking the above functions. The following timings are on a Colab T4 GPU:\npython\nCopy\ny = rng.uniform(size=1000)\nx_batched = rng.uniform(size=(200, 1000))\npython\nCopy\n%time jax.jit(batched_dot_for_loop).lower(x_batched, y).compile()\n# CPU times: user 4.96 s, sys: 55 ms, total: 5.01 s\n# Wall time: 7.24 s\n%timeit jax.jit(batched_dot_for_loop)(x_batched, y).block_until_ready()\n# 1.09 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\npython\nCopy\n%time jax.jit(batched_dot_lax_map).lower(x_batched, y).compile()\n# CPU times: user 117 ms, sys: 2.71 ms, total: 120 ms\n# Wall time: 172 ms\n%timeit jax.jit(batched_dot_lax_map)(x_batched, y).block_until_ready()\n# 2.67 ms ± 56.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\npython\nCopy\n%time jax.jit(batched_dot_vmap).lower(x_batched, y).compile()\n# CPU times: user 51 ms, sys: 941 µs, total: 52 ms\n# Wall time: 103 ms\n%timeit jax.jit(batched_dot_vmap)(x_batched, y).block_until_ready()\n# 719 µs ± 129 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
        ],
        "link": "https://stackoverflow.com/questions/76615802/what-are-the-tradeoffs-between-jax-lax-map-and-jax-vmap"
    },
    {
        "title": "How to create a jax array and store dictionaries inside it later?",
        "question": "I wish to create a Jax-based array and use this array later to store dictionaries inside it. Is it possible to do so?",
        "answers": [
            "JAX arrays cannot store dictionaries as items. They can only store items of simple numerical types, including:\nintegers: int8, int16, int32, int64\nunsigned integers: uint8, uint16, uint32, uint64\nfloating point: bfloat16, float16, float32, float64\ncomplex; complex64, complex128\nAdditionally, several experimental narrow-width float and integer types from the ml_dtypes library have support on some hardware.\nDepending on your use-case, you may be able to use a struct-of-arrays pattern rather than an array-of-structs pattern, but it's hard to say whether this is applicable without more information about what you're trying to do."
        ],
        "link": "https://stackoverflow.com/questions/76594409/how-to-create-a-jax-array-and-store-dictionaries-inside-it-later"
    },
    {
        "title": "Passing the returned stacked output to jax.lax.scan",
        "question": "I wish to pass on the returned stacked values from the jax.lax.scan back to one of its arguments. Is it possible to do so? For example:\npython\nCopy\nfrom jax import lax\n\n\ndef cumsum(res, el):\n    \"\"\"\n    - `res`: The result from the previous loop.\n    - `el`: The current array element.\n    \"\"\"\n    v, u = res\n    print(u)\n    v = v + el\n    return (v,u),v  # (\"carryover\", \"accumulated\")\n\n\nresult_init = 0\nresult = []\n\n(final,use), result = lax.scan(cumsum, (result_init,result), a)\nIn the above code, I want to extract the cumulated res values during the runtime and pass it back. Thus, I have passed the result as an argument in the lax function, but it always prints an empty list.",
        "answers": [
            "There's no built-in way to access the \"current state\" of the accumulated values in the course of a scan operation: in particular, the current state will be a dynamically-shaped array (it will have size 0 in the first iteration, size 1 in the second, etc.) and scan, like other JAX transformations and higher-order functions, requires static shapes.\nBut you could do something similar by passing along an array that you manually update. It might look something like this:\npython\nCopy\nfrom jax import lax\nimport jax.numpy as jnp\nfrom jax import debug\n\ndef cumsum(carry, el):\n    i, v, running_result = carry\n    v = v + el\n    running_result = running_result.at[i].set(v)\n    debug.print(\"iteration {}: running_result={}\", i, running_result)\n    return (i + 1, v, running_result), v\n\na = jnp.arange(5)\nrunning_result = jnp.zeros_like(a)\n(i, v, running_result), result = lax.scan(cumsum, (0, 0, running_result), a)\n\nprint(\"\\nfinal running result:\", running_result)\nprint(\"final result:\", result)\npython\nCopy\niteration 0: running_result=[0 0 0 0 0]\niteration 1: running_result=[0 1 0 0 0]\niteration 2: running_result=[0 1 3 0 0]\niteration 3: running_result=[0 1 3 6 0]\niteration 4: running_result=[ 0  1  3  6 10]\n\nfinal running result: [ 0  1  3  6 10]\nfinal result: [ 0  1  3  6 10]\nNotice that I used jax.debug.print to print the intermediate results, because this function is traced and compiled."
        ],
        "link": "https://stackoverflow.com/questions/76587199/passing-the-returned-stacked-output-to-jax-lax-scan"
    },
    {
        "title": "is there a more efficient equivalent of np.sum(np.cumprod(1 / (1 + y*x)))?",
        "question": "I have a 1D NumPy array, such as x:\npython\nCopy\nx = np.array([0.05, 0.06, 0.06, 0.04])\nI'm showing a small array, but in reality, x can be very large. To x, I want to perform the following calculation:\npython\nCopy\ny = 1./12.\nnp.sum(np.cumprod(1 / (1 + y * x)))\nBecause x is very large, I want to do this more efficiently. I tried to use np.exp(np.cumsum(np.log(1 / (1 + y * x)))).sum() but this makes it slower. Is there a more efficient NumPy/JAX function?",
        "answers": [
            "You pretty much vectorized as much as possible.\nWhat you could spare here, if your were writing in C, is the numerous (but implicit, that is done thanks to vectorization inside numpy's code, that is in C) for loops involved. Since what your code means is that you\nFirstly iterate all elements of x to multiply them by y. In pure python that would be compound list [y*a for a in x]\nSecondly iterate again all elements of former result, to add 1 to all of them. So, [1+z for z in [y*a for a in x]]\nThirdly, iterate again all elements of the former result to invert all of them. So [1/u for u in [1+z for z in [y*a for a in x]]]\nFourthly, iterate again to compute cumulative product. So p=1; [p:=v*p for v in [1/u for u in [1+z for z in [y*a for a in x]]]]\nFifthly, iterate again to compute sum of former\nSo, sure, all those for loops are in C, so very fast. But there are many (non-nested) of them. And each of them doesn't do much. So time spend in the iteration itself the for(int i=0; i<arr_len; i++) that occurs somewhere in numpy's C code, is not that negligible before the content of that iteration (the result[i] = y*x[i] that is repeated by this loop in numpy's C code).\nIf you were writing this in pure python\npython\nCopy\ndef cumsumprod(x,y):\n    z=[y*a for a in x]\n    u=[1+a for a in z]\n    v=[1/a for a in u]\n    p=1; w=[p:=p*a for a in v]\n    s=0\n        for a in w: s+=a\n    return s\nThat would be way less efficient than this other pure python implementation\npython\nCopy\ndef ff(x,y):\n    s=0\n    p=1\n    for a in x:\n        p/=(1+y*a)\n        s+=p\n    return s\nSame computation. But one for loop instead of 5.\nTo be quantitative in what I say, on your example, in microseconds, your code takes 9.8 μs on my machine. My 1st python code 3.6 μs. And my 2nd, 1.9 μs.\nAnd yes, with that small data, pure python codes are both faster than numpy. If array is size 1000 instead, those timings become 17.7, 464 and 288. But point is, my second code is faster than my first, unsurprisingly. And your numpy code is the equivalent of my first code, but in C.\nAnd that is even an understatement, since I just use the example of for loops. It is not the only redundant thing that numpy does. For example, it also allocates a new array for each intermediary operation.\nNot that you did anything wrong. You did exactly what you are supposed to do with numpy. Just that is what numpy does: if provides many vectorized operation that we can sequence, each of them being a for loop on all our data. And we pay the price of having several unnecessary for loops, in exchange of the reward of having them in C, when pure python would be way slower. That is pretty much the best you can have from numpy.\nIf you want to have more, a way is numba. Numba allows you to write, otherwise naive, code, and yet have it fast, in C.\nJust add @jit before my previous pure python's code\npython\nCopy\nfrom numba import jit\n\n@jit(nopython=True)\ndef ff(x,y):\n    s=0\n    p=1\n    for a in x:\n        p/=(1+y*a)\n        s+=p\n    return s\nAnd you get something that is both in C, and doesn't contain the unnecessary bu yet unavoidable operations that sequencing many numpy's vectorized operation does.\nTimings for this function is 0.25 μs for your list. So way better than 1.9 μs of the same in pure python, thanks to compilation.\nAnd for a size 1000 list, where numpy's beat pure python, timing is 3.4 μs. So not only way better than python 276 μs, but also better than numpy's 14.9 μs, thanks to the simplicity of the algorithm.\nSo long story short: numba allows to write plain, simple, naive algorithm on numpy array, that are compiled."
        ],
        "link": "https://stackoverflow.com/questions/76563019/is-there-a-more-efficient-equivalent-of-np-sumnp-cumprod1-1-yx"
    },
    {
        "title": "JAX vmap vs pmap vs Python multiprocessing",
        "question": "I am rewriting some code from pure Python to JAX. I have gotten to the point where in my old code, I was using Python's multiprocessing module to parallelize the evaluation of a function over all of the CPU cores in a single node as follows:\npython\nCopy\n# start pool process \npool = multiprocessing.Pool(processes=10) # if node has 10 CPU cores, start 10 processes\n\n# use pool.map to evaluate function(input) for each input in parallel\n# suppose len(inputs) is very large and 10 inputs are processed in parallel at a time\n# store the results in a list called out\nout = pool.map(function,inputs)\n\n# close pool processes to free memory\npool.close()\npool.join()\nI know that JAX has vmap and pmap, but I don't understand if either of those are a drop-in replacement for how I'm using multiprocessing.pool.map above.\nIs vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).",
        "answers": [
            "Is vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nNo, vmap has nothing to do with parallelization. It is a vectorizing transformation, not a parallelizing transformation. In the course of normal operation, JAX may use multiple cores via XLA, so vmapped operations may also do this. But there's no explicit parallelization in vmap.\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\npmap parallelizes over multiple XLA devices. vmap does not parallelize, but rather vectorizes on a single device. multiprocessing parallelizes over multiple Python processes.\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nYes, it could be described as SPMD across multiple python processes.\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\npmap parallelizes over multiple XLA devices, and you have configured only a single XLA device, so the requested operation is not possible.\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).\nYes, I believe that pmap can be used to compute on multiple CPU cores. Whether it's nested with vmap is irrelevant. See JAX pmap with multi-core CPU.\nNote also that jax.pmap is deprecated in favor of the newer jax.shard_map, which is a much more flexible transformation for multi-device/multi-host computation. There's some info here: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html and https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html"
        ],
        "link": "https://stackoverflow.com/questions/76536601/jax-vmap-vs-pmap-vs-python-multiprocessing"
    },
    {
        "title": "JAX's slow performance on simple loops",
        "question": "I'm learning JAX and trying to do a simple test on the performance of JAX. The run time of the following code using JAX is weirdly slower than numpy or even simple addition of list members using python lists. I mean the following code has no purpose other than testing the speed.\nI'm wondering what might be the reason for this? I see that there is a functionality of JAX called \"fori_loop\" that may help in this examples or I can vectorize my actual code, but I want to know why this simple loop is so slow and do I need to avoid writing code like this and completely understand things in the JAX way?\nHere is the code in JAX:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import random\nimport time\n\nkey = random.PRNGKey(0)\nx = random.uniform(key, shape=(100,3))\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\nwhich takes 11 seconds\npython\nCopy\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n11.254772663116455\nThe same code using Numpy:\npython\nCopy\nimport numpy as np\nimport time\n\nx = np.random.rand(100,3)\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\ntakes around a milli second:\npython\nCopy\n0.005955934524536133\nThank you!",
        "answers": [
            "If you're interested in comparing the speed of JAX vs. NumPy speed, the place to start is here: JAX FAQ: Is JAX Faster Than NumPy? Quoting from there:\nin summary: if you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead. If you’re running your code on GPU or TPU, or are benchmarking more complicated JIT-compiled sequences of operations on CPU, you can generally expect JAX to outperform NumPy.\nYour benchmark does many individual array operations, each of which is very inexpensive, so effectively all you're measuring is the single-operation dispatch time. This is precisely in the regime where we'd expect NumPy to be fast, and JAX to be slow.\nFor real use-cases involving repeated operations, there are several ways you might optimize the code (including wrapping the whole function in jit, using vmap for efficient batching, or using fori_loop for sequential operations) but for the example function you give it's hard to say what's best. Taking your example at face value, I'd optimize it this way:\npython\nCopy\ndef func(x):\n  return 0\nAs written, there's no need to do any operations on x at all (but I suspect that's not particularly helpful for your real use-case)."
        ],
        "link": "https://stackoverflow.com/questions/76474532/jaxs-slow-performance-on-simple-loops"
    },
    {
        "title": "How to vmap over cho_solve and cho_factor?",
        "question": "The following error appears because of the last line of code below:\njax.errors.ConcretizationTypeError Abstract tracer value encountered where concrete value is expected...\nThe problem arose with the bool function.\nIt looks like it is due to the lower return value from cho_factor, which _cho_solve (note underscore) requires as static.\nI'm new to jax, so I was hoping that vmap-ing cho_factor into cho_solve would just work. What have I done wrong here?\npython\nCopy\nimport jax\n\nkey = jax.random.PRNGKey(0)\nk_y = jax.random.normal(key, (100, 10, 10))\ny = jax.random.normal(key, (100, 10, 1))\n\nmatmul = jax.vmap(jax.numpy.matmul)\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor)\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve)\n\nk_y = matmul(k_y, jax.numpy.transpose(k_y, (0, 2, 1)))\nchol, lower = cho_factor(k_y)\nresult = cho_solve((chol, lower), y)",
        "answers": [
            "The issue is that in each case, lower is a static scalar that should not be mapped over. So if you specify in_axes and out_axes so that lower is mapped over axis None, the vmap should work:\npython\nCopy\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor, out_axes=(0, None))\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve, in_axes=((0, None), 0))",
            "So I didn't manage to get cho_factor and cho_solve working, but worked around it using cholesky and solve_triangular:\npython\nCopy\n  cholesky = jax.vmap(jax.scipy.linalg.cholesky, in_axes=(0, None))\n  solve_tri = jax.vmap(jax.scipy.linalg.solve_triangular, in_axes=(0, 0, None, None))\n\n  L = cholesky(k_y, True)\n  result2 = solve_tri(L, solve_tri(L, y, 0, True), 1, True)"
        ],
        "link": "https://stackoverflow.com/questions/76458629/how-to-vmap-over-cho-solve-and-cho-factor"
    },
    {
        "title": "How to rewrite this JAX snippet to prevent TypeError: unhashable type: 'DynamicJaxprTracer'?",
        "question": "I am rewriting some Python code in JAX with the goal of speeding it up using jax.jit. There is a part of my old code where I have a dict that maps some integer to a list of functions. Every integer has a unique list of functions assigned to it. The functions are all JAX compatible, but the problem is my use case. I have a master function whose arguments are the number to be fed into these functions and the integer that tells me which list of functions to grab from the dict.\nHere is a simple example to demonstrate what I mean:\npython\nCopy\nfrom jax import jit\n\n# set up a dict that maps integers to a list of functions\ntest_dict = {1122997037:[lambda x: x**2, lambda x: 2*x],\n             1124279607:[lambda x: x**3, lambda x: 3*x]}\n             \n@jit\ndef evaluate_functions(xval,index):\n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n    \n    # given the input index, pull the relevant list of functions from test_dict\n    func1, func2 = test_dict[index]\n    \n    # evaluate both functions at the input xval and return values\n    return func1(xval), func2(xval)\n\n\nprint(evaluate_functions(2,1122997037))\nprint(evaluate_functions(2,1124279607))\nIf you remove the @jit decorator, it works as expected and prints (4,4) and (8,6). But with the @jit decorator, it raises the following error:\npython\nCopy\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[60], line 16\n     11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n---> 16 print(evaluate_functions(2,1))\n     17 print(evaluate_functions(2,2))\n\n    [... skipping hidden 12 frame]\n\nCell In[60], line 11, in evaluate_functions(xval, index)\n      7 @jit\n      8 def evaluate_functions(xval,index):\n      9     \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n---> 11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n\nTypeError: unhashable type: 'DynamicJaxprTracer'\nIs there any way to rewrite this to make it work with @jit? For example using jax.lax.switch?",
        "answers": [
            "The problem with this is that you can't do conditional code execution in JAX based on input data (Edit: I was mistaken about the semantics of lax.cond). You could mark the index argument as static, which means that JAX will recompile the function each time a new value of it is used:\npython\nCopy\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef evaluate_functions(xval,index):\n    ...\nThis is the sort of compilation behavior you'll see:\npython\nCopy\nevaluate_functions(0, 0) # JIT runs, may be slow\nevaluate_functions(1, 0) # No compilation, fast \nevaluate_functions(1, 3) # JIT runs again\nevaluate_functions(7, 3) # No compilation\n# etc\nThis is a good solution if you'll be using each of the functions many times. If there's a large number of functions and you'll only be using each one once this won't help you.\nEdit: Here's a solution using jax.lax.switch, I'm not sure how the performance will be:\npython\nCopy\nkeys, branches = zip(*test_dict.items())    \nbranches1, branches2 = zip(*branches)    \n    \n@jit    \ndef evaluate_functions(xval,index):    \n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\"·    \n    ind_to_key = jnp.asarray(keys)    \n    \n    ind = jnp.where(ind_to_key == index, size=1)[0][0]    \n    out1 = jax.lax.switch(ind, branches1, xval)                                                                               \n    out2 = jax.lax.switch(ind, branches2, xval)                                                                               \n    return out1, out2 "
        ],
        "link": "https://stackoverflow.com/questions/76418151/how-to-rewrite-this-jax-snippet-to-prevent-typeerror-unhashable-type-dynamicj"
    },
    {
        "title": "JAX code for minimizing Lennard-Jones potential for 2 points in Python gives unexpected results",
        "question": "I am trying to practice using JAX fo optimization problem and I am trying to do a simple problem, which is to minimize Lennard-Jones potential for just 2 points and I set both epsilon and sigma in Lennard-Jones potential equal 1, so the potential is just: F = 4(1/r^12-1/r^6) and r is the distance between the two points. And the result should be r = 2^(1/6), which is approximately 1.12.\nUsing JAX, I wrote following code, which is pretty simple and short, my initial guess values for two points are [0,1], which I think it is reasonable(because for Lennard-Jones potential it could be a problem because it approach infinite if r guess is too small). As I mentioned, I am expecting a value of r around 1.12 after the minimization, however, the result I get is [-0.71276042 1.71276042], so the distance is 2.4, which is clearly too big and I am wondering how can I fix it. I original doubt it might be the precision so I change the data type to float64, but the results are still the same. Any help will be greatly appreciated! Here is my code\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\nfrom jax import vmap\nimport matplotlib.pyplot as plt\n\nN = 2\njax.config.update(\"jax_enable_x64\", True)\nx_init = jnp.arange(N, dtype=jnp.float64)\nepsilon = 1\nsigma = 1\n\ndef potential(r):\n    r = jnp.where(r == 0, jnp.finfo(jnp.float64).eps, r)\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\ndef F(x):\n    # Compute all pairwise distances\n    r = jnp.abs(x[:, None] - x[None, :])\n    # Compute all pairwise potentials\n    pot = vmap(vmap(potential))(r)\n    # Exclude the diagonal (distance = 0) and avoid double-counting by taking upper triangular part\n    pot = jnp.triu(pot, 1)\n    # Sum up all the potentials\n    total = jnp.sum(pot)\n    return total\n\n# Minimize the function\nprint(F)\nresult = minimize(F, x_init, method='BFGS')\n\n# Extract the optimized positions of the points\nx_solutions = result.x\nprint(x_solutions)",
        "answers": [
            "This function is one that would be very difficult for any unconstrained gradient-based optimizer to correctly optimize. Holding one point at zero and varying the other point on the range (0, 10], we see the potential looks like this:\npython\nCopy\nr = jnp.linspace(0.1, 5.0, 1000)\nplt.plot(r, jax.vmap(lambda ri: F(jnp.array([0, ri])))(r))\nplt.ylim(-2, 10)\nTo the left of the minimum, the gradient quickly diverges to negative infinity, meaning for nearly any reasonable step size, the optimizer will likely overshoot the minimum. Then on the right side, if the optimizer goes even a few units too far, the gradient tends to zero, meaning for nearly any reasonable step size, the optimizer will get stuck in a regime where the potential has almost no variation.\nAdd to this the fact that you've set up the model with two degrees of freedom in a degenerate potential, and it's not surprising that gradient-based optimization methods are failing.\nYou can make some progress here by minimizing the log of the shifted potential, which has the effect of smoothing the steep gradients, and lets the BFGS minimizer find an expected minimum:\npython\nCopy\nresult = minimize(lambda x: jnp.log(2 + F(x)), x_init, method='BFGS')\nprint(result.x)\n# [-0.06123102  1.06123102]\nBut in general my suggestion would probably be to opt for a constrained optimization approach instead, perhaps one of the JAXOpt constrained optimization methods, where you can rule-out problematic regions of the parameter space."
        ],
        "link": "https://stackoverflow.com/questions/76353392/jax-code-for-minimizing-lennard-jones-potential-for-2-points-in-python-gives-une"
    },
    {
        "title": "jax.numpy.delete assume_unique_indices unexpected keyword argument",
        "question": "I can not seem to get the assume_unique_indices from jax.numpy working. According to the documentation here, the jnp.delete has a keyword argument \"assume_unique_indices\" that is supposed to make this function jit compatible when we are sure that the index array is an integer array and is guaranteed to contain unique entries.\nHere is an minimum reproducible example\npython\nCopy\nimport jax\n\narr = jnp.array([1, 2, 3, 4, 5])\nidx = jnp.array([0, 2, 4])\n\nprint(jax.__version__)\n\n# Delete elements at indices idx\nout = jax.numpy.delete(arr, idx, assume_unique_indices=True)\n\nprint(out) # [2 4]\nThe error message\npython\nCopy\n0.4.8\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-bf0277118922> in <cell line: 9>()\n      7 \n      8 # Delete elements at indices idx\n----> 9 out = jax.numpy.delete(arr, idx, assume_unique_indices=False)\n     10 \n     11 print(out) # [2 4]\n\nTypeError: delete() got an unexpected keyword argument 'assume_unique_indices'\nDeleting the assume_unique_indices made it work as expected.",
        "answers": [
            "assume_unique_indices was added in https://github.com/google/jax/pull/15671, after JAX version 0.4.8 was released. If you update to version 0.4.9 or newer, your code should work.",
            "Ok, as it turns out, the 'assume_unique_indices' is only added rather recently, updating to jax version 0.4.10 did the trick"
        ],
        "link": "https://stackoverflow.com/questions/76244047/jax-numpy-delete-assume-unique-indices-unexpected-keyword-argument"
    },
    {
        "title": "CuDNN error when running JAX on GPU with apptainer",
        "question": "I have an application written in Python 3.10+ with JAX that I would like to run on GPU. I can run containers on my local computer cluster using apptainer (but not Docker) which has an NVIDIA A40 GPU. Based on the proposed Dockerfile for JAX I made an Ubuntu-based image from the following Dockerfile:\nFROM nvidia/cuda:11.8.0-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI then convert the Docker image to an apptainer image using apptainer pull docker://my-image and then run the container using apptainer run --nv docker://my-image as described in the apptainer GPU docs.\nError\nWhen I run the following code\npython\nCopy\nimport jax\njax.numpy.array(1.)\nJAX immediately crashes with the following error message:\n2023-05-11 14:41:50.580441: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1993, in array\n    out_array: Array = lax_internal._convert_element_type(out, dtype, weak_type=weak_type)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\", line 537, in _convert_element_type\n    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 360, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 363, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 817, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 117, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 253, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 246, in cached\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 208, in xla_primitive_callable\n    compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 254, in _xla_callable_uncached\n    return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 2816, in compile\n    self._executable = UnloadedMeshExecutable.from_hlo(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 3028, in from_hlo\n    xla_executable = dispatch.compile_or_get_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 526, in compile_or_get_cached\n    return backend_compile(backend, serialized_computation, compile_options,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 471, in backend_compile\n    return backend.compile(built_c, compile_options=options)\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I've tried\nBased on a github thread with a similar error message (https://github.com/google/jax/issues/4920), I have tried to add some CUDA paths:\nbash\nCopy\nexport PATH=/usr/local/cuda-11/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-11/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\nHowever, this did not resolve my problem.\nLocal docker container works without gpu\nWhen I test the image built from the Dockerfile on my local machine without GPU, everything works fine:\nbash\nCopy\n$ docker run -ti my-image python3 -c 'import jax; jax.numpy.array(1.)'\n$\nApptainer container detects GPU's\nI can confirm that the GPU's are detected in the apptainer container. I get the following output when I run nvidia-smi:\n$ apptainer run --nv docker://my-image nvidia-smi\nINFO:    Using cached SIF image\n\n==========\n== CUDA ==\n==========\n\nCUDA Version 11.8.0\n\nContainer image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nThu May 11 14:34:18 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A40          On   | 00000000:00:08.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A40          On   | 00000000:00:09.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A40          On   | 00000000:00:0A.0 Off |                    0 |\n|  0%   30C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A40          On   | 00000000:00:0B.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   4  NVIDIA A40          On   | 00000000:00:0C.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   5  NVIDIA A40          On   | 00000000:00:0D.0 Off |                    0 |\n|  0%   32C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   6  NVIDIA A40          On   | 00000000:00:0E.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   7  NVIDIA A40          On   | 00000000:00:0F.0 Off |                    0 |\n|  0%   30C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nEdit\nCuda 11 image with CuDNN 8.7 gives different error\nWhen I use a different base image with cudnn 8.7.0:\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI get a different error:\n$ apptainer run --nv docker://my-image python3 -c 'import jax; jax.numpy.array(1.)'\n\nCould not load library libcudnn_ops_infer.so.8. Error: libnvrtc.so: cannot open shared object file: No such file or directory\nAborted (core dumped)",
        "answers": [
            "Based on the pointers of jakevdp I managed to find a solution. What was needed was:\nThe CUDA 11 image with CuDNN 8.7.\nThe devel instead of the runtime image.\nTogether, I could succesfully run JAX on apptainer with the following Dockerfile:\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/76229252/cudnn-error-when-running-jax-on-gpu-with-apptainer"
    },
    {
        "title": "Local Jax variable not updating in `jit`ted function, but updating in standard?",
        "question": "So, I've got some code and I could really use help deciphering the behavior and how to get it to do what I want.\nSee my code as follows:\npython\nCopy\nfrom typing import Callable, List\n\nimport chex\nimport jax.numpy as jnp\nimport jax\n\nWeights = List[jnp.ndarray]\n\n\n@chex.dataclass(frozen=True)\nclass Model:\n    mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    jitted_mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    weight_updater: Callable[\n        [jnp.ndarray], None\n    ]\n\n\ndef create_weight():\n    return jnp.ones((2, 5))\n\n\ndef wrapper():\n    weights = create_weight()\n\n    def mult(input_var):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var):\n        return weights.dot(input_var)\n\n    def update_locally_created(new_weights):\n        nonlocal weights\n        weights = new_weights\n        return weights\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=update_locally_created\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult)))\n        print(jnp.sum(tester.jitted_mult(to_mult)))\n\n        if i % 2 == 0:\n            tester.weight_updater(jnp.zeros((2, 5)))\n        else:\n            tester.weight_updater(jnp.ones((2, 5)))\n\n        print(\"*\" * 10)\nTL;DR I'm defining some \"weights\" within a function closure, and I'm trying to modify the weights via a nonlocal. The problem seems to be that the jit-ted version (jitted_mult of the function doesn't recognize the \"updated\" weights, whereas the non-jit function (mult) does.\nWhat can I do to make it recognize the update? I think that I might be able to do what Build your own Haiku does, but that seems like a lot of work for an experiment",
        "answers": [
            "This is working as expected: the reason it's not respecting the update is because your function is not pure (see JAX Sharp Bits: Pure Functions). In your case, the function is not pure because the output depends on an input that is not explicitly passed to the function. This violates the assumptions made by jit and other JAX transformations, which leads to unexpected behavior.\nTo fix it I would make this implicit input explicit, so that your function is pure. It might look something like this:\npython\nCopy\ndef wrapper():\n    def mult(input_var, weights):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var, weights):\n        return weights.dot(input_var)\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=None\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    weights = create_weight()\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult, weights)))\n        print(jnp.sum(tester.jitted_mult(to_mult, weights)))\n\n        if i % 2 == 0:\n            weights = jnp.zeros((2, 5))\n        else:\n            weights = jnp.ones((2, 5))\n\n        print(\"*\" * 10)"
        ],
        "link": "https://stackoverflow.com/questions/76205477/local-jax-variable-not-updating-in-jitted-function-but-updating-in-standard"
    },
    {
        "title": "No module named 'jax.experimental.global_device_array' when running the official Flax Example on Colab with V100",
        "question": "I have been trying to understand this official flax example, based on a Coalb pro+ account with V100. When I execute the command python main.py --workdir=./imagenet --config=configs/v100_x8.py , the returned error is\nFile \"/content/FlaxImageNet/main.py\", line 29, in <module>\nimport train\nFile \"/content/FlaxImageNet/train.py\", line 30, in <module>\nfrom flax.training import checkpoints\nFile \"/usr/local/lib/python3.10/dist-packages/flax/training/checkpoints.py\", line 34, \nin <module>\nfrom jax.experimental.global_device_array import GlobalDeviceArray\nModuleNotFoundError: No module named 'jax.experimental.global_device_array'\nI am not sure whether global_device_array has been moved from jax.experimental package or it is no longer needed or replaced by other equivalent methods.",
        "answers": [
            "GlobalDeviceArray was deprecated in JAX version 0.4.1 and removed in JAX version 0.4.7.\nWith that in mind, it seems the code in question requires JAX version 0.4.6 or older. You might consider reporting this incompatibility to the flax project: http://github.com/google/flax/."
        ],
        "link": "https://stackoverflow.com/questions/76191911/no-module-named-jax-experimental-global-device-array-when-running-the-official"
    },
    {
        "title": "Fail to understand the usage of partial argument in Flax Resnet Official Example",
        "question": "I have been trying to understand this official example. However, I am very confused about the use of partial in two places.\nFor example, in line 94, we have the following:\nconv = partial(self.conv, use_bias=False, dtype=self.dtype)\nI am not sure why it is possible to apply a partial to a class, and where later in the code we fill in the missing argument (if we need to).\nComing to the final definition, I am even more confused. For example,\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n               block_cls=ResNetBlock)\nWhere do we apply the argument such as stage_size=[2,2,2,2]?\nThank you",
        "answers": [
            "functools.partial will partially evaluate a function, binding arguments to it for when it is called later. here's an example of it being used with a function:\npython\nCopy\nfrom functools import partial\n\ndef f(x, y, z):\n  print(f\"{x=} {y=} {z=}\")\n\ng = partial(f, 1, z=3)\ng(2)\n# x=1 y=2 z=3\nand here is an example of it being used on a class constructor:\npython\nCopy\nfrom typing import NamedTuple\n\nclass MyClass(NamedTuple):\n  a: int\n  b: int\n  c: int\n\nmake_class = partial(MyClass, 1, c=3)\nprint(make_class(b=2))\n# MyClass(a=1, b=2, c=3)\nThe use in the flax example is conceptually the same: partial(f) returns a function that when called, applies the bound arguments to the original callable, whether it is a function, a method, or a class constructor.\nFor example, the ResNet18 function created here:\npython\nCopy\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n                   block_cls=ResNetBlock)\nis a partially-evaluated ResNet constructor, and the function is called in a test here:\npython\nCopy\n  @parameterized.product(\n      model=(models.ResNet18, models.ResNet18Local)\n  )\n  def test_resnet_18_v1_model(self, model):\n    \"\"\"Tests ResNet18 V1 model definition and output (variables).\"\"\"\n    rng = jax.random.PRNGKey(0)\n    model_def = model(num_classes=2, dtype=jnp.float32)\n    variables = model_def.init(\n        rng, jnp.ones((1, 64, 64, 3), jnp.float32))\n\n    self.assertLen(variables, 2)\n    self.assertLen(variables['params'], 11)\nmodel here is the partially evaluated function ResNet18, and when it is called it returns the fully-instantiated ResNet object with the parameters specified in the ResNet18 partial definition."
        ],
        "link": "https://stackoverflow.com/questions/76178123/fail-to-understand-the-usage-of-partial-argument-in-flax-resnet-official-example"
    },
    {
        "title": "Calling an initialized function from a list inside a jitted JAX function",
        "question": "Given is a jitted function, which is calling another function that maps over a batch, which again calls a function, i.e. inner_function, to compute a certain property. Also given is a list of initialized functions intialized_functions_dic, from which we want to call the proper initialized function based on some information passed as argument, e.g. info_1. Is there a way to make this work? Thanks in advance.\npython\nCopy\ninitialized_functions_dic = {1:init_function1, 2:init_function_2, 3:init_function_3}\n\n\ndef inner_function(info_1, info_2, info_3):\n    return 5 + outside_dic[info_1]\nCalling outside_dic[info_1] will throw an error due to trying to access a dictionary with a traced value.\nTrying to pass info_1 as static_argnums also fails due to info_1 being an unhashable type 'ArrayImpl'.",
        "answers": [
            "It sounds like you're looking for jax.lax.switch, which will switch between entries in a list of functions given an index:\npython\nCopy\ninitialized_functions = [init_function_1, init_function_2, init_function_3]\n\ndef inner_function(info_1, info_2, info_3):\n    idx = info_1 - 1  # lists are zero-indexed\n    args = (info_2, info_3) # tuple of arguments to pass to the function\n    return 5 + lax.switch(idx, initialized_functions, *args)"
        ],
        "link": "https://stackoverflow.com/questions/76094143/calling-an-initialized-function-from-a-list-inside-a-jitted-jax-function"
    },
    {
        "title": "JAX VMAP Parallelization Details",
        "question": "I was wondering how vmap's internals work. When I vectorize code using jax.lax.map, I know that each element is executed consecutively. However when I use vmap I execute the vectorized operation apparently in parallel. Can someone provide me with a more detailed explanation of how the parallelization works? How does Jax determine the number of parallel processes, and can this behaviour be influenced by the user?\nThanks in advance.",
        "answers": [
            "jax.vmap is a vectorizing/batching transform, not a parallelizing transform. Internally, it converts an unbatched function to a batched function, lowering to efficient primitive calls rather than an explicit map or loop.\nFor example, here is a simple function, where we create both a manually-looped and an automatically vectorized batched version:\npython\nCopy\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\n\ndef f(x, y):\n  return x @ y\n\nnum_batches = 3\nnum_entries = 5\n\nnp.random.seed(0)\nx = np.random.rand(num_batches, num_entries)\ny = np.random.rand(num_batches, num_entries)\n\n\nf_loop = lambda x, y: jnp.stack([f(xi, yi) for xi, yi in zip(x, y)])\nf_vmap = jax.vmap(f)\n\nprint(f_loop(x, y))\n# [1.3567398 2.1908383 1.6315514]\nprint(f_vmap(x, y))\n# [1.3567398 2.1908383 1.6315514]\nThese return the same results (by design), but they are quite different under the hood; by printing the jaxpr, we see that the loop version requires a long sequence of XLA calls:\npython\nCopy\nprint(jax.make_jaxpr(f_loop)(x, y))\npython\nCopy\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] a\n    d:f32[5] = squeeze[dimensions=(0,)] c\n    e:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] b\n    f:f32[5] = squeeze[dimensions=(0,)] e\n    g:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d f\n    h:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] a\n    i:f32[5] = squeeze[dimensions=(0,)] h\n    j:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] b\n    k:f32[5] = squeeze[dimensions=(0,)] j\n    l:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] i k\n    m:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] a\n    n:f32[5] = squeeze[dimensions=(0,)] m\n    o:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] b\n    p:f32[5] = squeeze[dimensions=(0,)] o\n    q:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] n p\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n    s:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] l\n    t:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] q\n    u:f32[3] = concatenate[dimension=0] r s t\n  in (u,) }\nOn the other hand, the vmap version lowers to just a single generalized dot product:\npython\nCopy\nprint(jax.make_jaxpr(f_vmap)(x, y))\npython\nCopy\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[3] = dot_general[dimension_numbers=(([1], [1]), ([0], [0]))] a b\n  in (c,) }\nNotice that there is nothing concerning parallelization here; rather we've automatically created an efficient batched version of our operation. The same is true of vmap applied to more complicated functions: it does not involve parallelization, rather it outputs an efficient batched version of your operation in an automated manner."
        ],
        "link": "https://stackoverflow.com/questions/75891826/jax-vmap-parallelization-details"
    },
    {
        "title": "JAX performance problems",
        "question": "I am obviously not following best practices, but maybe that's because I don't know what they are. Anyway, my goal is to generate a tubular neighborhood about a curve in three dimensions. A curve is give by an array of length three f(t) = jnp.array([x(t), y(t), z(t)]).\nNow, first we compute the unit tangent:\npython\nCopy\ndef get_uvec2(f):\n  tanvec = jacfwd(f)\n  return lambda x: tanvec(x)/jnp.linalg.norm(tanvec(x))\nNext, we compute the derivative of the tangent:\npython\nCopy\ndef get_cvec(f):\n  return get_uvec2(get_uvec2(f))\nThird, we compute the orthogonal frame at a point:\npython\nCopy\ndef get_frame(f):\n  tt = get_uvec2(f)\n  tt2 = get_cvec(f)\n  def first2(t):\n    x = tt(t)\n    y = tt2(t)\n    tt3 = (jnp.cross(x, y))\n    return jnp.array([x, y, tt3])\n  return first2\nwhich we use to generate a point in the circle around a given point:\npython\nCopy\ndef get_point(frame, s):\n  v1 = frame[1, :]\n  v2 = frame[2, :]\n  return jnp.cos(s) * v1 + jnp.sin(s) * v2\nAnd now we generate the point on the tubular neighborhood corresponding to a pair of parameters:\npython\nCopy\ndef get_grid(f, eps):\n  ffunc = get_frame(f)\n  def grid(t, s):\n    base = f(t)\n    frame = ffunc(t)\n    return base + eps * get_point(frame, s)\n  return grid\nAnd finally, we put it all together:\npython\nCopy\ndef get_reg_grid(f, num1, num2, eps):\n  plist = []\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  for t in tarray:\n    for s in sarray:\n      plist.append(g(t, s))\n  return jnp.vstack(plist)\nFinally, use it to compute the tubular neighborhood around a circle in the xy-plane:\npython\nCopy\nf1 = lambda x: jnp.array([jnp.cos(2 * jnp.pi * x), jnp.sin(2 * jnp.pi * x), 0.0])\n\nfff = np.array(get_reg_grid(f1, 200, 200, 0.1))\nThe good news is that it all works. The bad news is that this computation takes well over an hour. Where did I go wrong?",
        "answers": [
            "JAX and numpy share one key rule-of-thumb for getting good performance: if you are writing for loops over array values, your code will probably be slow.\nTo make your code more performant, you should replace your loops with vectorized operations. One nice feature of JAX is jax.vmap, a vectorizing transform which makes this relatively easy. You can also use jax.jit to JIT-compile your function and get even faster execution.\nHere's a modified version of your get_reg_grid function that returns the same result with much faster execution:\npython\nCopy\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['f', 'num1', 'num2'])\ndef get_reg_grid(f, num1, num2, eps):\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  g = jax.vmap(g, in_axes=(None, 0))\n  g = jax.vmap(g, in_axes=(0, None))\n  return jnp.vstack(g(tarray, sarray))\nWith this approach, your code executes in about 300 microseconds:\npython\nCopy\n%timeit get_reg_grid(f1, 200, 200, 0.1).block_until_ready()\n# 296 µs ± 157 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
        ],
        "link": "https://stackoverflow.com/questions/75872342/jax-performance-problems"
    },
    {
        "title": "using jax lax scan with inputs that don't change across iterations within scan but are different each time scan is called",
        "question": "Jax lax scan operates on a function that takes two arguments, a carry and a sequence of inputs. I am wondering how scan should be called if some inputs don't change across iterations of the scan. Naively, I could create a sequence of identical inputs, but this seems wasteful/redundant and more importantly, this isn't always possible, as scan can only scan over arrays. For example, one of the inputs I want to pass to my function is a train state (e.g. from flax.training import train_state) that contains my model and its parameters, which cannot be put into array. As I say in the title, these inputs may also change each time I call scan (e.g. the model parameters will change).\nAny ideas on how best to do this?\nThanks.",
        "answers": [
            "In general, you have three possible approaches for this:\nConvert the single value into a sequence of identical inputs to scan over\nPut the single value in the carry to carry it along to each step of the scan\nClose over the single value\nHere are three examples of a computation using these strategies:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\na = jnp.arange(5)\nb = 2\npython\nCopy\n# Strategy 1: duplicate b across sequence\ndef f(carry, xs):\n  a, b = xs\n  result = a * b\n  return carry + result, result\n\nb_seq = jnp.full_like(a, b)\n\ntotal, cumulative = jax.lax.scan(f, 0, (a, b_seq))\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\npython\nCopy\n# Strategy 2: put b in the carry\ndef f(carry, xs):\n  carry, b = carry\n  a = xs\n  result = a * b\n  return (carry + result, b), result\n\n(total, _), cumulative = jax.lax.scan(f, (0, b), a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\npython\nCopy\n# Strategy 3: close over b\nfrom functools import partial\n\ndef f(carry, xs, b):\n  a = xs\n  result = a * b\n  return carry + result, result\n\ntotal, cumulative = jax.lax.scan(partial(f, b=b), 0, a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\nWhich you use probably depends on the context of where you are using it, but I personally think closure (option 3) is probably the cleanest approach."
        ],
        "link": "https://stackoverflow.com/questions/75776268/using-jax-lax-scan-with-inputs-that-dont-change-across-iterations-within-scan-b"
    },
    {
        "title": "What is the correct way to define a vectorized (jax.vmap) function in a class?",
        "question": "I want to add a function, which is vectorized by jax.vmap, as a class method. However, I am not sure where to define this function within the class. My main goal is to avoid, that the function is being redefined each time I call the class method.\nHere is a minimal example for a class that counts how often a value occurs in a jnp.array, with a non-vectorized and vectorized version:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\nclass ValueCounter():\n\n    def __init__(self): # for completeness, not used\n        self.attribute_1 = None\n\n    @staticmethod\n    def _count_value_in_array( # non-vectorized function\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n\n    # here comes the vectorized function\n    def count_values_in_array(self, array: jnp.array, value_array: jnp.array) -> jnp.array:\n        \"\"\"Count how often each value in an array of values occurs in an array\"\"\"\n        count_value_in_array_vec = jax.vmap(\n            self._count_value_in_array, in_axes=(None, 0)\n        ) # vectorized function is defined again each time the function is called\n        return count_value_in_array_vec(array, value_array)\nExample output & input:\npython\nCopy\nvalue_counter = ValueCounter()\nvalue_counter.count_values_in_array(jnp.array([0, 1, 2, 2, 1, 1]), jnp.array([0, 1, 2]))\nResult (correct as expected)\npython\nCopy\nArray([1, 3, 2], dtype=int32)\nThe vectorized function count_value_in_array_vecis redefined each time count_values_in_array - which seems unnecessary to me. However, I am a bit stuck on how to avoid this. Does someone know how the vectorized function could be integrated into the class in a more elegant way?",
        "answers": [
            "You can decorate the static method directly; for example:\npython\nCopy\nfrom functools import partial\n\n# ...\n\n    @staticmethod\n    @partial(jax.vmap, in_axes=(None, 0))\n    def _count_value_in_array(\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n# ..."
        ],
        "link": "https://stackoverflow.com/questions/75696639/what-is-the-correct-way-to-define-a-vectorized-jax-vmap-function-in-a-class"
    },
    {
        "title": "Python function minimization not changing optimization variables",
        "question": "I need to minimize a simple function that divides two values. The optimization paramter x is a (n,m) numpy array from which I calculate a float.\npython\nCopy\n# An initial value\nnormX0 = calculate_normX(x_start)\n\ndef objective(x) -> float:\n    \"\"\"Objective function \"\"\"\n    x = x.reshape((n,m))\n    normX = calculate_normX(x)\n    return -(float(normX) / float(normX0))\ndef calculate_normX() is a wrapper function to an external (Java-)API that takes the ndarray as an input and outputs a float, in this case, the norm of a vector. For the optimization, I was using jax and jaxopt, since it supports automatic differentiation of objective.\npython\nCopy\nsolver = NonlinearCG(fun=objective, maxiter=5, verbose=True)\nres = solver.run(x.flatten())\nor the regular scipy minimize\npython\nCopy\nobjective_jac = jax.jacrev(objective)\nminimize(objective, jac=objective_jac, x0=`x.flatten(), method='L-BFGS-B', options={'maxiter': 2})\nIn both cases, however, x is not changed during the optimization step. Even initializing x with random values the optimizer does not seem to work. I also tried other solvers like Jaxopt NonlinearCG. What am I doing wrong?",
        "answers": [
            "The external, non-JAX function call is almost certainly the source of the problem. Non-JAX function calls within JAX transforms like jacrev effectively get replaced with a trace-time constant (and in most cases will error), and so it makes sense that your optimization will not change its value.\nThe best approach would be to define your calculate_normX function using JAX rather than calling out to an external API, and then everything should work automatically.\nIf you must call to an external API, one way to do this in JAX is to use pure_callback along with custom_jvp to define the autodiff rule for your external callback. There is an example of this in the JAX docs: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html#example-pure-callback-with-custom-jvp"
        ],
        "link": "https://stackoverflow.com/questions/75651326/python-function-minimization-not-changing-optimization-variables"
    },
    {
        "title": "Nested vmap in pmap - JAX",
        "question": "I currently can run simulations in parallel on one GPU using vmap. To speed things up, I want to batch the simulations over multiple GPU devices using pmap. However, when pmapping the vmapped function I get a tracing error.\nThe code I use to get a trajectory state is:\npython\nCopy\ntraj_state = vmap(run_trajectory, in_axes=(0, None, 0))(sim_state, timings, lambda_array)\n                                                                        \nwhere lambda_array parameterises each simulation, which is run by the function run_trajectory which runs a single simulation. I then try to nest this inside a pmap:\npython\nCopy\npmap(vmap(run_trajectory, in_axes=(0, None, 0)),in_axes=(0, None, 0))(reshaped_sim_state, timings, reshaped_lambda_array)                                                                                       \nIn doing so I get the error:\npython\nCopy\nWhile tracing the function run_trajectory for pmap, this concrete value was not available in Python because it depends on the value of the argument 'timings'.\nI'm quite new to JAX and although there are documentations on errors with traced values, I'm not very sure on how to navigate this problem.",
        "answers": [
            "vmap and pmap have slightly different APIs when it comes to in_axes. In vmap, setting in_axes=None causes inputs to be unmapped and static (i.e. un-traced), while in pmap even inputs with in_axes=None will be unmapped but still traced:\npython\nCopy\nfrom jax import vmap, pmap\nimport jax.numpy as jnp\n\ndef f(x, condition):\n  # requires untraced condition:\n  return x if condition else x + 1\n\nx = jnp.arange(4)\nvmap(f, in_axes=(0, None))(x, True)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(f, in_axes=(0, None))(x, True)\n# ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: \nTo ensure that your variable is untraced in pmap, you can partially evaluate the function; for example:\npython\nCopy\nfrom functools import partial\n\nvmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\nIn your case, applying this solution might look like this:\npython\nCopy\ndef run(sim_state, lambda_array, timings=timings):\n  return run_trajectory(sim_state, timings, lambda_array)\n\nvmap(run)(sim_state, lambda_array)\n\npmap(vmap(run))(reshaped_sim_state, reshaped_lambda_array)",
            "I can seemingly avoid this problem by passing the timing values prior to vmapping using partial, that is:\npython\nCopy\nrun_trajectory = partial(run_trajectory, starting_time=timings)\n\ntraj_state = pmap(vmap(run_trajectory, in_axes=(0, 0)))(reshaped_sim_state, reshaped_lambda_array)"
        ],
        "link": "https://stackoverflow.com/questions/75625305/nested-vmap-in-pmap-jax"
    },
    {
        "title": "Defining the correct vectorization axes for JAX vmap with arrays of different shapes and sizes",
        "question": "Following the answer to this post, the following function that 'f_switch' that dynamically switches between multiple functions based on an index array is defined (based on 'jax.lax.switch'):\npython\nCopy\nimport jax\nfrom jax import vmap;\nimport jax.random as random\n\ndef g_0(x, y, z, u): return x + y + z + u\ndef g_1(x, y, z, u): return x * y * z * u\ndef g_2(x, y, z, u): return x - y + z - u\ndef g_3(x, y, z, u): return x / y / z / u\ng_i = [g_0, g_1, g_2, g_3]\n\n\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g)(i)\nWith input arrays: i_ar of shape (len_i,), x_ar y_ar and z_ar of shapes (len_xyz,) and u_ar of shape (len_u, len_xyz), out = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar), yields out of shape (len_i, len_xyz, len_u):\npython\nCopy\nlen_i = 50\ni_ar = random.randint(random.PRNGKey(5), shape=(len_i,), minval=0, maxval= len(g_i)) #related to \n\nlen_xyz = 3000\nx_ar = random.uniform(random.PRNGKey(0), shape=(len_xyz,))\ny_ar = random.uniform(random.PRNGKey(1), shape=(len_xyz,))\nz_ar = random.uniform(random.PRNGKey(2), shape=(len_xyz,))\n\nlen_u = 1000\nu_0 = random.uniform(random.PRNGKey(3), shape=(len_u,))\nu_1 = jnp.repeat(u_0, len_xyz)\nu_ar = u_1.reshape(len_u, len_xyz)\n\nout = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nprint('The shape of out is', out.shape)\nThis worked. **But, How can the f_switch function be defined such that the result out of out = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar) has a shape of (j_len, k_len, l_len) when the function is applied along the following axes: i_ar[j], x_ar[j], y_ar[j, k], z_ar[j, k], u_ar[l]? I am not sure about how ** Examples of these input arrays are here:\npython\nCopy\nj_len = 82;\nk_len = 20;\nl_len = 100;\ni_ar = random.randint(random.PRNGKey(0), shape=(j_len,), minval=0, maxval=len(g_i))\nx_ar = random.uniform(random.PRNGKey(1), shape=(j_len,))\ny_ar = random.uniform(random.PRNGKey(2), shape=(j_len,k_len))\nz_ar = random.uniform(random.PRNGKey(3), shape=(j_len,k_len))\nu_ar = random.uniform(random.PRNGKey(4), shape=(l_len,))\nI tried to resolve this (i.e. with given input array to get output of shape: (j_len, k_len, l_len), with a nested vmap:\npython\nCopy\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  g_map = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n  wrapper = lambda x, y, z, u: g_map(i, x, y, z, u)\n  return jax.vmap(wrapper, in_axes=(0, None, None, None, 0))(x, y, z, u)\nand to broadcast u_ar: u_ar_broadcast = jnp.broadcast_to(u_ar, (j_len, k_len, l_len)), and then apply it inside of the original f_switch. But, both of these attempts failed.",
        "answers": [
            "It looks like maybe you want something like this?\npython\nCopy\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  g = jax.vmap(g, (None, None, None, None, 0))\n  g = jax.vmap(g, (None, None, 0, 0, None))\n  g = jax.vmap(g, (0, 0, 0, 0, None))\n  return g(i, x, y, z, u)\n\nout = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nprint(out.shape)\n# (82, 20, 100)\nYou should read the in_axes from bottom to top (because the bottom vmap is the outer one, and is therefore applied to the inputs first). Schematically, you can think of the effect of the maps on the shapes as something like this:\npython\nCopy\n                               (i[82], x[82], y[82,20], z[82,20], u[100])\n(0, 0, 0, 0, None)          -> (i,     x,     y[20],    z[20],    u[100])\n(None, None, 0, 0, None)    -> (i,     x,     y,        z,        u[100])\n(None, None, None, None, 0) -> (i,     x,     y,        z,        u)\nThat said, often it is easier to rely on numpy-style broadcasting rather than on multiple nested vmaps. For example, you could also do something like this:\npython\nCopy\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g, in_axes=(0, 0, 0, 0, None))(i, x, y, z, u)\n\nout = f_switch(i_ar, x_ar[:, None, None], y_ar[:, :, None], z_ar[:, :, None], u_ar)\nprint(out.shape)\n# (82, 20, 100)"
        ],
        "link": "https://stackoverflow.com/questions/75465486/defining-the-correct-vectorization-axes-for-jax-vmap-with-arrays-of-different-sh"
    },
    {
        "title": "using jax.vmap to vectorize along with broadcasting",
        "question": "Consider the following toy example:\npython\nCopy\nx = np.arange(3)\n# np.sum(np.sin(x - x[:, np.newaxis]), axis=1)\n\ncfun = lambda x: np.sum(np.sin(x - x[:, np.newaxis]), axis=1)\ncfuns = jax.vmap(cfun)\n\n# for a 2d x:\nx = np.arange(6).reshape(3,2)\ncfuns(x)\nwhere x-x[:,None] is the broadcasting part and give a 3x3 array. I want cfuns to be vectorized over each row of x.\nbash\nCopy\nThe numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int64[2,2])>with<BatchTrace(level=1/0)> with\n  val = Array([[[ 0,  1],\n        [-1,  0]],\n\n       [[ 0,  1],",
        "answers": [
            "JAX transformations like vmap, jit, grad, etc. are not compatible with standard numpy operations. Instead you should use jax.numpy, which provides a similar API built on JAX-compatible operations:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(3)\n\ncfun = lambda x: jnp.sum(jnp.sin(x - x[:, jnp.newaxis]), axis=1)\ncfuns = jax.vmap(cfun)\n\n# for a 2d x:\nx = jnp.arange(6).reshape(3,2)\n\nprint(cfuns(x))\n# [[ 0.84147096 -0.84147096]\n#  [ 0.84147096 -0.84147096]\n#  [ 0.84147096 -0.84147096]]"
        ],
        "link": "https://stackoverflow.com/questions/75386255/using-jax-vmap-to-vectorize-along-with-broadcasting"
    },
    {
        "title": "JAX best way to iterate RNGKeys?",
        "question": "In JAX I find myself needing a PRNGKey that changes on each iteration of a loop. I'm not sure of the best pattern. I've considered\na) split\npython\nCopy\nfor i in range(N):\n  rng, _ = jax.random.split(rng)\n\n  # Alternatively.\n  rng = jax.random.split(rng, 1)[0]\nb) fold_in\npython\nCopy\nfor i in range(N):\n  rng = jax.random.fold_in(rng, i)\nc) use the iterator index? seems bad since the rng doesn't depend on a prior rng.\npython\nCopy\nfor i in range(N):\n rng = jax.random.PRNGKey(i)\nWhich of these is the best pattern and why? I am leaning towards (b) as it maintains dependency on the previous rng key (e.g. passed in as an argument) but im not sure if this is really the intended use-case for jax.random.fold_in",
        "answers": [
            "JAX docs (including the PRNG design doc) recommend something similar to (a):\npython\nCopy\nfor i in range(N):\n  key, subkey = jax.random.split(key)\n  values = random.uniform(subkey, shape)\n  # key carries over to the next iteration\nThe reason this is better than splitting and throwing away the subkey is that it ensures that the streams in each iteration are independent.\nYour option (b) is also safe, and in fact is the pattern that developers had in mind when creating fold_in (see e.g. https://github.com/google/jax/discussions/12395).\nIf you have a fixed number of iterations, it may be better to do all the splits once; for example:\npython\nCopy\nfor i, key in enumerate(random.split(key, N)):\n  values = random.uniform(key, shape)\nOr if your iterations do not have sequential dependence, it's better to use vmap to vectorize the operation:\npython\nCopy\ndef f(key):\n  return random.uniform(key, shape)\n\njax.vmap(f)(random.split(key, N))"
        ],
        "link": "https://stackoverflow.com/questions/75338838/jax-best-way-to-iterate-rngkeys"
    },
    {
        "title": "computational complexity of higher order derivatives with AD in jax",
        "question": "Let f: R -> R be an infinitely differentiable function. What is the computational complexity of calculating the first n derivatives of f in Jax? Naive chain rule would suggest that each multiplication gives a factor of 2 increase, hence the nth derivative would require at least 2^n more operations. I imagine though that clever manipulation of formal series would reduce the number of required calculations and eliminate duplications, esspecially if the derivaives are Jax jitted? Is there a different between the Jax, Tensorflow and Torch implementations?\nhttps://openreview.net/forum?id=SkxEF3FNPH discusses this topic, but doesn t provide a computational complexity.",
        "answers": [
            "What is the computational complexity of calculating the first n derivatives of f in Jax?\nThere's not much you can say in general about computational complexity of Nth derivatives. For example, with a function like jnp.sin, the Nth derivative is O[1], oscillating between negative and positive sin and cos calls as N grows. For an order-k polynomial, the Nth derivative is O[0] for N > k. Other functions may have complexity that is linear or polynomial or even exponential with N depending on the operations they contain.\nI imagine though that clever manipulation of formal series would reduce the number of required calculations and eliminate duplications, esspecially if the derivaives are Jax jitted\nYou imagine correctly! One implementation of this idea is the jax.experimental.jet module, which is an experimental transform designed for computing higher-order derivatives efficiently and accurately. It doesn't cover all JAX functions, but it may be complete enough to do what you have in mind.",
            "If L is the complexity of evaluating the scalar function f, then L*(n+1)^2 is an upper bound for the complexity of finding the first n derivatives of f as coefficients of a truncated Taylor series.\nThe general idea is that each elementary function can be implemented for truncated Taylor series in the equivalent of one or two truncated series multiplications."
        ],
        "link": "https://stackoverflow.com/questions/75324482/computational-complexity-of-higher-order-derivatives-with-ad-in-jax"
    },
    {
        "title": "Why is this JAX jitted function so much slower than the non-jitted JAX version?",
        "question": "I'm in the process of rewriting some code from pure Python to JAX. I have a function that I need to call a lot. Why is the jitted version of the following function so much slower than the non-jitted version?\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef regular(M,R,a):\n    return (3+a)*M*R**a / (4*jnp.pi * R**(3+a))\n\n@jit\ndef jitted(M,R,a):\n    return (3+a)*M*R**a / (4*jnp.pi * R**(3+a))\n\n%timeit regular(1e10,100.,-2.)\n# 346 ns ± 2.07 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n%timeit jitted(1e10,100.,-2.)\n# 4.2 µs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)",
        "answers": [
            "There is some helpful information on benchmarking in JAX's FAQ: Benchmarking JAX Code: in particular note the discussion of JAX dispatch overhead for individual operations.\nRegarding your particular example, the first thing to point out is that you're not comparing jit-compiled JAX code against non-jit-compiled JAX code; you're comparing jit-compiled JAX code against pure Python code. Because you're passing python scalars to the function, none of the operations in regular have anything to do with JAX (even jnp.pi is just a Python float), so you're just executing built-in Python arithmetic operators on Python scalars.\nIf you want to compare JAX jit to non-jit code, you can use JAX values rather than scalar values as inputs; for example:\npython\nCopy\na = jnp.array(1e10)\nb = jnp.array(100.)\nc = jnp.array(-2.)\n\n%timeit regular(a, b, c).block_until_ready()\n# 86.8 µs ± 1.56 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n%timeit jitted(a, b, c).block_until_ready()\n# 3.71 µs ± 59.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\nHere you see that JIT gives you about a 20x speedup over un-jitted JAX code.\nBut JIT or not, why is JAX so much slower than the native Python version? The reason is because each JAX function call incurs a few microseconds of dispatch overhead, while each native Python operation has much less dispatch overhead. You've written a function where the actual computations are so small they are virtually free; to first order all you are measuring is dispatch overhead.\nIn situations JAX was designed for (executing JIT-compiled sequences of operations over large arrays on accelerators), this one-time, few-microsecond dispatch cost is generally not significant in comparison to the full computation."
        ],
        "link": "https://stackoverflow.com/questions/75318282/why-is-this-jax-jitted-function-so-much-slower-than-the-non-jitted-jax-version"
    },
    {
        "title": "jax automatic differentiation",
        "question": "I have the following three functions implements in JAX.\npython\nCopy\ndef helper_1(params1):\n   ...calculations...\n   return z\n\ndef helper_2(z, params2):\n   ...calculations...\n   return y\n\ndef main(params1, params2):\n   z = helper_1(params1)\n   y = helper_2(z, params2)\n   return z,y\nI am interested in the partial derivatives of the output from main, i.e. z and y, with respect to both params1 and params2. As params1 and params2 are low dimensional and z and y are high dimensional, I am using the jax.jacfwd function.\nWhen calling\npython\nCopy\njax.jacfwd(main,argnums=(0,1))(params1,params2)\nJax computes the derivatives of z with respect to params1 (and params2, which in this case is just a bunch of zeros). My question is: does Jax recompute dz/d_param1 for the derivatives of y with respect to params1 and params2, or does it somehow figure out this has already been computed?\nI don't know if this is relevant, but the 'helper_1' function contains functions from the TensorFlow library for Jax. Thanks!",
        "answers": [
            "In general, in the situation you describe JAX's forward-mode autodiff approach will re-use the derivative of z when computing the derivative of y. If you wish, you can confirm this by looking at the jaxpr of your differentiated function:\npython\nCopy\nprint(jax.make_jaxpr(jax.jacfwd(main, (0, 1)))(params1, params2))\nThough if your function is more than moderately complicated, the output might be hard to understand.\nAs a general note, though, JAX's autodiff implementation does tend to produce a small number of unnecessary or duplicated computations. As a simple example consider this:\npython\nCopy\nimport jax\nprint(jax.make_jaxpr(jax.grad(jax.lax.sin))(1.0))\n# { lambda ; a:f32[]. let\n#     _:f32[] = sin a\n#     b:f32[] = cos a\n#     c:f32[] = mul 1.0 b\n#   in (c,) }\nHere the primal value sin(a) is computed even though it is never used in computing the final output.\nIn practice this can be addressed by wrapping your computation in jit, in which case the XLA compiler takes care of optimization, including dead code elimination and de-duplication when applicable:\npython\nCopy\nresult = jit(jax.jacfwd(main, (0, 1)))(params1, params2)"
        ],
        "link": "https://stackoverflow.com/questions/75144091/jax-automatic-differentiation"
    },
    {
        "title": "Given batch of samples find x,y positions - python, jax",
        "question": "I would like to find the positions of where the is a '1' in the following batch of sampled array with dimension [batch, 4,4] = [2,4,4].\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\na = jnp.array([[[0., 0., 0., 1.],\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.],\n                [0., 0., 1., 1.]],\n             \n               [[1., 0., 1., 0.],\n                [1., 0., 0., 0.],\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.]]])\nI tried going through the dimension of batches (with vmap) and use the jax function to find the coordinates with\npython\nCopy\nb = jax.vmap(jnp.where)(a)\nprint('b', b)\nbut I get an error that I don't know how to fix:\npython\nCopy\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThis Tracer was created on line /home/imi/Desktop/Backflow/backflow/src/debug.py:17 (<module>)\nI expect the following output:\npython\nCopy\nb = [[[0,3], [2,1],[2,3],[3,2],[3,3]],\n\n     [[0,0],[0,2],[1,0],[3,1],[3,3]]\nThe first line of [x,y] coordinates correspond to the positions of where there is a '1' in the first batch, and for the second line in the second batch.",
        "answers": [
            "JAX transformations like vmap require arrays to be statically-sized, so there is no way to do exactly the computation you have in mind (because the number of 1 entries, and thus the size of the output array, is data-dependent).\nBut if you know a priori that there are five entries per batch, you can do something like this:\npython\nCopy\nfrom functools import partial\nindices = jax.vmap(partial(jnp.where, size=5))(a)\nprint(jnp.stack(indices, axis=2))\npython\nCopy\n[[[0 3]\n  [2 1]\n  [2 3]\n  [3 2]\n  [3 3]]\n\n [[0 0]\n  [0 2]\n  [1 0]\n  [3 1]\n  [3 3]]]\nIf you don't know a priori how many 1 entries there are, then you have a few options: one is to avoid JAX transformations and call an un-transformed jnp.where on each batch:\npython\nCopy\nresult = [jnp.column_stack(jnp.where(b)) for b in a]\nprint(result)\npython\nCopy\n[DeviceArray([[0, 3],\n             [2, 1],\n             [2, 3],\n             [3, 2],\n             [3, 3]], dtype=int32), DeviceArray([[0, 0],\n             [0, 2],\n             [1, 0],\n             [3, 1],\n             [3, 3]], dtype=int32)]\nNote that for this case, it's not possible in general to store the results in a single array, because there may be different numbers of 1 entries in each batch, and JAX does not support ragged arrays.\nThe other option is to set the size to some maximum value, and output padded results:\npython\nCopy\nmax_size = a[0].size  # size of slice is the upper bound\nfill_value = a[0].shape  # fill with out-of-bound indices\nindices = jax.vmap(partial(jnp.where, size=max_size, fill_value=fill_value))(a)\nprint(jnp.stack(indices, axis=2))\npython\nCopy\n[[[0 3]\n  [2 1]\n  [2 3]\n  [3 2]\n  [3 3]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]]\n\n [[0 0]\n  [0 2]\n  [1 0]\n  [3 1]\n  [3 3]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]]]\nWith padded results, you could then write the remainder of your code to anticipate these padded values."
        ],
        "link": "https://stackoverflow.com/questions/75134947/given-batch-of-samples-find-x-y-positions-python-jax"
    },
    {
        "title": "Negative Sampling in JAX",
        "question": "I'm implementing a negative sampling algorithm in JAX. The idea is to sample negatives from a range excluding from this range a number of non-acceptable outputs. My current solution is close to the following:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nmax_range = 5\nn_samples = 2\ntrue_cases = jnp.array(\n    [\n        [1,2],\n        [1,4],\n        [0,5]\n    ]\n)\n# i combine the true cases in a dictionary of the following form:\nnon_acceptable_as_negatives = {\n    0: jnp.array([5]),\n    1: jnp.array([2,4]),\n    2: jnp.array([]),\n    3: jnp.array([]),\n    4: jnp.array([]),\n    5: jnp.array([])\n}\nnegatives = []\nkey = jax.random.PRNGKey(42)\nfor i in true_cases[:,0]:\n    key,use_key  = jax.random.split(key,2)\n    p = jnp.ones((max_range+1,))\n    p = p.at[non_acceptable_as_negatives[int(i)]].set(0)\n    p = p / p.sum()\n    negatives.append(\n        jax.random.choice(use_key,\n            jnp.arange(max_range+1),\n            (1, n_samples),\n            replace=False,\n            p=p,\n            )\n    )\nHowever this seems\nrather complicated and\nis not very performant as the true cases in the original contain ~200_000 entries and max range is ~ 50_000.\nHow can I improve this solution? And is there a more JAX way to store arrays of varying size which I currently store in the non_acceptable_as_negatives dict?",
        "answers": [
            "You'll generally achieve better performance in JAX (as in NumPy) if you can avoid loops and use vectorized operations instead. If I'm understanding your function correctly, I think the following does roughly the same thing, but using vmap.\nSince JAX does not support dictionary lookups based on traced values, I replaced your dict with a padded array\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nmax_range = 5\nn_samples = 2\nfill_value = max_range + 1\n\ntrue_cases = jnp.array([\n  [1,2],\n  [1,4],\n  [0,5]\n])\n\nnon_acceptable_as_negatives = jnp.array([\n    [5, fill_value],\n    [2, 4],\n])\n\n@jax.vmap\ndef func(key, true_case):\n  p = jnp.ones(max_range + 1)\n  idx = true_cases[0]\n  replace = non_acceptable_as_negatives.at[idx].get(fill_value=fill_value)\n  p = p.at[replace].set(0, mode='drop')\n  return jax.random.choice(key, max_range + 1, (n_samples,), replace=False, p=p)\n\n\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, len(true_cases))\nresult = func(keys, true_cases)\nprint(result)\npython\nCopy\n[[3 1]\n [5 1]\n [1 5]]",
            "Jax array are immutable. It means that you can't edit it without copying the entire array. Here the main problem is that you create the vector p two times at each iteration. I advice you to compute the probabilities only once via numpy:\npython\nCopy\nimport numpy as np\n\nnon_acceptable_as_negatives = {\n    0: np.array([5]),\n    1: np.array([2,4]),\n    2: np.array([]),\n    3: np.array([]),\n    4: np.array([]),\n    5: np.array([])\n}\n\nprobas = np.ones((max_range+1, max_range+1))\nfor k, idx in non_acceptable_as_negatives.items():\n    for i in idx:\n        probas[k, i] = 0\nprobas = probas / probas.sum(axis=1, keepdims=True)\nprobas = jnp.array(probas)\nThen, to further speed-up the algorithm, you can compile the choice function. You can try:\npython\nCopy\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef sample(key, max_range, probas):\n    key, use_key  = jax.random.split(key, 2)\n    return jax.random.choice(use_key,\n            jnp.arange(max_range+1),\n            (1, n_samples),\n            replace=False,\n            p=probas[i],\n            ), key\nAnd finally:\npython\nCopy\nfor i in true_cases[:,0]:\n    neg, key = aux(key, max_range, probas)\n    negatives.append(neg)"
        ],
        "link": "https://stackoverflow.com/questions/75081519/negative-sampling-in-jax"
    },
    {
        "title": "Is there a way to return tuple of mixed variables in Jax helper function?",
        "question": "On my path learning Jax, I tried to achieve something like\ndef f(x):\n    return [x + 1, [1,2,3], \"Hello\"]\n\nx = 1\nnew_x, a_list, str = jnp.where(\n    x > 0,\n    test(x),\n    test(x + 1)\n)\nWell, Jax clearly does not support this. I tried searching online and went through quite a few docs, but I couldn't find a good answer.\nAny help on how can I achieve this in Jax?",
        "answers": [
            "In general, JAX functions like jnp.where only accept array arguments, not list or string arguments. Since you're using a function that is not compatible with JAX in the first place, it might be better to just avoid JAX conditionals and just use standard Python conditionals instead:\npython\nCopy\nimport jax.numpy as jnp\n\ndef f(x):\n    return [x + 1, [1,2,3], \"Hello\"]\n\nx = 1\n\nnew_x, a_list, str_ = f(x) if x > 0 else f(x + 1)"
        ],
        "link": "https://stackoverflow.com/questions/75065495/is-there-a-way-to-return-tuple-of-mixed-variables-in-jax-helper-function"
    },
    {
        "title": "jax.lax.select vs jax.numpy.where",
        "question": "Was taking a look at the dropout implementation in flax:\npython\nCopy\ndef __call__(self, inputs, deterministic: Optional[bool] = None):\n    \"\"\"Applies a random dropout mask to the input.\n\n    Args:\n      inputs: the inputs that should be randomly masked.\n      deterministic: if false the inputs are scaled by `1 / (1 - rate)` and\n        masked, whereas if true, no mask is applied and the inputs are returned\n        as is.\n\n    Returns:\n      The masked inputs reweighted to preserve mean.\n    \"\"\"\n    deterministic = merge_param(\n        'deterministic', self.deterministic, deterministic)\n\n    if (self.rate == 0.) or deterministic:\n      return inputs\n\n    # Prevent gradient NaNs in 1.0 edge-case.\n    if self.rate == 1.0:\n      return jnp.zeros_like(inputs)\n\n    keep_prob = 1. - self.rate\n    rng = self.make_rng(self.rng_collection)\n    broadcast_shape = list(inputs.shape)\n    for dim in self.broadcast_dims:\n      broadcast_shape[dim] = 1\n    mask = random.bernoulli(rng, p=keep_prob, shape=broadcast_shape)\n    mask = jnp.broadcast_to(mask, inputs.shape)\n    return lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs))\nParticularly, I'm interested in last line lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs)). Wondering why lax.select is used here instead of:\npython\nCopy\nreturn jnp.where(mask, inputs / keep_prob, 0)\nor even more simply:\npython\nCopy\nreturn mask * inputs / keep_prob",
        "answers": [
            "jnp.where is basically the same as lax.select, except more flexible in its inputs: for example, it will broadcast inputs to the same shape or cast to the same dtype, whereas lax.select requires more strict matching of inputs:\npython\nCopy\n>>> import jax.numpy as jnp\n>>> from jax import lax\n>>> x = jnp.arange(3)\npython\nCopy\n# Implicit broadcasting\n>>> jnp.where(x < 2, x[:, None], 0)\nDeviceArray([[0, 0, 0],\n             [1, 1, 0],\n             [2, 2, 0]], dtype=int32)\n\n>>> lax.select(x < 2, x[:, None], 0)\nTypeError: select cases must have the same shapes, got [(), (3, 1)].\npython\nCopy\n# Implicit type promotion\n>>> jnp.where(x < 2, jnp.zeros(3), jnp.arange(3))\nDeviceArray([0., 0., 2.], dtype=float32)\n\n>>> lax.select(x < 2, jnp.zeros(3), jnp.arange(3))\nTypeError: lax.select requires arguments to have the same dtypes, got int32, float32. (Tip: jnp.where is a similar function that does automatic type promotion on inputs).\nLibrary code is one place where the stricter semantics can be useful, because rather than smoothing-over potential implementation bugs and returning an unexpected output, it will complain loudly. But performance-wise (especially once JIT-compiled) the two are essentially equivalent.\nAs for why the flax developers chose lax.select vs. multiplying by a mask, I can think of two reasons:\nMultiplying by a mask is subject to implicit type promotion semantics, and it takes a lot more thought to anticipate problematic outputs than a simple select, which is specifically-designed for the intended operation.\nUsing multiplication causes the compiler to treat this operation as a multiplication, which it is not. A select is a much more narrow and precise operation than a multiplication, and by specifying operations precisely it often allows the compiler to optimize the results to a greater extent."
        ],
        "link": "https://stackoverflow.com/questions/74972850/jax-lax-select-vs-jax-numpy-where"
    },
    {
        "title": "JAX: JIT compatible sparse matrix slicing",
        "question": "I have a boolean sparse matrix that I represent with row indices and column indices of True values.\npython\nCopy\nimport numpy as np\nimport jax\nfrom jax import numpy as jnp\nN = 10000\nM = 1000\nX = np.random.randint(0, 100, size=(N, M)) == 0  # data setup\nrows, cols = np.where(X == True)\nrows = jax.device_put(rows)\ncols = jax.device_put(cols)\nI want to get a column slice of the matrix like X[:, 3], but just from rows indices and column indices.\nI managed to do that by using jnp.isin like below, but the problem is that this is not JIT compatible because of the data-dependent shaped array rows[cols == m].\npython\nCopy\ndef not_jit_compatible_slice(rows, cols, m):\n  return jnp.isin(jnp.arange(N), rows[cols == m])\nI could make it JIT compatible by using jnp.where in the three-argument form, but this operation is much slower than the previous one.\npython\nCopy\ndef jit_compatible_but_slow_slice(rows, cols, m):\n  return jnp.isin(jnp.arange(N), jnp.where(cols == m, rows, -1))\nIs there any fast and JIT compatible solution to acheive the same output?",
        "answers": [
            "You can do a bit better than the first answer by using the mode argument of set() to drop out-of-bound indices, eliminating the final slice:\npython\nCopy\nout = jnp.zeros(N, bool).at[jnp.where(cols==3, rows, N)].set(True, mode='drop')",
            "I figured out that the implementation below returns the same output much faster, and it’s JIT compatible.\npython\nCopy\ndef slice(rows, cols, m):\n  res = jnp.zeros(N + 1, dtype=bool)\n  res = res.at[jnp.where(cols == m, rows, -1)].set(True)\n  return res[:-1]"
        ],
        "link": "https://stackoverflow.com/questions/74946632/jax-jit-compatible-sparse-matrix-slicing"
    },
    {
        "title": "Why does jax.grad(lambda v: jnp.linalg.norm(v-v))(jnp.ones(2)) produce nans?",
        "question": "Can someone explain the following behaviour? Is it a bug?\npython\nCopy\nfrom jax import grad\nimport jax.numpy as jnp\n\nx = jnp.ones(2)\ngrad(lambda v: jnp.linalg.norm(v-v))(x) # returns DeviceArray([nan, nan], dtype=float32)\n\ngrad(lambda v: jnp.linalg.norm(0))(x) # returns DeviceArray([0., 0.], dtype=float32)\nI've tried looking up the error online but didn't find anything relevant.\nI also skimmed through https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html",
        "answers": [
            "When you compute grad(lambda v: jnp.linalg.norm(v-v))(x), your function looks roughly like this:\npython\nCopy\nf(x) = sqrt[(x - x)^2]\nso, evaluating with the chain rule, the derivative is\npython\nCopy\ndf/dx = (x - x) / sqrt[(x - x)^2]\nwhich, when you plug-in any finite x evaluates to\npython\nCopy\n0 / sqrt(0)\nwhich is undefined, and represented by NaN in floating point arithmetic.\nWhen you compute grad(lambda v: jnp.linalg.norm(0))(x), your function looks roughly like this:\npython\nCopy\ng(x) = sqrt[0.0^2]\nand because it has no dependence on x the derivative is simply\npython\nCopy\ndg/dx = 0.0\nDoes that answer your question?"
        ],
        "link": "https://stackoverflow.com/questions/74864427/why-does-jax-gradlambda-v-jnp-linalg-normv-vjnp-ones2-produce-nans"
    },
    {
        "title": "Is there a way to update multiple indexes of Jax array at once?",
        "question": "Since array is immutable in Jax, so when one updates N indexes, it creates N arrays with\nx = x.at[idx].set(y)\nWith hundreds of updates per training cycle, it will ultimately create hundreds of arrays if not millions. This seems a little wasteful, is there a way to update multiple index at one go? Does anyone know if there is overhead or if it's significant? Am I overlook at this?",
        "answers": [
            "You can perform multiple updates in a single operation using the syntax you mention. For example:\npython\nCopy\nimport jax.numpy as jnp\n\nx = jnp.zeros(10)\nidx = jnp.array([3, 5, 7, 9])\ny = jnp.array([1, 2, 3, 4])\n\nx = x.at[idx].set(y)\nprint(x)\n# [0. 0. 0. 1. 0. 2. 0. 3. 0. 4.]\nYou're correct that outside JIT, each update operation will create an array copy. But within JIT-compiled functions, the compiler is able to perform such updates in-place when it is possible (for example, when the original array is not referenced again). You can read more at JAX Sharp Bits: Array Updates.",
            "This sounds very like a job for scatter update. I'm not really familiar with Jax itself, but major frameworks have it:\nhttps://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html\nWhat it does in a nutshell:\nsetup your output tensor (x)\naccumulate required updates in the other tensor (y in your case)\naccumulate in list/tensor indices where to apply you updates (create tensor/list full of index)\nfeed 1)-3) to scatter_updated"
        ],
        "link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once"
    },
    {
        "title": "How to improve Julia's performance using just in time compilation (JIT)",
        "question": "I have been playing with JAX (automatic differentiation library in Python) and Zygote (the automatic differentiation library in Julia) to implement Gauss-Newton minimisation method. I came upon the @jit macro in Jax that runs my Python code in around 0.6 seconds compared to ~60 seconds for the version that does not use @jit. Julia ran the code in around 40 seconds. Is there an equivalent of @jit in Julia or Zygote that results is a better performance?\nHere are the codes I used:\nPython\ncss\nCopy\nfrom jax import grad, jit, jacfwd\nimport jax.numpy as jnp\nimport numpy as np\nimport time\n\ndef gaussian(x, params):\n    amp = params[0]\n    mu  = params[1]\n    sigma = params[2]\n    amplitude = amp/(jnp.abs(sigma)*jnp.sqrt(2*np.pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*jnp.exp(-0.5*(arg**2))\n\ndef myjacobian(x, params):\n    return jacfwd(gaussian, argnums = 1)(x, params)\n\ndef op(jac):\n    return jnp.matmul(\n        jnp.linalg.inv(jnp.matmul(jnp.transpose(jac),jac)),\n        jnp.transpose(jac))\n                         \ndef res(x, data, params):\n    return data - gaussian(x, params)\n@jit\ndef step(x, data, params):\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    temp = jnp.matmul(jacobian_operation, residuals)\n    return params + temp\n\nN = 2000\nx = np.linspace(start = -100, stop = 100, num= N)\ndata = gaussian(x, [5.65, 25.5, 37.23])\n\nini = jnp.array([0.9, 5., 5.0])\nt1 = time.time()\nfor i in range(5000):\n    ini = step(x, data, ini)\nt2 = time.time()\nprint('t2-t1: ', t2-t1)\nini\nJulia\njulia\nCopy\nusing Zygote\n\nfunction gaussian(x::Union{Vector{Float64}, Float64}, params::Vector{Float64})\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]\n    \n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x.-mu)./sigma)\n    return amplitude.*exp.(-0.5.*(arg.^2))\n    \nend\n\nfunction myjacobian(x::Vector{Float64}, params::Vector{Float64})\n    output = zeros(length(x), length(params))\n    for (index, ele) in enumerate(x)\n        output[index,:] = collect(gradient((params)->gaussian(ele, params), params))[1]\n    end\n    return output\nend\n\nfunction op(jac::Matrix{Float64})\n    return inv(jac'*jac)*jac'\nend\n\nfunction res(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    return data - gaussian(x, params)\nend\n\nfunction step(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    \n    temp = jacobian_operation*residuals\n    return params + temp\nend\n\nN = 2000\nx = collect(range(start = -100, stop = 100, length= N))\nparams = vec([5.65, 25.5, 37.23])\ndata = gaussian(x, params)\n\nini = vec([0.9, 5., 5.0])\n@time for i in range(start = 1, step = 1, length = 5000)\n    ini = step(x, data, ini)\nend\nini",
        "answers": [
            "Your Julia code doing a number of things that aren't idiomatic and are worsening your performance. This won't be a full overview, but it should give you a good idea to start.\nThe first thing is passing params as a Vector is a bad idea. This means it will have to be heap allocated, and the compiler doesn't know how long it is. Instead, use a Tuple which will allow for a lot more optimization. Secondly, don't make gaussian act on a Vector of xs. Instead, write the scalar version and broadcast it. Specifically, with these changes, you will have\nlua\nCopy\nfunction gaussian(x::Number, params::NTuple{3, Float64})\n    amp, mu, sigma = params\n    \n    # The next 2 lines should probably be done outside this function, but I'll leave them here for now.\n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*exp(-0.5*(arg^2))\nend",
            "One straightforward way to speed this up is to use ForwardDiff not Zygote, since you are taking a gradient of a vector of length 3, many times. Here this gets me from 16 to 3.5 seconds, with the last factor of 2 involving Chunk(3) to improve type-stability. Perhaps this can be improved further.\ncss\nCopy\nfunction myjacobian(x::Vector, params)\n    # return rand(eltype(x), length(x), length(params))  # with no gradient, takes 0.5s\n    output = zeros(eltype(x), length(x), length(params))\n    config = ForwardDiff.GradientConfig(nothing, params, ForwardDiff.Chunk(3))\n    for (i, xi) in enumerate(x)\n        # grad = gradient(p->gaussian(xi, p), params)[1]       # original, takes 16s\n        # grad = ForwardDiff.gradient(p-> gaussian(xi, p))     # ForwardDiff, takes 7s\n        grad = ForwardDiff.gradient(p-> gaussian(xi, p), params, config)  # takes 3.5s\n        copyto!(view(output,i,:), grad)  # this allows params::Tuple, OK for Zygote, no help\n    end\n    return output\nend\n# This needs gaussian.(x, Ref(params)) elsewhere to use on many x, same params\nfunction gaussian(x::Real, params)\n    # amp, mu, sigma = params  # with params::Vector this is slower, 19 sec\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]  # like this, 16 sec\n    T = typeof(x)  # avoids having (2*pi)::Float64 promote everything\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nHowever, this is still computing many small gradient arrays in a loop. It could easily compute one big gradient array instead.\nWhile in general Julia is happy to compile loops to something fast, loops that make individual arrays tend to be a bad idea. And this is especially true for Zygote, which is fastest on matlab-ish whole-array code.\nHere's how this looks, it gets me under 1s for the whole program:\nphp\nCopy\nfunction gaussian(x::Real, amp::Real, mu::Real, sigma::Real)\n    T = typeof(x)\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nfunction myjacobian2(x::Vector, params)  # with this, 0.9s\n    amp = fill(params[1], length(x))\n    mu  = fill(params[2], length(x))\n    sigma = fill(params[3], length(x))  # use same sigma & different x value at each row:\n    grads = gradient((amp, mu, sigma) -> sum(gaussian.(x, amp, mu, sigma)), amp, mu, sigma)\n    hcat(grads...)\nend\n# Check that it agrees:\nmyjacobian2(x, params) ≈ myjacobian(x, params)\nWhile this has little effect on the speed, I think you probably also want op(jac::Matrix) = Hermitian(jac'*jac) \\ jac' rather than inv."
        ],
        "link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit"
    },
    {
        "title": "jax.lax.fori_loop Abstract tracer value encountered where concrete value is expected",
        "question": "I've a JAX loop that looks like this where inside the step function I use min between the two arguments\npython\nCopy\nimport jax\n\ndef step(timestep: int, order: int = 4) -> int:\n    order = min(timestep + 1, order)\n    return order\n\nnum_steps = 10\norder = 100\norder = jax.lax.fori_loop(0, num_steps, step, order)\nThe above code fails with a jax._src.errors.ConcretizationTypeError. This is is the full stacktrace:\npython\nCopy\nWARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n---------------------------------------------------------------------------\nUnfilteredStackTrace                      Traceback (most recent call last)\n<ipython-input-4-9ec280f437cb> in <module>\n      2 order = 100\n----> 3 order = jax.lax.fori_loop(0, num_steps, step, order)\n\n16 frames\n/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)\n    161     try:\n--> 162       return fun(*args, **kwargs)\n    163     except Exception as e:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in fori_loop(lower, upper, body_fun, init_val)\n   1691 \n-> 1692     (_, result), _ = scan(_fori_scan_body_fun(body_fun), (lower_, init_val),\n   1693                           None, length=upper_ - lower_)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)\n    161     try:\n--> 162       return fun(*args, **kwargs)\n    163     except Exception as e:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in scan(f, init, xs, length, reverse, unroll)\n    258   # necessary, a second time with modified init values.\n--> 259   init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n    260   new_init_flat, changed = _promote_weak_typed_inputs(init_flat, carry_avals, carry_avals_out)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in _create_jaxpr(init)\n    244     carry_avals = tuple(_map(_abstractify, init_flat))\n--> 245     jaxpr, consts, out_tree = _initial_style_jaxpr(\n    246         f, in_tree, (*carry_avals, *x_avals), \"scan\")\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/common.py in _initial_style_jaxpr(fun, in_tree, in_avals, primitive_name)\n     59                          primitive_name: Optional[str] = None):\n---> 60   jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n     61       fun, in_tree, in_avals, primitive_name)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/common.py in _initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name)\n     53   debug = pe.debug_info(fun, in_tree, False, primitive_name or \"<unknown>\")\n---> 54   jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n     55   return jaxpr, consts, out_tree()\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py in wrapper(*args, **kwargs)\n    313     with TraceAnnotation(name, **decorator_kwargs):\n--> 314       return func(*args, **kwargs)\n    315     return wrapper\n\n/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)\n   1980     main.jaxpr_stack = ()  # type: ignore\n-> 1981     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n   1982       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n\n/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)\n   1997     in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n-> 1998     ans = fun.call_wrapped(*in_tracers_)\n   1999     out_tracers = map(trace.full_raise, ans)\n\n/usr/local/lib/python3.8/dist-packages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)\n    166     try:\n--> 167       ans = self.f(*args, **dict(self.params, **kwargs))\n    168     except:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in scanned_fun(loop_carry, _)\n   1607     i, x = loop_carry\n-> 1608     return (i + 1, body_fun()(i, x)), None\n   1609   return scanned_fun\n\n<ipython-input-2-2e3345899235> in step(timestep, order)\n      1 def step(timestep: int, order: int = 100) -> int:\n----> 2     order = min(timestep + 1, order)\n      3     return order\n\n/usr/local/lib/python3.8/dist-packages/jax/core.py in __bool__(self)\n    633   def __nonzero__(self): return self.aval._nonzero(self)\n--> 634   def __bool__(self): return self.aval._bool(self)\n    635   def __int__(self): return self.aval._int(self)\n\n/usr/local/lib/python3.8/dist-packages/jax/core.py in error(self, arg)\n   1266   def error(self, arg):\n-> 1267     raise ConcretizationTypeError(arg, fname_context)\n   1268   return error\n\nUnfilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 for scan. This concrete value was not available in Python because it depends on the values of the argument 'loop_carry'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nConcretizationTypeError                   Traceback (most recent call last)\n<ipython-input-4-9ec280f437cb> in <module>\n      1 num_steps = 10\n      2 order = 100\n----> 3 order = jax.lax.fori_loop(0, num_steps, step, order)\n\n<ipython-input-2-2e3345899235> in step(timestep, order)\n      1 def step(timestep: int, order: int = 100) -> int:\n----> 2     order = min(timestep + 1, order)\n      3     return order\n\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 for scan. This concrete value was not available in Python because it depends on the values of the argument 'loop_carry'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nEverything works fine if instead of using jax.lax.fori_loop i use a simple python loop, but my original code will end up very slow. How can I fix this issue?",
        "answers": [
            "Use jax.numpy.minimum in place of min:\npython\nCopy\ndef step(timestep: int, order: int = 4) -> int:\n    order = jax.numpy.minimum(timestep + 1, order)\n    return order\nThe reason min does not work is that in the course of executing code within jit, grad, vmap, fori_loop, etc., JAX replaces concrete values with abstract tracers, and Python functions like min don't know how to handle these abstract values. See How to Think in JAX for more background on this."
        ],
        "link": "https://stackoverflow.com/questions/74640431/jax-lax-fori-loop-abstract-tracer-value-encountered-where-concrete-value-is-expe"
    },
    {
        "title": "`jax.jit` not improving in place update performance for large arrays?",
        "question": "I am trying to apply a number of in place updates to a 2D matrix.\nIt appears that using jit to the in place update does not have any effect in computation time (which is many orders of magnitude longer than the equivalent numpy implementation).\nHere is code that demonstrates my problem and research.\npython\nCopy\nnode_count = 10000\n\n# NUMPY IMPLEMENTATION\nb = onp.zeros([node_count,node_count])\nprint(\"`numpy` in place update.\")\n%timeit b[1,1] = 1.\n# 86.9 ns ± 1.42 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\n# JAX IN PLACE IMPLEMENTATION\na = np.zeros([node_count,node_count])\nprint(\"`jax.np` in place update.\")\n%timeit a.at[1,1].set(1.)\n# 112 ms ± 14.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n## TEST JIT IMPLEMENTATION\ndef update(mat, index, val):\n    return mat.at[tuple(index)].set(val)\nupdate_jit = jit(update)\n\n# Run once for trace.\nupdate_jit(a, [1,1], 1.).block_until_ready()\n\nprint(\"`jax.np` jit in place update.\")\n%timeit update_jit(a, [1,1],1.).block_until_ready()\n# 99.6 ms ± 358 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)",
        "answers": [
            "This has nothing to do with inlining of inplace updates. This has to do with the fact that, unless otherwise requested, a JIT-compiled function will always return its result in a new, distinct buffer. The only exception to this is if you use buffer donation to explicitly mark that the input buffer can be re-used in the output:\npython\nCopy\nupdate_jit = jit(update, donate_argnums=[0])\nNote, however, that buffer donation is currently only available on GPU and TPU runtimes.\nYou'll not be able to use %timeit in this case, because the donated input buffer is no longer available for use after the first iteration, but you can confirm via %time that this improves the computation speed:\npython\nCopy\n# Following is run on a Colab T4 GPU runtime\n\nupdate_jit = jit(update)\n_ = update_jit(b, [1,1], 1.)\n%time _ = update_jit(b, [1,1], 1.).block_until_ready()\n# CPU times: user 607 µs, sys: 112 µs, total: 719 µs\n# Wall time: 5.89 ms\n\nupdate_jit_donate = jit(update, donate_argnums=[0])\nb = update_jit_donate(b, [1,1], 1.)\n%time _ = update_jit_donate(b, [1,1], 1.).block_until_ready()\n# CPU times: user 467 µs, sys: 86 µs, total: 553 µs\n# Wall time: 332 µs\nThe buffer donation version is still quite a bit slower than the NumPy version, but this is expected for the reasons discussed at FAQ: Is JAX Faster Than Numpy?.\nI suspect you're performing these micro-benchmarks to assure yourself that the compiler performs updates in-place within a JIT-compiled sequence of operations rather than making internal copies, as is mentioned in Sharp Bits: Array Updates. If so, you can confirm this by other means; for example:\npython\nCopy\n@jit\ndef sum(x):\n  return x.sum()\n\n@jit\ndef update_and_sum(x):\n  return x.at[0, 0].set(1).sum()\n\n_ = sum(b)\n%timeit sum(b).block_until_ready()\n# 1.66 ms ± 7.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n_ = update_and_sum(b)\n%timeit update_and_sum(b).block_until_ready()\n# 1.66 ms ± 20.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\nThe identical timings here show that the update operation is being performed in-place rather than causing the input buffer to be copied.",
            "The JIT version is slower because it does not operate in-place as opposed to the Numpy version: it creates a copy of the array and then modify the items. You can see that by growing the array: the execution time is proportional to the size of the array. You can also check the array a is left unmodified after the call to set. You can also see that the time of b.fill(42.0) is very close to the speed of the JAX function (but strangely not b.copy()). The out-of-place version is significantly slower because the RAM is slow and it takes a lot of time to operate on the whole array than just setting 1 value."
        ],
        "link": "https://stackoverflow.com/questions/74587875/jax-jit-not-improving-in-place-update-performance-for-large-arrays"
    },
    {
        "title": "Execute function specifically on CPU in Jax",
        "question": "I have a function that will instantiate a huge array and do other things. I am running my code on TPUs so my memory is limited.\nHow can I execute my function specifically on the CPU?\nIf I do:\npython\nCopy\ny = jax.device_put(my_function(), device=jax.devices(\"cpu\")[0])\nI guess that my_function() is first executed on TPU and the result is put on CPU, which gives me memory error.\nand using jax.config.update('jax_platform_name', 'cpu') at the beginning of my code seems to have no effect.\nAlso please note that I can't modify my_function()\nThanks!",
        "answers": [
            "To directly specify the device on which a function should be executed, use the device argument of jax.jit. For example (using a GPU runtime because it's the accelerator I have access to at the moment):\npython\nCopy\nimport jax\n\ngpu_device = jax.devices('gpu')[0]\ncpu_device = jax.devices('cpu')[0]\n\ndef my_function(x):\n  return x.sum()\n\nx = jax.numpy.arange(10)\n\nx_gpu = jax.jit(my_function, device=gpu_device)(x)\nprint(x_gpu.device())\n# gpu:0\n\nx_cpu = jax.jit(my_function, device=cpu_device)(x)\nprint(x_cpu.device())\n# TFRT_CPU_0\nThis can also be controlled with the jax.default_device decorator around the call-site:\npython\nCopy\nwith jax.default_device(cpu_device):\n  print(jax.jit(my_function)(x).device())\n  # TFRT_CPU_0\n\nwith jax.default_device(gpu_device):\n  print(jax.jit(my_function)(x).device())\n  # gpu:0",
            "I'm going to make a guess here. I can't run it either so you may have to fiddle with it\npython\nCopy\nwith jax.default_device(jax.devices(\"cpu\")[0]):\n    y = my_function()\nSee the docs here and here."
        ],
        "link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax"
    },
    {
        "title": "Test jax.pmap before deploying on multi-device hardware",
        "question": "I am coding on a single-device laptop and I am using jax.pmap because my code will run on multiple TPUs. I would like to \"fake\" having multiple devices to test my code and try different things.\nIs there any way to achieve this? Thanks!",
        "answers": [
            "You can spoof multiple XLA devices backed by a single device by setting the following environment variable:\n$ set XLA_FLAGS=\"--xla_force_host_platform_device_count=8\"\nIn Python, you could do it like this\npython\nCopy\n# Note: must set this env variable before jax is imported\nimport os\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\n\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3),\n#  CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n\nimport jax.numpy as jnp\nout = jax.pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\n# [ 0  1  4  9 16 25 36 49]\nNote that when a only a single physical device is present, all the \"devices\" here will be backed by the same threadpool. This will not improve performance of the code, but it can be useful for testing the semantics of parallel implementations on a single-device machine."
        ],
        "link": "https://stackoverflow.com/questions/74466352/test-jax-pmap-before-deploying-on-multi-device-hardware"
    },
    {
        "title": "Getting a type error while using fori_loop with JAX",
        "question": "I'm developing a code using JAX, and I wanted to JIT some parts of that had big loops. I didn't want the code to be unrolled so I used fori_loop, but I'm getting an error and can't figure out what I am doing wrong.\nThe error is:\npython\nCopy\n  self.arr = self.arr.reshape(new_shape+new_shape)\nTypeError: 'aval_method' object is not callable\nI was able to reduce the code to the following:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\nclass UB():\n    def __init__(self, arr, new_shape):\n\n        self.arr = arr\n        self.shape = new_shape\n        if type(arr) is not object:\n            self.arr = self.arr.reshape(new_shape+new_shape)\n\n        \n    def _tree_flatten(self):\n        children = (self.arr,)  # arrays / dynamic values\n        aux_data = {\n            'new_shape': self.shape            \n        }  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\n\nclass UM():\n    def __init__(self, arr, r=None):\n\n        self.arr = arr\n        self.r = tuple(r)\n    \n    def _tree_flatten(self):\n        children = (self.arr,)  # arrays / dynamic values\n        aux_data = {\n            'r': self.r\n        }  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\n\n\nfor C in [UB, UM]:\n    jax.tree_util.register_pytree_node(\n        C,\n        C._tree_flatten,\n        C._tree_unflatten,\n    )\n\n\ndef s_w(ub, ums):\n    e  = jnp.identity(2)\n    u = UM(e, [2])\n    ums[0] = u\n    return ub, ums\n\ndef s_c(t, uns):\n    n = 20\n    ums = []\n    for un in uns:\n        ums.append(UM(un, [2]))\n\n    tub = UB(t.arr, t.r)\n    \n    s_loop_body = lambda i,x: s_w( ub=x[0], ums=x[1])\n    \n    tub, ums = jax.lax.fori_loop(0, n, s_loop_body, (tub, ums))\n    # for i in range(n):\n    #     tub, ums = s_loop_body(i, (tub, ums))\n\n    return jnp.array([u.arr.flatten() for u in ums])\n\n\nuns = jnp.array([jnp.array([1, 2, 3, 4]) for _ in range(6)])\nt = UM(jnp.array([1, 0, 0, 1]), r=[2])\nuns = s_c(t, uns)\nHas anyone encountered this issue or can explain how to fix it?",
        "answers": [
            "The issue is discussed here: https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization\nNamely, in JAX pytrees are used as general containers, and are sometimes initialized with abstract values or other place-holders, and so you cannot assume that arguments to a custom PyTree will be of array type. You might account for this by doing something like the following:\npython\nCopy\nclass UB():\n    def __init__(self, arr, new_shape):\n        self.arr = arr\n        self.shape = new_shape\n        if isinstance(arr, jnp.ndarray):\n            self.arr = self.arr.reshape(new_shape+new_shape)\nWhen I run your code with this modification, it gets past the error you asked about, but unfortunately does trigger another error due to the body function of the fori_loop not having a valid signature (namely, the arr attributes of the ums have different shapes on input and output, which is not supported by fori_loop).\nHopefully this gets you on the path toward working code!"
        ],
        "link": "https://stackoverflow.com/questions/74296697/getting-a-type-error-while-using-fori-loop-with-jax"
    },
    {
        "title": "Indexing a BCOO in Jax",
        "question": "I came across another problem in my attempts to learn jax: I have a sparse BCOO array, and an array holding indices. I need to obtain all values at these indices in the BCOO array. It would be ideal if the returned array would be a sparse BCOO as well. Using the usual slicing syntax seems to not work. Is there a standard way to achieve this? e.g.\npython\nCopy\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\n\nindices = jnp.array([\n    1,1,0\n])\n\nfull_array = jnp.array(\n        [\n            [\n                [0,0,0],\n                [2,2,2],\n                [0,0,0],\n                [0,0,0]\n            ],\n            [\n                [1,1,1],\n                [0,0,0],\n                [0,0,0],\n                [0,0,0]\n            ],\n            [\n                [1,1,1],\n                [0,0,0],\n                [0,0,0],\n                [0,0,0]\n            ]\n        ]\n)\nfull_array[jnp.arange(3),indices]\n# results in:\n#    [2,2,2],\n#    [0,0,0],\n#    [1,1,1] \n\nsparse_array = sparse.bcoo_fromdense(full_array)\n\n# Trying the same thing on a sparse array:\nsparse_array[jnp.arange(3),indices]\n# produces an NotImplementedError",
        "answers": [
            "[Edit: 2022-11-15] As of jax version 0.3.25, this kind of sparse indexing is directly supported in JAX:\npython\nCopy\nimport jax\nprint(jax.__version__)\n# 0.3.25\n\nsparse_array = sparse.bcoo_fromdense(full_array)\nresult = sparse_array[jnp.arange(3),indices]\n\nprint(result.todense())\n# [[2 2 2]\n#  [0 0 0]\n#  [1 1 1]]\nOriginal answer:\nThanks for the question. Unfortunately, general indexing support has not been added yet to jax.experimental.sparse. The types of indexing operations currently supported are limited to static scalars and slices; for example:\npython\nCopy\nprint(sparse_array[0].todense())\n# [[0 0 0]\n#  [2 2 2]\n#  [0 0 0]\n#  [0 0 0]]\nWith this in mind, you may be able to build the operation you have in mind using concatenation. For example:\npython\nCopy\nresult = sparse.sparsify(jnp.vstack)([\n    sparse_array[0][1],  # only single indices supported currently\n    sparse_array[1][1],\n    sparse_array[2][0],\n])\nprint(result.todense())\n# [[2 2 2]\n#  [0 0 0]\n#  [1 1 1]]\nAdmittedly only supporting static indices is not very convenient, but we hope to add more indexing support in the future."
        ],
        "link": "https://stackoverflow.com/questions/74281298/indexing-a-bcoo-in-jax"
    },
    {
        "title": "How can I redefine a subfunction of a JAX-jitted function?",
        "question": "I have a function foo which is jitted with JAX. foo calls bar.\npython\nCopy\nfrom jax import jit\n\ndef bar(x):\n  return x ** 2\n\n@jit\ndef foo(x):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4)}')\nPrints foo(4) = 17 as expected.\nIf I redefine bar at runtime, what is the best way to re-jit foo?\nBonus: Is it possible to just tell jax bar changed, so that it can re-jit everything dependent upon bar?\nDetails\nIf I redefine bar and print again,\npython\nCopy\ndef bar(x):\n  return 2 * x\n\nprint(f'foo(4) = {foo(4)}')\nthe output is still foo(4) = 17. Clearly, the old bar is still in the jitted foo.\nIn order to get the desired foo with updated bar, I need to re-jit. I can do this by rewriting foo, so that it is re-jitted\npython\nCopy\n@jit\ndef foo(x):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4)}')\nwhich now prints foo(4) = 9 as desired. But rewriting all of foo is silly. I can also get the desired output by just rewrapping foo as\npython\nCopy\nfoo = jit(foo)\nBut this feels dangerous, since we're passing an already-jitted foo into the jit compiler again. I was surprised to see that it worked, and I suspect weird corner cases with this approach.\nAlso, both of these approaches require knowing that foo called bar. It's trivial in this toy example, but in more complex software, there might be many functions which need to be re-jitted. It's cumbersome to do it individually, and easy to forget some as well. Hence the 'bonus' part of my question.\nThe jit docs and jit caching docs did not answer my question, AFAICT. Nor did a reasonable search of stackoverflow or the github discussion pages.",
        "answers": [
            "The reason this is not working as expected is because the function is not pure: that is, its output depends not just on the inputs, but also on some global state. JAX transforms like JIT only work correctly for pure functions; see JAX Sharp Bits: Pure Functions for more discussion of this.\nThe best way to approach this is probably to change the function's call signature so that all relevant state is explicitly passed to the function. For example, you could pass bar explicitly:\npython\nCopy\nfrom jax import jit\nfrom functools import partial\n\ndef bar(x):\n  return x ** 2\n\n# Mark `bar` as a static argument: when it changes it will trigger re-compilation\n@partial(jit, static_argnames=['bar'])\ndef foo(x, bar):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4, bar)}')\n# foo(4) = 17\n\ndef bar(x):\n  return 2 * x\n\nprint(f'foo(4) = {foo(4, bar)}')\n# foo(4) = 9\nNow when you change bar the output of the function changes."
        ],
        "link": "https://stackoverflow.com/questions/74185207/how-can-i-redefine-a-subfunction-of-a-jax-jitted-function"
    },
    {
        "title": "multidimensional jax.isin()",
        "question": "i am trying to filter an array of triples. The criterion by which I want to filter is whether another array of triples contains at least one element with the same first and third element. E.g\npython\nCopy\nimport jax.numpy as jnp\narray1 = jnp.array(\n  [\n    [0,1,2],\n    [1,0,2],\n    [0,3,3],\n    [3,0,1],\n    [0,1,1],\n    [1,0,3],\n  ]\n)\narray2 = jnp.array([[0,1,3],[0,3,2]])\n# the mask to filter the first array1 should look like this:\njnp.array([True,False,True,False,False,False])\nWhat would be a computationally efficient way to achieve this mask using jax? I am looking forward to your input.",
        "answers": [
            "You can do this by reducing over a broadcasted equality check:\npython\nCopy\nimport jax.numpy as jnp\narray1 = jnp.array(\n  [\n    [0,1,2],\n    [1,0,2],\n    [0,3,3],\n    [3,0,1],\n    [0,1,1],\n    [1,0,3],\n  ]\n)\narray2 = jnp.array([[0,1,2],[0,3,2]])  # note adjustment to match first entry of array1\n\nmask = (array1[:, None] == array2[None, :]).all(-1).any(-1)\nprint(mask)\n# [ True False False False False False]\nXLA doesn't have any binary search-like primitive, so the best approach in general is to generate the full equality matrix and reduce. If you're running the code on an accelerator like a GPU/TPU, this sort of vectorized operation is efficiently parallelized and so it will be computed quite efficiently in practice."
        ],
        "link": "https://stackoverflow.com/questions/74154196/multidimensional-jax-isin"
    },
    {
        "title": "Issues with non-hashable static arguments when forming",
        "question": "I have a vector-jacobian product that I want to compute.\nThe function func takes four arguments, the final two of which are static:\npython\nCopy\ndef func(variational_params, e, A, B):\n    ...\n    return model_params, dlogp, ...\nThe function jits perfectly fine via\npython\nCopy\nfunc_jitted = jit(func, static_argnums=(2, 3))\nThe primals are the variational_params, and the cotangents are dlogp (the second output of the function).\nCalculating the vector-jacobian product naively (by forming the jacobian) works fine:\npython\nCopy\njacobian_func = jacobian(func_jitted, argnums=0, has_aux=True)\njacobian_jitted = jit(jacobian_func, static_argnums=(2, 3))\njac, func_output = jacobian_jitted(variational_params, e, A, B)\nnaive_vjp = func_output.T @ jac \nWhen trying to form the vjp in an efficient manner via\npython\nCopy\nf_eval, vjp_function, aux_output = vjp(func_jitted, variational_params, e, A, B, has_aux=True)\nI get the following error:\npython\nCopy\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.ad.JVPTracer'> for function func is non-hashable.\nI am a little confused as the function func jitted perfectly fine... there is no option for adding static_argnums to the vjp function, so I am not too sure what this means.",
        "answers": [
            "For higher-level transformation APIs like jit, JAX generally provides a mechanism like static_argnums or argnums to allow specification of static vs. dynamic variables.\nFor lower-level transformation routines like jvp and vjp, these mechanisms are not provided, but you can still accomplish the same thing by passing partially-evaluated functions. For example:\npython\nCopy\nfrom functools import partial\n\nf_eval, vjp_function, aux_output = vjp(partial(func_jitted, A=A, B=B), variational_params, e, has_aux=True)\nThis is effectively how transformation parameters like argnums and static_argnums are implemented under the hood."
        ],
        "link": "https://stackoverflow.com/questions/74065210/issues-with-non-hashable-static-arguments-when-forming"
    },
    {
        "title": "Execute Markov chains with tree-structured state in parallel with JAX",
        "question": "I have a Markov chain function implemented in JAX that advances the chain from state s -> s' based on some training data (X_train).\npython\nCopy\ndef step(state: dict, key, X_train) -> dict:\n    new_state = advance(state, key, X_train)\n    return new_state\nHere, state is a fairly complicated tree-structured dict of array's that was generated by Haiku. For example,\npython\nCopy\nstate = {\n    'layer1': {\n        'weights': array(...),\n        'bias': array(...),\n    },\n    'layer2': {\n        'weights': array(...),\n        'bias': array(...),\n    },\n}\nI would like to run multiple Markov chains, with different states, in parallel. At first glance, jax.vmap function looks like a good candidate. However, state is not an array but a (tree-structured) dict.\nWhat is the best way to approach this?\nThanks!",
        "answers": [
            "Yes, you could use vmap for any pytree. But this is how you should construct it:\npython\nCopy\nstates = {'layer1':{'weights':jnp.array([[1, -2, 3],\n                                         [4, 5, 6]])},\n          'layer2':{'weights':jnp.array([[1, .2, 3],\n                                         [.4, 5, 6]])}}\nSo in your first run, your weights will be [1, -2, 3] and [1, .2, 3] for layer1 and layer2 respectively (second run will be [4, 5, 6] and [.4, 5, 6]). But markov chain should be handled by jax.lax.scan. And you could use jit compilation to speed things up. Here is a trivial example. In each step chain calculates the following:\npython\nCopy\nimport jax\nimport jax.numpy as jnp \nfrom functools import partial\n\n@jax.jit\ndef step(carry, k):\n    # this function runs a single step in the chain\n    # X_train dim:(3,3)\n    # w1 dim: (1,3)\n    # w2 dim: (3,1)\n    # X_new = log(Relu(w1@X_old)@w2) + e\n    # e~Normal(0, 1)\n    \n    state, X_train, rng = carry\n    rng, rng_input = jax.random.split(rng)\n    e = jax.random.normal(rng) # generate pseudorandom\n    w1 = state['layer1']['weights'] # it is a column vector\n    w2 = state['layer2']['weights'][None, :] # make it a row vector\n    \n    X_train = jax.nn.relu(w1@X_train)[:, None]+1\n    X_train = jnp.log(X_train@w2)\n    X_train = X_train + e\n    \n    return [state, X_train, rng], e\n\n@partial(jax.jit, static_argnums = 3)\ndef fi(state, X_train, key, number_of_steps):\n    rng = jax.random.PRNGKey(key)\n    carry = [state, X_train, rng]\n    carry, random_normals = jax.lax.scan(step, carry, xs = jnp.arange(number_of_steps))\n    state, X, rng = carry\n    return X\n\nX_train = jnp.array([[1., -1., 0.5],\n                     [1., 1, 2.],\n                     [4, 2, 0.1]])\n\nstates = {'layer1':{'weights':jnp.array([[1, -2, 3],\n                                         [4, 5, 6]])},\n          'layer2':{'weights':jnp.array([[1, .2, 3],\n                                         [.4, 5, 6]])}}\n\nvmap_fi = jax.vmap(fi, (0, None, None, None)) # only map on first argument axis 0\n\nkey = 42 # random seed\nnumber_of_steps = 100 # chain runs 100 steps\n\nlast_states = vmap_fi(states, X_train, key, number_of_steps)\nprint(last_states)\nOutput:\npython\nCopy\n[[[ 1.8478627   0.23842478  2.946475  ]\n  [ 1.3278859  -0.28155205  2.4264982 ]\n  [ 2.0921988   0.48276085  3.1908112 ]]\n\n [[ 2.9374144   5.4631433   5.645465  ]\n  [ 3.4333894   5.959118    6.1414394 ]\n  [ 3.4612248   5.9869533   6.169275  ]]]\nIn this example, you could make states dictionaries more complicated. You just need to parallelize on their 0th axis."
        ],
        "link": "https://stackoverflow.com/questions/73995540/execute-markov-chains-with-tree-structured-state-in-parallel-with-jax"
    },
    {
        "title": "Error when trying to jit the computation of the Jacobian in JAX: \"ValueError: Non-hashable static arguments are not supported\"",
        "question": "This question is similar to the question here, but I cannot link with what I should alter.\nI have a function\npython\nCopy\ndef elbo(variational_parameters, eps, a, b):\n    ...\n    return theta, _\n\nelbo = jit(elbo, static_argnames=[\"a\", \"b\"])\nwhere variational_parameters is a vector (one-dimensional array) of length P, eps is a two-dimensional array of dimensions K by N, and a, b are fixed values.\nThe elbo has been successfully vmapped over the rows of eps, and has been jitted by setting by passing a and b to static_argnames, to return theta, which is a two-dimensional array of dimensions K by P.\nI want to take the Jacobian of the output theta with respect to variational_parameters through the elbo function. The first value returned by\npython\nCopy\njacobian(elbo, argnums=0, has_aus=True)(variational_parameters, eps, a, b)\ngives me a three-dimensional array of dimensions K by P by N. This is what I want. As soon as I try to jit this function\npython\nCopy\njit(jacobian(elbo, argnums=0, has_aus=True))(variational_parameters, eps, a, b)\nI get the error\npython\nCopy\nValueError: Non-hashable static arguments are not supported, which can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.partial_eval.DynamicJaxprTracer'> for function elbo is non-hashable.\nAny help would be greatly appreciated; thanks!",
        "answers": [
            "Any parameters you pass to a JIT-compiled function will no longer be static, unless you explicitly mark them as such. So this line:\npython\nCopy\njit(jacobian(elbo, argnums=0, has_aus=True))(variational_parameters, eps, a, b)\nMakes variational_parameters, eps, a, and b non-static. Then within the transformed function these non-static parameters are passed to this function:\npython\nCopy\nelbo = jit(elbo, static_argnames=[\"a\", \"b\"])\nwhich means that you are attempting to pass non-static values as static arguments, which causes an error.\nTo fix this, you should mark the static parameters as static any time they enter a jit-compiled function. In your case it might look something like this:\npython\nCopy\njit(jacobian(elbo, argnums=0, has_aus=True),\n    static_argnums=(2, 3))(variational_parameters, eps, a, b)"
        ],
        "link": "https://stackoverflow.com/questions/73941657/error-when-trying-to-jit-the-computation-of-the-jacobian-in-jax-valueerror-no"
    },
    {
        "title": "How to use JAX vmap to efficiently calculate importance sampling estimate",
        "question": "I have code to calculate the off-policy importance sampling estimate commonly used in reinforcement learning. It is not important to know what that is, but for someone who does it might help them understand this question a little better. Basically, I have a 1D array of instances of a custom Episode class. An Episode has four attributes, all of which are arrays of floats. I have a function which loops over all episodes and for each one, it does a computation based only on the arrays in that episode. The result of that computation is a float, which I then store in a result array. Don't worry about what model.get_prob_this_action() does, you can consider it a black box that takes two floats as input and returns a float. The code for this function before optimizing with JAX is:\npython\nCopy\ndef IS_estimate(model, theta, episodes):\n    \"\"\" Calculate the unweighted importance sampling estimate\n    for each episode in episodes.\n    Return as an array, one element per episode\n    \"\"\"\n    # episodes is an array of custom Python class instances\n    \n    gamma = 1.0\n    result = np.zeros(len(episodes))\n    for ii, ep in enumerate(episodes):\n        obs = ep.observations # 1D array of floats\n        actions = ep.actions # 1D array of floats\n        rewards = ep.rewards # 1D array of floats\n        action_probs = ep.action_probs # 1D array of floats\n\n        pi_news = np.zeros(len(obs))\n        for jj in range(len(obs)):\n            pi_news[jj] = model.get_prob_this_action(obs[jj],actions[jj])\n\n        pi_ratio_prod = np.prod(pi_news / action_probs)\n\n        weighted_return = weighted_sum_gamma(rewards, gamma)\n        result[ii] = pi_ratio_prod * weighted_return\n\n    return np.array(result)\nUnfortunately, I cannot just rewrite the function to work on a single episode and then use jax.vmap to vectorize over that function. The reason is that the argument I want to vectorize is a custom Episode object, which JAX won't support.\nI can get rid of the inner loop to get pi_news using vmap, like:\npython\nCopy\ndef IS_estimate(model, theta, episodes):\n    \"\"\" Calculate the unweighted importance sampling estimate\n    for each episode in episodes.\n    Return as an array, one element per episode\n    \"\"\"\n    # episodes is an array of custom Python class instances\n    \n    gamma = 1.0\n    result = np.zeros(len(episodes))\n    for ii, ep in enumerate(episodes):\n        obs = ep.observations # 1D array of floats\n        actions = ep.actions # 1D array of floats\n        rewards = ep.rewards # 1D array of floats\n        action_probs = ep.action_probs # 1D array of floats\n\n        vmapped_get_prob_this_action = vmap(model.get_prob_this_action,in_axes=(0,0))\n        pi_news = vmapped_get_prob_this_action(obs,actions)\n\n        pi_ratio_prod = np.prod(pi_news / action_probs)\n\n        weighted_return = weighted_sum_gamma(rewards, gamma)\n        result[ii] = pi_ratio_prod * weighted_return\n\n    return np.array(result)\nand this does help some. But ideally, I'd like to vmap my outer loop as well. Does anyone know how I would do this?",
        "answers": [
            "The computation you're describing is an \"array-of-structs\" style computation; JAX's vmap does not support this. What it does support is a \"struct-of-arrays` style computation.\nAs a quick demonstration of this, here's how you might do a simple per-episode computation using first the array-of-structs pattern (with Python for-loops) and then the struct-of-arrays pattern (with jax.vmap):\npython\nCopy\nfrom typing import NamedTuple\nimport jax.numpy as jnp\nimport numpy as np\nimport jax\n\nclass Episode(NamedTuple):\n  observations: jnp.ndarray\n  actions: jnp.ndarray\n\n  def compute_result(self):\n    # stand-in for computing some value from attributes\n    return jnp.dot(self.observations, self.actions)\n\n# Computing result per episode on array of structs:\nrng = np.random.RandomState(42)\nepisodes = [\n    Episode(\n        observations=jnp.array(rng.rand(4)),\n        actions=jnp.array(rng.rand(4)))\n    for i in range(5)\n]\nresult1 = jnp.array([ep.compute_result() for ep in episodes])\nprint(result1)\n# [0.767802   0.83237386 0.49223748 0.5156544  1.1290307 ]\n\n# Computing results on struct of arrays via vmap:\nepisodes_struct_of_arrays = Episode(\n    observations = jnp.vstack([ep.observations for ep in episodes]),\n    actions = jnp.vstack([ep.actions for ep in episodes])\n)\nresult2 = jax.vmap(lambda self: self.compute_result())(episodes_struct_of_arrays)\nprint(result2)\n# [0.767802   0.83237386 0.49223748 0.5156544  1.1290307 ]\nIf you want to use JAX's vmap for this computation, you'll have to use a struct-of-arrays approach like the second one. Note that this also assumes that your Episode class is registered as a pytree (see extending pytrees) which is true by default for NamedTuple types."
        ],
        "link": "https://stackoverflow.com/questions/73921013/how-to-use-jax-vmap-to-efficiently-calculate-importance-sampling-estimate"
    },
    {
        "title": "Exception occurred while installing jaxlib on Ubuntu x86_64",
        "question": "I am unable to figure out how to troubleshoot these errors in jaxlib installation.\nIf somebody could please guide me on how to go about it, it will be much appreciated, Thanks.\nBelow are the commands I am using with the corresponding outputs.\n:~$ uname -a\nLinux pc-name-15-3567 5.15.0-47-generic #51-Ubuntu SMP Thu Aug 11 07:51:15 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n\n:~$ pip install --upgrade pip\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pip in ./.local/lib/python3.10/site-packages (22.2.2)\n\n:~$ pip install --upgrade \"jax\"\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: jax in ./.local/lib/python3.10/site-packages (0.3.17)\nCollecting jax\n  Downloading jax-0.3.19.tar.gz (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 221.8 kB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: absl-py in ./.local/lib/python3.10/site-packages (from jax[cpu]) (1.2.0)\nRequirement already satisfied: etils[epath] in ./.local/lib/python3.10/site-packages (from jax[cpu]) (0.8.0)\nRequirement already satisfied: numpy>=1.20 in /usr/lib/python3/dist-packages (from jax) (1.21.5)\nRequirement already satisfied: opt_einsum in ./.local/lib/python3.10/site-packages (from jax[cpu]) (3.3.0)\nRequirement already satisfied: scipy>=1.5 in /usr/lib/python3/dist-packages (from jax) (1.8.0)\nRequirement already satisfied: typing_extensions in ./.local/lib/python3.10/site-packages (from jax[cpu]) (4.3.0)\nCollecting jaxlib==0.3.15\n  Downloading jaxlib-0.3.15-cp310-none-manylinux2014_x86_64.whl (72.0 MB)\n     ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/72.0 MB 248.9 kB/s eta 0:04:33\nERROR: Exception:\nTraceback (most recent call last):\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 435, in _error_catcher\n    yield\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 516, in read\n    data = self._fp.read(amt) if not fp_closed else b\"\"\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n    data = self.__fp.read(amt)\n  File \"/usr/lib/python3.10/http/client.py\", line 465, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/python3.10/ssl.py\", line 1273, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/usr/lib/python3.10/ssl.py\", line 1129, in read\n    return self._sslobj.read(len, buffer)\nTimeoutError: The read operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n    status = run_func(*args)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\n    return func(self, options, args)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 369, in run\n    requirement_set = resolver.resolve(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n    result = self._result = resolver.resolve(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n    state = resolution.resolve(requirements, max_rounds=max_rounds)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\n    failure_causes = self._attempt_to_pin_criterion(name)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n    criteria = self._get_updated_criteria(candidate)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\n    self._add_to_criteria(criteria, requirement, parent=candidate)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n    if not criterion.candidates:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n    return bool(self._sequence)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n    return any(self)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n    return (c for c in iterator if id(c) not in self._incompatible_ids)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n    candidate = func()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n    self._link_candidate_cache[link] = LinkCandidate(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\n    super().__init__(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\n    self.dist = self._prepare()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\n    dist = self._prepare_distribution()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 308, in _prepare_distribution\n    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 438, in prepare_linked_requirement\n    return self._prepare_linked_requirement(req, parallel_builds)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 483, in _prepare_linked_requirement\n    local_file = unpack_url(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 165, in unpack_url\n    file = get_http_url(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 106, in get_http_url\n    from_path, content_type = download(link, temp_dir.path)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/network/download.py\", line 147, in __call__\n    for chunk in chunks:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n    for chunk in iterable:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n    for chunk in response.raw.stream(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 573, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 509, in read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 440, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\npip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.",
        "answers": [
            "The issue got solved for me by running these commands to install jaxlib:\npip install --upgrade \"jax\"\npip install --upgrade \"jaxlib\""
        ],
        "link": "https://stackoverflow.com/questions/73874153/exception-occurred-while-installing-jaxlib-on-ubuntu-x86-64"
    },
    {
        "title": "How to write this function jax.jit-able>",
        "question": "def factory(points):\n  points.sort()\n  @jax.jit\n  def fwd(x):\n    for i in range(1, len(points)):\n      if x < points[i][0]:\n        return (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * x + (points[i][1] - (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * points[i][0])\n    i = len(points) - 1\n    return (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * x + (points[i][1] - (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * points[i][0])\n  return fwd\nI want to write a function that creates jitted function, given argument: points, a list contain pairs of numbers. I aware that if/else statement can't be jitted and jax.lax.cond() allow conditions but I want something like a break as you can see in the above code. Is there any way to work with conditions?",
        "answers": [
            "The challenge in converting this to JAX-compatible is that your function relies on control flow triggered by values in the array; to make this compatible with JAX you should convert it to a vector-based operation. Here's how you might express your operation in terms of np.where rather than for loops:\npython\nCopy\ndef factory_v2(points):\n  points.sort()\n  def fwd(x):\n    matches = np.where(x < points[1:, 0])[0]\n    i = matches[0] + 1 if len(matches) else len(points) - 1\n    return (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * x + (points[i, 1] - (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * points[i, 0])\n  return fwd\n\nx = 2\npoints = np.array([[4, 0], [2, 1], [6, 5], [4, 6], [5, 7]])\nprint(factory(points)(x))\n# 3.0\n\nprint(factory_v2(points)(x))\n# 3.0\nThis is closer to a JAX-compatible operation, but unfortunately it relies on creating dynamically-shaped arrays. You can get around this by using the size argument to jnp.where. Here's a JAX-compatible version that uses the JAX-only size and fill_value arguments to jnp.where to work around this dynamic size issue:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef factory_jax(points):\n  points = jnp.sort(points)\n  @jax.jit\n  def fwd(x):\n    i = 1 + jnp.where(x < points[1:, 0], size=1, fill_value=len(points) - 2)[0][0]\n    return (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * x + (points[i, 1] - (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * points[i, 0])\n  return fwd\n\nprint(factory_jax(points)(x))\n# 3.0\nIf I've understood the intended input shapes for your code, I believe this should compute the same results as your orginal function."
        ],
        "link": "https://stackoverflow.com/questions/73693195/how-to-write-this-function-jax-jit-able"
    },
    {
        "title": "How to map the kronecker product along array dimensions?",
        "question": "Given two tensors A and B with the same dimension (d>=2) and shapes [A_{1},...,A_{d-2},A_{d-1},A_{d}] and [A_{1},...,A_{d-2},B_{d-1},B_{d}] (shapes of the first d-2 dimensions are identical).\nIs there a way to calculate the kronecker product over the last two dimensions? The shape of my_kron(A,B)should be [A_{1},...,A_{d-2},A_{d-1}*B_{d-1},A_{d}*B_{d}]. For example with d=3,\npython\nCopy\nA.shape=[2,3,3]\nB.shape=[2,4,4]\nC=my_kron(A,B)\nC[0,...] should be the kronecker product of A[0,...] and B[0,...] and C[1,...] the kronecker product of A[1,...] and B[1,...].\nFor d=2 this is simply what the jnp.kron(or np.kron) function does.\nFor d=3 this can be achived with jax.vmap. jax.vmap(lambda x, y: jnp.kron(x[0, :], y[0, :]))(A, B)\nBut I was not able to find a solution for general (unknown) dimensions. Any suggestions?",
        "answers": [
            "In numpy terms I think this is what you are doing:\npython\nCopy\nIn [104]: A = np.arange(2*3*3).reshape(2,3,3)\nIn [105]: B = np.arange(2*4*4).reshape(2,4,4)\n\nIn [106]: C = np.array([np.kron(a,b) for a,b in zip(A,B)])\nIn [107]: C.shape\nOut[107]: (2, 12, 12)\nThat treats the initial dimension, the 2, as a batch. One obvious generalization is to reshape the arrays, reducing the higher dimensions to 1, e.g. reshape(-1,3,3), etc. And then afterwards, reshape C back to the desired n-dimensions.\nnp.kron does accept 3d (and higher), but it's doing some sort of outer on the shared 2 dimension:\npython\nCopy\nIn [108]: np.kron(A,B).shape\nOut[108]: (4, 12, 12)\nAnd visualizing that 4 dimension as (2,2), I can take the diagonal and get your C:\npython\nCopy\nIn [109]: np.allclose(np.kron(A,B)[[0,3]], C)\nOut[109]: True\nThe full kron does more calculations than needed, but is still faster:\npython\nCopy\nIn [110]: timeit C = np.array([np.kron(a,b) for a,b in zip(A,B)])\n108 µs ± 2.23 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nIn [111]: timeit np.kron(A,B)[[0,3]]\n76.4 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nI'm sure it's possible to do your calculation in a more direct way, but doing that requires a better understanding of how the kron works. A quick glance as the np.kron code suggest that is does an outer(A,B)\npython\nCopy\nIn [114]: np.outer(A,B).shape\nOut[114]: (18, 32)\nwhich has the same number of elements, but it then reshapes and concatenates to produce the kron layout.\nBut following a hunch, I found that this is equivalent to what you want:\npython\nCopy\nIn [123]: D = A[:,:,None,:,None]*B[:,None,:,None,:]\nIn [124]: np.allclose(D.reshape(2,12,12),C)\nOut[124]: True\nIn [125]: timeit np.reshape(A[:,:,None,:,None]*B[:,None,:,None,:],(2,12,12))\n14.3 µs ± 184 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\nThat is easily generalized to more leading dimensions.\npython\nCopy\ndef my_kron(A,B):\n   D = A[...,:,None,:,None]*B[...,None,:,None,:]\n   ds = D.shape\n   newshape = (*ds[:-4],ds[-4]*ds[-3],ds[-2]*ds[-1])\n   return D.reshape(newshape)\n\nIn [137]: my_kron(A.reshape(1,2,1,3,3),B.reshape(1,2,1,4,4)).shape\nOut[137]: (1, 2, 1, 12, 12)"
        ],
        "link": "https://stackoverflow.com/questions/73673599/how-to-map-the-kronecker-product-along-array-dimensions"
    },
    {
        "title": "How to vmap over specific funciton in jax?",
        "question": "I have this function which works for single vector:\npython\nCopy\ndef vec_to_board(vector, player, dim, reverse=False):\n    player_board = np.zeros(dim * dim)\n    player_pos = np.argwhere(vector == player)\n    if not reverse:\n        player_board[mapping[player_pos.T]] = 1\n    else:\n        player_board[reverse_mapping[player_pos.T]] = 1\n    return np.reshape(player_board, [dim, dim])\nHowever, I want it to work for a batch of vectors.\nWhat I have tried so far:\npython\nCopy\nstates = jnp.array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]])\n batch_size = 1\nb_states = vmap(vec_to_board)((states, 1, 4), batch_size)\nThis doesn't work. However, if I understand correctly vmap should be able to handle this transformation for batches?",
        "answers": [
            "There are a couple issues you'll run into when trying to vmap this function:\nThis function is defined in terms of numpy arrays, not jax arrays. How do I know? JAX arrays are immutable, so things like arr[idx] = 1 will raise errors. You need to replace these with equivalent JAX operations (see JAX Sharp Bits: in-place updates) and ensure your function works with JAX array operations rather than numpy array operations.\nYour function makes used of dynamically-shaped arrays; e.g. player_pos, has a shape dependent on the number of nonzero entries in vector == player. You'll have to rewrite your function in terms of statically-shaped arrays. There is some discussion of this in the jnp.argwhere docstring; for example, if you know a priori how many True entries you expect in the array, you can specify the size to make this work.\nGood luck!"
        ],
        "link": "https://stackoverflow.com/questions/73588309/how-to-vmap-over-specific-funciton-in-jax"
    },
    {
        "title": "Rewriting for loop with jax.lax.scan",
        "question": "I'm having troubles understanding the JAX documentation. Can somebody give me a hint on how to rewrite simple code like this with jax.lax.scan?\npython\nCopy\nnumbers = numpy.array( [ [3.0, 14.0], [15.0, -7.0], [16.0, -11.0] ])\nevenNumbers = 0\nfor row in numbers:\n      for n in row:\n         if n % 2 == 0:\n            evenNumbers += 1",
        "answers": [
            "Assuming a solution should demonstrate the concepts rather than optimize the example shown, the function to be jax.lax.scanned must match the expected signature and any dynamic condition has to be replaced with jax.lax.cond. The code below is the closest to the original I could think of, but please be aware that I'm anything but an jaxpert.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(carry, row):\n\n    even = 0\n    for n in row:\n        even += jax.lax.cond(n % 2 == 0, lambda: 1, lambda: 0)\n\n    return carry + even, even\n\nnumbers = jnp.array([[3.0, 14.0], [15.0, -7.0], [16.0, -11.0]])\njax.lax.scan(f, 0, numbers)\nOutput\npython\nCopy\n(DeviceArray(2, dtype=int32, weak_type=True),\n DeviceArray([1, 0, 1], dtype=int32, weak_type=True))"
        ],
        "link": "https://stackoverflow.com/questions/73564732/rewriting-for-loop-with-jax-lax-scan"
    },
    {
        "title": "JAX: Getting rid of zero-gradient",
        "question": "Is there a way how to modify this function (MyFunc) so that it gives the same result, but its derivative is not zero gradient?\npython\nCopy\nfrom jax import grad\nimport jax.nn as nn\nimport numpy as np\n\ndef MyFunc(coefs):\n   a = coefs[0]\n   b = coefs[1]\n   c = coefs[2]\n   \n   if a > b:\n      return 30.0\n   elif b > c:\n      return 20.0\n   else:\n      return 10.0   \n   \nmyFuncDeriv = grad (MyFunc)   \n\n# prints [0. 0. 0.]\nprint (myFuncDeriv(np.random.sample(3)))\n# prints [0. 0. 0.]\nprint (myFuncDeriv(np.array([1.0, 2.0, 3.0])))\nEDIT: Similar function which doesn't give zero gradient - but it doesn't return 30/20/10\npython\nCopy\ndef MyFunc2(coefs):\n    a = coefs[0]\n    b = coefs[1]\n    c = coefs[2]\n    if a > b:\n        return nn.sigmoid(a)*30.0\n    if b > c:\n        return nn.sigmoid(b)*20.0\n    else:\n        return nn.sigmoid(c)*10.0\n\n\nmyFunc2Deriv = grad (MyFunc2)   \n\n# prints [0.         0.         0.45176652]\nprint (myFuncDeriv(np.array([1.0, 2.0, 3.0])))\n# prints for example [6.1160526 0.        0.       ]\nprint (myFunc2Deriv(np.random.sample(3)))",
        "answers": [
            "The gradient of your function is zero because this is the correct result for the gradient as your function is defined. For more information on this phenomenon, see FAQ: Why are gradients zero for functions based on sort order?\nIf you want a sort-based function with non-zero gradients, you can achieve this by replacing your step-wise function with a smooth approximation. The sigmoid version you included in your question seems like a reasonable approach for this approximation.\nBut note that the answer to your exact question – how to make a function that produces the same output but has nonzero gradients – is impossible, because a function returning the same outputs as yours for all inputs has a zero gradient by definition."
        ],
        "link": "https://stackoverflow.com/questions/73507935/jax-getting-rid-of-zero-gradient"
    },
    {
        "title": "How to install objax with GPU support?",
        "question": "I have followed the objax documentation to install the library with GPU support: https://objax.readthedocs.io/en/stable/installation_setup.html\ni.e.\npip install --upgrade objax\nCUDA_VERSION=11.6\npip install -f https://storage.googleapis.com/jax-releases/jax_releases.html jaxlib==`python3 -c 'import jaxlib; print(jaxlib.__version__)'`+cuda`echo $CUDA_VERSION | sed s:\\\\\\.::g` \nHowever the last step doesn't work. I get the following error message:\nERROR: Could not find a version that satisfies the requirement jaxlib==0.3.15+cuda116 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15) ERROR: No matching distribution found for jaxlib==0.3.15+cuda116\nI have tried with multiple versions of python/CUDA, but I always get this error.\nExecuting pip install --upgrade pip at the begining does not help.\nSystem description:\nOperating system: Ubuntu 20.04.4 LTS\nCUDA Version: 11.6\nPython version: 3.8.13",
        "answers": [
            "JAX recently updated its GPU installation instructions, which you can find here: https://github.com/google/jax#pip-installation-gpu-cuda\nIn particular, the CUDA wheels are now located at https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo, for example, you can install JAX with\npython\nCopy\n$ pip install \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nand replace cuda11 and cudnn805 respectively with the appropriate CUDA and CUDNN version for your system, ensuring that they match the versions listed in the index at the above URL.\nI've sent a pull request to the objax repository to update the instructions you were following: https://github.com/google/objax/pull/246",
            "The doc page was last updated 2 year ago. It's outdated and due to this it currently lies. Take a close look at https://storage.googleapis.com/jax-releases/jax_releases.html — there're currently no releases with CUDA.\nAs for no-CUDA versions you can install them directly from PyPI:\npython\nCopy\npip install jaxlib\nFor CUDA version — try to compile from sources. Or use conda: https://anaconda.org/search?q=jaxlib"
        ],
        "link": "https://stackoverflow.com/questions/73472417/how-to-install-objax-with-gpu-support"
    },
    {
        "title": "Local devices VS non local devices in multi GPU processing",
        "question": "I'm reading JAX documentation on jax.local_devices and in it, it is written:\nLike jax.devices(), but only returns devices local to a given process.\nAnd in jax.devices() it is written:\nReturns a list of all devices for a given backend.\nI don't know what exactly are these local and non-local devices. Could you please elaborate on the difference between these?",
        "answers": [
            "This is discussed in JAX's documentation in Using JAX in multi-host and multi-process environments:\nA process’s local devices are those that it can directly address and launch computations on. For example, on a GPU cluster, each host can only launch computations on the directly attached GPUs. On a Cloud TPU pod, each host can only launch computations on the 8 TPU cores attached directly to that host (see the Cloud TPU System Architecture documentation for more details). You can see a process’s local devices via jax.local_devices().\nThe global devices are the devices across all processes. A computation can span devices across processes and perform collective operations via the direct communication links between devices, as long as each process launches the computation on its local devices. You can see all available global devices via jax.devices(). A process’s local devices are always a subset of the global devices."
        ],
        "link": "https://stackoverflow.com/questions/73458553/local-devices-vs-non-local-devices-in-multi-gpu-processing"
    },
    {
        "title": "JAX: average per-example gradients different from aggregated gradients",
        "question": "I want to compute per-example gradients to perform per-example clipping before the final gradient descent step.\nI wanted to ensure that the per-example gradients are correct. Therefore I implemented this minimal example below to test standard gradient computation and per-example gradient computation with JAX.\nThe problem I have is, that the average of the per-example gradients differs from the gradients of the standard computation.\nDoes someone see where I went wrong?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\n\ndef loss(params, x, t):\n    w, b = params\n    y = jnp.dot(w, x.T) + b\n    return ((t.T - y)**2).sum()\n\n\ndef main():\n\n    n_samples = 3\n    dims_in = 7\n    dims_out = 5\n\n    key = random.PRNGKey(0)\n\n    # Random data\n    x = random.normal(key, (n_samples, dims_in), dtype=jnp.float32)\n    t = random.normal(key, (n_samples, dims_out), dtype=jnp.float32)\n    \n    # Random weights\n    w = random.normal(key, (dims_out, dims_in), dtype=jnp.float32)\n    b = random.normal(key, (dims_out, 1), dtype=jnp.float32)\n    params = (w, b)\n\n    # Standard gradient\n    reduced_grads = jax.grad(loss)\n    dw0, db0 = reduced_grads(params, x, t)\n    print(f\"{dw0.shape = }\") \n    print(f\"{db0.shape = }\") \n\n    # Per-example gradients\n    perex_grads = jax.vmap(jax.grad(loss), in_axes=((None, None), 0, 0))\n    dw1, db1 = perex_grads(params, x, t)\n    print(f\"{dw1.shape = }\") \n    print(f\"{db1.shape = }\")\n\n    # Gradients are different!\n    print(jnp.allclose(dw0, jnp.mean(dw1, axis=0)))  # should be True\n    print(jnp.allclose(db0, jnp.mean(db1, axis=0)))  # should be True\n    \n\nif __name__ == \"__main__\":\n    main()",
        "answers": [
            "THe issue is that vmap effectively passes 1D inputs to your function, and your loss function operates differently on 1D vs. 2D inputs. You can see this by comparing the following:\npython\nCopy\nprint(loss(params, x[0], t[0]))\n# 87.5574\n\nprint(loss(params, x[:1], t[:1]))\n# 18.089672\nIf you inspect the shapes of the intermediate results within the loss function, it should become clear why these differ (e.g. you're summing over a shape (5, 5) array of differences instead of a shape (5, 1) array of differences).\nTo use per-example gradients, you'll need to change the implementation of your loss function so that it returns the correct result whether the input is 1D or 2D. The easiest way to do this is probably to use jnp.atleast_2d to ensure that the inputs are two-dimensional:\npython\nCopy\ndef loss(params, x, t):\n    x = jnp.atleast_2d(x)\n    t = jnp.atleast_2d(t)\n    w, b = params\n    y = jnp.dot(w, x.T) + b\n    return ((t.T - y)**2).sum()\nAt this point the sum (not the mean) of the per-example gradients will match the full computation:\npython\nCopy\nprint(jnp.allclose(dw0, jnp.sum(dw1, axis=0)))\n# True\nprint(jnp.allclose(db0, jnp.sum(db1, axis=0)))\n# True"
        ],
        "link": "https://stackoverflow.com/questions/73435394/jax-average-per-example-gradients-different-from-aggregated-gradients"
    },
    {
        "title": "Struggling to understand nested vmaps in JAX",
        "question": "I just about understand unnested vmaps, but try as I may, and I have tried my darnedest, nested vmaps continue to elude me. Take the snippet from this text for example\nI don't understand what the axis are in this case. Is the nested vmap(kernel, (0, None)) some sort of partial function application? Why is the function mapped twice? Can someone please explain what is going on behind the scene in other words. What does a nested vmap desugar to?? All the answers that I have found are variants of the same curt explanation: mapping over both axis, which I am struggling with.",
        "answers": [
            "Each time vmap is applied, it maps over a single axis. So say for simplicity that you have a function that takes two scalars and outputs a scalar:\npython\nCopy\ndef f(x, y):\n  assert jnp.ndim(x) == jnp.ndim(y) == 0  # x and y are scalars\n  return x + y\n\nprint(f(1, 2))\n# 0\nIf you want to apply this function to an array of x values and a single y value, you can do this with vmap:\npython\nCopy\nf_mapped_over_x = jax.vmap(f, in_axes=(0, None))\n\nx = jnp.arange(5)\nprint(f_mapped_over_x(x, 1))\n# [1 2 3 4 5]\nin_axes=(0, None) means that it is mapped along the leading axis of the first argument, x, and there is no mapping of the second argument, y.\nLikewise, if you want to apply this function to a single x value and an array of y values, you can specify this via in_axes:\npython\nCopy\nf_mapped_over_y = jax.vmap(f, in_axes=(None, 0))\n\ny = jnp.arange(5, 10)\nprint(f_mapped_over_y(1, y))\n# [ 6  7  8  9 10]\nIf you wish to map the function over both arrays at once, you can do this by specifying in_axes=(0, 0), or equivalently in_axes=0:\npython\nCopy\nf_mapped_over_x_and_y = jax.vmap(f, in_axes=(0, 0))\n\nprint(f_mapped_over_x_and_y(x, y))\n# [ 5  7  9 11 13]\nBut suppose you want to map first over x, then over y, to get a sort of \"outer-product\" version of the function. You can do this via a nested vmap, first mapping over just x, then mapping over just y:\npython\nCopy\nf_mapped_over_x_then_y = jax.vmap(jax.vmap(f, in_axes=(None, 0)), in_axes=(0, None))\n\nprint(f_mapped_over_x_then_y(x, y))\n# [[ 5  6  7  8  9]\n#  [ 6  7  8  9 10]\n#  [ 7  8  9 10 11]\n#  [ 8  9 10 11 12]\n#  [ 9 10 11 12 13]]\nThe nesting of vmaps is what lets you map over two axes separately."
        ],
        "link": "https://stackoverflow.com/questions/73212780/struggling-to-understand-nested-vmaps-in-jax"
    },
    {
        "title": "Is it safe to read the value of numpy.empty or jax.numpy.empty?",
        "question": "In Flax, we typically initialize a model by passing in a random vector and let the library figure the correct shape for the parameters via shape inference. For example, this is what the tutorial did\npython\nCopy\ndef create_train_state(rng, learning_rate, momentum):\n  \"\"\"Creates initial `TrainState`.\"\"\"\n  cnn = CNN()\n  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return train_state.TrainState.create(\n      apply_fn=cnn.apply, params=params, tx=tx)\nIt is worth noting that the concrete value of jnp.ones([1, 28, 28, 1]) does not matter, as shape inference only relies on its shape. I can replace it with jnp.zeros([1, 28, 28, 1]) or jnp.random(jax.random.PRNGKey(42), [1, 28, 28, 1]), and it will give me the exactly same result.\nMy question is, can I use jnp.empty([1, 28, 28, 1]) instead? I want to use jnp.empty to clarify that we don't care about the value (and it could also be faster but the speedup is negligible). However, there is something called trap representation in C, and it looks like reading from jnp.empty without overwriting it first could trigger undefined behavior. Since Numpy is a light wrapper around C, should I worry about that?\nBonus question: let's forget about Jax and focus on vanilla Numpy. It is safe to read from np.empty([...])? Again, I don't care about the value, but I do care about not getting a segfault.",
        "answers": [
            "Because XLA does not provide a mechanism to create uninitialized memory, in JAX jnp.empty is currently (v0.3.14) equivalent to jnp.zeros (see https://github.com/google/jax/blob/jax-v0.3.14/jax/_src/numpy/lax_numpy.py#L2007-L2009)\nSo at least in the current release, it is safe to refer to the contents of jnp.empty arrays. But if you're going to rely on that property, I'd suggest using jnp.zeros instead, so that if the jnp.empty implementation changes in the future your assumptions will still be valid.\nnp.empty is different: it will include uninitialized values, and so your program's behavior may change unpredictably from run to run if you rely on those values. There's no danger of memory corruption/segfaults when accessing these uninitialized values: the memory is allocated, it's just that the contents are uninitialized and so the values will reflect whatever bits happened to be stored there at the time the block was allocated."
        ],
        "link": "https://stackoverflow.com/questions/72905359/is-it-safe-to-read-the-value-of-numpy-empty-or-jax-numpy-empty"
    },
    {
        "title": "How to reorder different sets of parameters in dm-haiku",
        "question": "In dm-haiku, parameters of neural networks are defined in dictionaries where keys are module (and submodule) names. If you would like to traverse through the values, there are multiple ways of doing so as shown in this dm-haiku issue. However, the dictionary doesn't respect the ordering of the modules and makes it hard to parse submodules. For example, if I have 2 linear layers, each followed by a mlp layer, then using hk.data_structures.traverse(params) will (roughly) return:\npython\nCopy\n['linear', 'linear_2', 'mlp/~/1', 'mlp/~/2'].\nwhereas I would like it to return:\npython\nCopy\n['linear', 'mlp/~/1', 'linear_2', 'mlp/~/2'].\nMy reason for wanting this form is if creating an invertible neural network and wanting to reverse the order the params are called, isolating substituent parts for other purposes (e.g. transfer learning), or, in general, wanting more control of how and where to (re)use trained parameters.\nTo deal with this, I've resorted to regex the names and put them in the order that I want, then using hk.data_structures.filter(predicate, params) to filter by the sorted module names. Although, this is quite tedious if I have to remake a regex every time I want to do this.\nI'm wondering if there is a way to convert a dm-haiku dictionary of params to something like a pytree with a hierarchy and ordering that makes this easier? I believe equinox handles parameters in this manner (and I'm going to look more into how that is done soon), but wanted to check to see if I'm overlooking a simple method to allow grouping, reversing, and other permutations of the params's dictionary?",
        "answers": [
            "According to source code https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/filtering.py#L42#L46 haiku use the sorted function of dict (haiku parameters are vanilla dict since 0.0.6) for hk.data_structures.traverse. Therefore you can't get the result you want without modifying the function itself. By the way, I don't get precisely what do you mean by \"to reverse the order the params are called\". All parameters are passed together in input and then the only thing that determines the order of use is the architecture of the function itself so you should manually invert the forward pass but you don't need to change something in params."
        ],
        "link": "https://stackoverflow.com/questions/72860276/how-to-reorder-different-sets-of-parameters-in-dm-haiku"
    },
    {
        "title": "Using JAX with NetworkX",
        "question": "Can I use JIT from JAX with NetworkX algorithms? For instance, if were to compute the average clustering coefficient for a NetworkX graph object, is it possible to use the @jit decorator to speed up my analysis pipeline?",
        "answers": [
            "No, JAX's JIT and other transforms only work with functions implemented via JAX primitives (generally operations defined in jax.lax, jax.numpy, and related submodules). They cannot be used to compile/transform arbitrary Python code."
        ],
        "link": "https://stackoverflow.com/questions/72807939/using-jax-with-networkx"
    },
    {
        "title": "Modify an array from indexes contained in another array",
        "question": "I have an array of the shape (2,10) such as:\npython\nCopy\narr = jnp.ones(shape=(2,10)) * 2\nor\npython\nCopy\n[[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\nand another array, for example [2,4].\nI want the second array to tell from which index the elements of arr should be masked. Here the result would be:\npython\nCopy\n[[2. 2. -1. -1. -1. -1. -1. -1. -1. -1.]\n [2. 2. 2. 2.  -1. -1. -1. -1. -1. -1.]]\nI need to use jax.numpy and the answer to be vectorized and fast if possible, i.e. not using loops.",
        "answers": [
            "You can do this with a vmapped three-term jnp.where statement. For example:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\narr = jnp.ones(shape=(2,10)) * 2\nidx = jnp.array([2, 4])\n\n@jax.vmap\ndef f(row, ind):\n  return jnp.where(jnp.arange(len(row)) < ind, row, -1)\n\nf(arr, idx)\n# DeviceArray([[ 2.,  2., -1., -1., -1., -1., -1., -1., -1., -1.],\n#              [ 2.,  2.,  2.,  2., -1., -1., -1., -1., -1., -1.]], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/72553056/modify-an-array-from-indexes-contained-in-another-array"
    },
    {
        "title": "Get dictionary keys for given batched values - Python",
        "question": "I defined a dictionary A and would like to find the keys given a batch of values a:\npython\nCopy\ndef dictionary(r):\n return dict(enumerate(r))\n\ndef get_key(val, my_dict):\n   for key, value in my_dict.items():\n      if np.array_equal(val,value):\n          return key\n    \n\n # dictionary\n A = jnp.array([[0, 0],[1,1],[2,2],[3,3]])\n A = dictionary(A)\n\n a = jnp.array([[[1, 1],[2, 2], [3,3]],[[0, 0],[3, 3], [2,2]]])\n keys = jax.vmap(jax.vmap(get_key, in_axes=(0,None)), in_axes=(0,None))(a, A)\nThe expected output should be: keys = [[1,2,3],[0,3,2]]\nWhy am I getting Noneas an output?",
        "answers": [
            "JAX transforms like vmap work by tracing the function, meaning they replace the value with an abstract representation of the value to extract the sequence of operations encoded in the function (See How to think in JAX for a good intro to this concept).\nWhat this means is that to work correctly with vmap, a function can only use JAX methods, not numpy methods, so your use of np.array_equal breaks the abstraction.\nUnfortunately, there's not really any replacement for it, because there's no mechanism to look up an abstract JAX value in a concrete Python dictionary. If you want to do dict lookups of JAX values, you should avoid transforms and just use Python loops:\npython\nCopy\nkeys = jnp.array([[get_key(x, A) for x in row] for row in a])\nOn the other hand, I suspect this is more of an XY problem; your goal is not to look up dictionary values within a jax transform, but rather to solve some problem. Perhaps you should ask a question about how to solve the problem, rather than how to get around an issue with the solution you have tried.\nBut if you're willing to not directly use the dict, an alternative get_key implementation that is compatible with JAX might look something like this:\npython\nCopy\ndef get_key(val, my_dict):\n  keys = jnp.array(list(my_dict.keys()))\n  values = jnp.array(list(my_dict.values()))\n  return keys[jnp.where((values == val).all(-1), size=1)]"
        ],
        "link": "https://stackoverflow.com/questions/72390628/get-dictionary-keys-for-given-batched-values-python"
    },
    {
        "title": "Get batched indices from stacked matrices - Python Jax",
        "question": "I would like to extract the indices of stacked matrices.\nLet us say we have an array a of dimension (3, 2, 4), meaning that we have three arrays of dimension (2,4) and a list of indices (3, 2).\npython\nCopy\ndef get_cols(x,idx):  \n  x = x[:,idx]\n  return x\n\n\nidx = jnp.array([[0,1],[2,3],[1,2]])\n\na = jnp.array([[[1,2,3,4],\n            [3,2,2,4]],\n           \n           [[100,20,3,50],\n            [5,5,2,4]],\n                         \n           [[1,2,3,4],\n            [3,2,2,4]]\n           ])\n\n\n\ne = jax.vmap(get_cols, in_axes=(None,0))(a,idx)\nI want to extract the columns of the different matrices given a batch of indices. I expect the following result:\npython\nCopy\ne = [[[[1,2],\n  [3,2]],\n\n  [[100,20],\n  [5,5]],\n\n  [[1,2],\n  [3,2]]],\n \n \n \n [[[3,4],\n  [2,4]],\n  \n  [[3,50],\n  [2,4]],\n  \n  [[3,4],\n  [2,4]]],\n \n \n \n \n[[[2,3],\n[2,2]],\n           \n[[20,3],\n [5,2]],\n                         \n[[2,3],\n[2,2]]]]\nWhat am I missing?",
        "answers": [
            "It looks like you're interested in a double vmap over the inputs; e.g. something like this:\npython\nCopy\ne = jax.vmap(jax.vmap(get_cols, in_axes=(0, None)), in_axes=(None, 0))(a, idx)\nprint(e)\npython\nCopy\n[[[[  1   2]\n   [  3   2]]\n\n  [[100  20]\n   [  5   5]]\n\n  [[  1   2]\n   [  3   2]]]\n\n\n [[[  3   4]\n   [  2   4]]\n\n  [[  3  50]\n   [  2   4]]\n\n  [[  3   4]\n   [  2   4]]]\n\n\n [[[  2   3]\n   [  2   2]]\n\n  [[ 20   3]\n   [  5   2]]\n\n  [[  2   3]\n   [  2   2]]]]"
        ],
        "link": "https://stackoverflow.com/questions/72351994/get-batched-indices-from-stacked-matrices-python-jax"
    },
    {
        "title": "JAX pmap with multi-core CPU",
        "question": "What is the correct method for using multiple CPU cores with jax.pmap?\nThe following example creates an environment variable for SPMD on CPU core backends, tests that JAX recognises the devices, and attempts a device lock.\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'\n\nimport jax as jx\nimport jax.numpy as jnp\n\njx.local_device_count()\n# WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n# 2\n\njx.devices(\"cpu\")\n# [CpuDevice(id=0), CpuDevice(id=1)]\n\ndef sfunc(x): while True: pass\n\njx.pmap(sfunc)(jnp.arange(2))\nExecuting from a jupyter kernel and observing htop shows that only one core is locked\nI receive the same output from htop when omitting the first two lines and running:\n$ env XLA_FLAGS=--xla_force_host_platform_device_count=2 python test.py\nReplacing sfunc with\ndef sfunc(x): return 2.0*x\nand calling\njx.pmap(sfunc)(jnp.arange(2))\n# ShardedDeviceArray([0., 2.], dtype=float32, weak_type=True)\ndoes return a SharedDeviecArray.\nClearly I am not correctly configuring JAX/XLA to use two cores. What am I missing and what can I do to diagnose the problem?",
        "answers": [
            "As far as I can tell, you are configuring the cores correctly (see e.g. Issue #2714). The problem lies in your test function:\npython\nCopy\ndef sfunc(x): while True: pass\nThis function gets stuck in an infinite loop at trace-time, not at run-time. Tracing happens in your host Python process on a single CPU (see How to think in JAX for an introduction to the idea of tracing within JAX transformations).\nIf you want to observe CPU usage at runtime, you'll have to use a function that finishes tracing and begins running. For that you could use any long-running function that actually produces results. Here is a simple example:\npython\nCopy\ndef sfunc(x):\n  for i in range(100):\n    x = (x @ x)\n  return x\n\njx.pmap(sfunc)(jnp.zeros((2, 1000, 1000)))"
        ],
        "link": "https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu"
    },
    {
        "title": "JAX return types after transformations",
        "question": "Why is the return type of jax.grad different to other jax transformations in the following scenario?\nConsider a function to be transformed by JAX which takes a custom container as an argument\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom collections import namedtuple\n\n# Define NTContainer using namedtuple factory\nNTContainer = namedtuple(\n    'NTContainer',['name','flag','array']\n)\n\n# Instantiate container\ncontainer = NTContainer(\n   name='composite-container',\n   flag=0,\n   array=jnp.reshape(jnp.arange(6,dtype='f4'),(3,2)),\n)\n\n# Test function\ndef test_func(container : NTContainer):\n    if container.flag == 0:\n        return container.array.sum()\n    return container.array.prod()\nJAX needs to be informed how to handle NTContainer correctly (default namedtuple pytree cannot be used)\npython\nCopy\n# Register NTContainer pytree to handle static and traceable members\n\ndef unpack_NTC(c):\n    active, passive = (c.array,), (c.name, c.flag)\n    return active, passive\n\ndef repack_NTC(passive, active):\n    (name, flag), (array,) = passive, active\n    return NTContainer(name,flag,value)\n\njax.tree_util.register_pytree_node(\n    NTContainer, unpack_NTC, repack_NTC,\n)\nNow performing several jax transforms and calling with container results in\npython\nCopy\njax.jit(test_func)(container)\n# DeviceArray(15., dtype=float32)\n\njax.vmap(test_func)(container)\n# DeviceArray([1., 5., 9.], dtype=float32)\n\njax.grad(test_func)(container)\n# NTContainer(name='composite-container',\n#     flag=0,\n#     array=DeviceArray([[1., 1.],\n#         [1., 1.],\n#         [1., 1.]], dtype=float32))\nWhy does the jax.grad transform call return a NTContainer rather than a DeviceArray?",
        "answers": [
            "The short answer is that this is the return value because this is how grad is defined, and to be honest I'm not sure how else it could be defined.\nThinking about these transforms;\njit(f)(x) computes the output of f applied to x: the result is a scalar.\nvmap(f)(x) computes the output of f applied to each element of the arrays in x: the result is a vector.\ngrad(f)(x) computes the gradient of f with respect to each component of x, and the result is one value per component of x.\nSo it makes sense that grad(f)(x) in this case has to return a collection of multiple values, but how should this collection of gradients be packaged?\nJAX could have defined grad so that it always returns these as a list or tuple, but this might make it hard to understand how the output gradients relate to the inputs, particularly for more complicated structures like nested lists, tuples, and dicts. So instead JAX takes these per-component gradients and re-bundles them in the same structure as the input argument: therefore, in this case the result is a NTContainer containing those per-component gradients."
        ],
        "link": "https://stackoverflow.com/questions/72066135/jax-return-types-after-transformations"
    },
    {
        "title": "Repeating rows from array",
        "question": "I have a problem becase I would like to repeat n time all rows from array(X, Y) without using loops and get array(n*X, Y)\npython\nCopy\nimport jax.numpy as jnp\n\narr = jnp.array([[12, 14, 12, 0, 1],\n                [0, 14, 12, 0, 1],\n                [0, 0, 12, 0, 1]])\nn = 3\n\nresult = jnp.array([[12 14 12 0 1],\n                    [12 14 12 0 1],\n                    [12 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 0 12 0 1],\n                    [0 0 12 0 1],\n                    [0 0 12 0 1]])\nI haven't found any built-in method to perform this operation, tried with jnp.tile, jnp.repeat.\njnp.repeat\npython\nCopy\narr_r = jnp.repeat(arr, n, axis=1)\n\nOutput:\n[[12 12 12 14 14 14 12 12 12  0  0  0  1  1  1]\n [ 0  0  0 14 14 14 12 12 12  0  0  0  1  1  1]\n [ 0  0  0  0  0  0 12 12 12  0  0  0  1  1  1]]\n\narr_t = jnp.tile(arr, n)\n\nOutput:\n[[12 14 12  0  1 12 14 12  0  1 12 14 12  0  1]\n [ 0 14 12  0  1  0 14 12  0  1  0 14 12  0  1]\n [ 0  0 12  0  1  0  0 12  0  1  0  0 12  0  1]]\nMaybe I may construct result array from array_t...",
        "answers": [
            "You say you tried jnp.repeat but don't explain why it doesn't do what you want. I'm guessing you're neglecting the axis parameter:\npython\nCopy\njnp.repeat(arr, n, axis=0)"
        ],
        "link": "https://stackoverflow.com/questions/71974684/repeating-rows-from-array"
    },
    {
        "title": "Parameters do not converge at a lower tolerance in nonlinear least square implementation in python",
        "question": "I am translating some of my R codes to Python as a learning process, especially trying JAX for autodiff.\nIn functions to implement non-linear least square, when I set tolerance at 1e-8, the estimated parameters are nearly identical after several iterations, but the algorithm never appear to converge.\nHowever, the R codes converge at the 12th inter at tol=1e-8 and 14th inter at tol=1e-9. The estimated parameters are almost the same as the ones resulted from Python implementation.\nI think this has something to do with floating point, but not sure which step I could improve to make the converge as quickly as seen in R.\nHere are my codes, and most steps are the same as in R\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport scipy.linalg as ola\n\n\ndef update_parm(X, y, fun, dfun, parm, theta, wt):\n    len_y = len(y)\n    mean_fun = fun(X, parm)\n\n    if (type(wt) == bool):\n        if (wt):\n            var_fun = np.exp(theta * np.log(mean_fun))\n            sqrtW = 1 / np.sqrt(var_fun ** 2)\n        else:\n            sqrtW = 1\n    else:\n        sqrtW = wt\n        \n    gradX = dfun(x, parm)\n    weighted_X = sqrtW.reshape(len_y, 1) * gradX\n    z = gradX @ parm + (y - mean_fun)\n    weighted_z = sqrtW * z\n    qr_gradX = ola.qr(weighted_X, mode=\"economic\")\n    Q = qr_gradX[0]\n    R = qr_gradX[1]\n    new_parm = ola.solve(R, np.dot(Q.T, weighted_z))\n    \n    return new_parm\n\n\ndef nls_irwls(X, y, fun, dfun, init, theta = 1, tol = 1e-8, maxiter = 500):\n\n    old_parm = init\n    iter = 0\n\n    while (iter < maxiter):\n        new_parm = update_parm(X, y, fun, dfun, parm=old_parm, theta=theta, wt=True)\n        parm_diff = np.max(np.abs(new_parm - old_parm) / np.abs(old_parm))\n        print(parm_diff)\n\n        if (parm_diff < tol) :\n            break\n        else:\n            old_parm = new_parm\n            iter += 1\n            print(new_parm)\n\n    if (iter == maxiter):\n        print(\"The algorithm failed to converge\")\n    else:\n        return {\"Estimated coefficient\": new_parm}\n\n\nx = np.array([0.25, 0.5, 0.75, 1, 1.25, 2, 3, 4, 5, 6, 8])\ny = np.array([2.05, 1.04, 0.81, 0.39, 0.30, 0.23, 0.13, 0.11, 0.08, 0.10, 0.06])\n\ndef model(x, W):\n    comp1 = jnp.exp(W[0])\n    comp2 = jnp.exp(-jnp.exp(W[1]) * x)\n    comp3 = jnp.exp(W[2])\n    comp4 = jnp.exp(-jnp.exp(W[3]) * x)\n    return comp1 * comp2 + comp3 * comp4\n\n\ninit = np.array([0.69, 0.69, -1.6, -1.6])\n\n#autodiff\nmodel_grad = jax.jit(jax.jacfwd(model, argnums=1))\n\n#manual derivative\ndef dModel(x, W):\n    e1 = np.exp(W[1])\n    e2 = np.exp(W[3])\n    e5 = np.exp(-(x * e1))\n    e6 = np.exp(-(x * e2))\n    e7 = np.exp(W[0])\n    e8 = np.exp(W[2])\n    b1 = e5 * e7\n    b2 = -(x * e5 * e7 * e1) \n    b3 = e6 * e8 \n    b4 = -(x * e6 * e8 * e2)\n\n    return np.array([b1, b2, b3, b4]).T\n\nnls_irwls(x, y, model, model_grad, init=init, theta=1, tol=1e-8, maxiter=50)\nnls_irwls(x, y, model, dModel, init=init, theta=1, tol=1e-8, maxiter=50)",
        "answers": [
            "One thing to be aware of is that by default, JAX performs computations in 32-bit, while tools like R and numpy perform computations in 64-bit. Since 1E-8 is at the edge of 32-bit floating point precision, I suspect this is why your program is failing to converge.\nYou can enable 64-bit computation by putting this at the beginning of your script:\npython\nCopy\nfrom jax import config\nconfig.update('jax_enable_x64', True)\nAfter doing this, your program converges as expected. For more information, see JAX Sharp Bits: Double Precision."
        ],
        "link": "https://stackoverflow.com/questions/71902257/parameters-do-not-converge-at-a-lower-tolerance-in-nonlinear-least-square-implem"
    },
    {
        "title": "What is the best way to index 3d matrix with vectors?",
        "question": "python\nCopy\nimport jax.numpy as jnp\nvectors and array are jnp.array(dtype=jnp.int32)\nI have an array with shape [x, d, y] (3x3x3)\npython\nCopy\n[[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]]]\nand vectors x = [2 0 3], y = [ 2 0 1], d = [0 0 1]\nI want to have something like this by indexing but I tried and don't really know how, with jax.numpy.\npython\nCopy\n[[[0 0 2],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 3 0],\n [0 0 0]]]\nEdit: I would like to specify that I wanted to put number from x with its index to the array but only when x > 0. I tried with boolean mask. Something like this\npython\nCopy\nmask = x > 0\narray = array.at[mask, d, y].set(array[mask, d, y] + x)",
        "answers": [
            "You have a three-dimensional array, so you can index it with three arrays of indices. Since you want d and y to be associated with the second and third dimensions, you'll need to create another array of indices for the first dimension:\npython\nCopy\nimport jax.numpy as jnp\n\narr = jnp.zeros((3, 3, 3), dtype='int32')\nx = jnp.array([2, 0, 3])\ny = jnp.array([2, 0, 1])\nd = jnp.array([0, 0, 1])\n\ni = jnp.arange(len(x))\nmask = x > 0\n\nout = arr.at[i[mask], d[mask], y[mask]].set(x[mask])\nprint(out)\n# [[[0 0 2]\n#   [0 0 0]\n#   [0 0 0]]\n\n#  [[0 0 0]\n#   [0 0 0]\n#   [0 0 0]]\n\n#  [[0 0 0]\n#   [0 3 0]\n#   [0 0 0]]]\nIn this case the result will be the same whether or not you use the mask (i.e. arr.at[i, d, y].set(x) will give the same result) but because your question explicitly specified that you only want to use values x > 0 I included it."
        ],
        "link": "https://stackoverflow.com/questions/71880876/what-is-the-best-way-to-index-3d-matrix-with-vectors"
    },
    {
        "title": "DenseElementsAttr could not be constructed from the given buffer",
        "question": "I try to run a code written with JAX. At one part of the code, key for training set is defined as\nkey_train = random.PRNGKey(0).\nHere the type of the key jaxlib.xla_extension.DeviceArray. Then in the following part, keys are defined as keys = random.split(key_train, N). Here N is an integer which is equal to 10000. At that part of the code it gives an error like:\nDenseElementsAttr could not be constructed from the given buffer. This may mean that the Python buffer layout does not match that MLIR expected layout and is a bug.\nCould you please help me about the error?\nEdit: I try to run the code on Win10. Here (https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets/blob/main/Antiderivative/DeepONet_antideriv.ipynb) you can find the code that I try to run. For simplicity you can try to run the code below as well. You will get the exact same error.\nfrom jax import random\nN=10000\nkey_train=random.PRNGKey(0)\nkeys=random.split(key_train, N)\nJax and Jaxlib versions are 0.3.5 with cuda 11",
        "answers": [
            "I had the same error. Deleting c:\\python37\\lib\\site-packages\\jaxlib\\cuda_prng.py fixed the issue (replace the prefix by your python path). It could be cuda_prng.py was an old file.",
            "By following Erwin Coumans's advice and indeed the mistake disappeared. The version installed on Windows is ( jaxlib 0.3.7 with cuda 11.3 and cudnn 8.2 , on anaconda python 3.9.0) the latest found on : https://whls.blob.core.windows.net/unstable/index.html"
        ],
        "link": "https://stackoverflow.com/questions/71827814/denseelementsattr-could-not-be-constructed-from-the-given-buffer"
    },
    {
        "title": "JAX(XLA) vs Numba(LLVM) Reduction",
        "question": "Is it possible to make CPU only reductions with JAX comparable to Numba in terms of computation time?\nThe compilers come straight from conda:\npython\nCopy\n$ conda install -c conda-forge numba jax\nHere is a 1-d NumPy array example\npython\nCopy\nimport numpy as np\nimport numba as nb\nimport jax as jx\n\n@nb.njit\ndef reduce_1d_njit_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\n@jx.jit\ndef reduce_1d_jax_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\nN = 2**10\na = np.random.randn(N)\nUsing timeit on the following\nnp.add.reduce(a) gives 1.99 µs ...\nreduce_1d_njit_serial(a) gives 1.43 µs ...\nreduce_1d_jax_serial(a).item() gives 23.5 µs ...\nNote that jx.numpy.sum(a) and using jx.lax.fori_loop gives comparable (marginally slower) comp. times to reduce_1d_jax_serial.\nIt seems there is a better way to craft the reduction for XLA.\nEDIT: compile times were not included as a print statement proceeded to check results.",
        "answers": [
            "When performing these kinds of microbenchmarks with JAX, you have to be careful to ensure you're measuring what you think you're measuring. There are some tips in the JAX Benchmarking FAQ. Implementing some of these best practices, I find the following for your benchmarks:\npython\nCopy\nimport jax.numpy as jnp\n\n# Native jit-compiled XLA sum\njit_sum = jx.jit(jnp.sum)\n\n# Avoid including device transfer cost in the benchmarks\na_jax = jnp.array(a)\n\n# Prevent measuring compilation time\n_ = reduce_1d_njit_serial(a)\n_ = reduce_1d_jax_serial(a_jax)\n_ = jit_sum(a_jax)\n\n%timeit np.add.reduce(a)\n# 100000 loops, best of 5: 2.33 µs per loop\n\n%timeit reduce_1d_njit_serial(a)\n# 1000000 loops, best of 5: 1.43 µs per loop\n\n%timeit reduce_1d_jax_serial(a_jax).block_until_ready()\n# 100000 loops, best of 5: 6.24 µs per loop\n\n%timeit jit_sum(a_jax).block_until_ready()\n# 100000 loops, best of 5: 4.37 µs per loop\nYou'll see that for these microbenchmarks, JAX is a few milliseconds slower than both numpy and numba. So does this mean JAX is slow? Yes and no; you'll find a more complete answer to that question in JAX FAQ: is JAX faster than numpy?. The short summary is that this computation is so small that the differences are dominated by Python dispatch time rather than time spent operating on the array. The JAX project has not put much effort into optimizing for Python dispatch of microbenchmarks: it's not all that important in practice because the cost is incurred once per program in JAX, as opposed to once per operation in numpy."
        ],
        "link": "https://stackoverflow.com/questions/71701041/jaxxla-vs-numballvm-reduction"
    },
    {
        "title": "jax: sample many observations from random.choice with replacement between them",
        "question": "I'd like to pick two indices out of an array. These indices must not be the same. One such sample can be obtained with:\nrandom.choice(next(key), num_items, (2,), replace=False)\nFor performance reasons, I'd like to batch the sampling:\nnum_samples = 100\nsamples = random.choice(next(key), num_items, (num_samples, 2), replace=False)\nThis doesn't work because of replace=False. It raises the error:\nValueError: Cannot take a larger sample than population when 'replace=False'\nFor each new sample, I'd like to have replace=True. Within one sample, I'd like to have replace=False. Is there a way to do this?\nThe next(key) in my random sampling is syntactic sugar. I'm using this snippet for convenience:\ndef reset_key(seed=42):\n    key = random.PRNGKey(seed)\n    while True:\n        key, subkey = random.split(key)\n        yield subkey\n        \nkey = reset_key()",
        "answers": [
            "The best way to do this is using jax.vmap to map across individual samples. For example:\npython\nCopy\nfrom jax import random, vmap\n\ndef sample_two(key, num_items):\n  return random.choice(key , num_items, (2,), replace=False)\n\nkey = random.PRNGKey(0)\nnum_samples = 10\nnum_items = 5\n\nkey_array = random.split(key, num_samples)\nprint(vmap(sample_two, in_axes=(0, None))(key_array, num_items))\n# [[2 0]\n#  [1 4]\n#  [2 1]\n#  [3 4]\n#  [4 2]\n#  [2 0]\n#  [1 3]\n#  [2 1]\n#  [1 0]\n#  [2 4]]\nFor more information on jax.vmap, see Automatic Vectorization in JAX."
        ],
        "link": "https://stackoverflow.com/questions/71679151/jax-sample-many-observations-from-random-choice-with-replacement-between-them"
    },
    {
        "title": "How to print with JAX",
        "question": "I have a JAX Boolean array and want to print a statement combined with sum of Trues:\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.host_callback import id_print\n\n@jax.jit\ndef overlaps_jax():\n    mask_cp = jnp.array([True, False, False, True, False, True, False, True, True])\n    id_print(jnp.sum(mask_cp))\n\noverlaps_jax()\nThere are 5 Trues in the mask_cp; I want to print as:\nWith jax accelerator\nThere are 5 true bools\nAs this function is jitted, I tried to print this by using id_print, but I couldn't. id_print(jnp.sum(mask_cp)) will print 5, but I couldn't use this with the strings. I have tried the followings:\nid_print(jnp.sum(mask_cp))\n# print:\n# 5\n\nid_print(\"\\nWith jax accelerator\\nThere are \" + jnp.sum(mask_cp) + \" true bools\\n\")\n# error:\n# TypeError: can only concatenate str (not \"DynamicJaxprTracer\") to str\n\nprint(\"\\nWith jax accelerator\\nThere are {} true bools\\n\".format(jnp.sum(mask_cp)))\n# print:\n# With jax accelerator\n# There are Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)> true bools\nHow could I print such statements in this code?",
        "answers": [
            "Please note that id_print is experimental, and its API and capabilities are subject to change. That said, I don't believe id_print has the capability to add text like this, but you can do it via a more general host_callback.call:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.host_callback import call\n\n@jax.jit\ndef overlaps_jax():\n    mask_cp = jnp.array([True, False, False, True, False, True, False, True, True])\n    call(lambda x: print(f\"There are {x} true bools\"), jnp.sum(mask_cp))\n\noverlaps_jax()\nThe output is\nThere are 5 true bools"
        ],
        "link": "https://stackoverflow.com/questions/71548823/how-to-print-with-jax"
    },
    {
        "title": "vmap gives inconsistent shape error when trying to calculate gradient per sample",
        "question": "I am trying to implement a two layer neural network and get the gradient of the second layer per sample.\nMy code looks like this:\npython\nCopy\nx = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.float32)\ny = jnp.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=jnp.float32)\n\nW1 = jnp.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=jnp.float32)\nW2 = random.uniform(key, shape=(10,), minval=1, maxval=2, dtype=jnp.float32)\nb = jnp.linspace(0, -9, 10, dtype=jnp.float32)\nb = jnp.reshape(b, (1,10))\n\ndef predict(W1, W2, b, x):\n  f1 = jnp.einsum('i,j->ji', W1, x)+b\n  f1 = nn.relu(f1)\n  f2 = jnp.einsum('i,ji->j', W2, f1)\n  return f2\n\ndef loss(W1, W2, b, x, y):\n  preds = predict(W1, W2, b, x)\n  return jnp.mean(jnp.square(y-preds))\n\nperex_grads = vmap(grad(loss, argnums=1), in_axes= (0, None, 0, 0, 0))\npers_grads = perex_grads(W1, W2, b, x, y)\nI ran loss and can do grad(loss) just fine. Running vmap is the actual problem.\nThe exact error I get is:\npython\nCopy\nValueError: vmap got inconsistent sizes for array axes to be mapped:\narg 0 has shape (10,) and axis 0 is to be mapped\narg 1 has shape (10,) and axis None is to be mapped\narg 2 has shape (1, 10) and axis 0 is to be mapped\narg 3 has shape (11,) and axis 0 is to be mapped\narg 4 has shape (11,) and axis 0 is to be mapped\nso\narg 0 has an axis to be mapped of size 10\narg 2 has an axis to be mapped of size 1\nargs 3, 4 have axes to be mapped of size 11\nIt is my first time using Jax, and my google search didn't help me resolve the issue, plus the documentation was not very clear to me. I'd appreciate if anyone can help me.",
        "answers": [
            "The issue is exactly what the error message says: in order to vmap an operation over multiple arrays, the dimension of the mapped axes in each array must be equal. In your arrays, the dimensions are not equal: you passed in_axes=(0, None, 0, 0, 0) for arguments W1, W2, b, x, y, but W1.shape[0] = 10, b.shape[0] = 1, x.shape[0] = 11, and y.shape[0] = 11.\nBecause these are not equal, you get this error. To prevent this error, you should only vmap over array axes of the same length.\nFor example, if you want the gradients with respect to W2 computed per pair of W1, W2 inputs, it might look something like this (note the updated predict function and updated in_axes):\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import random, nn, grad, vmap\n\nkey = random.PRNGKey(0)\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.float32)\ny = jnp.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=jnp.float32)\n\nW1 = jnp.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=jnp.float32)\nW2 = random.uniform(key, shape=(10,), minval=1, maxval=2, dtype=jnp.float32)\nb = jnp.linspace(0, -9, 10, dtype=jnp.float32)\nb = jnp.reshape(b, (1,10))\n\ndef predict(W1, W2, b, x):\n  # Since you're vmapping over W1 and W2, your function needs to\n  # handle scalar values, so we cast to 1D if necessary.\n  W1 = jnp.atleast_1d(W1)\n  W2 = jnp.atleast_1d(W2)\n\n  f1 = jnp.einsum('i,j->ji', W1, x)+b\n  f1 = nn.relu(f1)\n  f2 = jnp.einsum('i,ji->j', W2, f1)\n  return f2\n\ndef loss(W1, W2, b, x, y):\n  preds = predict(W1, W2, b, x)\n  return jnp.mean(jnp.square(y-preds))\n\nperex_grads = vmap(grad(loss, argnums=1), in_axes= (0, 0, None, None, None))\npers_grads = perex_grads(W1, W2, b, x, y)"
        ],
        "link": "https://stackoverflow.com/questions/71168030/vmap-gives-inconsistent-shape-error-when-trying-to-calculate-gradient-per-sample"
    },
    {
        "title": "Merge dataclasses in python",
        "question": "I have a dataclass like:\npython\nCopy\nimport dataclasses\nimport jax.numpy as jnp\n\n@dataclasses.dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\nIn my code, I create multiple instances of it, is there an easy way to merge two of them attribute per attribute? For example if I have:\npython\nCopy\na = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nb = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nI'd like to merge them such as having a single Metric containing:\nscore1=jnp.array([10,10,10,10,10,10]), score2=jnp.array([20,20,20,20,20,20]) and score3=jnp.array([30,30,30,30,30,30])",
        "answers": [
            "It is possible to do so in a \"jax-centric\" manner by registering the class Metric as a pytree_node. google/flax, a neural network library built on top of jax, provides the flax.struct.dataclass helper to do so. Once registered, you can use the jax.tree_util package to manipulate Metric instances:\npython\nCopy\nfrom flax.struct import dataclass as flax_dataclass\nfrom jax.tree_util import tree_multimap\nimport jax.numpy as jnp\n\n@flax_dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\n\na = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nb = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\n\ntree_multimap(lambda x, y: jnp.concatenate([x, y]), a, b)\nGives:\npython\nCopy\nMetric(score1=DeviceArray([10, 10, 10, 10, 10, 10], dtype=int32), score2=DeviceArray([20, 20, 20, 20, 20, 20], dtype=int32), score3=DeviceArray([30, 30, 30, 30, 30, 30], dtype=int32))",
            "The easiest thing is probably just to define a method:\npython\nCopy\nimport dataclasses\nimport jax.numpy as jnp\n\n\n@dataclasses.dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\n\n    def concatenate(self, other):\n        return Metric(\n            jnp.concatenate((self.score1, other.score1)),\n            jnp.concatenate((self.score2, other.score2)),\n            jnp.concatenate((self.score3, other.score3)),\n        )\nand then just do a.concatenate(b). You could also instead call the method __add__, which would make it possible just to use a + b. This is neater, but could potentially be confused with element-wise addition."
        ],
        "link": "https://stackoverflow.com/questions/71109587/merge-dataclasses-in-python"
    },
    {
        "title": "Resulting array is zero when jax.numpy is used",
        "question": "I wrote the code below using numpy and got the correct output as shown in program 1. However when I switch to jax.numpy as jnp (in Program 2) the resulting output is an array of zeros. My MWE is shown below. I would like to know where I got the computation wrong? PS: the codes were run in different python files.\npython\nCopy\n#Program 1 (using numpy as np):\nimport numpy as np\n\nnum_rows = 5\nnum_cols = 20\nsmf = np.array([np.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = np.array([1,2,3,4,5])\nlb = np.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = np.array([10, 10, 10, 10, 10])\npar = np.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = np.where(np.isinf(smf), 1, num_cols)\nkvals = np.insert(kvals, 0, 0)\nkvals = np.cumsum(kvals)\n\npar0_col = np.zeros(num_rows*num_cols - (num_cols-1) * np.sum(np.isinf(smf)))\nlb_col = np.zeros(num_rows*num_cols - (num_cols-1) * np.sum(np.isinf(smf)))\nub_col = np.zeros(num_rows*num_cols- (num_cols-1) * np.sum(np.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col[kvals[i]:kvals[i+1]] = par[i, :kvals[i+1]-kvals[i]]\n    lb_col[kvals[i]:kvals[i+1]] = lb[i]\n    ub_col[kvals[i]:kvals[i+1]] = ub[i]\n\narr_1 = np.zeros(shape = (num_rows, num_cols))\narr_2 = np.zeros(shape = (num_rows, num_cols))\n\n\npar_log = np.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\nk = 0\nfor i in range(num_rows):\n\n    arr_1[i, :] = (par_log[kvals[i]:kvals[i+1]])\n    arr_2[i, :] = 10**par_log[kvals[i]:kvals[i+1]]\n  \n\nprint(arr_1)\n\n# [[0.         0.         0.         0.         0.         0.\n#   0.         0.         0.         0.         0.         0.\n#   0.         0.         0.         0.         0.         0.\n#   0.         0.        ]\n#  [0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361]\n#  [0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996]\n#  [0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336]\n#  [0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608]]\n\n# Program 2 (using jax.numpy as jnp):\n\nimport jax\nimport jax.numpy as jnp\njax.config.update(\"jax_enable_x64\", True)\n\nsmf = jnp.array([jnp.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = jnp.array([1.0,2.0,3.0,4.0,5.0])\nlb = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = jnp.array([10.0, 10.0, 10.0, 10.0, 10.0])\npar = jnp.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = jnp.where(jnp.isinf(smf), 1, num_cols)\nkvals = jnp.insert(kvals, 0, 0)\nkvals = jnp.cumsum(kvals)\n\npar0_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nlb_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nub_col = jnp.zeros(num_rows*num_cols- (num_cols-1) * jnp.sum(jnp.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col.at[kvals[i]:kvals[i+1]].set(par[i, :kvals[i+1]-kvals[i]])\n    lb_col.at[kvals[i]:kvals[i+1]].set(lb[i])\n    ub_col.at[kvals[i]:kvals[i+1]].set(ub[i])\n\narr_1 = jnp.zeros(shape = (num_rows, num_cols))\narr_2 = jnp.zeros(shape = (num_rows, num_cols))\n\n\npar_log = jnp.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\nfor i in range(num_rows):\n \n    arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\n    arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\n  \n\nprint(arr_1)\n\n# #[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]",
        "answers": [
            "The issue is that ndarray.at expressions don't operate in-place, but rather return a modified value.\nSo instead of this:\npython\nCopy\narr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\narr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\nYou should write this:\npython\nCopy\narr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\narr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\nRead more at JAX sharp bits: in-place updates.",
            "Oh I already figured it out. I needed to make an explicit assignment."
        ],
        "link": "https://stackoverflow.com/questions/71016283/resulting-array-is-zero-when-jax-numpy-is-used"
    },
    {
        "title": "Compute efficiently Hessian matrices in JAX",
        "question": "In JAX's Quickstart tutorial I found that the Hessian matrix can be computed efficiently for a differentiable function fun using the following lines of code:\nfrom jax import jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\nHowever, one can compute the Hessian also by computing the following:\ndef hessian(fun):\n  return jit(jacrev(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacfwd(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacrev(jacrev(fun)))\nHere is a minimal working example:\nimport jax.numpy as jnp\nfrom jax import jit\nfrom jax import jacfwd, jacrev\n\ndef comp_hessian():\n\n    x = jnp.arange(1.0, 4.0)\n\n    def sum_logistics(x):\n        return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\n    def hessian_1(fun):\n        return jit(jacfwd(jacrev(fun)))\n\n    def hessian_2(fun):\n        return jit(jacrev(jacfwd(fun)))\n\n    def hessian_3(fun):\n        return jit(jacrev(jacrev(fun)))\n\n    def hessian_4(fun):\n        return jit(jacfwd(jacfwd(fun)))\n\n    hessian_fn = hessian_1(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_2(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_3(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_4(sum_logistics)\n    print(hessian_fn(x))\n\n\ndef main():\n    comp_hessian()\n\n\nif __name__ == \"__main__\":\n    main()\nI would like to know which approach is best to use and when? I also would like to know if it is possible to use grad() to compute the Hessian? And how does grad() differ from jacfwd and jacrev?",
        "answers": [
            "The answer to your question is within the JAX documentation; see for example this section: https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev\nTo quote its discussion of jacrev and jacfwd:\nThese two functions compute the same values (up to machine numerics), but differ in their implementation: jacfwd uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while jacrev uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, jacfwd probably has an edge over jacrev.\nand further down,\nTo implement hessian, we could have used jacfwd(jacrev(f)) or jacrev(jacfwd(f)) or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function 𝑓:ℝⁿ→ℝ), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since ∇𝑓:ℝⁿ→ℝⁿ), which is where forward-mode wins out.\nSince your function looks like 𝑓:ℝⁿ→ℝ, then jit(jacfwd(jacrev(fun))) is likely the most efficient approach.\nAs for why you can't implement a hessian with grad, this is because grad is only designed for derivatives of functions with scalar outputs. A hessian by definition is a composition of vector-valued jacobians, not a composition of scalar gradients."
        ],
        "link": "https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax"
    },
    {
        "title": "in_axes keyword in JAX's vmap",
        "question": "I'm trying to understand JAX's auto-vectorization capabilities using vmap and implemented a minimal working example based on JAX's documentation.\nI don't understand how in_axes is used correctly. In the example below I can set in_axes=(None, 0) or in_axes=(None, 1) leading to the same results. Why is that the case?\nAnd why do I have to use in_axes=(None, 0) and not something like in_axes=(0, )?\nimport jax.numpy as jnp\nfrom jax import vmap\n\n\ndef predict(params, input_vec):\n    assert input_vec.ndim == 1\n    activations = input_vec\n    for W, b in params:\n        outputs = jnp.dot(W, activations) + b\n        activations = jnp.tanh(outputs)\n    return outputs\n\n\nif __name__ == \"__main__\":\n\n    # Parameters\n    dims = [2, 3, 5]\n    input_dims = dims[0]\n    batch_size = 2\n\n    # Weights\n    params = list()\n    for dims_in, dims_out in zip(dims, dims[1:]):\n        params.append((jnp.ones((dims_out, dims_in)), jnp.ones((dims_out,))))\n\n    # Input data\n    input_batch = jnp.ones((batch_size, input_dims))\n\n    # With vmap\n    predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n    print(predictions)",
        "answers": [
            "in_axes=(None, 0) means that the first argument (here params) will not be mapped, while the second argument (here input_vec) will be mapped along axis 0.\nIn the example below I can set in_axes=(None, 0) or in_axes=(None, 1) leading to the same results. Why is that the case?\nThis is because input_vec is a 2x2 matrix of ones, so whether you map along axis 0 or axis 1, the input vectors are length-2 vectors of ones. In more general cases, the two specifications are not equivalent, which you can see by either (1) making batch_size differ from input_dims[0], or (2) filling your arrays with non-constant values.\nwhy do I have to use in_axes=(None, 0) and not something like in_axes=(0, )?\nIf you set in_axes=(0, ) for a function with two arguments, you get an error because the length of the in_axes tuple must match the number of arguments passed to the function. That said, it is possible to pass a scalar in_axes=0 as a shorthand for in_axes=(0, 0), but for your function this would lead to a shape error because the leading dimension of the arrays in params does not match the leading dimension of input_vec."
        ],
        "link": "https://stackoverflow.com/questions/70564419/in-axes-keyword-in-jaxs-vmap"
    },
    {
        "title": "Is there a module to convert a tensorflow NN to Jax?",
        "question": "There is a libary to convert Jax functions to Tensorflow functions. Is there a similar library to convert TensorFlow functions to Jax functions?",
        "answers": [
            "No, there is no library supported by the JAX team to convert tensorflow into JAX in a manner similar to how jax.experimental.jax2tf converts JAX code to tensorflow, and I have not seen any such library developed by others.",
            "See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md.\n\"jax2tf.call_tf: for using TensorFlow functions in a JAX context, e.g., to call a TensorFlow library or a SavedModel inside a JAX function.\"\nThat is what you need. So you can call tf function under jax context.\nfor example:\npython\nCopy\n# Compute cos with TF and sin with JAX\ndef cos_tf_sin_jax(x):\n  return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))",
            "Is https://github.com/google-deepmind/tf2jax what you were looking for? It only works for TF v2 though.",
            "To my knowledge there is no library similar to the one you mentioned to convert TensorFlow functions to Jax functions. I'm sorry"
        ],
        "link": "https://stackoverflow.com/questions/70356126/is-there-a-module-to-convert-a-tensorflow-nn-to-jax"
    },
    {
        "title": "Python - time difference (JAX library)",
        "question": "I'm trying to compare execution times between functions:\nsimpleExponentialSmoothing - which is my implementation of SES in JAX library\nsimpleExponentialSmoothingJax - as above, but boosted with JIT from JAX library\nSimpleExpSmoothing - implementation from Statsmodels library\nI have tried using %timeit, time and writing my own function to measure time using datetime, however I'm quite confused. My function to measure time and %timeit are returning the same exec time, however %time is showing much, much different exec time. I have found that %time checks only single run and is less accurate than %timeit, but how does it apply to asynchronous functions like those in JAX? Although I've blocked them until finishing calculations, I'm not sure if that is enough.\nI need advice about this measure, which should I take as actual execution time?\n%time\npython\nCopy\n%time timeSeriesSes = simpleExponentialSmoothing(params, timeSeries, initState).block_until_ready()\n%time timeSeriesSesJit = simpleExponentialSmoothingJit(params, timeSeries, initState).block_until_ready()\n%time timeSeriesSesSm = SimpleExpSmoothing(timeSeries).fit()\npython\nCopy\nCPU times: user 82.4 ms, sys: 4.03 ms, total: 86.4 ms\nWall time: 97.6 ms\nCPU times: user 199 µs, sys: 0 ns, total: 199 µs\nWall time: 214 µs\nCPU times: user 6.12 ms, sys: 0 ns, total: 6.12 ms\nWall time: 6.2 ms\n%timeit\npython\nCopy\n%timeit timeSeriesSes = simpleExponentialSmoothing(params, timeSeries, initState).block_until_ready()\n%timeit timeSeriesSesJit = simpleExponentialSmoothingJit(params, timeSeries, initState).block_until_ready()\n%timeit timeSeriesSesSm = SimpleExpSmoothing(timeSeries).fit()\npython\nCopy\n48.8 ms ± 904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n15.5 µs ± 222 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n3.4 ms ± 62.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)",
        "answers": [
            "For your JAX-specific question: using block_until_ready() should be enough to account for JAX's asynchronous execution.\nBe careful also about JIT compilation: the first time you call a JIT-compiled function with arguments of a particular shape, the compilation time will affect the speed of execution. After that, the cached compilation will be used.\nAs to your more general question: the difference between %timeit and %time is covered in the IPython docs:\nBy default, timeit() temporarily turns off garbage collection during the timing. The advantage of this approach is that it makes independent timings more comparable. The disadvantage is that GC may be an important component of the performance of the function being measured.\n(From https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit)\nSo if you want to measure performance with Python garbage collection, and with only a single execution, use %time. If you want to measure performance without Python garbage collection, and with multiple executions for more statistical rigor, use %timeit."
        ],
        "link": "https://stackoverflow.com/questions/70161804/python-time-difference-jax-library"
    },
    {
        "title": "JAX: avoid just-in-time recompilation for a function evaluated with a varying number of elements along one axis",
        "question": "Is it possible to avoid recompiling a JIT function when the structure of its input remains essentially unchanged, aside from one axis having a varying number of elements?\npython\nCopy\nimport jax\n\n@jax.jit\ndef f(x):\n    print('recompiling')\n    return (x + 10) * 100\n\na = f(jax.numpy.arange(300000000).reshape((-1, 2, 2)).block_until_ready()) # recompiling\nb = f(jax.numpy.arange(300000000).reshape((-1, 2, 2)).block_until_ready())\nc = f(jax.numpy.arange(450000000).reshape((-1, 2, 2)).block_until_ready()) # recompiling. It would be nice if it weren't\nRequirements: pip install jax, jaxlib",
        "answers": [
            "No, there is no way to avoid recompilation when you call a function with arrays of a different shape. Fundamentally, JAX compiles functions for statically-shaped inputs and outputs, and calling a JIT-compiled function with an array of a new shape will always trigger re-compilation.\nThere is some ongoing work on relaxing this requirement (search \"dynamic shapes\" in JAX's github repository) but no such APIs are available at the moment."
        ],
        "link": "https://stackoverflow.com/questions/70126391/jax-avoid-just-in-time-recompilation-for-a-function-evaluated-with-a-varying-nu"
    },
    {
        "title": "Turn a tf.data.Dataset to a jax.numpy iterator",
        "question": "I am interested about training a neural network using JAX. I had a look on tf.data.Dataset, but it provides exclusively tf tensors. I looked for a way to change the dataset into JAX numpy array and I found a lot of implementations that use Dataset.as_numpy_generator() to turn the tf tensors to numpy arrays. However I wonder if it is a good practice, as numpy arrays are stored in CPU memory and it is not what I want for my training (I use the GPU). So the last idea I found is to manually recast the arrays by calling jnp.array but it is not really elegant (I am afraid about the copy in GPU memory). Does anyone have a better idea for that?\nQuick code to illustrate:\npython\nCopy\nimport os\nimport jax.numpy as jnp\nimport tensorflow as tf\n\ndef generator():\n    for _ in range(2):\n        yield tf.random.uniform((1, ))\n\nds = tf.data.Dataset.from_generator(generator, output_types=tf.float32,\n                                    output_shapes=tf.TensorShape([1]))\n\nds1 = ds.take(1).as_numpy_iterator()\nds2 = ds.skip(1)\n\nfor i, batch in enumerate(ds1):\n    print(type(batch))\n\nfor i, batch in enumerate(ds2):\n    print(type(jnp.array(batch)))\n\n# returns:\n\n<class 'numpy.ndarray'> # not good\n<class 'jaxlib.xla_extension.DeviceArray'> # good but not elegant",
        "answers": [
            "Both tensorflow and JAX have the ability to convert arrays to dlpack tensors without copying memory, so one way you can create a JAX array from a tensorflow array without copying the underlying data buffer is to do it via dlpack:\nimport numpy as np\nimport tensorflow as tf\nimport jax.dlpack\n\ntf_arr = tf.random.uniform((10,))\ndl_arr = tf.experimental.dlpack.to_dlpack(tf_arr)\njax_arr = jax.dlpack.from_dlpack(dl_arr)\n\nnp.testing.assert_array_equal(tf_arr, jax_arr)\nBy doing the round-trip to JAX, you can compare unsafe_buffer_pointer() to ensure that the arrays point at the same buffer, rather than copying the buffer along the way:\ndef tf_to_jax(arr):\n  return jax.dlpack.from_dlpack(tf.experimental.dlpack.to_dlpack(arr))\n\ndef jax_to_tf(arr):\n  return tf.experimental.dlpack.from_dlpack(jax.dlpack.to_dlpack(arr))\n\njax_arr = jnp.arange(20.)\ntf_arr = jax_to_tf(jax_arr)\njax_arr2 = tf_to_jax(tf_arr)\n\nprint(jnp.all(jax_arr == jax_arr2))\n# True\nprint(jax_arr.unsafe_buffer_pointer() == jax_arr2.unsafe_buffer_pointer())\n# True",
            "From Flax example:\nhttps://github.com/google/flax/blob/6ae22681ef6f6c004140c3759e7175533bda55bd/examples/imagenet/train.py#L183\npython\nCopy\ndef prepare_tf_data(xs):\n  local_device_count = jax.local_device_count()\n  def _prepare(x):\n    x = x._numpy() \n    return x.reshape((local_device_count, -1) + x.shape[1:])\n  return jax.tree_util.tree_map(_prepare, xs)\n\nit = map(prepare_tf_data, ds)\nit = jax_utils.prefetch_to_device(it, 2)"
        ],
        "link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator"
    },
    {
        "title": "Jax/Flax (very) slow RNN-forward-pass compared to pyTorch?",
        "question": "I recently implemented a two-layer GRU network in Jax and was disappointed by its performance (it was unusable).\nSo, i tried a little speed comparison with Pytorch.\nMinimal working example\nThis is my minimal working example and the output was created on Google Colab with GPU-runtime. notebook in colab\npython\nCopy\nimport flax.linen as jnn \nimport jax\nimport torch\nimport torch.nn as tnn\nimport numpy as np \nimport jax.numpy as jnp\n\ndef keyGen(seed):\n    key1 = jax.random.PRNGKey(seed)\n    while True:\n        key1, key2 = jax.random.split(key1)\n        yield key2\nkey = keyGen(1)\n\nhidden_size=200\nseq_length = 1000\nin_features = 6\nout_features = 4\nbatch_size = 8\n\nclass RNN_jax(jnn.Module):\n\n    @jnn.compact\n    def __call__(self, x, carry_gru1, carry_gru2):\n        carry_gru1, x = jnn.GRUCell()(carry_gru1, x)\n        carry_gru2, x = jnn.GRUCell()(carry_gru2, x)\n        x = jnn.Dense(4)(x)\n        x = x/jnp.linalg.norm(x)\n        return x, carry_gru1, carry_gru2\n\nclass RNN_torch(tnn.Module):\n    def __init__(self, batch_size, hidden_size, in_features, out_features):\n        super().__init__()\n\n        self.gru = tnn.GRU(\n            input_size=in_features, \n            hidden_size=hidden_size,\n            num_layers=2\n            )\n        \n        self.dense = tnn.Linear(hidden_size, out_features)\n\n        self.init_carry = torch.zeros((2, batch_size, hidden_size))\n\n    def forward(self, X):\n        X, final_carry = self.gru(X, self.init_carry)\n        X = self.dense(X)\n        return X/X.norm(dim=-1).unsqueeze(-1).repeat((1, 1, 4))\n\nrnn_jax = RNN_jax()\nrnn_torch = RNN_torch(batch_size, hidden_size, in_features, out_features)\n\nXj = jax.random.normal(next(key), (seq_length, batch_size, in_features))\nYj = jax.random.normal(next(key), (seq_length, batch_size, out_features))\nXt = torch.from_numpy(np.array(Xj))\nYt = torch.from_numpy(np.array(Yj))\n\ninitial_carry_gru1 = jnp.zeros((batch_size, hidden_size))\ninitial_carry_gru2 = jnp.zeros((batch_size, hidden_size))\n\nparams = rnn_jax.init(next(key), Xj[0], initial_carry_gru1, initial_carry_gru2)\n\ndef forward(params, X):\n    \n    carry_gru1, carry_gru2 = initial_carry_gru1, initial_carry_gru2\n\n    Yhat = []\n    for x in X: # x.shape = (batch_size, in_features)\n        yhat, carry_gru1, carry_gru2 = rnn_jax.apply(params, x, carry_gru1, carry_gru2)\n        Yhat.append(yhat) # y.shape = (batch_size, out_features)\n\n    #return jnp.concatenate(Y, axis=0)\n\njitted_forward = jax.jit(forward)\nResults\npython\nCopy\n# uncompiled jax version\n%time forward(params, Xj)\nCPU times: user 7min 17s, sys: 8.18 s, total: 7min 25s Wall time: 7min 17s\npython\nCopy\n# time for compiling\n%time jitted_forward(params, Xj)\nCPU times: user 8min 9s, sys: 4.46 s, total: 8min 13s Wall time: 8min 12s\npython\nCopy\n# compiled jax version\n%timeit jitted_forward(params, Xj)\nThe slowest run took 204.20 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 115 µs per loop\npython\nCopy\n# torch version\n%timeit lambda: rnn_torch(Xt)\n10000000 loops, best of 5: 65.7 ns per loop\nQuestions\nWhy is my Jax-implementation so slow? What am i doing wrong?\nAlso, why is compiling taking so long? The sequence is not that long..\nThank you :)",
        "answers": [
            "The reason the JAX code compiles slowly is that during JIT compilation JAX unrolls loops. So in terms of XLA compilation, your function is actually very large: you call rnn_jax.apply() 1000 times, and compilation times tend to be roughly quadratic in the number of statements.\nBy contrast, your pytorch function uses no Python loops, and so under the hood it is relying on vectorized operations that run much faster.\nAny time you use a for loop over data in Python, a good bet is that your code will be slow: this is true whether you're using JAX, torch, numpy, pandas, etc. I'd suggest finding an approach to the problem in JAX that relies on vectorized operations rather than relying on slow Python looping."
        ],
        "link": "https://stackoverflow.com/questions/69767707/jax-flax-very-slow-rnn-forward-pass-compared-to-pytorch"
    },
    {
        "title": "how to do curve fitting using google jax?",
        "question": "Extending the examples from http://implicit-layers-tutorial.org/neural_odes/ I am tying to mimic the curve fitting function in scipy , scipy.optimize.curve_fit ,using google jax. The function to be fitted is a first order ODE.\npython\nCopy\n#Generate toy data for first order ode.\n\nimport jax.numpy as jnp\nimport jax\nimport numpy as np\n\n\n#input  data \nu = np.zeros(100)  \nu[10:50] = 1\nt = np.arange(len(u))\nu = jnp.array(u)\n\n#first order ODE\ndef f(y,t,k,tau,u):\n \n  return (k*u[t]-y)/tau\n  \n#Euler integration\ndef odeint_euler(f, y0, t, *args):\n  def step(state, t):\n    y_prev, t_prev = state\n    dt = t - t_prev\n    y = y_prev + dt * f(y_prev, t_prev, *args)\n    return (y, t), y\n  _, ys = jax.lax.scan(step, (y0, t[0]), t[1:])\n  return ys\n\npred = odeint_euler(f, jnp.array([0.0]),t,2.,5.,u) \npred_noise = pred.reshape(-1) +  0.05* np.random.randn(len(pred)) # this is the  data to be fitted\n\n# define loss function \ndef loss_function(params,u,targets):\n  k,tau = params\n  \n  pred = odeint_euler(f, jnp.array([0.0]),t,k,tau,u)\n  return jnp.sum((pred-targets)**2)      \n\n\ndef update(params, u, targets):\n  grads = jax.grad(loss_function)(params,u, targets)\n  return [w - 0.0001 * dw for w,dw  in zip(params, grads)] \n\n\nupdated_params = jnp.array([1.0,2.0]) #initial parameters\nfor i in range(100):\n  updated_params = update(updated_params, u, pred_noise)\nprint(updated_params)\nThe code works fine. However , this runs pretty slow when compared to scipy curve fit. The accuracy of the solution is not good even after 500, 1000 iterations. What is wrong with the above code ? Any idea how to make the code run faster and to get more accurate solution? Is there any better way of doing the curve fitting with jax?",
        "answers": [
            "I see two overall issues with your approach:\nThe reason your code is running slowly is because you are doing your looping in Python, which incurs JAX's dispatch overhead every loop. I'd recommend using JAX's built-in tools for minimization of loss functions; for example:\npython\nCopy\nfrom jax.scipy.optimize import minimize\nresult = minimize(\n    loss_function, x0=jnp.array([1.0,2.0]),\n    method='BFGS', args=(u, pred_noise))\nThe reason your accuracy does not approach that of scipy is likely because JAX defaults to 32-bit computations (See Double (64 bit) Precision). To run your code in 64-bit, you can run this block before any other imports:\npython\nCopy\nfrom jax import config\nconfig.update('jax_enable_x64', True)"
        ],
        "link": "https://stackoverflow.com/questions/69641423/how-to-do-curve-fitting-using-google-jax"
    },
    {
        "title": "How to use grad convolution in google-jax?",
        "question": "Thanks for reading my question!\nI was just learning about custom grad functions in Jax, and I found the approach JAX took with defining custom functions is quite elegant.\nOne thing troubles me though.\nI created a wrapper to make lax convolution look like PyTorch conv2d.\nfrom jax import numpy as jnp\nfrom jax.random import PRNGKey, normal \nfrom jax import lax\nfrom torch.nn.modules.utils import _ntuple\nimport jax\nfrom jax.nn.initializers import normal\nfrom jax import grad\n\ntorch_dims = {0: ('NC', 'OI', 'NC'), 1: ('NCH', 'OIH', 'NCH'), 2: ('NCHW', 'OIHW', 'NCHW'), 3: ('NCHWD', 'OIHWD', 'NCHWD')}\n\ndef conv(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n    n = len(input.shape) - 2\n    if type(stride) == int:\n        stride = _ntuple(n)(stride)\n    if type(padding) == int: \n        padding = [(i, i) for i in _ntuple(n)(padding)]\n    if type(dilation) == int:\n        dilation = _ntuple(n)(dilation)\n    return lax.conv_general_dilated(lhs=input, rhs=weight, window_strides=stride, padding=padding, lhs_dilation=dilation, rhs_dilation=None, dimension_numbers=torch_dims[n], feature_group_count=1, batch_group_count=1, precision=None, preferred_element_type=None)\nThe problem is that I could not find a way to use its grad function:\ninit = normal()\nrng = PRNGKey(42)\nx = init(rng, [128, 3, 224, 224])\nk = init(rng, [64, 3, 3, 3])\ny = conv(x, k)\ngrad(conv)(y, k)\nThis is what I got.\nValueError: conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 64 // 1 != 3.\nPlease help!",
        "answers": [
            "When I run your code with the most recent releases of jax and jaxlib (jax==0.2.22; jaxlib==0.1.72), I see the following error:\nTypeError: Gradient only defined for scalar-output functions. Output had shape: (128, 64, 222, 222).\nIf I create a scalar-output function that uses conv, the gradient seems to work:\npython\nCopy\nresult = grad(lambda x, k: conv(x, k).sum())(x, k)\nprint(result.shape)\n# (128, 3, 224, 224)\nIf you are using an older version of JAX, you might try updating to a more recent version – perhaps the error you're seeing is due to a bug that has already been fixed."
        ],
        "link": "https://stackoverflow.com/questions/69571976/how-to-use-grad-convolution-in-google-jax"
    },
    {
        "title": "Is there a CUDA threadId alike in Jax (google)?",
        "question": "I'm trying to understand the behaviour of jax.vmap/pmap, (jax: https://jax.readthedocs.io/). CUDA has threadId to let you know which thread is executing the code, is there a similar concept in jax? (jax.process_id is not)",
        "answers": [
            "No, there is no real analog to CUDA threadid in JAX. Details about GPU thread assignment are handled at a lower level by the XLA compiler, and I don't know of any straightforward API to plumb this information back to JAX's Python runtime.\nOne case where JAX does offer higher-level handling of device assignment is when using pmap; in this case you can explicitly pass a set of device IDs to the pmapped function if you want logic that depends on the device on which the mapped code is being executed. For example, I ran the following on an 8-device system:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nnum_devices = jax.device_count()\n\ndef f(device, data):\n  return data + device\n\ndevice_index = jnp.arange(num_devices)\ndata = jnp.zeros((num_devices, 10))\n\njax.pmap(f)(device_index, data)\n\n# ShardedDeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n#                     [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n#                     [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n#                     [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n#                     [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n#                     [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.],\n#                     [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.],\n#                     [7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/69450145/is-there-a-cuda-threadid-alike-in-jax-google"
    },
    {
        "title": "How can I utilize JAX library to speed up my code?",
        "question": "I have written a code that gets some vertex and rearranged them based on some rules. When the input contains big data, the code runs very slowly e.g. for 60000 loops it will take about 15 hours on google colab TPU runtime. I have found JAX is one of the best libraries to do so and trying to use it, but due to lack of experience in dealing with such big data and its related methods such as parallelization, I have faced to some problems. The following small sample is created to show what does the code doing:\npython\nCopy\nimport numpy as np\n\n# <class 'numpy.ma.core.MaskedArray'> <class 'numpy.ma.core.MaskedArray'> (m, 4) <class 'numpy.int64'>\nnodes = np.ma.masked_array(np.array([[0, 1, 2, 3], [4, 0, 5, 1], [6, 4, 7, 5], [8, 6, 9, 7]],\n                                    dtype=np.int64), mask=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# <class 'numpy.ndarray'> <class 'numpy.ndarray'> (n, 3) <class 'numpy.float64'>\nvert = np.array([[0.06944111, -0.12027553, -0.3], [0., -0.13888221, -0.3], [0.05, -0.08660254, -0.3],\n                [0.06944111, -0.12027553, -0.5], [0.06944111, -0.12027553, -0.1], [0., -0.13888221, -0.1],\n                [0.06944111, -0.12027553,  0.1], [0., -0.13888221,  0.1], [0.06944111, -0.12027553,  0.3],\n                [0., -0.13888221,  0.3]])\n\n\ndef ali_sh():\n    mod_array = []\n    mod_idx = []\n\n    for cell in range(len(nodes)):\n        vertex_idx = []\n\n        B_face = sorted(nodes[cell], key=lambda v: [vert[v][0]], reverse=True)\n        if round(vert[B_face[1]][0], 7) == round(vert[B_face[2]][0], 7):\n            if vert[B_face[1]][1] > vert[B_face[2]][1]:\n                B_face[1], B_face[2] = B_face[2], B_face[1]\n\n        mod_array.append(B_face)\n\n        for vertex in B_face:\n            vertex_idx.append(np.where(nodes[cell] == vertex)[0][0])\n\n        mod_idx.append(vertex_idx)\n\n    return mod_idx\n\nmod_idx = ali_sh()\nThe above code is just a view of my code and have some differences e.g. in this code jnp.where run correctly but using the main code and the big data it will stuck and must use np.where instead. In my first try, I have added jax_r = jit(ali_sh) and mod_idx = jax_r().block_until_ready() to the end of the code, but I did not get any better performance. I have used FiPy library and its methods, which where in numpy types e.g. 'fipy.mesh.vertexCoords.T' is a numpy ndarray. I have tried to convert the used fipy numpy arrays to JAX ones by jnp.array(fipy numpy arrays) to check if it could help, but I get errors due to using lambda by sorted command. How can I implement JAX on my code to get a better run time.\nDoes colab need anything to do to get its maximum capability on TPU or GPU for such codes? Does using JAX could have significant effects on my code to speeding up? I would be appreciated if someone could help to find out how to speed up the code.",
        "answers": [
            "Writing efficient JAX code is very similar to writing efficient NumPy code: generally if you are using a for loop over rows of your data, your code will not be very efficient. Instead, you should strive to write your computations in terms of vectorized operations.\nIn your code, it looks like you are relying on many non-JAX elements (e.g. NumPy masked arrays, operations in FiPy, etc.) so it's unlikely that JAX will be able to improve your runtime. I'd focus instead on rewriting your code to make efficient use of NumPy, replacing the for-loop logic with NumPy vectorized operations.\nHere is an example of expressing your function in terms of vectorized operations:\npython\nCopy\ndef ali_sh_vectorized():\n  i_sort = np.argsort(vert[nodes, 0], axis=1)[:, ::-1]\n  B_face = nodes[np.arange(nodes.shape[0])[:, None], i_sort]\n  close = np.isclose(vert[B_face[:, 1],1], vert[B_face[:, 2], 2])\n  larger = np.greater(vert[B_face[:, 1],1], vert[B_face[:, 2], 2])\n  col_1 = np.where(close & larger, B_face[:, 2], B_face[:, 1])\n  col_2 = np.where(close & larger, B_face[:, 1], B_face[:, 2])\n  B_face[:, 1] = col_1\n  B_face[:, 2] = col_2\n  mod_idx = np.where(nodes[:, :, None] == B_face[:, None, :])[2].reshape(nodes.shape)\n  return mod_idx\nThe differences in the output compared to the original function are due to differences in how the Python sort and the NumPy sort handle equivalent elements, but I believe the overall logic is the same."
        ],
        "link": "https://stackoverflow.com/questions/69422078/how-can-i-utilize-jax-library-to-speed-up-my-code"
    },
    {
        "title": "How does one get a parameter from a params (pytree) in haiku? (jax framework)",
        "question": "For example you set up a module and that has params. But if you want do regularize something in a loss what is the pattern?\npython\nCopy\nimport jax.numpy as jnp\nimport jax\ndef loss(params, x, y):\n   l = jnp.sum((y - mlp.apply(params, x)) ** 2)\n   w = hk.get_params(params, 'w') # does not work like this\n   l += jnp.sum(w ** w)\n   return l\nThere is some pattern missing in the examples.",
        "answers": [
            "params is essentially a read-only dictionary, so you can get the value of a parameter by treating it as a dictionary:\npython\nCopy\nprint(params['w'])\nIf you want to update the parameters, you cannot do it in-place, but have to first convert it to a mutable dictionary:\npython\nCopy\nparams_mutable = hk.data_structures.to_mutable_dict(params)\nparams_mutable['w'] = 3.14\nparams_new = hk.data_structures.to_immutable_dict(params_mutable)"
        ],
        "link": "https://stackoverflow.com/questions/69031947/how-does-one-get-a-parameter-from-a-params-pytree-in-haiku-jax-framework"
    },
    {
        "title": "Error message in Python with differentiation",
        "question": "I am computing these derivatives using the Montecarlo approach for a generic call option. I am interested in this combined derivative (with respect to both S and Sigma). Doing this with the algorithmic differentiation, I get an error that can be seen at the end of the page. What could be a possible solution? Just to explain something regarding the code, I am going to attach the formula used to compute the \"X\" in the code below:\npython\nCopy\nfrom jax import jit, grad, vmap\nimport jax.numpy as jnp\nfrom jax import random\nUnderlying_asset = jnp.linspace(1.1,1.4,100)\nvolatilities = jnp.linspace(0.5,0.6,100)\ndef second_derivative_mc(S,vol):\n    N = 100\n    j,T,q,r,k = 10000,1.,0,0,1.\n    S0 = jnp.array([S]).T #(Nx1) vector underlying asset\n    C = jnp.identity(N)*vol    #matrix of volatilities with 0 outside diagonal \n    e = jnp.array([jnp.full(j,1.)])#(1xj) vector of \"1\"\n    Rand = np.random.RandomState()\n    Rand.seed(10)\n    U= Rand.normal(0,1,(N,j)) #Random number for Brownian Motion\n    sigma2 = jnp.array([vol**2]).T #Vector of variance Nx1\n\n    first = jnp.dot(sigma2,e) #First part equation\n    second = jnp.dot(C,U)     #Second part equation\n\n    X = -0.5*first+jnp.sqrt(T)*second\n\n    St = jnp.exp(X)*S0\n\n    P = jnp.maximum(St-k,0)\n    payoff = jnp.average(P, axis=-1)*jnp.exp(-q*T)\n    return payoff \n\n\ngreek = vmap(grad(grad(second_derivative_mc, argnums=1), argnums=0)(Underlying_asset,volatilities)\nThis is the error message:\npython\nCopy\n> UnfilteredStackTrace                      Traceback (most recent call\n> last) <ipython-input-78-0cc1da97ae0c> in <module>()\n>      25 \n> ---> 26 greek = vmap(grad(grad(second_derivative_mc, argnums=1), argnums=0))(Underlying_asset,volatilities)\n> \n> 18 frames UnfilteredStackTrace: TypeError: Gradient only defined for\n> scalar-output functions. Output had shape: (100,).\nThe stack trace below excludes JAX-internal frames. The preceding is the original exception that occurred, unmodified.\nThe above exception was the direct cause of the following exception:\npython\nCopy\n> TypeError                                 Traceback (most recent call\n> last) /usr/local/lib/python3.7/dist-packages/jax/_src/api.py in\n> _check_scalar(x)\n>     894     if isinstance(aval, ShapedArray):\n>     895       if aval.shape != ():\n> --> 896         raise TypeError(msg(f\"had shape: {aval.shape}\"))\n>     897     else:\n>     898       raise TypeError(msg(f\"had abstract value {aval}\"))\n\n> TypeError: Gradient only defined for scalar-output functions. Output had shape: (100,).",
        "answers": [
            "As the error message indicates, gradients can only be computed for functions that return a scalar. Your function returns a vector:\npython\nCopy\nprint(len(second_derivative_mc(1.1, 0.5)))\n# 100\nFor vector-valued functions, you can compute the jacobian (which is similar to a multi-dimensional gradient). Is this what you had in mind?\npython\nCopy\nfrom jax import jacobian\ngreek = vmap(jacobian(jacobian(second_derivative_mc, argnums=1), argnums=0))(Underlying_asset,volatilities)\nAlso, this is not what you asked about, but the function above will probably not work as you intend even if you solve the issue in the question. Numpy RandomState objects are stateful, and thus will generally not work correctly with jax transforms like grad, jit, vmap, etc., which require side-effect-free code (see Stateful Computations In JAX). You might try using jax.random instead; see JAX: Random Numbers for more information."
        ],
        "link": "https://stackoverflow.com/questions/68908160/error-message-in-python-with-differentiation"
    },
    {
        "title": "Websockets messages only sent at the end and not in instances using async / await, yield in nested for loops",
        "question": "I have a computationally heavy process that takes several minutes to complete in the server. So I want to send the results of every iteration to the client via websockets.\nThe overall application works but my problem is that all the messages are arriving at the client in one big chunk after the entire simulation finishes. I must be missing something here as I expect the await websocket.send_json() to send the message during the process and not all of them at the end.\nServer python (FastAPI)\npython\nCopy\n# A very simplified abstraction of the actual app.\n\ndef simulate_intervals(data):\n  for t in range(data.n_intervals):\n    state = interval(data) # returns a JAX NumPy array\n    yield state\n\ndef simulate(data):\n  for key in range(data.n_trials):\n     trial = simulate_intervals(data)\n     yield trial\n\n@app.websocket(\"/ws\")\nasync def socket(websocket: WebSocket):\n\n  await websocket.accept()\n  while True:\n    # Get model inputs from client\n    data = await websocket.receive_text()\n    # Minimal computation\n    nodes = distributions(data)\n\n    nodosJson = json.dumps(nodes, cls=NumpyEncoder)\n    # I expect this message to be sent early on,\n    # but the client gets it at the end with all the other messages. \n    await websocket.send_json({\"tipo\": \"nodos\", \"datos\": json.loads(nodosJson)})\n    \n    # Heavy computation\n    trials = simulate(data)\n\n    for trialI, trial in enumerate(trials):\n      for stateI, state in enumerate(trial):\n        stateString = json.dumps(state, cls=NumpyEncoder)\n\n        await websocket.send_json(\n          {\n            \"tipo\": \"estado\",\n            \"datos\": json.loads(stateString),\n            \"trialI\": trialI,\n            \"stateI\": stateI,\n          }\n        )\n\n    await websocket.send_json({\"tipo\": \"estado\", \"msg\": \"fin\"})\nFor completeness, here is the basic client code.\nClient\njavascript\nCopy\nconst ws = new WebSocket('ws://localhost:8000/ws');\n\nws.onopen = () => {\n  console.log('Conexión exitosa');\n};\n\nws.onmessage = (e) => {\n  const mensaje = JSON.parse(e.data);\n  console.log(mensaje);\n};\n\nbotonEnviarDatos.onclick = () => {\n   ws.send(JSON.stringify({...}));\n}",
        "answers": [
            "I got a similar issue, and was able to resolve it by adding a small await asyncio.sleep(0.1) after sending json messages. I have not dived into asyncios internals yet, but my guess is that websocker.send shedules a message to be sent, but since the async function continues to run it never has a chance to do it in the background. Sleeping the async function makes asyncio pick up other tasks while it is waiting.",
            "I was not able to make it work as posted in my question, still interested in hearing from anyone who understands why it is not possible to send multiple async messages without them getting blocked.\nFor anyone interested, here is my current solution:\nPing pong messages from client and server\nI changed the logic so the server and client are constantly sending each other messages and not trying to stream the data in a single request from the client.\nThis actually works much better than my original attempt because I can detect when a sockets gets disconnected and stop processing in the server. Basically, if the client disconnects, no new requests for data are sent from that client and the server never continues the heavy computation.\nServer\npython\nCopy\n# A very simplified abstraction of the actual app.\n\ndef simulate_intervals(data):\n  for t in range(data.n_intervals):\n    state = interval(data) # returns a JAX NumPy array\n    yield state\n\ndef simulate(data):\n  for key in range(data.n_trials):\n     trial = simulate_intervals(data)\n     yield trial\n\n@app.websocket(\"/ws\")\nasync def socket(websocket: WebSocket):\n\n  await websocket.accept()\n  while True:\n    # Get messages from client\n    data = await websocket.receive_text()\n    \n    # \"tipo\" is basically the type of data being sent from client or server to the other one.\n    # In this case, \"tipo\": \"inicio\" is the client sending inputs and requesting for a certain data in response.\n    if data[\"tipo\"] == \"inicio\":\n      # Minimal computation\n      nodes = distributions(data)\n\n      nodosJson = json.dumps(nodes, cls=NumpyEncoder)\n      # In this first interaction, the client gets the first message without delay. \n      await websocket.send_json({\"tipo\": \"nodos\", \"datos\": json.loads(nodosJson)})\n\n      # Since this is a generator (def returns yield) it does not actually\n      # trigger that actual computationally heavy process. \n      trials = simulate(data)\n      \n      # define some initial variables to count the iterations\n      trialI = 0\n      stateI = 0\n      trialsLen = args.number_trials\n      statesLen = 600\n      \n      # load the first trial (also a generator)\n      # without the for loop used before, the counters and next()\n      # allow us to do the same as being done before in the for loop\n      trial = next(trials)\n\n      # With the use of generators and next() it is possible to keep\n      # this first message light on the server and send the first response\n      # as quickly as possible.\n    \n    # This type of message asks for the next instance of the simluation\n    # without processing the entire model.\n    elif data[\"tipo\"] == \"sim\":\n      # check if we are within the limits (before this was a nested for loop)\n      if trialI < trialsLen and stateI < statesLen:\n        # Trigger the next instance of the simulation\n        state = next(trial)\n        # update counter\n        stateI = stateI + 1\n        \n        # Send the message with 1 instance of the simulation.\n        # \n        stateString = json.dumps(state, cls=NumpyEncoder)\n        await websocket.send_json(\n          {\n             \"tipo\": \"estado\",\n             \"datos\": json.loads(stateString),\n             \"trialI\": trialI,\n             \"stateI\": stateI,\n          }\n        )\n        \n        # Check if the second loop is done\n        if stateI == statesLen:\n          # update counter of first loop\n          trialI = trialI + 1\n          # update counter of second loop\n          stateI = 0\n          \n          # Check if there are more pending trials,\n          # otherwise stop and notify the client we are done.\n          try:\n            trial = next(trials)\n          except StopIteration:\n            await websocket.send_json({\"tipo\": \"fin\"})\nClient\nJust the part that actually changed:\njavascript\nCopy\nws.onmessage = (e) => {\n  const mensaje = JSON.parse(e.data);\n  \n  // Simply check the type of incoming message so it can be processed\n  if (mensaje.tipo === 'fin') {\n    viz.calcularResultados();\n  } else if (mensaje.tipo === 'nodos') {\n    viz.pintarNodos(mensaje.datos);\n  } else if (mensaje.tipo === 'estado') {\n    viz.sumarEstado(mensaje.datos);\n  }\n\n  // After receiving a message, ping the server for the next one \n  ws.send(\n    JSON.stringify({\n      tipo: 'sim',\n    })\n  );\n};\nThis seems like reasonable solution to keep the server and client working together. I am able to show in the client the progress of a long simulation and the user experience is much better than having to wait for a long time for the server to respond. Hope it helps other with a similar problem."
        ],
        "link": "https://stackoverflow.com/questions/68884040/websockets-messages-only-sent-at-the-end-and-not-in-instances-using-async-awai"
    },
    {
        "title": "Is there a way to disable forward evaluation while using VJP in JAX?",
        "question": "I use VJP frequently in my project. It runs the function that is subject to Jacobian computation and returns a primals_out together with the callable vjp function. For example, custom VJP definition in JAX documentation is given like this:\npython\nCopy\nfrom jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n# Returns primal output and residuals to be used in backward pass by f_bwd.\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res # Gets residuals computed in f_fwd\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\nIn this example, we see that evaluation of the forward function is required when using VJP. This is also the case when using regular VJP instead of a custom defined one. However, when the evaluation of function costs highly and since I have already run that function somewhere in my code, I don't want VJP to evaluate that function one more time.\nSo, is there any way to indicate that a function will not be evaluated when computing its VJP?",
        "answers": [
            "I don't think there is any way to explicitly disable forward evaluation in this context, but if you wrap your computation in a jit compilation, the XLA compiler will automatically do dead code elimination and trim unused branches from the computation graph."
        ],
        "link": "https://stackoverflow.com/questions/68491225/is-there-a-way-to-disable-forward-evaluation-while-using-vjp-in-jax"
    },
    {
        "title": "Apply function only on slice of array under jit",
        "question": "I am using JAX, and I want to perform an operation like\npython\nCopy\n@jax.jit\ndef fun(x, index):\n    x[:index] = other_fun(x[:index])\n    return x\nThis cannot be performed under jit. Is there a way of doing this with jax.ops or jax.lax? I thought of using jax.ops.index_update(x, idx, y) but I cannot find a way of computing y without incurring in the same problem again.",
        "answers": [
            "The previous answer by @rvinas using dynamic_slice works well if your index is static, but you can also accomplish this with a dynamic index using jnp.where. For example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef other_fun(x):\n    return x + 1\n\n@jax.jit\ndef fun(x, index):\n  mask = jnp.arange(x.shape[0]) < index\n  return jnp.where(mask, other_fun(x), x)\n\nx = jnp.arange(5)\nprint(fun(x, 3))\n# [1 2 3 3 4]",
            "It seems there are two issues in your implementation. First, the slices are producing dynamically shaped arrays (not allowed in jitted code). Second, unlike numpy arrays, JAX arrays are immutable (i.e. the contents of the array cannot be changed).\nYou can overcome the two problems by combining static_argnums and jax.lax.dynamic_update_slice. Here is an example:\npython\nCopy\ndef other_fun(x):\n    return x + 1\n\n@jax.partial(jax.jit, static_argnums=(1,))\ndef fun(x, index):\n    update = other_fun(x[:index])\n    return jax.lax.dynamic_update_slice(x, update, (0,))\n\nx = jnp.arange(5)\nprint(fun(x, 3))  # prints [1 2 3 3 4]\nEssentially, the example above uses static_argnums to indicate that the function should be recompiled for different index values and jax.lax.dynamic_update_slice creates a copy of x with updated values at :len(update)."
        ],
        "link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit"
    },
    {
        "title": "sum matrix elementwise using vmap (jax)?",
        "question": "I'm trying to understand the in_axes and out_axes options in vmap. For example, I want to sum two matrix and get the output with the same shape.\npython\nCopy\nX = np.arange(9).reshape(3,3)\nY = np.arange(0,-9,-1).reshape(3,3)\ndef sum2(x,y):\n    return x + y\nvmap(sum2,in_axes=((0,1),(0,1)))(X,Y)\nI think I mapped both axes 0 and 1 for X and Y respectively. The output will have the same shape as X,Y. But i get the error,\npython\nCopy\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-403-103694166574> in <module>\n      3 def sum2(x,y):\n      4     return x + y\n----> 5 vmap(sum2,in_axes=((0,1),(0,1)))(X,Y)\n\n    [... skipping hidden 2 frame]\n\n~/anaconda3/lib/python3.8/site-packages/jax/api_util.py in flatten_axes(name, treedef, axis_tree, kws)\n    276       assert treedef_is_leaf(leaf)\n    277       axis_tree, _ = axis_tree\n--> 278     raise ValueError(f\"{name} specification must be a tree prefix of the \"\n    279                      f\"corresponding value, got specification {axis_tree} \"\n    280                      f\"for value tree {treedef}.\") from None\n\nValueError: vmap in_axes specification must be a tree prefix of the corresponding value, got specification ((0, 1), (0, 1)) for value tree PyTreeDef((*, *)).",
        "answers": [
            "First of all, the easiest way to do an element-wise sum is to use the built-in broadcasting of binary operations, and call sum2(X, Y) directly.\nThat said, if you're trying to understand vmap: the issue is that vmap can only map one axis at a time. If you want to map multiple axes, you can nest multiple vmaps. I believe what you intended to do can be expressed this way:\npython\nCopy\nfrom jax import vmap\nimport jax.numpy as np\n\nX = np.arange(9).reshape(3,3)\nY = np.arange(0,-9,-1).reshape(3,3)\n\ndef sum2(x,y):\n    assert x.ndim == y.ndim == 0\n    return x + y\n\nvmap(vmap(sum\n  vmap(sum2, in_axes=(0, 0), out_axes=0),\n  in_axes=(1, 1), out_axes=1\n)(X,Y)\nNote: I added the assertion about number of dimensions to demonstrate that the mapped function is being called on scalar values.\nAlso, notice that when the mapped axes match, e.g. in_axes=(0, 0) can be equivalently written in_axes=0, but I left it as a tuple because it was closer to the syntax you were trying.\nIn fact, a far more concise way to do the same computation with nested vmap would be to use the default arguments: vmap(vmap(sum2))(X, Y) will do the same elementwise sum."
        ],
        "link": "https://stackoverflow.com/questions/68332924/sum-matrix-elementwise-using-vmap-jax"
    },
    {
        "title": "Not able to import python package jax in Google TPU",
        "question": "I am working on linux console and typing python takes me into the python console. When I use the following command in TPU machine\npython\nCopy\nimport jax\nthen it generates following mss and get out of the python prompt.\nbash\nCopy\nparamjeetsingh80@t1v-n-1c883486-w-0:~$ python3\nPython 3.8.5 (default, Jan 27 2021, 15:41:15)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import jax\n2021-07-08 17:41:39.660523: F external/org_tensorflow/tensorflow/core/tpu/tpu_executor_init_fns.inc:110] TpuTransferManager_ReadDynamicShapes not available in this library.\nAborted (core dumped)\nparamjeetsingh80@t1v-n-1c883486-w-0:~$\nThis issue is causing problem in my code so I would like to figure out, what is this issue and how to get rid of this?",
        "answers": [
            "It may be that your system does not have the correct version of libtpu. Try installing the version listed here.\nYou should be able to do this automatically with\npython\nCopy\n$ pip install -U pip  # older pip may not support extra requirements\n$ pip install -U jax  # newer jax required for [tpu] extras declaration\n$ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/jax_releases.html",
            "Above command give some error but I researched and below command worked for me. But your answer give me the direction that it is a package issue.\npython\nCopy\npip install --upgrade pip\npip install \"jax[tpu]>=0.2.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/68306484/not-able-to-import-python-package-jax-in-google-tpu"
    },
    {
        "title": "How to install just XLA?",
        "question": "I want to use XLA as a backend for my project. Is there a recommended way to install it on its own (without the rest of TensorFlow). Jax probably does this, but looking in their repository it's not obvious how.",
        "answers": [
            "There is no supported way to install XLA on its own apart from tensorflow.\nThat said, JAX does extract, build, and bundle XLA separately from tensorflow within the jaxlib package. You can see the relevant build scripts for jaxlib on various platforms here: https://github.com/google/jax/tree/main/build\nIn particular, take a look at build_wheel.py, which contains the scripts that extract relevant pieces of XLA from the tensorflow source as part of the jaxlib build.",
            "XLA has been moved to the OpenXLA GitHub organization. It can be installed on its own from there."
        ],
        "link": "https://stackoverflow.com/questions/68290128/how-to-install-just-xla"
    },
    {
        "title": "Gradient Accumulation with JAX",
        "question": "I made a simple script to try to do gradient accumulation with JAX. The idea is to have large batch size (e.g. 64) that are split in small chunks (e.g. 4) that fit in the GPU's memory. For each chunck, the resulting gradient, stored in a pytree, is added to the current batch gradient. The update is done only when all chunks of the large batch are computed. In this particular example, we simply try to fit random 512-dimensional vectors to random booleans with a linear layer. Here is the script:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, random\nfrom jax.experimental import optimizers\nfrom functools import partial\nfrom jax.nn.initializers import normal, zeros\nfrom typing import Callable\nfrom dataclasses import dataclass\n\n@dataclass\nclass Jax_model:\n    init_fun: Callable\n    apply_fun: Callable\n\n\ndef Dense(input_size: int, output_size: int, init_kernel=normal(), init_bias=zeros):\n\n    def init_fun(key):\n        key, sub_key1, sub_key2 = jax.random.split(key, 3)\n        params = {\n            'I': init_kernel(sub_key1, (input_size, output_size) ),\n            'I_b': init_bias(sub_key2, (1,output_size) ),\n        }\n        return params\n\n    def apply_fun(params, inputs):\n        I, I_b, = params['I'], params['I_b']\n        logits = inputs @ I + I_b\n        return logits\n\n    return Jax_model(init_fun, apply_fun)\n\n\ndef divide_pytree(pytree, div):\n    for pt in jax.tree_util.tree_leaves(pytree):\n        pt = pt / div\n    return pytree\n\n\ndef add_pytrees(pytree1, pytree2):\n    for pt1, pt2 in zip( jax.tree_util.tree_leaves(pytree1), jax.tree_util.tree_leaves(pytree2) ):\n        pt1 = pt1 + pt2\n    return pytree1\n\n\nrng_key = random.PRNGKey(42)\nbatch_size = 64\naccumulation_size = 4\nmodel_dim = 512\nn_iter = 50\n\nmodel = Dense(model_dim, 1)\nrng_key, sub_key = random.split(rng_key)\ninit_params = model.init_fun(sub_key)\nopt_init, opt_update, get_params = optimizers.adam(0.001)\nopt_state = opt_init(init_params)\n\n@jit\ndef update(i, current_opt_state, current_batch):\n    N = current_batch[0].shape[0]\n    K = accumulation_size\n    num_gradients = N//K\n    accumulation_batch = (current_batch[ib][0:K] for ib in range(len(current_batch)))\n    value, grads = jax.value_and_grad(loss_func)(get_params(current_opt_state), accumulation_batch)\n    value = value / num_gradients\n    grads = divide_pytree(grads, num_gradients)\n    for k in range(K,N,K):\n        accumulation_batch = (current_batch[ib][k:k+K] for ib in range(len(current_batch)))\n        new_value, new_grads = jax.value_and_grad(loss_func)(get_params(current_opt_state), accumulation_batch)\n        value = value + (new_value / num_gradients)\n        grads = add_pytrees(grads, divide_pytree(new_grads, num_gradients))\n    return opt_update(i, grads, current_opt_state), value\n\ndef loss_func(current_params, current_batch):\n    inputs, labels = current_batch\n    predictions = model.apply_fun(current_params, inputs)\n    loss = jnp.square(labels-predictions).sum()\n    return loss\n\nfor i in range(n_iter):\n    rng_key, sub_key1, sub_key2 = random.split(rng_key, 3)\n    inputs = jax.random.uniform(sub_key1, (batch_size, model_dim))\n    labels = jax.random.uniform(sub_key2, (batch_size, 1)) > 0.5\n    batch = inputs, labels\n    opt_state, batch_loss = update(i, opt_state, batch)\n    print(i, batch_loss)\nI have doubts about the divide_pytree and add_pytrees. Does it actually modify the current batch gradient or am I missing something ? Moreover, do you see any speed issue with this code ? In particular, should I use the jax.lax.fori_loop in place of the traditional python for loop ?\nRelated links:\nhttps://github.com/google/jax/issues/1488\nhttps://github.com/google-research/long-range-arena/issues/4",
        "answers": [
            "Regarding the pytree computations: as written your functions are returning the input unmodified. The better approach for this is to use jax.tree_util.tree_map; for example:\npython\nCopy\nfrom jax.tree_util import tree_map\n\ndef divide_pytree(pytree, div):\n  return tree_map(lambda pt: pt / div, pytree)\n\ndef add_pytrees(pytree1, pytree2):\n  return tree_map(lambda pt1, pt2: pt1 + pt2, pytree1, pytree2)\nRegarding performance: anything in the for loop will be flattened when JIT-compiled, with one repeated copy of all XLA instructions per iteration of the loop. If you have 5 iterations, that's not really an issue. If you have 5000, that would significantly slow down compilation times (because XLA needs to analyze & optimize 5000 explicit copies of the instructions in the loop).\nfori_loop can help, but does not lead to optimal code, particularly when running on CPU and GPU.\nBetter would be to use broadcasted or vmapped operations where possible to express the logic of the loops without explicit looping."
        ],
        "link": "https://stackoverflow.com/questions/68016425/gradient-accumulation-with-jax"
    },
    {
        "title": "derivatives by a and b, using using algorithmic differentiation",
        "question": "I've been tasked to find the derivatives by a and b, using jax, for this function\nnow, the reason I'm here is because I don't know enough Python, and this for the course in question, we haven't been thought python either.\nthe assignment is:\npython\nCopy\nreturn a tuple (dfa, dfb) such that dfa is the partial derivatives of f by a,\n           and dfb is the partial derivative of f by b\nnow, I was able to do it the normal way:\npython\nCopy\ndef function(a, b):\n   dfa = sym.diff((2/b)*sym.cos(a)*sym.exp(-a*a/b*b), a)\n   dfb = sym.diff((2/b)*sym.cos(a)*sym.exp(-a*a/b*b), a)\n   return (dfa, dfb)\nbut im not familiar with algorithmic differentiation, using the example we were given, i've tried this:\npython\nCopy\ndef foo():\n\n   x = (2/b)*sym.cos(a)\n   y = sym.exp(-sym.Pow(a/b,2))\n   return (x*y)\n\ndef f_partial_derviatives_algo():\n   return jax.grad(foo)\nbut I'm getting this error:\ncannot unpack non-iterable function object\nIf anyone can help with understanding how i can do something like that, It would be greatly appreciated",
        "answers": [
            "JAX and sympy are not compatible. You should either use one or the other, and not try to combine the two.\nIf you want to compute the partial derivatives of this function at some value using JAX, you can write something like this:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import grad\n\ndef f(a, b):\n  return (2 / b) * jnp.cos(a) * jnp.exp(- a ** 2 / b ** 2)\n\ndf_da = grad(f, argnums=0)\ndf_db = grad(f, argnums=1)\n\nprint(df_da(1.0, 1.0), df_db(1.0, 1.0))\n# -1.4141841 0.3975322"
        ],
        "link": "https://stackoverflow.com/questions/67922307/derivatives-by-a-and-b-using-using-algorithmic-differentiation"
    },
    {
        "title": "Jax and train Neural Networks",
        "question": "I am a beginner in JAX and I am trying to learn how to train a neural network. I saw some blogs, but as I understood there isn't a library that you can train it easily, like 'fit' as in sklearn. I am interested about classification task, could you please reccommend me any blogs in order to adopt his/her algorithm into my problem?",
        "answers": [
            "JAX is an array manipulation library, not a deep learning library: in that respect, you should think of it as more similar to NumPy than similar to scikit-learn. If you want neural networks built on JAX, there are several other good projects available such as haiku and flax."
        ],
        "link": "https://stackoverflow.com/questions/67778124/jax-and-train-neural-networks"
    },
    {
        "title": "is it possible to jit a function which uses jax.numpy.unique?",
        "question": "The following code does not work:\ndef get_unique(arr):\n    return jnp.unique(arr)\n\nget_unique = jit(get_unique)\nget_unique(jnp.ones((10,)))\nThe error message compains about the use of jnp.unique:\nFilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=0/1)>\nThe error arose in jnp.unique()\nThe documentation on sharp bits explains that jit doesn't work if the shape of internal arrays depends on argument values. This is exactly the case here.\nAccording to the docs, a potential workaround is to specify static parameters. But this doesn't apply to my case. The parameters will change for almost every function call. I have split up my code into a preprocessing step, which performs calculations such as this jnp.unique, and a computation step which can be jitted.\nBut still I'd like to ask, is there some workaround that I'm not aware of?",
        "answers": [
            "No, for the reasons you mention, there's currently no way to use jnp.unique on a non-static value.\nIn similar cases, JAX sometimes adds extra parameters that can be used to specify a static size for the output (for example, the size parameter in jax.numpy.nonzero) but nothing like that is currently implemented for jnp.unique. If that is something you'd like, it would be worth filing a feature request."
        ],
        "link": "https://stackoverflow.com/questions/67739742/is-it-possible-to-jit-a-function-which-uses-jax-numpy-unique"
    },
    {
        "title": "What is JaxNumpy-compatible equivalent to this Python function?",
        "question": "How do I implement the below in a JAX-compatable way (e.g., using jax.numpy)?\npython\nCopy\ndef actions(state: tuple[int, ...]) -> list[tuple[int, ...]]:\n    l = []\n    iterables = [range(1, i+1) for i in state]\n    ns = list(range(len(iterables)))\n    for i, iterable in enumerate(iterables):\n        for value in iterable:\n            action = tuple(value if n == i else 0 for n in ns)\n            l.append(action)\n    return l\n\n>>> state = (3, 1, 2)\n>>> actions(state)\n[(1, 0, 0), (2, 0, 0), (3, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 2)]",
        "answers": [
            "Jax, like numpy, cannot efficiently operate on Python container types like lists and tuples, so there's not really any JAX-compatible way to create a function with the exact signature you specify above.\nBut if you're alright with the return value being a two-dimensional array, you could do something like this, based on jnp.vstack:\npython\nCopy\nfrom typing import Tuple\nimport jax.numpy as jnp\nfrom jax import jit, partial\n\n@partial(jit, static_argnums=0)\ndef actions(state: Tuple[int, ...]) -> jnp.ndarray:\n  return jnp.vstack([\n    jnp.zeros((val, len(state)), int).at[:, i].set(jnp.arange(1, val + 1))\n    for i, val in enumerate(state)])\npython\nCopy\n>>> state = (3, 1, 2)\n>>> actions(state)\nDeviceArray([[1, 0, 0],\n             [2, 0, 0],\n             [3, 0, 0],\n             [0, 1, 0],\n             [0, 0, 1],\n             [0, 0, 2]], dtype=int32)\nNote that because the size of the output array depends on the content of state, state must be a static quantity, so a tuple is a good option for the input."
        ],
        "link": "https://stackoverflow.com/questions/67290650/what-is-jaxnumpy-compatible-equivalent-to-this-python-function"
    },
    {
        "title": "All pairwise cross products of the rows of two matrices",
        "question": "I would like to efficiently calculate all pairwise cross products of the rows of two matrices, A and B, which are nx3 and mx3 in size. And would ideally like to achieve this in einsum notation.\ni.e. the output Matrix C, would be (n X m x 3),\nwhere\nC[0][0] = cross(n[0],m[0])\nC[0][1] = cross(n[0],m[1])\n...\nC[1][0] = cross(n[1],m[0])\n...\nDue to the approach I am taking, using for loops aren't an option.\nAny help would be much appreciated.",
        "answers": [
            "Looks like cross broadcasts the leading dimensions.\npython\nCopy\nnp.cross(A[:, None,:], B[None, :,:])"
        ],
        "link": "https://stackoverflow.com/questions/67205068/all-pairwise-cross-products-of-the-rows-of-two-matrices"
    },
    {
        "title": "sampling univariate gausssian with specific mean and standard deviation using jax.random.normal",
        "question": "I'm trying to sample from a gaussian with specific standard deviation and mean, I know the following function is sampling from a gaussian with zero mean and standard deviation equals to 1:\npython\nCopy\nimport jax\nfrom jax import random\n\nkey = random.PRNGKey(0)\nmu = 20\nstd = 4\n\nx1 = jax.random.normal(key, (1000,))\nAnd I can adjust the mean by doing: x1 = x1 + mu, but how can I adjust the standard deviation?",
        "answers": [
            "Create your samples this way:\npython\nCopy\nx1 = mu + std * jax.random.normal(key, (1000,))\nIf you do this, the histogram of samples will follow the expected distribution:\npython\nCopy\nimport jax\nfrom jax import random\nfrom jax.scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nkey = random.PRNGKey(0)\nmu = 20\nstd = 4\n\nx1 = mu + std * jax.random.normal(key, (1000,))\nplt.hist(x1, bins=50, density=True)\n\nx = jnp.linspace(5, 35, 100)\ny = norm.pdf(x, loc=mu, scale=std)\nplt.plot(x, y)",
            "This\npython\nCopy\nx1 = std * x1 + mu\nwill give you want you want"
        ],
        "link": "https://stackoverflow.com/questions/66664455/sampling-univariate-gausssian-with-specific-mean-and-standard-deviation-using-ja"
    },
    {
        "title": "JAX vmap behaviour",
        "question": "I'm trying to understand the behaviour of JAX vmap, so I wrote the following code:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import vmap\n\ndef what(a,b,c):\n  z = jnp.dot(a,b)\n  return z + c\n\nv_what = vmap(what, in_axes=(None,0,None))\n\na = jnp.array([1,1,3])\nb = jnp.array([2,2])\nc = 1.0\n\nv_what(a,b,c)\nAnd the output is:\npython\nCopy\nDeviceArray([[3., 3., 7.],\n             [3., 3., 7.]], dtype=float32)\nI understand that the only input that is being altered is b, but Can someone shed some light on why this is the result? And how the dot product behaves after I vectorized the function?",
        "answers": [
            "You have specified that the transformed function should map over the first axis of b, and not map over any axis of a or c. So roughly, you've created a mapped function that does this:\npython\nCopy\ndef v_what(a, b, c):\n  return jnp.stack([what(a, b_i, c) for b_i in b], axis=0)\nFor your inputs, within each row the dot product looks like jnp.dot(a, 2), and the result is equivalent to a * 2."
        ],
        "link": "https://stackoverflow.com/questions/66548897/jax-vmap-behaviour"
    },
    {
        "title": "Understanding JAX argnums parameter in its gradient function",
        "question": "I'm trying to understand the behaviour of argnums in JAX's gradient function. Suppose I have the following function:\npython\nCopy\ndef make_mse(x, t):  \n  def mse(w,b): \n    return np.sum(jnp.power(x.dot(w) + b - t, 2))/2\n  return mse \nAnd I'm taking the gradient in the following way:\npython\nCopy\nw_gradient, b_gradient = grad(make_mse(train_data, y), (0,1))(w,b)\nargnums= (0,1) in this case, but what does it mean? With respect to which variables the gradient is calculated? What will be the difference if I will use argnums=0 instead? Also, can I use the same function to get the Hessian matrix?\nI looked at JAX help section about it, but couldn't figure it out",
        "answers": [
            "When you pass multiple argnums to grad, the result is a function that returns a tuple of gradients, equivalent to if you had computed each separately:\npython\nCopy\ndef f(x, y):\n  return x ** 2 + x * y + y ** 2\n\ndf_dxy = grad(f, argnums=(0, 1))\ndf_dx = grad(f, argnums=0)\ndf_dy = grad(f, argnums=1)\n\nx = 3.0\ny = 4.25\nassert df_dxy(x, y) == (df_dx(x, y), df_dy(x, y))\nIf you want to compute a mixed second derivatives, you can do this by repeatedly applying the gradient:\npython\nCopy\nd2f_dxdy = grad(grad(f, argnums=0), argnums=1)\nassert d2f_dxdy(x, y) == 1"
        ],
        "link": "https://stackoverflow.com/questions/66445754/understanding-jax-argnums-parameter-in-its-gradient-function"
    },
    {
        "title": "Non-hashable static arguments are not supported in Jax when using vmap",
        "question": "This is related to this question. After some work, I managed to change it down to the last error. The code looks like this now.\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, 0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha, T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n\n@jax.partial(jax.jit, static_argnums=(0,2,))    \ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\nThis leads to (what I hope) is the final error, that is\npython\nCopy\nNon-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.batching.BatchTracer'> for function map_row is non-hashable.\nThis is very strange, as I think I have marked every everywhere axis appears to be static, yet it still tells me that it is traced.",
        "answers": [
            "When you write a partial function with positional arguments, those arguments are passed first. So this:\npython\nCopy\njax.partial(map_row, alpha, T)\nis essentially equivalent to this:\npython\nCopy\nlambda z_input: map_row(alpha, T, z_input)\nNotice the incorrect order of the arguments – this is what's causing your error: you're passing z_input, a non-hashable tracer, to an argument that is expected to be static.\nYou can fix this by replacing the partial statement above with:\npython\nCopy\nlambda z: map_row(z, alpha, T)\nand then your code will run correctly."
        ],
        "link": "https://stackoverflow.com/questions/65685211/non-hashable-static-arguments-are-not-supported-in-jax-when-using-vmap"
    },
    {
        "title": "Jax cannot find the static argnums",
        "question": "This is related with this question. I manage to make the most of the code work, except one of the strange thing.\nHere is the modified code.\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, a_min=0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha=alpha, T=T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n    \n@jax.partial(jax.jit, static_argnums=(0,2,))\ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, axis=0, alpha=1.5, T=20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\nThis code will produce an error as follows:\npython\nCopy\ntuple index out of range\nwhich is cause by this line of code\npython\nCopy\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\nEven if I replace the function body with nothing but an entity function, the error persists. This is a really strange behavior. However, it is very important for me to get this thing to be static as it will help to unrolled loops.",
        "answers": [
            "This error is due to a wart that I hope will be fixed soon in JAX: static arguments cannot be passed by keyword. In other words, you should change this:\npython\nCopy\ndef toy(input, weight):\n    return (weight*entmax(input, axis=0, alpha=1.5, T=20)).sum()\nto this:\npython\nCopy\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\nThe same fix should be applied in calls to max_row.\nAt this point, you end up with a ValueError because of passing traced variables to functions that require static arguments; the solution will be similar to that in How to handle JAX reshape with JIT.\nOne additional note: this static_argnums error has recently been improved, and in the next release will be a bit more clear:\npython\nCopy\nValueError: jitted function has static_argnums=(2,), donate_argnums=() but was called with only 1 positional arguments."
        ],
        "link": "https://stackoverflow.com/questions/65612989/jax-cannot-find-the-static-argnums"
    },
    {
        "title": "Unable to Install Specific JAX jaxlib GPU version",
        "question": "I'm trying to install a particular version of jaxlib to work with my CUDA and cuDNN versions. Following the README, I'm trying\npip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html\nThis returns the following error:\nERROR: Requested jaxlib==0.1.52+cuda101 from https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.52%2Bcuda101-cp37-none-manylinux2010_x86_64.whl has different version in metadata: '0.1.52'\nDoes anyone know what causes this or how to get around the error?",
        "answers": [
            "This error appears to be from a new check in pip version 20.3.X and higher, likely related to the new dependency resolver. I can reproduce this error with pip version 20.3.3, but the package installs correctly with pip version 20.2.4.\nThe easiest way to proceed would probably be to first downgrade pip; i.e.\npip install pip==20.2.4\nand then proceed with your jaxlib install.",
            "Note that both the versions of jax and jaxlib has to match. You can use something like:\n$ pip install --upgrade jax==0.3.2 jaxlib==0.3.2+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nAnother workaround would be to first choose a specific version of jax and jaxlib from the available wheel files and then install those.\n$ pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.76+cuda11.cudnn82-cp39-none-manylinux2010_x86_64.whl",
            "Maybe you need to change the link to: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html. So, pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version"
    },
    {
        "title": "How to resolve ValueError `vector::reserve` in JAX/Python?",
        "question": "EDIT: GitHub issue here: https://github.com/google/jax/issues/5190\nI am trying to optimize the following function using jit:\npython\nCopy\n@partial(jit, static_argnums=(0, 1,))\ndef coocurrence_helper(pairs: np.array, label_map: Dict) -> lil_matrix:\n    uniques = lil_matrix(np.zeros((len(label_map), len(label_map))).astype(\"int32\"))\n    for item in pairs:\n        if item[0]!=item[1]:\n            uniques[label_map[item[0]], label_map[item[1]]] += 1\n    return uniques\nthe routine above is used here:\npython\nCopy\ndef _get_pairwise_frequencies(\n     data: pd.DataFrame, crosstab=False\n    ) -> pd.DataFrame:\n        values = data.stack()\n        values.index = values.index.droplevel(1)\n        values.name = \"vals\"\n        values = optimize(values.to_frame())\n        pair = optimize(values.join(values, rsuffix=\"_2\"))\n        label_map = dict()\n        for lbl, each in enumerate(values.vals.unique()):\n            label_map[each] = lbl\n        if not crosstab:\n            freq = coocurrence_helper(pairs = pair.values, label_map=label_map)\n            return ((freq / freq.sum(1).ravel()).astype(np.float32))\n        else:\n            freq = pd.crosstab(pair[\"vals\"], pair[\"vals_2\"])\n            self.index = freq.index\n            return csr_matrix((freq / freq.sum(1)).astype(np.float32))\nBut i get the following error:\npython\nCopy\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-42-f8e638fc2bb6> in <module>\n----> 1 _get_pairwise_frequencies(data)\n\n<ipython-input-30-43adeb39c76c> in _get_pairwise_frequencies(data, crosstab)\n     25             label_map[each] = lbl\n     26         if not crosstab:\n---> 27             freq = coocurrence_helper(pairs = pair.values, label_map=label_map)\n     28             return csr_matrix((freq / freq.sum(1).ravel()).astype(np.float32))\n     29         else:\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/jax/api.py in f_jitted(*args, **kwargs)\n    369         return cache_miss(*args, **kwargs)[0]  # probably won't return\n    370     else:\n--> 371       return cpp_jitted_f(*args, **kwargs)\n    372   f_jitted._cpp_jitted_f = cpp_jitted_f\n    373 \n\nValueError: vector::reserve\nWhat can be the source of the issue here? Without using static_argnums the error message is\npython\nCopy\nRuntimeError: Invalid argument: Unknown NumPy type O size 8\nwith the same traceback.",
        "answers": [
            "The issue is that you are returning a scipy.sparse.lil_matrix, which is not a valid JAX type. The JAX jit decorator cannot be used as a compiler for arbitrary Python code; it is designed to optimize sequences of operations on JAX arrays.\nThe best way to proceed in this case would probably be to remove the @partial(jit, ...) decorator from your function; if you wanted to use JAX jit compilation here, you would first have to rewrite your code to avoid scipy.sparse matrices and use JAX arrays instead."
        ],
        "link": "https://stackoverflow.com/questions/65307334/how-to-resolve-valueerror-vectorreserve-in-jax-python"
    },
    {
        "title": "Jaxlib pip installation failure",
        "question": "From the command line, I've tried following this installation tutorial I'd like to avoid building from source if at all possible. Currently, I'm not sure what the issue is. Could anyone verify that they get the same/different response when trying to install Jaxlib?\nFor awareness, Jax installed fine without any issues, but some supporting components are found in Jaxlib which is installed separately.\npython\nCopy\nC:\\Users\\john.smith>pip install jaxlib\nERROR: Could not find a version that satisfies the requirement jaxlib (from versions: none)\nERROR: No matching distribution found for jaxlib",
        "answers": [
            "It appears you are using Windows. JAX currently does not provide jaxlib builds for Windows (see google/jax#438) though there is work in progress to rectify this; see google/jax#4843.\nThere are some comments in the above issue that suggest a possible approach to build jaxlib for Windows if you wish to build it yourself."
        ],
        "link": "https://stackoverflow.com/questions/64863194/jaxlib-pip-installation-failure"
    },
    {
        "title": "vmap ops.index_update in Jax",
        "question": "I have the following code below and it's using a simple for loop. I was just wondering if there was a way to vmap it? Here is the original code:\npython\nCopy\nimport numpy as np \nimport jax.numpy as jnp\nimport jax.scipy.signal as jscp\nfrom scipy import signal\nimport jax\n\ndata = np.random.rand(192,334)\n\na = [1,-1.086740193996892,0.649914553946275,-0.124948974636730]\nb = [0.054778173164082,0.164334519492245,0.164334519492245,0.054778173164082]\nimpulse = signal.lfilter(b, a, [1] + [0]*99) \nimpulse_20 = impulse[:20]\nimpulse_20 = jnp.asarray(impulse_20)\n\n@jax.jit\ndef filter_jax(y):\n    for ind in range(0, len(y)):\n      y = jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19])\n    return y\n\njnpData = jnp.asarray(data)\n\n%timeit filter_jax(jnpData).block_until_ready()\nAnd here is my attempt at using vmap:\npython\nCopy\ndef paraUpdate(y, ind):\n    return jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19])\n\n@jax.jit\ndef filter_jax2(y):\n  ranger = range(0, len(y))\n  return jax.vmap(paraUpdate, y)(ranger)\nBut I receive the following error:\nTypeError: vmap in_axes must be an int, None, or (nested) container with those types as leaves, but got Traced<ShapedArray(float32[192,334])>with<DynamicJaxprTrace(level=0/1)>.\nI'm a little confused since the range is of type int so I'm not too sure what's going on.\nIn the end, I'm trying to get this little piece optimized as best as possible to get the lowest time.",
        "answers": [
            "jax.vmap can express functionality in which a single operation is independently applied across multiple axes of an input. Your function is a bit different: you have a single operation iteratively applied to a single input.\nFortunately JAX provides lax.scan which can handle this situation. The implementation would look something like this:\npython\nCopy\nfrom jax import lax\n\ndef paraUpdate(y, ind):\n    return jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19]), ind\n\n@jax.jit\ndef filter_jax2(y):\n  ranger = jnp.arange(len(y))\n  return lax.scan(paraUpdate, y, ranger)[0]\n\nprint(np.allclose(filter_jax(jnpData), filter_jax2(jnpData)))\n# True\n\n%timeit filter_jax(jnpData).block_until_ready()\n# 10 loops, best of 3: 28.6 ms per loop\n\n%timeit filter_jax2(jnpData).block_until_ready()\n# 1000 loops, best of 3: 519 µs per loop\nIf you change your algorithm so that you'e applying the operation to every column in the array rather than the first N columns, it can be expressed with vmap like this:\npython\nCopy\n@jax.jit\ndef filter_jax3(y):\n  f = lambda col: jscp.convolve(impulse_20, col)[:-19]\n  return jax.vmap(f, in_axes=1, out_axes=1)(y)"
        ],
        "link": "https://stackoverflow.com/questions/64655749/vmap-ops-index-update-in-jax"
    },
    {
        "title": "Alternative to scipy stats zmap function",
        "question": "Is there any alternative to scipy stats module of the zmap function? I'm currently using it to obtain the zmap scores for two really large arrays and it's taking quite some time.\nAre there any libraries or alternatives that could boost its performance? Or even another of obtaining what the zmap function does?\nYour ideas and comments would be appreciated!\nHere's my minimal reproducible code below:\npython\nCopy\nfrom scipy import stats\nimport numpy as np\n\nFeatureData = np.random.rand(483, 1)\ngoodData = np.random.rand(4640, 483)\nFeatureNorm= stats.zmap(FeatureData, goodData)\nAnd here's what the scipy stats.zmap does under the hood:\npython\nCopy\ndef zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\nAny ideas on how to optimize it for my use case? Could I use libraries like numba or JAX to boost this further?",
        "answers": [
            "Fortunately, the zmap code is pretty straightforward. The overhead in numpy, however, will come from the fact that it must instantiate intermediate arrays. If you use a numerical compiler such as that available in numba or jax, it can fuse these operations and do the computation with less overhead.\nUnfortunately, numba doesn't support optional arguments to mean and std, so let's take a look at JAX. For reference, here are benchmarks for scipy and for the raw numpy version of the function, computed on a Google Colab CPU runtime:\npython\nCopy\nimport numpy as np\nfrom scipy import stats\n\nFeatureData = np.random.rand(483, 1)\ngoodData = np.random.rand(4640, 483)\n\n%timeit stats.zmap(FeatureData, goodData)\n# 100 loops, best of 3: 13.9 ms per loop\n\ndef np_zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\n%timeit np_zmap(FeatureData, goodData)\n# 100 loops, best of 3: 13.8 ms per loop\nHere is the equivalent code executed in JAX, both eager mode and JIT compiled:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef jnp_zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(jnp.asarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\njit_jnp_zmap = jit(jnp_zmap)\n\nFeatureData = jnp.array(FeatureData)\ngoodData = jnp.array(goodData)\n%timeit jnp_zmap(FeatureData, goodData).block_until_ready()\n# 100 loops, best of 3: 8.59 ms per loop\n\njit_jnp_zmap(FeatureData, goodData)  # trigger compilation\n%timeit jit_jnp_zmap(FeatureData, goodData).block_until_ready()\n# 100 loops, best of 3: 2.78 ms per loop\nThe JIT-compiled version is about a factor of 5 faster than the scipy or numpy code. On a Colab T4 GPU runtime, the compiled version gains another factor of 10:\npython\nCopy\n%timeit jit_jnp_zmap(FeatureData, goodData).block_until_ready()\n1000 loops, best of 3: 286 µs per loop\nIf this kind of operation is a bottleneck in your analysis, a compiler like JAX might be a good option."
        ],
        "link": "https://stackoverflow.com/questions/64437380/alternative-to-scipy-stats-zmap-function"
    },
    {
        "title": "Efficiently fill an array from a function",
        "question": "I want to construct a 2D array from a function in such a way that I can utilize jax.jit.\nThe way I would normally do this using numpy is to create an empty array, and then fill that array in-place.\npython\nCopy\nxx = jnp.empty((num_a, num_b))\nyy = jnp.empty((num_a, num_b))\nzz = jnp.empty((num_a, num_b))\n\nfor ii_a in range(num_a):\n    for ii_b in range(num_b):\n        a = aa[ii_a, ii_b]\n        b = bb[ii_a, ii_b]\n\n        xyz = self.get_coord(a, b)\n\n        xx[ii_a, ii_b] = xyz[0]\n        yy[ii_a, ii_b] = xyz[1]\n        zz[ii_a, ii_b] = xyz[2]\nTo make this work within jax I have attempted to use the jax.opt.index_update.\npython\nCopy\n        xx = xx.at[ii_a, ii_b].set(xyz[0])\n        yy = yy.at[ii_a, ii_b].set(xyz[1])\n        zz = zz.at[ii_a, ii_b].set(xyz[2])\nThis runs without errors but is very slow when I try to use a @jax.jit decorator (at least an order of magnitude slower than the pure python/numpy version).\nWhat is the best way to fill a multi-dimensional array from a function using jax?",
        "answers": [
            "JAX has a vmap transform that is designed specifically for this kind of application.\nAs long as your get_coords function is compatible with JAX (i.e. is a pure function with no side-effects), you can accomplish this in one line:\npython\nCopy\nfrom jax import vmap\nxx, yy, zz = vmap(vmap(get_coord))(aa, bb)",
            "This can be achieved efficiently by using either the jax.vmap or the jax.numpy.vectorize functions.\nAn example using vectorize:\npython\nCopy\nimport jax.numpy as jnp\n\ndef get_coord(a, b):\n    return jnp.array([a, b, a+b])\n\nf0 = jnp.vectorize(get_coord, signature='(),()->(i)')\nf1 = jnp.vectorize(f0, excluded=(1,), signature='()->(i,j)')\n\nxyz = f1(a,b)\nThe vectorize function uses vmap under the hood, so this should be exactly equivalent to:\npython\nCopy\nf0 = jax.vmap(get_coord, (None, 0))\nf1 = jax.vmap(f0, (0, None)) \nThe advantage of using vectorize is that the code can be still be run in standard numpy. The disadvantage is less concise code and possibly a small amount of overhead because of the wrapper."
        ],
        "link": "https://stackoverflow.com/questions/64221771/efficiently-fill-an-array-from-a-function"
    },
    {
        "title": "Efficient way to compute Jacobian x Jacobian.T",
        "question": "Assume J is the Jacobian of some function f with respect to some parameters. Are there efficient ways (in PyTorch or perhaps Jax) to have a function that takes two inputs (x1 and x2) and computes J(x1)*J(x2).transpose() without instantiating the entire J matrices in memory?\nI have come across something like jvp(f, input, v=vjp(f, input)) but don't quite understand it and not sure is what I want.",
        "answers": [
            "In JAX, you can compute a full jacobian matrix using jax.jacfwd or jax.jacrev, or you can compute a jacobian operator and its transpose using jax.jvp and jax.vjp.\nSo, for example, say you had a function Rᴺ → Rᴹ that looks something like this:\npython\nCopy\nimport jax.numpy as jnp\nimport numpy as np\n\nnp.random.seed(1701)\nN, M = 10000, 5\nf_mat = np.array(np.random.rand(M, N))\ndef f(x):\n  return jnp.sqrt(f_mat @ x / N)\nGiven two vectors x1 and x2, you can evaluate the Jacobian matrix at each using jax.jacfwd\npython\nCopy\nimport jax\nx1 = np.array(np.random.rand(N))\nx2 = np.array(np.random.rand(N))\nJ1 = jax.jacfwd(f)(x1)\nJ2 = jax.jacfwd(f)(x2)\nprint(J1 @ J2.T)\n# [[3.3123782e-05 2.5001222e-05 2.4946943e-05 2.5180108e-05 2.4940484e-05]\n#  [2.5084497e-05 3.3233835e-05 2.4956826e-05 2.5108084e-05 2.5048916e-05]\n#  [2.4969209e-05 2.4896170e-05 3.3232871e-05 2.5006309e-05 2.4947023e-05]\n#  [2.5102483e-05 2.4947576e-05 2.4906987e-05 3.3327218e-05 2.4958186e-05]\n#  [2.4981882e-05 2.5007204e-05 2.4966144e-05 2.5076926e-05 3.3595043e-05]]\nBut, as you note, along the way to computing this 5x5 result, we instantiate two 5x10,000 matrices. How might we get around this?\nThe answer is in jax.jvp and jax.vjp. These have somewhat unintuitive call signatures for the purposes of your question, as they are designed primarily for use in forward-mode and reverse-mode automatic differentiation. But broadly, you can think of them as a way to compute J @ v and J.T @ v for a vector v, without having to actually compute J explicitly.\nFor example, you can use jax.jvp to compute the effect of J1 operating on a vector, without actually computing J1:\npython\nCopy\nJ1_op = lambda v: jax.jvp(f, (x1,), (v,))[1]\n\nvN = np.random.rand(N)\nnp.allclose(J1 @ vN, J1_op(vN))\n# True\nSimilarly, you can use jax.vjp to compute the effect of J2.T operating on a vector, without actually computing J2:\npython\nCopy\nJ2T_op = lambda v: jax.vjp(f, x2)[1](v)[0]\n\nvM = np.random.rand(M)\nnp.allclose(J2.T @ vM, J2T_op(vM))\n# True\nPutting these together and operating on an identity matrix gives you the full jacobian matrix product that you're after:\npython\nCopy\ndef direct(f, x1, x2):\n  J1 = jax.jacfwd(f)(x1)\n  J2 = jax.jacfwd(f)(x2)\n  return J1 @ J2.T\n\ndef indirect(f, x1, x2, M):\n  J1J2T_op = lambda v: jax.jvp(f, (x1,), jax.vjp(f, x2)[1](v))[1]\n  return jax.vmap(J1J2T_op)(jnp.eye(M)).T\n\nnp.allclose(direct(f, x1, x2), indirect(f, x1, x2, M))\n# True\nAlong with the memory savings, this indirect method is also a fair bit faster than the direct method, depending on the sizes of the jacobians involved:\npython\nCopy\n%time direct(f, x1, x2)\n# CPU times: user 1.43 s, sys: 14.9 ms, total: 1.44 s\n# Wall time: 886 ms\n%time indirect(f, x1, x2, M)\n# CPU times: user 311 ms, sys: 0 ns, total: 311 ms\n# Wall time: 158 ms"
        ],
        "link": "https://stackoverflow.com/questions/63559139/efficient-way-to-compute-jacobian-x-jacobian-t"
    },
    {
        "title": "Get and Post API call in java with basic authentication",
        "question": "I want to call GET and POST API in java without using any framework. I need to use basic authentication. Can anybody help me with some tutorial link. In google I found code only in spring framework, But I am not using Spring. I am looking for code to call API with basic authentication.\nI have to add new url with authentication in the below code. What modification is required if API is secured with basic auth and it is POST method. I am new to java so not much aware.\njava\nCopy\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.Proxy;\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class NetClientGet {\n\n    public static void main(String[] args)  {\n        \n        try\n        {\n            System.out.println(\"Inside the main function\");\n             URL weburl=new URL(\"http://dummy.restapiexample.com/api/v1/employees\");\n             HttpURLConnection conn = (HttpURLConnection) weburl.openConnection();\n             conn.setRequestMethod(\"GET\");\n             conn.setRequestProperty(\"Accept\", \"application/json\");\n             System.out.println(\"Output is: \"+conn.getResponseCode());\n             System.out.println(\"Output is: \");\n             System.setProperty(\"http.proxyHost\", null);\n             //conn.setConnectTimeout(60000);\n             if(conn.getResponseCode()!=200)\n             {\n                 System.out.println(conn.getResponseCode());\n                 throw new RuntimeException(\"Failed : HTTP Error Code: \"+conn.getResponseCode());\n             }\n             System.out.println(\"After the 2 call \");\n             InputStreamReader in=new InputStreamReader(conn.getInputStream());\n             BufferedReader br =new BufferedReader(in);\n             String output;\n             while((output=br.readLine())!=null)\n             {\n                 System.out.println(output);\n             }\n             conn.disconnect();\n             \n        }\n        catch(Exception e)\n        {\n            System.out.println(e.getMessage());\n        }\n        \n    }\n}",
        "answers": [
            "Basic Authentication\nSee the RFC #2617 section 2: Basic Authentication Scheme\nAdd Authentication header into the request. Here's an example:\njava\nCopy\nString username = \"john\";\nString password = \"pass\";\n// ...\nURL weburl=new URL(\"http://dummy.restapiexample.com/api/v1/employees\");\nHttpURLConnection conn = (HttpURLConnection) weburl.openConnection();\nconn.setRequestMethod(\"GET\");\nconn.setRequestProperty(\"Accept\", \"application/json\");\n// snippet begins\nconn.setRequestProperty(\"Authorization\",\n  \"Basic \" + Base64.getEncoder().encodeToString(\n    (username + \":\" + password).getBytes()\n  )\n);\n// snippet ends\nSystem.out.println(\"Output is: \"+conn.getResponseCode());\nPOST Method\nSee this answer for more information about using POST method with HttpURLConnection."
        ],
        "link": "https://stackoverflow.com/questions/62564502/get-and-post-api-call-in-java-with-basic-authentication"
    },
    {
        "title": "vmap over a list in jax",
        "question": "Using jax, I try to calculate gradients per sample, process them and then bring them in the normal form to calculate a normal parameter update. My working code looks like\npython\nCopy\ndifferentiate_per_sample = jit(vmap(grad(loss), in_axes=(None, 0, 0)))\ngradients = differentiate_per_sample(params, x, y)\n\n# some code\n\ngradients_summed_over_samples = []\n    for layer in gradients:\n        (dw, db) = layer\n        (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n        gradients_summed_over_samples.append((dw, db))\nwhere gradients is of the form list(tuple(DeviceArray(...), DeviceArray(...)), ...).\nNow I tried to rewrite the loop as vmap (not sure if it brings a speedup in the end)\npython\nCopy\ndef sum_samples(layer):\n    (dw, db) = layer\n    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n\nvmap(sum_samples)(gradients)\nbut sum_samples is called only once and not for each element in the list.\nIs the list the problem or do I understand something else wrong?",
        "answers": [
            "jax.vmap will only be mapped over jax array inputs, not inputs that are lists of arrays or tuples. In addition, vmapped functions cannot modify inputs in-place; the functions should return a value, and this return value will be stacked with other return values to construct the output\nFor example, you could modify the function you defined and use it like this:\npython\nCopy\nimport jax.numpy as np\nfrom jax import random\n\ndef sum_samples(layer):\n    (dw, db) = layer\n    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n    return np.array([dw, db])\n\nkey = random.PRNGKey(1701)\ndata = random.uniform(key, (10, 2, 20))\n\nresult = vmap(sum_samples)(data)\nprint(result.shape)\n# (10, 2)\nSide note: if you're using this approach, the vmapped function above can be more concisely expressed as:\npython\nCopy\ndef sum_samples(layer):\n    return layer.sum(1)"
        ],
        "link": "https://stackoverflow.com/questions/61786831/vmap-over-a-list-in-jax"
    },
    {
        "title": "What's the best way to compute row-wise (or axis-wise) dot products with jax?",
        "question": "I have two numerical arrays of shape (N, M). I'd like to compute a row-wise dot product. I.e. produce an array of shape (N,) such that the nth row is the dot product of the nth row from each array.\nI'm aware of numpy's inner1d method. What would the best way be to do this with jax? jax has jax.numpy.inner, but this does something else.",
        "answers": [
            "You can define your own jit-compiled version of inner1d in a few lines of jax code:\npython\nCopy\nimport jax\n@jax.jit\ndef inner1d(X, Y):\n  return (X * Y).sum(-1)\nTesting it out:\npython\nCopy\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.core import umath_tests\n\n\nX = np.random.rand(5, 10)\nY = np.random.rand(5, 10)\n\nprint(umath_tests.inner1d(X, Y))\nprint(inner1d(jnp.array(X), jnp.array(Y)))\n# [2.23219571 2.1013316  2.70353783 2.14094973 2.62582531]\n# [2.2321959 2.1013315 2.703538  2.1409497 2.6258256]",
            "You can try jax.numpy.einsum. Here the implementaion using numpy einsum\npython\nCopy\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\narr1 = np.random.randint(0,10,[5,5])\narr2 = np.random.randint(0,10,[5,5])\n\narr = np.inner1d(arr1,arr2)\narr\narray([ 87, 200, 229,  81,  53])\nnp.einsum('...i,...i->...',arr1,arr2)\narray([ 87, 200, 229,  81,  53])"
        ],
        "link": "https://stackoverflow.com/questions/61314443/whats-the-best-way-to-compute-row-wise-or-axis-wise-dot-products-with-jax"
    },
    {
        "title": "JAX: time to jit a function grows superlinear with memory accessed by function",
        "question": "Here is a simple example, which numerically integrates the product of two Gaussian pdfs. One of the Gaussians is fixed, with mean always at 0. The other Gaussian varies in its mean:\npython\nCopy\nimport time\n\nimport jax.numpy as np\nfrom jax import jit\nfrom jax.scipy.stats.norm import pdf\n\n# set up evaluation points for numerical integration\nintegr_resolution = 6400\nlower_bound = -100\nupper_bound = 100\nintegr_grid = np.linspace(lower_bound, upper_bound, integr_resolution)\nproba = pdf(integr_grid)\nintegration_weight = (upper_bound - lower_bound) / integr_resolution\n\n\n# integrate with new mean\ndef integrate(mu_new):\n    x_new = integr_grid - mu_new\n\n    proba_new = pdf(x_new)\n    total_proba = sum(proba * proba_new * integration_weight)\n\n    return total_proba\n\n\nprint('starting jit')\nstart = time.perf_counter()\nintegrate = jit(integrate)\nintegrate(1)\nstop = time.perf_counter()\nprint('took: ', stop - start)\nThe function looks seemingly simple, but it doesn't scale at all. The following list contains pairs of (value for integr_resolution, time it took to run the code):\n100 | 0.107s\n200 | 0.23s\n400 | 0.537s\n800 | 1.52s\n1600 | 5.2s\n3200 | 19s\n6400 | 134s\nFor reference, the unjitted function, applied to integr_resolution=6400 takes 0.02s.\nI thought that this might be related to the fact that the function is accessing a global variable. But moving the code to set up the integration points inside of the function has no notable influence on the timing. The following code takes 5.36s to run. It corresponds to the table entry with 1600 which previously took 5.2s:\npython\nCopy\n# integrate with new mean\ndef integrate(mu_new):\n    # set up evaluation points for numerical integration\n    integr_resolution = 1600\n    lower_bound = -100\n    upper_bound = 100\n    integr_grid = np.linspace(lower_bound, upper_bound, integr_resolution)\n    proba = pdf(integr_grid)\n    integration_weight = (upper_bound - lower_bound) / integr_resolution\n\n    x_new = integr_grid - mu_new\n\n    proba_new = pdf(x_new)\n    total_proba = sum(proba * proba_new * integration_weight)\n\n    return total_proba\nWhat is happening here?",
        "answers": [
            "I also answered this at https://github.com/google/jax/issues/1776, but adding the answer here too.\nIt's because the code uses sum where it should use np.sum.\nsum is a Python built-in that extracts each element of a sequence and sums them one by one using the + operator. This has the effect of building a large, unrolled chain of adds which XLA takes a long time to compile.\nIf you use np.sum, then JAX builds a single XLA reduction operator, which is much faster to compile.\nAnd just to show how I figured this out: I used jax.make_jaxpr, which dumps JAX's internal trace representation of a function. Here, it shows:\npython\nCopy\nIn [3]: import jax\n\nIn [4]: jax.make_jaxpr(integrate)(1)\nOut[4]:\n{ lambda b c ;  ; a.\n  let d = convert_element_type[ new_dtype=float32\n                                old_dtype=int32 ] a\n      e = sub c d\n      f = sub e 0.0\n      g = pow f 2.0\n      h = div g 1.0\n      i = add 1.8378770351409912 h\n      j = neg i\n      k = div j 2.0\n      l = exp k\n      m = mul b l\n      n = mul m 2.0\n      o = slice[ start_indices=(0,)\n                 limit_indices=(1,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      p = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] o\n      q = add p 0.0\n      r = slice[ start_indices=(1,)\n                 limit_indices=(2,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      s = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] r\n      t = add q s\n      u = slice[ start_indices=(2,)\n                 limit_indices=(3,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      v = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] u\n      w = add t v\n      x = slice[ start_indices=(3,)\n                 limit_indices=(4,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      y = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] x\n      z = add w y\n... similarly ...\nand it's then obvious why this is slow: the program is very big.\nContrast the np.sum version:\npython\nCopy\nIn [5]: def integrate(mu_new):\n   ...:     x_new = integr_grid - mu_new\n   ...:\n   ...:     proba_new = pdf(x_new)\n   ...:     total_proba = np.sum(proba * proba_new * integration_weight)\n   ...:\n   ...:     return total_proba\n   ...:\n\nIn [6]: jax.make_jaxpr(integrate)(1)\nOut[6]:\n{ lambda b c ;  ; a.\n  let d = convert_element_type[ new_dtype=float32\n                                old_dtype=int32 ] a\n      e = sub c d\n      f = sub e 0.0\n      g = pow f 2.0\n      h = div g 1.0\n      i = add 1.8378770351409912 h\n      j = neg i\n      k = div j 2.0\n      l = exp k\n      m = mul b l\n      n = mul m 2.0\n      o = reduce_sum[ axes=(0,)\n                      input_shape=(100,) ] n\n  in [o] }\nHope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/59068666/jax-time-to-jit-a-function-grows-superlinear-with-memory-accessed-by-function"
    }
]