[
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Is it expected that vmapping over different input sizes for the same function impacts the accuracy of the result?",
        "question": "I was suprised to see that depending on the size of an input matrix, which is vmapped over inside of a function, the output of the function changes slightly. That is, not only does the size of the output change (which is what I would expect from vmapping) but also the numerics changed slightly. (Note that this only occurs in float32 and only on the GPU)\nI wrote a minimally reproducible example to illustrate the behaviour:\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\ndef equinox_vmap(x, mlp):\n    out = eqx.filter_vmap(mlp.__call__)(x)\n    return out\n\nkey = jax.random.PRNGKey(0)\nkey, network_key = jax.random.split(key, 2)\nmlp = eqx.nn.MLP(2, 2, 10, 2, key=network_key)\n\nkey, key_x = jax.random.split(key, 2)\nx = jax.random.normal(key_x, (10000, 2))\n\nerror_eqx = equinox_vmap(x[:10], mlp) - equinox_vmap(x, mlp)[:10]\nprint(\"eqx error:\", error_eqx)\nWhen running this example I get the output:\neqx error: [[-1.4442205e-04  1.0999292e-04]\n [-5.9515238e-05 -9.1716647e-06]\n [ 1.4841557e-05  5.6132674e-05]\n [ 0.0000000e+00  0.0000000e+00]\n [-9.1642141e-06 -2.5466084e-05]\n [ 3.8832426e-05 -3.3110380e-05]\n [ 3.3825636e-05 -2.4946406e-05]\n [ 4.0918589e-05 -3.2216311e-05]\n [ 1.3601780e-04  8.7693334e-06]\n [ 0.0000000e+00  0.0000000e+00]]\nI understand that the numerics of float32 are not fully accurate and some error is to be expected. However, I was suprised that the result changes depending on how much of the input array is put into the function. I was expecting that the first row of the x array, i.e., x[0,:] would still be filled with the same values and therefore the first row in the output would be the same.\nFurther notes:\nI enabled the use of float64 (jax.config.update(\"jax_enable_x64\", False)) which completely removed this from occuring. I understand that this is a numerical problem, but I am a little bit confused how the vmapping interacts with the example.\nWhen I run the same example on the CPU (using jax.config.update(\"jax_platform_name\", \"cpu\")) this problem also disappears which I also find difficult to understand.\nQuestions:\nIs this to be expected?\nWhere does this \"inconsistency\" come from?\nWhy does it not occur on the CPU and only on the GPU?\nSetup:\nGPU: NVIDIA RTX 6000 Ada Generation 48 GB\nPython 3.11.11 with\nequinox                  0.13.0\njax                      0.7.0\njax-cuda12-pjrt          0.7.0\njax-cuda12-plugin        0.7.0\njaxlib                   0.7.0\njaxtyping                0.3.2\nml_dtypes                0.5.3\nnumpy                    2.3.2\nnvidia-cublas-cu12       12.9.1.4\nnvidia-cuda-cupti-cu12   12.9.79\nnvidia-cuda-nvcc-cu12    12.9.86\nnvidia-cuda-nvrtc-cu12   12.9.86\nnvidia-cuda-runtime-cu12 12.9.79\nnvidia-cudnn-cu12        9.11.0.98\nnvidia-cufft-cu12        11.4.1.4\nnvidia-cusolver-cu12     11.7.5.82\nnvidia-cusparse-cu12     12.5.10.65\nnvidia-nccl-cu12         2.27.6\nnvidia-nvjitlink-cu12    12.9.86\nnvidia-nvshmem-cu12      3.3.9\nopt_einsum               3.4.0\npip                      24.0\nscipy                    1.16.1\nsetuptools               65.5.0\ntyping_extensions        4.14.1\nwadler_lindig            0.1.7\nAny explanations are greatly appreachiated.",
        "answers": [
            "This is behaving as expected. This is not fundamentally about vmap; this is about floating point math. Whenever you're doing floating point operations, you will accumulate rounding errors, and when you do the \"same\" computation in two different ways, you will accumulate rounding errors differently (see Is floating-point math broken? for some discussion of this).\nRunning vmap over different batch sizes results in different sequences of operations, which in turn results in different rounding errors.\nAs for why this differs between CPU and GPU, it's all about how the floating point operations are sequenced. CPU is a serial architecture, so it's likely computing matrix products row-by-row with the same accumulation orders regardless of input size. GPU is a parallel architecture, and will generally distribute and accumulate results differently depending on the size of the inputs."
        ],
        "link": "https://stackoverflow.com/questions/79726091/is-it-expected-that-vmapping-over-different-input-sizes-for-the-same-function-im"
    },
    {
        "title": "Are JAX operations already vectorized?",
        "question": "In the documentation, JAX provides vectorization. However, aren't JAX operations already vectorized? For example, to add two vectors, I thought that the element-wise additions were vectorized internally already.\nMy guess is that vectorization is useful when: it's hard for us to add a dimension for broadcasting, so we resort to a more explicit vectorization.\nEDIT: for example, instead of vectorizing convolution2d with different kernels, I simply stack the kernels, copy and stack the channel, then perform the convolution2d with this stack of kernels.",
        "answers": [
            "I have also raised a similar question here: https://github.com/jax-ml/jax/issues/26212 By now I think there is no universal answer to this and it will remain a matter of taste to a certain degree. However in some cases there is a clearer answer:\nSome operations in JAX are not natively vectorized, such as e.g. jnp.histogram or jnp.bincount, in this case you can use vmap to get a \"batched\" version of that function (for example search for \"batched_histogram\" here http://axeldonath.com/jax-diffusion-models-pydata-boston-2025/). This is really convenient and avoids loops to improve readability as well as performance.\nvmap works over PyTrees. Some libraries (most notably equinox) use this to avoid the need for handling a batch axis in models completely and just finally vmap over the whole parameter tree by convention. This frees developers from thinking about the batch axis at all, but when working with equinox you have to stick to that convention. It also only works if operations are independent across different batches. It does not work for operations such as a \"batch norm\" (see also https://docs.kidger.site/equinox/examples/stateful/)\nIn some cases one introduces a local(!) extra dimension to an array to avoid writing a Python loop and optionally reduce after. This can often be implemented more shortly and with clearer intent using vmap (basically what you said).\nAs broadcasting and batch axes are universally accepted convention in deep learning I mostly stick with them. But I rely on vmap whenever there is no native vectorization, whenever I work with libraries that rely on vmap by convention, or whenever I need to vectorize operations along non-conventional axes (basically everything except batch axis)."
        ],
        "link": "https://stackoverflow.com/questions/79718029/are-jax-operations-already-vectorized"
    },
    {
        "title": "Does vmap correctly split the RNG keys?",
        "question": "In the following code, when I remove the vmap, I have the right randomized behavior. However, with vmap, I don't anymore. Isn't this supposed to be one of the features of nnx.vmap?\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\n@nnx.vmap(\n    in_axes=(None, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((4, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")",
        "answers": [
            "To make dropout work together with vmap in flax, we need to use split_rngs and StateAxes :\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\nbs = 4\n\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=bs, only='dropout')\n@nnx.vmap(\n    in_axes=(state_axes, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\nmodel.train()\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\nOutput with jax: 0.7.0.dev20250704, flax: 0.10.6\nOutput batch:\n[[0.         0.1736668  1.6533196  0.         0.        ]\n [0.         0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.",
            "I'm not sure nnx.vmap and nnx.split_rngs are necessary in vfdev's answer. Also, having a train kwarg is unnecessary in most situations since NNX models can dynamically jump between train=True, train=False with .train() and .eval()\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    x = self.linear(x)\n    x = self.dropout(x)\n    return x\n\nkey = jax.random.PRNGKey(42)\n\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n@nnx.jit\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  return model(x)\n\nbs = 4\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Enable training. This works because Dropout layers have a .deterministic property\n# that can be modified.\nmodel.train()\n\noutput_batch = batched_apply(model, batch_input)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\noutput:\nModel initialized successfully.\nDropout Rate: 0.5\n------------------------------\nInput batch shape: (4, 10)\nInput batch:\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n------------------------------\nRunning the batched model in training mode (dropout is active)...\nOutput batch shape: (4, 5)\n\nOutput batch:\n[[0.         0.1736668  0.         0.         0.        ]\n [0.         0.         1.6533196  1.0752656  0.        ]\n [0.         0.         0.         0.         0.7218913 ]\n [0.09358063 0.         0.         1.0752656  0.        ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.\nand if instead you do model.eval()\nOutput batch:\n[[0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]]\n------------------------------\n❌ Verification failed: All outputs were the same."
        ],
        "link": "https://stackoverflow.com/questions/79698307/does-vmap-correctly-split-the-rng-keys"
    },
    {
        "title": "Configuration options varying between jax installs?",
        "question": "I have a laptop I do work on for a program that includes jax, the program ends up getting run here on a small scale to test it, then it is sent off to a server for batch processing.\nIn the program I have set these flags for jax:\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\n(as well as others but these are the relevant ones)\nThis runs fine on my laptop (using sharding to CPU parallelise), but when running on the server on GPU, I get an error message:\nAttributeError: Unrecognized config option: jax_captured_constants_report_frames\n(and the same for jax_captured_constants_warn_bytes if that were to run first)\nWhy is there this discrepancy? Can I use these flags some other way that is generalised between different jax installs?\npip list | grep jax, on laptop:\njax                       0.6.2\njaxlib                    0.6.2\njaxtyping                 0.3.2\non server:\njax                       0.6.0\njax-cuda12-pjrt           0.6.0\njax-cuda12-plugin         0.6.0\njaxlib                    0.6.0\njaxtyping                 0.3.2\nEDIT: As a side note, what is the scope of jax flags? I have a jax initialisation function to set os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=\" + str(cpu_count()) before the rest of the code runs, if I set jax.config.update(..., ...) options in here, will they hold in files called after it that also import jax? Or do I have to set them again? Is there a function to check the current value of these flags?",
        "answers": [
            "The jax_captured_constants_report_frames and jax_captured_constants_warn_bytes configurations were added in JAX version 0.6.1 (Relevant PR: https://github.com/jax-ml/jax/pull/28157) If you want to use them on your server, you'll have to update JAX to v0.6.1 or later."
        ],
        "link": "https://stackoverflow.com/questions/79693916/configuration-options-varying-between-jax-installs"
    },
    {
        "title": "Unable to set cpu device count for jax parallelisation?",
        "question": "I have been trying to generalise this jax program for solving on both CPU and GPU depending on the machine it's running on (essentially need cpu parallelisation to speed up testing versus gpu for production). I can get jax to parallelise on the GPU, but no matter what I do jax will not detect my cpu_count and thus cannot be sharded across cores (for context am running on 8 core, 16 thread laptop processor).\nI found out that XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT had to be set before jax was initialised (was previously set in the if statement included in the code), but it is still not working. I also tried setting at the very start of my code (this is a snippet from the only file using jax itself, but some other files use jnp as a jax drop in for numpy).\nCan anyone tell me why jax will not pick up on the flag? (Relevant code snippet and jupyter notebook output included below). Thanks.\nRelevant code snippet:\nfrom multiprocessing import cpu_count\ncore_count = cpu_count()\n\n### THIS NEEDS TO BE SET BEFORE JAX IS INITIALISED IN ANY WAY, INCLUDING IMPORTING\n# - XLA_FLAGS are read WHEN jax is IMPORTED\n\n# you can see other ways of setting the environment variable that I've tried here\n\n#jax.config.update('xla_force_host_platform_device_count', core_count)\n#os.environ[\"XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT\"] = '16'#str(core_count)\n#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=' + str(core_count)\nos.environ[\"XLA_FLAGS\"] = f\"--xla_force_host_platform_device_count={cpu_count()}\"\n\nimport jax\n\n# defaults float data types to 64-bit instead of 32 for greater precision\njax.config.update('jax_enable_x64', True)\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\njax.config.update('jax_traceback_filtering', 'off')\n# https://docs.jax.dev/en/latest/gpu_memory_allocation.html\n#jax.config.update('xla_python_client_allocator', '\\\"platform\\\"')\n# can't set via jax.config.update for some reason\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = '\\\"platform\\\"'\n\nprint(\"\\nDefault jax backend:\", jax.default_backend())\n\navailable_devices = jax.devices()\nprint(f\"Available devices: {available_devices}\")\n\nrunning_device = xla_bridge.get_backend().platform\nprint(\"Running device:\", running_device, end='')\n\nif running_device == 'cpu':\n    print(\", with:\", core_count, \"cores.\")\n\n    from jax.sharding import PartitionSpec as P, NamedSharding\n\n    # Create a Sharding object to distribute a value across devices:\n    # Assume core_count is the no. of core devices available\n    mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n\n    # Example matrix shape (9, N), e.g., N = 1e7\n    #x = jax.random.normal(jax.random.key(0), (9, Np))\n\n    # Specify sharding: don't split axis 0 (rows), split axis 1 (columns) across devices\n    # then apply sharding to produce a sharded array from the matrix input\n    # and use jax.device_put to distribute it across devices:\n    s0_sharded = jax.device_put(s0, NamedSharding(mesh, P(None, 'cols')))  # 'None' means don't shard axis 0\n\n    print(s0_sharded.sharding)            # See the sharding spec\n    print(s0_sharded.addressable_shards)  # Check each device's shard\n    jax.debug.visualize_array_sharding(s0_sharded)\nOutput:\nDefault jax backend: cpu\nAvailable devices: [CpuDevice(id=0)]\nRunning device: cpu, with: 16 cores.\n\n...\n\nrelevant line of my code: --> 258 mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n... jax backend trace\nValueError: Number of devices 1 must be >= the product of mesh_shape (16,)",
        "answers": [
            "I tried running your snippet and got a number of errors related to missing imports and undefined names (os is not defined, xla_bridge is not defined, s0 is undefined). This, along with the fact that you're running in Jupyter notebook, makes me think that you've already imported JAX in your runtime before running this cell.\nAs mentioned in your code comments, the XLA device count must be set before JAX is imported in your runtime. You should try restarting the Jupyter kernel, then fix the missing imports and variables and rerun your cell as the first execution in your fresh runtime.\nHere's a simple recipe that should work to set the device count while asserting that you've not already imported JAX in another cell in your notebook:\nimport os\nimport sys\n\nassert \"jax\" not in sys.modules, \"jax already imported: you must restart your runtime\"\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\nIf running this results in an assertion error, then you'll have to restart your kernel/runtime before running it again."
        ],
        "link": "https://stackoverflow.com/questions/79691728/unable-to-set-cpu-device-count-for-jax-parallelisation"
    },
    {
        "title": "Jax vmapping while loop [closed]",
        "question": "Closed. This question needs debugging details. It is not currently accepting answers.\nEdit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\nClosed last month.\nImprove this question\nI have a function that has jax.lax.while_loop. Now, I want to vmap it. However, vmap makes the execution time very slow compared to the original one.\nI understand that in the case of lax.cond, it is transformed into select, which evaluates all branches and thus may decrease the computational speed.\nIs a similar thing happening here? If so, what is the best practice to do do xx while y is true with vmap?",
        "answers": [
            "A while_loop under vmap becomes a single while_loop over a batched body_fun and cond_fun, meaning effectively that every loop in the batch executes for the same number of iterations. If different batches lead to vastly different iteration times, this can result in extra computation compared to executing individual while_loops in sequence."
        ],
        "link": "https://stackoverflow.com/questions/79660448/jax-vmapping-while-loop"
    },
    {
        "title": "Looking for an efficent JAX function to reconstruct an image from patches",
        "question": "I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.\nI am trying to reconstruct the original images from the patches. Here is vanilla python code that works:\nkernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)\n\ndef fwd(x):\n    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')\n\n    # return channels last\n    return jnp.transpose(xcv, [0,2,3,1])\n\npatches = fwd(bfrc)\n\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\n\n# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT\n# H_PATCHES == IMG_WIDTH // PATCH_WIDTH\n\nreconstructed = np.zeros(EXPECTED_IMG_SHAPE)\n\nfor vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):\n    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):\n        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):\n            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):\n                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):\n                    row = vpatch * PATCH_HEIGHT + prow\n                    col = hpatch * PATCH_WIDTH + pcol\n                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]\n\n# This assert passes\nassert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0\nOf course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?",
        "answers": [
            "I'm not sure what happened here:\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\nbut I assume it's some kind of mistake.\nAssuming bfrc has shape of (batch, channels, height, width), and\nV_PATCHES = IMG_HEIGHT // PATCH_HEIGHT\nH_PATCHES = IMG_WIDTH // PATCH_WIDTH\nthen patch_reshaped_pn_c_h_w will have the shape of (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH).\nKeeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.\nV, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape\n\nH_total = V * PH\nW_total = H * PW\n\npatches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)\n\nreconstructed = patches.reshape(V, H, PH, PW, C)\nreconstructed = reconstructed.transpose(0, 2, 1, 3, 4)\nreconstructed = reconstructed.reshape(H_total, W_total, C)\nreconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)\nYou can additionally wrap it into @jax.jit, which should be slightly faster."
        ],
        "link": "https://stackoverflow.com/questions/79647350/looking-for-an-efficent-jax-function-to-reconstruct-an-image-from-patches"
    },
    {
        "title": "Would using lists rather than jax.numpy arrays lead to more accurate numerical transformations?",
        "question": "I am doing a project with RNNs using jax and flax and I have noticed some behavior that I do not really understand.\nMy code is basically an optimization loop where the user provides the initial parameters for the system they want to optimize. This system is divided onto several time steps. He feeds the initial input into the first time step of the the system, gets a certain output, feeds this output into a RNN which returns the parameters for the following time step and so on. Then it is optimized using adam (particularly using optax).\nNow the user inputs his initial parameters as a dict and then there is a function called prepare_parameters_from_dict that basically converts this dict into a list of lists (or a list of jnp arrays for that matter).\nMy question/observation is when I make this function return a list of jnp.arrays instead of a list of lists, the property I am optimizing is an order of magnitude worse!\nFor example, using a list of lists outputs 0.9997 and a list of jnp.arrays outputs 0.998 (the closer to one the better).\nNoting: the RNN output a list of jnp.arrays (it is using flax linnen) and everything in the code remains the same.\nHere are said function:\nOutputing list of lists:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(flat_params)\n        shapes.append(len(flat_params))\n    return res, shapes\nUsing list of jnp.arrays:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(jnp.array(flat_params))\n        shapes.append(jnp.array(flat_params).shape[0])\n    return res, shapes\nand this is an example of the users input initial params:\ninitial_params = {\n    \"param1\": {\n        \"gamma\": 0.1,\n        \"delta\": -3 * jnp.pi / 2,\n    }\n}\nThe rest of the code remains exactly the same for both.\nAfter optimization if for example there were five time steps, this is how the final optimized params for each time step would look like:\nusing list of jnp.arrays:\n[[Array([ 0.1       , -4.71238898], dtype=float64)],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nusing list of lists:\n[[ [0.1       , -4.71238898] ]],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nWould such a difference in behavior be due to how jax handles grad and jit and others with lists compared to jnp.arrays or am I missing something?",
        "answers": [
            "The main operative difference between these two cases is that Python floats are treated as weakly-typed, meaning that the list version of your code could result in operations being performed at a lower precision. For example:\nIn [1]: import jax\n\nIn [2]: import jax.numpy as jnp\n\nIn [3]: jax.config.update('jax_enable_x64', True)\n\nIn [4]: list_values = [0.1, -4.71238898]\n\nIn [5]: array_values = jax.numpy.array(list_values)\n\nIn [6]: x = jax.numpy.float32(1.0)\n\nIn [7]: x + list_values[1]\nOut[7]: Array(-3.712389, dtype=float32)\n\nIn [8]: x + array_values[1]\nOut[8]: Array(-3.71238898, dtype=float64)\nNotice that the array version leads to higher-precision computations in this case. If I had to guess what the main difference is in your two runs, I'd guess something to do with the precision implied by strict vs weak types."
        ],
        "link": "https://stackoverflow.com/questions/79634990/would-using-lists-rather-than-jax-numpy-arrays-lead-to-more-accurate-numerical-t"
    },
    {
        "title": "How to select between using a `jax.lax.scan` vs a `for` loop when using JAX?",
        "question": "I am a JAX beginner and someone experienced with JAX told me that if we have repeated calls to a scan/for loop (e.g. when these are themselves wrapped by another for loop), it might be better to leave the loop as a for instead of converting it to a scan because the for loop is unrolled completely and only has the 1-time huge compilation cost while the scan is not unrolled by default and even though its compilation cost will be small, the fact that it is rolled will mean that the cost of repeatedly running this loop will end up making the scan more expensive than the for. This did not strike me immediately when I started writing my code, but made sense upon thinking about it.\nSo, I tested this assumption using code based on the following pseudo-code (the full code is really long and I hope these relevant parts I provide here are easier to understand):\nfor i in range(num_train_steps):  # Currently fixed to be a for loop\n  for j in range(num_env_steps):  # Currently fixed to be a for loop\n    act()\n\ndef act():\n  for k in range(num_algo_iters):  # Currently playing around with making this one either a scan or a for loop\n    jax.lax.scan(rollout_func)  # Currently fixed to be a scan\nThe only loop in the above code that I tested switching between scan and for was the k loop and then I varied the variable num_env_steps to be 1, 100, 1000 and 10000 to see whether increasing the number of times the act() (and thus the k loop) was executed made a difference to the timing. (The testing was done with 5 iterations for the k for loop and 2 iterations for the innermost scan although these are variable in general, if that matters.) The times taken for act() for the different repeats were 1.5, 11.3,, 99.0, 956.2 seconds for the scan version and 5.1, 14.5, 103.6, 972.7 seconds for the for version. So the for version never ended up faster for the number of repeats I tried.\nSo, now I am wondering if for any number of repeats (i.e. num_env_steps), the unrolling of the for actually makes the program faster than with scan. My questions:\nWould maybe increasing the repeats even more by setting num_env_steps to 100k or 1 million make it faster or can we always just replace a for with a scan? I have this question because I wonder if I am trying to over-optimise my code by converting every for to a scan.\nIf I set unroll = True for the scan, would it then always be fine to replace all fors with scans and expect speed-ups?\nIs there a rule of thumb that can help me decide when to use for and when to use scan if I am only interested in such speed-ups?\nact was jitted by the way.",
        "answers": [
            "scan vs for loop is essentially a tradeoff between compilation cost and runtime cost.\nJAX unrolls Python control flow, meaning that a for loop with 100 iterations leads to a single linear program with 100 copies of the loop body. The benefit of this is that it leaves the compiler free to optimize code across loop iterations, e.g. fusing operations between one iteration and the next; or noticing that one output is unused and eliding every computation in its graph. The downside is that compilation cost grows super-linearly with the size of the program, so for loops with large loop bodies and/or many iterations can lead to very long compilation times.\nWith scan or fori_loop on the other hand, the looping logic is pushed into the HLO, and the loop body is only parsed and compiled once. This results in much more efficient compilation, but may leave some runtime performance on the table compared to a for loop, because the compiler has fewer degrees of freedom to work with.\nThe best option will depend on the details of your program, and the relative importance of runtime and compile time costs in your particular application. Speaking very generally, though: for a smaller loop body with fewer iterations, for loops are often the better choice. For a larger loop body with more iterations, scan / fori_loop is likely better.\nNote also that scan has an unroll parameter that gives you the ability to tune the tradeoff between these extremes: unroll=True is effectively equivalent to a for loop, while unroll=n for 1 < n < N_iterations effectively puts a small for loop within each step of the larger scan."
        ],
        "link": "https://stackoverflow.com/questions/79633608/how-to-select-between-using-a-jax-lax-scan-vs-a-for-loop-when-using-jax"
    },
    {
        "title": "JAX Point Cloud Processing: Slow index_points_3d operation causing extreme XLA fusion loops in backpropagation",
        "question": "I'm trying to use JAX for implementing point cloud processing. However, I found that training becomes extremely slow due to my implementation of the following index_points_3d operation, which performs selection of features based on 3D indices.\nHere's my current implementation:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    features_expanded = features[..., None, :]\n    idx_expanded = indices[..., None]\n    return jnp.take_along_axis(features_expanded, idx_expanded, axis=1)\nWhen I traced the profiler, I found that this operation triggers extreme repetitions of loop_dynamic_update_slice_fusion, loop_add_fusion, input_reduce_fusion, and loop_select_fusion in the backpropagation stage as in following.\nThe forward pass is not a problem since the learning went fast when I stopped the gradient of the output features.\nI've tried different implementations such as using vmap on the batch dimension, but failed to achieve any performance gains.\nI'm not deeply familiar with JAX's low-level operations, so I'm unsure if this is a fundamental limitation of JAX/XLA or if there's a more efficient approach. Any help or guidance on optimizing this operation would be greatly appreciated!",
        "answers": [
            "Thanks to jakevdp's comment, I got a significant speedup using one-hot matrix multiplication. I changed to the following code:\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    B, N, C = features.shape\n    _, S, K = indices.shape\n    one_hot = jax.nn.one_hot(indices, num_classes=N, dtype=features.dtype)\n    return jnp.einsum('bskn,bnc->bskc', one_hot, features)"
        ],
        "link": "https://stackoverflow.com/questions/79631678/jax-point-cloud-processing-slow-index-points-3d-operation-causing-extreme-xla-f"
    },
    {
        "title": "Why some nested python functions are defined as `def _():`",
        "question": "I understand internal functions are prefixed with '_' to indicate they are helper/internal functions. It also helps with tooling etc. But I find some functions with just '_' as their name. Can't even find where they are called from. e.g., from\nhttps://github.com/jax-ml/jax/blob/7412adec21c534f8e4bcc627552f28d162decc86/jax/_src/pallas/mosaic/helpers.py#L72\ndef run_on_first_core(core_axis_name: str):\n  \"\"\"Runs a function on the first core in a given axis.\"\"\"\n  num_cores = jax.lax.axis_size(core_axis_name)\n  if num_cores == 1:\n    return lambda f: f()\n\n  def wrapped(f):\n    core_id = jax.lax.axis_index(core_axis_name)\n\n    @pl_helpers.when(core_id == 0)\n    @functools.wraps(f)\n    def _(): ## How is this called?\n      return f()\n\n  return wrapped\nThere are several of them in an internal code base but here are some references\nhttps://github.com/search?q=repo%3Ajax-ml%2Fjax%20def%20_()%3A&type=code\nhttps://github.com/jax-ml/jax/blob/7412adec21c534f8e4bcc627552f28d162decc86/docs/pallas/tpu/distributed.ipynb#L1125",
        "answers": [
            "A name of _ is different from a name prefixed with _. A name that is only _ means, by convention, \"I need to supply a name to satisfy the syntax, but I don't actually need to use the name\"*. That would be the case here, since the _ is never actually used anywhere.\nIn terms of how this function is actually called, the when decorator appears to be here:\ndef when(condition):\n  def _wrapped(f):\n    if isinstance(condition, bool):\n      if condition:\n        f()\n    else:\n      jax.lax.cond(condition, f, lambda: None)\n  return _wrapped\nYou can see that the decorator has a handle on the function via f, and calls it internally if condition is satisfied.\n* I could have sworn that this convention comes from PEP8, but I've skimmed the document twice now, and can't find where it says it."
        ],
        "link": "https://stackoverflow.com/questions/79625948/why-some-nested-python-functions-are-defined-as-def"
    },
    {
        "title": "Why is array manipulation in JAX much slower?",
        "question": "I'm working on converting a transformation-heavy numerical pipeline from NumPy to JAX to take advantage of JIT acceleration. However, I’ve found that some basic operations like broadcast_to and moveaxis are significantly slower in JAX—even without JIT—compared to NumPy, and even for large batch sizes like 3,000,000 where I would expect JAX to be much quicker.\n### Benchmark: moveaxis + broadcast_to ###\nNumPy: moveaxis + broadcast_to → 0.000116 s\nJAX: moveaxis + broadcast_to → 0.204249 s\nJAX JIT: moveaxis + broadcast_to → 0.054713 s\n\n### Benchmark: broadcast_to only ###\nNumPy: broadcast_to → 0.000059 s\nJAX: broadcast_to → 0.062167 s\nJAX JIT: broadcast_to → 0.057625 s\nAm I doing something wrong? Are there better ways of performing these kind of manipulations?\nHere's a minimal benchmark ChatGPT generated, comparing broadcast_to and moveaxis in NumPy, JAX, and JAX with JIT:\nimport timeit\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit\n\n# Base transformation matrix\nM_np = np.array([[1, 0, 0, 0.5],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1]])\n\nM_jax = jnp.array(M_np)\n\n# Batch size\nn = 1_000_000\n\nprint(\"### Benchmark: moveaxis + broadcast_to ###\")\n\n# NumPy\nt_numpy = timeit.timeit(\n    lambda: np.moveaxis(np.broadcast_to(M_np[:, :, None], (4, 4, n)), 2, 0),\n    number=10\n)\nprint(f\"NumPy: moveaxis + broadcast_to → {t_numpy:.6f} s\")\n\n# JAX\nt_jax = timeit.timeit(\n    lambda: jnp.moveaxis(jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)), 2, 0).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: moveaxis + broadcast_to → {t_jax:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_and_move_jax(M):\n    return jnp.moveaxis(jnp.broadcast_to(M[:, :, None], (4, 4, n)), 2, 0)\n\n# Warm-up\nbroadcast_and_move_jax(M_jax).block_until_ready()\n\nt_jit = timeit.timeit(\n    lambda: broadcast_and_move_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: moveaxis + broadcast_to → {t_jit:.6f} s\")\n\nprint(\"\\n### Benchmark: broadcast_to only ###\")\n\n# NumPy\nt_numpy_b = timeit.timeit(\n    lambda: np.broadcast_to(M_np[:, :, None], (4, 4, n)),\n    number=10\n)\nprint(f\"NumPy: broadcast_to → {t_numpy_b:.6f} s\")\n\n# JAX\nt_jax_b = timeit.timeit(\n    lambda: jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: broadcast_to → {t_jax_b:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_only_jax(M):\n    return jnp.broadcast_to(M[:, :, None], (4, 4, n))\n\nbroadcast_only_jax(M_jax).block_until_ready()\n\nt_jit_b = timeit.timeit(\n    lambda: broadcast_only_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: broadcast_to → {t_jit_b:.6f} s\")",
        "answers": [
            "There are a couple things happening here that come from the different execution models of NumPy and JAX.\nFirst, NumPy operations like broadcasting, transposing, reshaping, slicing, etc. typically return views of the original buffer. In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies. I suspect this is the largest contribution to the timing difference here.\nSecond, NumPy tends to have very fast dispatch time for individual operations. JAX has much slower dispatch time for individual operations, and this can become important when the operation itself is very cheap (like \"return a view of the array with different strides/shape\")\nYou might wonder given these points how JAX could ever be faster than NumPy. The key is JIT compilation of sequences of operations: within JIT-compiled code, sequences of operations are fused so that the output of each individual operation need not be allocated (or indeed, need not even exist at all as a buffer of intermediate values). Additionally, for JIT compiled sequences of operations the dispatch overhead is paid only once for the whole program. Compare this to NumPy where there's no way to fuse operations or to avoid paying the dispatch cost of each and every operation.\nSo in microbenchmarks like this, you can expect JAX to be slower than NumPy. But for real-world sequences of operations wrapped in JIT, you should often find that JAX is faster, even when executing on CPU.\nThis type of question comes up enough that there's a section devoted to it in JAX's FAQ: FAQ: is JAX faster than NumPy?\nAnswering the followup question:\nIs the statement \"In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies\", within a jitted environment?\nThis question is not really well-formulated, because in a jitted environment, array objects do not necessarily correspond to buffers of values. Let's make this more concrete with a simple example:\nimport jax\n\n@jax.jit\ndef f(x):\n  y = x[::2]\n  return y.sum()\nYou might ask: in this program, is y a copy or a view of x? The answer is neither, because y is never explicitly created. Instead, JIT fuses the slice and the sum into a single operation: the array x is the input, and the array y.sum() is the output, and the intermediate array y is never actually created.\nYou can see this by printing the compiled HLO for this function:\nx = jax.numpy.arange(10)\nprint(f.lower(x).compile().as_text())\nHloModule jit_f, is_scheduled=true, entry_computation_layout={(s32[10]{0})->s32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.9 (Arg_0.10: s32[], Arg_1.11: s32[]) -> s32[] {\n  %Arg_0.10 = s32[] parameter(0), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  %Arg_1.11 = s32[] parameter(1), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  ROOT %add.12 = s32[] add(s32[] %Arg_0.10, s32[] %Arg_1.11), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\n%fused_computation (param_0.2: s32[10]) -> s32[] {\n  %param_0.2 = s32[10]{0} parameter(0)\n  %iota.0 = s32[5]{0} iota(), iota_dimension=0, metadata={op_name=\"jit(f)/jit(main)/iota\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.1 = s32[] constant(2)\n  %broadcast.0 = s32[5]{0} broadcast(s32[] %constant.1), dimensions={}\n  %multiply.0 = s32[5]{0} multiply(s32[5]{0} %iota.0, s32[5]{0} %broadcast.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %bitcast.1 = s32[5,1]{1,0} bitcast(s32[5]{0} %multiply.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %gather.0 = s32[5]{0} gather(s32[10]{0} %param_0.2, s32[5,1]{1,0} %bitcast.1), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}, indices_are_sorted=true, metadata={op_name=\"jit(f)/jit(main)/gather\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.0 = s32[] constant(0)\n  ROOT %reduce.0 = s32[] reduce(s32[5]{0} %gather.0, s32[] %constant.0), dimensions={0}, to_apply=%region_0.9, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\nENTRY %main.14 (Arg_0.1: s32[10]) -> s32[] {\n  %Arg_0.1 = s32[10]{0} parameter(0), metadata={op_name=\"x\"}\n  ROOT %gather_reduce_fusion = s32[] fusion(s32[10]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\nThe output is complicated, but the main thing to look at here is the ENTRY %main section, which is the \"main\" program generated by compilation. It consists of two steps: %Arg0.1 identifies the input argument, and ROOT %gather_reduce_fusion is essentially a single compiled kernel that sums every second element of the input. No intermediate arrays are generated. The blocks above this (e.g. the %fused_computation (param_0.2: s32[10]) -> s32[] definition) give you information about what operations are done within this kernel, but represent a single fused operation.\nNotice that the sliced array represented by y in the Python code never actually appears in the main function block, so questions about its memory layout cannot be answered except by saying \"y doesn't exist in the compiled program\".",
            "According to the Jax Docs (emphasis mine):\nif you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead"
        ],
        "link": "https://stackoverflow.com/questions/79615872/why-is-array-manipulation-in-jax-much-slower"
    },
    {
        "title": "Freezing filtered parameter collections with Flax.nnx",
        "question": "I'm trying to work out how to do transfer learning with flax.nnx. Below is my attempt to freeze the kernel of my nnx.Linear instance and optimize the bias. I think maybe I'm not correctly setting up the 'wrt' argument to my optimizer.\nfrom jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\ndef f(x,m=2.234,b=-1.123):\n    return m*x+b\n\ndef compute_loss(model, inputs, obs):\n    prediction = model(inputs)\n    error = obs - prediction\n    loss = jnp.mean(error ** 2)\n    mae = jnp.mean(jnp.abs(error ) )\n    return loss, mae\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    model.kernel.value = jnp.array([[2.0]]) #load pretrained kernel  \n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 ) \n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n    \n    loss_grad = nnx.value_and_grad(compute_loss, has_aux = True)\n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None,),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( optimizer, x, obs ):\n        (loss,mae), grads = loss_grad( optimizer.model, x, obs )        \n        optimizer.update( grads )\n        return optimizer, (loss,mae)\n\n    transfer_params = nnx.All(nnx.PathContains(\"bias\"))\n    optimizer_transfer = nnx.Optimizer(model, optax.adam(learning_rate=1e-3), wrt = transfer_params)\n\n    optimizer, (losses,maes) = optimizer_scan( optimizer_transfer, x1, obs1 )\n\n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = optimizer.model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', optimizer.model.kernel.value )\n    print( 'b approximation:', optimizer.model.bias.value )\nAnd this results in the following error:\nValueError: Mismatch custom node data: ('bias', 'kernel') != ('bias',); value: State({\n  'bias': VariableState(\n    type=Param,\n    value=Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=1/0)>\n  )\n}).",
        "answers": [
            "The missing link for me was nnx.DiffState. For clarification on DiffState, see the documentation for nnx.grad() on the nnx \"transforms\" page:\nhttps://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html\nAnyway, effectively the only changes that need be made for the code to work as intended are:\nMove the declaration of transfer_params to before the value_and_grad call,\nCreate an nnx.DiffState object diff_state = nnx.DiffState(0,transfer_params)\nGive diff_state as the argnums keyword for nnx.value_and_grad.\nAnd that does it!\nAnother helpful example of how to use nnx.DiffState with parameter filtering can be found here:\nhttps://github.com/google/flax/issues/4167\nAnd lastly here is the complete fixed example:\nfrom jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\ndef f(x,m=2.234,b=-1.123):\n    return m*x+b\n\ndef compute_loss(model, inputs, obs):\n    prediction = model(inputs)\n    error = obs - prediction\n    loss = jnp.mean(error ** 2)\n    mae = jnp.mean(jnp.abs(error ) )\n    return loss, mae\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    model.kernel.value = jnp.array([[2.0]]) #load pretrained kernel\n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 ) \n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n\n    transfer_params = nnx.All(nnx.PathContains(\"bias\"))\n    diff_state = nnx.DiffState(0,transfer_params)\n    \n    loss_grad = nnx.value_and_grad(compute_loss, argnums = diff_state, has_aux = True)\n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None,),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( optimizer, x, obs ):\n        (loss,mae), grads = loss_grad( optimizer.model, x, obs )        \n        optimizer.update( grads )\n        return optimizer, (loss,mae)\n\n    optimizer_transfer = nnx.Optimizer(model, optax.adamw(learning_rate = 1e-3), wrt = transfer_params)\n\n    optimizer, (losses,maes) = optimizer_scan( optimizer_transfer, x1, obs1 )\n\n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = optimizer.model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', optimizer.model.kernel.value )\n    print( 'b approximation:', optimizer.model.bias.value )"
        ],
        "link": "https://stackoverflow.com/questions/79580101/freezing-filtered-parameter-collections-with-flax-nnx"
    },
    {
        "title": "DIfference in variable values in jax non-jit runtime and jit transformed runtime",
        "question": "I have a deep learning mode which I am running in the jit transformed manner by:\nmy_function_checked = checkify.checkify(model.apply)\n    model_jitted = jax.jit(my_function_checked)\n    err, pred = model_jitted({\"params\": params}, batch, training=training, rng=rng)\n    err.throw()\nThe code is compiling fine, but now I want to debug the intermediate values after every few steps, save the arrays, and then compare them with pytorch tensors. For this, I need to repeatedly save the arrays. The easiest way to do this is to use any IDE's inbuilt debugger and evaluate the save expression after every few steps. But jax.jit transformed code doesn't allow external debuggers. But, I can do this after disabling the jit. Should I be expecting any discrepancies between the two runs? Can I assume that the values in jit and non-jit runs will remain same?",
        "answers": [
            "In general when comparing the same JAX operation with and without JIT, you should expect equivalence up to typical floating point rounding errors, but you should not expect bitwise equivalence, as the compiler may fuse operations in a way that leads to differing float error accumulation."
        ],
        "link": "https://stackoverflow.com/questions/79571227/difference-in-variable-values-in-jax-non-jit-runtime-and-jit-transformed-runtime"
    },
    {
        "title": "Reproducibility of JAX calculations",
        "question": "I am using JAX in running Reinforcement Learning (RL) & Multi-Agent Reinforcement Learning (MARL) calculations. I have noticed the following behaviour:\nIn RL, my results are always fully reproducible.\nIn MARL, where computations become significantly heavier, my results are reprodicible when running on CPU.\nHowever, when running MARL in GPU I encounter a different behaviour. I have noticed that repeating the same calculation within a script execution leads to identical results. However, executing the same script twice leads to different results. The latter problem is only mitigated when I use:\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_ops\"\nos.environ[\"JAX_DISABLE_MOST_FASTER_PATHS\"] = \"1\"\nUnfortunately, this measure significantly reduces the computation speed.\nAny idea about dealing with this issue?",
        "answers": [
            "On an accelerator like GPU, there will generally be a tradeoff between strict bit-wise reproducibility and speed of computation.\nWhy is this? Fundamentally, this is because of the fact that floating point arithmetic only approximates real arithmetic, and so the order in which operations are executed can change the results, and order of operations is a degree of freedom that the GPU can exploit to execute code faster.\nAs a simple example, consider summing the same array in different orders:\nIn [1]: import numpy as np\n\nIn [2]: rng = np.random.default_rng(0)\n\nIn [3]: x = rng.normal(size=10000).astype('float32')\n\nIn [4]: x.sum()\nOut[4]: np.float32(63.11888)\n\nIn [5]: x[::-1].sum()\nOut[5]: np.float32(63.118877)\nThe results differ slightly.\nThis is relevant to your question because of the way a GPU works: GPUs do fast vector operations by automatically running them in parallel. So, for example, to compute a sum, it might chunk the array across N cores, sum each chunk individually, and then accumulate the intermediate sums to get the final result.\nIf you only care mainly about speed, you can sacrifice reproducibility and accumulate those intermediate sums in the order they're ready, which might vary from run to run, and therefore produce slightly different results. If you care mainly about reproducibility, then you have to sacrifice some speed by ensuring that you accumulate those intermediate sums in exactly the same order every time, which may leave the process waiting for a slower chunk even if a faster chunk is already ready. This is a simplistic example but the same principal applies for any computation parallelized on a GPU.\nSo fundamentally speaking, there will always be a tradeoff between bitwise reproducibility and speed of computation. You've already discovered the primary flags for controlling this tradeoff (XLA_FLAGS=\"--xla_gpu_deterministic_ops\" and JAX_DISABLE_MOST_FASTER_PATHS=1 ). Your question seems to be \"can I somehow get both speed and strict bitwise reproducibility at once\": the answer to that question is No."
        ],
        "link": "https://stackoverflow.com/questions/79563698/reproducibility-of-jax-calculations"
    },
    {
        "title": "Flax nnx / jax: tree.map for layers of incongruent size",
        "question": "I am trying to figure out how to use nnx.split_rngs. Can somebody give a version of the code below that uses nnx.split_rngs with jax.tree.map to produce an arbitrary number of Linear layers with different out_features?\nimport jax\nfrom flax import nnx\nfrom functools import partial\n\nif __name__ == '__main__':\n\n    session_sizes = {\n        'a':2,\n        'b':3,\n        'c':4,\n        'd':5,\n        'e':6,\n    }\n    dz = 2\n\n    rngs = nnx.Rngs(0)\n    \n    my_linear = partial(\n        nnx.Linear,\n        use_bias = False,\n        in_features = dz,\n        rngs=rngs )\n    \n    def my_linear_wrapper(a):\n        return my_linear( out_features=a )\n\n    q_s = jax.tree.map(my_linear_wrapper, session_sizes)\n\n    for k in session_sizes.keys():\n        print(q_s[k].kernel)\nSo in this case, we would need a tree of layers that will take our 2 in_features into spaces of 2, ..., 6 out_features.\nThe function my_linear_wrapper is sort of a workaround for the original solution we had in mind, which is to map in very much the same fashion as we're doing, but instead use (something like) the @nnx.split_rngs function decorator.\nIs there a way to use nnx.split_rngs on my_linear in order to map over the rng argument to nnx.Linear?",
        "answers": [
            "split_rngs is mostly useful when you are going to pass the Rngs through a transform like vmap, here you want to produce variable sized Modules so the current solution is the way to go. Because of how partial works you can simplify this to:\ndin = 2\nrngs = nnx.Rngs(0)\n\nmy_linear = functools.partial(\n  nnx.Linear, din, use_bias=False, rngs=rngs\n)\n\nq_s = jax.tree.map(my_linear, session_sizes)\n\nfor k in session_sizes.keys():\n  print(q_s[k].kernel)"
        ],
        "link": "https://stackoverflow.com/questions/79551198/flax-nnx-jax-tree-map-for-layers-of-incongruent-size"
    },
    {
        "title": "Why is Jax treating floating point values as tracers rather than concretizing them when nesting jitted functions?",
        "question": "I am doing some physics simulations using jax, and this involves a function called the Hamiltonian defined as follows:\n# Constructing the Hamiltonian\n@partial(jit, static_argnames=['n', 'omega'])\ndef hamiltonian(n: int, omega: float):\n    \"\"\"Construct the Hamiltonian for the system.\"\"\"\n    H = omega *  create(n) @ annhilate(n)\n    return H \nand then a bigger function def solve_diff(n, omega, kappa, alpha0): that is defined as follows:\n@partial(jit, static_argnames=['n', 'omega'])\ndef solve_diff(n, omega, kappa, alpha0):\n    # Some functionality that uses kappa and alpha0\n    \n    H = hamiltonian(n, omega)\n\n    # returns an expectation value\nWhen I try to compute the gradient of this function using jax.grad\nn = 16   \nomega = 1.0   \nkappa = 0.1  \nalpha0 = 1.0 \n\n# Compute gradients with respect to omega, kappa, and alpha0\ngrad_population = grad(solve_diff, argnums=(1, 2, 3))\ngrads = grad_population(n, omega, kappa, alpha0)\n\nprint(f\"Gradient w.r.t. omega: {grads[0]}\")\nprint(f\"Gradient w.r.t. kappa: {grads[1]}\")\nprint(f\"Gradient w.r.t. alpha0: {grads[2]}\")\nit outputs the following error:\nValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jax._src.interpreters.ad.JVPTracer'>, Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n  primal = 1.0\n  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n    pval = (ShapedArray(float32[], weak_type=True), None)\n    recipe = LambdaBinding(). The error was:\nTypeError: unhashable type: 'JVPTracer'\nThough, running solve_diff(16,1.0,0.1,1.0) on its own works as expected.\nNow if I remove omega from the list of static variables for both the hamiltonian function and the solve_diff, the grad is output as expected.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls, both n and omega are constants and indeed should not change between function calls.",
        "answers": [
            "The fundamental issue is that you cannot differentiate with respect to a static variable, and if you try to do so you will get the error you observed.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls\nIn JAX, the term \"static\" does not have to do with whether the variable is changed between function calls. Rather, a static variable is a variable that does not participate in tracing, which is the mechanism used to compute transformations like vmap, grad, jit, etc. When you differentiate with respect to a variable, it is no longer static because it is participating in the autodiff transformation, and trying to treat it as static later in the computation will lead to an error.\nFor a discussion of transformations, tracing, and related concepts, I'd start with JAX Key Concepts: transformations."
        ],
        "link": "https://stackoverflow.com/questions/79550040/why-is-jax-treating-floating-point-values-as-tracers-rather-than-concretizing-th"
    },
    {
        "title": "General way to define JAX functions with non-differentiable arguments",
        "question": "For a particular JAX function func, one can define non-differentiable arguments by using the decorator @partial(jax.custom_jvp, nondiff_argnums=...). However, in order to make it work, one must also explicitly define the differentiation rules in a custom jvp function by using the decorator @func.defjvp. I'm wondering if there is a generic way to define non-differentiable arguments for any given func, without defining a custom jvp (or vjp) function? This will be useful when the differentiation rules are too complicated to write out.",
        "answers": [
            "In JAX's design, non-differentiated arguments are a property of the gradient transformation being used, not a property of the function being differentiated. custom_jvp is fundamentally about customizing the gradient behavior, and using it to mark non-differentiable arguments without actually customizing the gradient is not an intended use.\nThe way to ensure that arguments do not participate in an autodiff transformation is to specify the arguments you want to differentiate against when you call the jax.grad, jax.jacobian, or other autodiff transformation; e.g.\njax.grad(func, argnums=(0,))  # differentiate with respect to argument 0.\nRegardless of what func is, this will attempt to differentiate with respect to the 0th argument, and if that argument is either explicitly or implicitly not differentiable due to how func is defined, an error will be raised."
        ],
        "link": "https://stackoverflow.com/questions/79516990/general-way-to-define-jax-functions-with-non-differentiable-arguments"
    },
    {
        "title": "How can I apply member functions of a list of objects across slices of a JAX array using vmap?",
        "question": "I have a list of a objects, each of which has a function to be applied on a slice of a jax.numpy.array. There are n objects and n corresponding slices. How can I vectorise this using vmap?\nFor example, for the following code snippet:\nimport jax\nimport jax.numpy as jnp\n\nclass Obj:\n    def __init__(self, i):\n        self.i = i\n\n    def f1(self, x): return (x - self.i)\n\nx = jnp.arange(9).reshape(3, 3).astype(jnp.float32)\n\nfunctions_obj = [Obj(1).f1, Obj(2).f1, Obj(3).f1]\nhow would I apply the functions in functions_obj to slices of x?\nMore details, probably not relevant: My specific use-case is running the member functions of a lot of Reinforcement Learning Gym environment objects on slices of an actions array, but I believe my problem is more general and I formulated it as above. (P.S.: I know about AsyncVectorEnv by the way but that does not solve my problem as I am not trying to run the step function).",
        "answers": [
            "Use jax.lax.switch to select between the functions in the list and map over the desired axis of x at the same time:\ndef apply_func_obj(i, x_slice):\n    return jax.lax.switch(i, functions_obj, x_slice)\n\nindices = jnp.arange(len(functions_obj)) \n# Use vmap to apply the function element-wise\nresults = jax.vmap(apply_func_obj, in_axes=(0, 0))(indices, x)"
        ],
        "link": "https://stackoverflow.com/questions/79499056/how-can-i-apply-member-functions-of-a-list-of-objects-across-slices-of-a-jax-arr"
    },
    {
        "title": "Why does JAX's grad not always print inside the cost function?",
        "question": "I am new to JAX and trying to use it with PennyLane and optax to optimize a simple quantum circuit. However, I noticed that my print statement inside the cost function does not execute in every iteration. Specifically, it prints only once at the beginning and then stops appearing.\nThe quantum circuit itself does not make sense; I just wanted to simplify the example as much as possible. I believe the circuit is not actually relevant to the question, but it's included as an example.\nHere is my code:\nimport pennylane as qml\nimport jax\nimport jax.numpy as jnp\nimport optax\n\njax.config.update(\"jax_enable_x64\", True)\n\ndevice = qml.device(\"default.qubit\", wires=1)\n\n\n@qml.qnode(device, interface='jax')\ndef circuit(params):\n    qml.RX(params, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(params):\n    print('Evaluating')\n    return circuit(params)\n\n# Define optimizer\nparams = jnp.array(0.1)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\n\n# JIT the gradient function\ngrad = jax.jit(jax.grad(cost))\n\nfor epoch in range(5):\n    print(f'{epoch = }')\n    grad_value = grad(params)\n    updates, opt_state = opt.update(grad_value, opt_state)\n    params = optax.apply_updates(params, updates)\nExpected output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nEvaluating\nepoch = 3\nEvaluating\nepoch = 4\nEvaluating\nActual output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nepoch = 3\nepoch = 4\nQuestion:\nWhy is the print statement inside cost not executed after the first iteration? Is JAX caching the function call or optimizing it in a way that skips execution? How can I ensure that cost is evaluated in every iteration?",
        "answers": [
            "When working with JAX it is important to understand the difference between \"trace time\" and \"runtime\". For JIT compilation JAX does an abstract evaluation of the function when it is called first. This is used to \"trace\" the computational graph of the function and then create a fully compiled replacement, which is cached and then invoked on the next calls (\"runtime\") of the function. Now, Python's print statements are only evaluated at trace time and not at runtime, because the code of the function has been effectively replaced by a compiled version.\nFor the case of printing during runtime, JAX has a special jax.debug.print function, you can use:\ndef cost(params):\n    jax.debug.print('Evaluating')\n    return circuit(params)\nMore on the jax.debug utilities: https://docs.jax.dev/en/latest/debugging/index.html\nAnd JIT compilation: https://docs.jax.dev/en/latest/jit-compilation.html"
        ],
        "link": "https://stackoverflow.com/questions/79498911/why-does-jaxs-grad-not-always-print-inside-the-cost-function"
    },
    {
        "title": "Jax numpy extracting non-nan values gives NonConcreteBooleanIndexError",
        "question": "I have a jax 2d array with some nan-values\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\nand want to get an array which contains for each row only the non-nan values. The resulting array has thus the same number of rows, and either less columns or the same number but with nan values padded at the end. So in this case, the result should be\narray_2d = jnp.array([\n    [1,   2,      3],\n    [10  20,jnp.nan]\n    ])\nThe order (among non-nan values) should stay the same.\nTo make things easier, I know that each row has at most k (in this case 3) non-nan values. Getting the indices for the non-nan values is very easy, but ``moving them to the front'' is harder.\nI tried to work on a row-by-row basis; the following function works indeed:\n# we want to vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\nHowever, I can't vmap this. Even though I payed attention that the returned array always has the same size, the line ret_vals = row_vals[row_mask] makes problems, since this has a dynamic size. Does anyone know how to circumvent this? I believe that functions like `jnp.where' etc don't help either.\nHere is the full MWE:\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n# we want to get -- efficiently -- all non-nan values per row.\n# we know that each row has at most 3 non-nan values\n\n# we will vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\n\n# we now vmap\nnon_nan_vals = jax.vmap(get_non_nan_values)(array_2d) # this gives error: NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[5])\nNB: The array will be very large in practice and have many nan values, while k (the number of non-nan values) is on the order of 10 or 100.\nThank you very much!",
        "answers": [
            "By padding the array with a fill value at the end of each row first, you can rely on jnp.nonzero and its size and fill_value arguments, which define a fixed output size and fill value index, when the size requirement is not met. Here is a minimal example:\nimport jax.numpy as jnp\nimport jax\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n\n@jax.vmap\ndef get_non_nan_values(row_vals, size=3):\n    padded = jnp.pad(row_vals, (0, 1), constant_values=jnp.nan)\n    non_nan = jnp.nonzero(~jnp.isnan(padded), size=size, fill_value=-1)\n    return padded[non_nan]\n\nget_non_nan_values(array_2d)\nWhich returns:\nArray([[ 1.,  2.,  3.],\n       [10., 20., nan]], dtype=float32)\nI think this solution is a bit more compact and clearer in intend, however I have not checked the performance.\nI hope this helps!",
            "I think you can do what you want with this function, which rather than sorting the array (as I commented), sorts and masks the indices of the non-nan values:\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(1,))\ndef func(array, k=3):\n    m, n = array.shape[-2:]\n    indices = jnp.broadcast_to(jnp.arange(n)[None, :], (m, n))\n    sorted_masked_indices = jnp.sort(jnp.where(jnp.isnan(array), jnp.nan, indices))\n    array_rearranged = array[jnp.arange(m)[:, None], sorted_masked_indices.astype(int)]\n    return jnp.where(jnp.isnan(sorted_masked_indices), jnp.nan, array_rearranged)[:, :k]\nTest:\nimport numpy as np\nrng = np.random.default_rng(0)\nk = 3\n\na = rng.random((12, 6))\na[np.arange(12)[:, None], rng.integers(0, 6, (12, 6))] = np.nan\n\nprint(a)\nprint(func(a, k=k))\nGives:\n[[0.63696169        nan        nan 0.01652764 0.81327024        nan]\n [       nan 0.72949656        nan        nan 0.81585355        nan]\n [       nan 0.03358558        nan        nan        nan        nan]\n [0.29971189        nan        nan        nan        nan 0.64718951]\n [       nan        nan        nan 0.98083534        nan 0.65045928]\n [       nan        nan 0.13509651 0.72148834        nan        nan]\n [       nan 0.88948783 0.93404352 0.3577952         nan        nan]\n [       nan 0.33791123 0.391619   0.89027435        nan        nan]\n [       nan 0.83264415        nan        nan 0.87648423        nan]\n [0.33611706        nan        nan 0.79632427        nan 0.0520213 ]\n [       nan        nan 0.09075305 0.58033239        nan        nan]\n [       nan 0.94211311        nan        nan 0.62910815        nan]]\n[[0.6369617  0.01652764 0.8132702 ]\n [0.72949654 0.81585354        nan]\n [0.03358557        nan        nan]\n [0.29971188 0.6471895         nan]\n [0.9808353  0.6504593         nan]\n [0.1350965  0.72148836        nan]\n [0.88948786 0.9340435  0.3577952 ]\n [0.33791122 0.391619   0.89027435]\n [0.83264416 0.8764842         nan]\n [0.33611706 0.79632425 0.0520213 ]\n [0.09075305 0.5803324         nan]\n [0.9421131  0.62910813        nan]]",
            "With the stable=True option, argsort on a boolean array is guaranteed to preserve the relative order between True and False elements. So this should do the trick:\ndef get_non_nan_values(row_vals):\n    return row_vals[jnp.argsort(jnp.isnan(rowvals), stable=True)[:3]]\nHowever, for wide rows, sorting the entire row seems unnecessary when we already know there are only at most 3 non-nan values. So another simple approach using jax.lax.top_k:\ndef get_top_3_non_nan(row_vals):\n  return row_vals[jax.lax.top_k(~jnp.isnan(row_vals), 3)[1]]",
            "I would do this using vmap of argsort of isnan:\nimport jax\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n])\n\nresult = jax.vmap(lambda x: x[jnp.argsort(jnp.isnan(x))])(array_2d)\nprint(result)\n# [[ 1.  2.  3. nan nan]\n#  [10. 20. nan nan nan]]\nThis approach uses static shapes, and thus will be compatible with jit."
        ],
        "link": "https://stackoverflow.com/questions/79443943/jax-numpy-extracting-non-nan-values-gives-nonconcretebooleanindexerror"
    },
    {
        "title": "Problems when boolean indexing in Jax, getting NonConcreteBooleanIndexError",
        "question": "I'm currently trying to create a CustomProblem inheriting from the BaseProblem class in TensorNEAT which is a Jax based library. In trying to implement the evaluate function of this class, I'm using a boolean mask, but I have problems getting it to work. My code results in jax.errors.NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[n,n]) which I think is due to some of my arrays not having a definite shape. How do I circumvent this?\nConsider this example in np:\nimport numpy as np\n\nran_int = np.random.randint(1, 5, size=(2, 2))\nprint(ran_int)\n\nran_bool = np.random.randint(0,2, size=(2,2), dtype=bool)\nprint(ran_bool)\n\na = (ran_int[ran_bool]>0).astype(int)\nprint(a)\nIt could give an output like this:\n[[2 2]\n [3 4]]\n[[ True False]\n [ True  True]]\n[1 1 1] #Is 1D and has less elements than before boolean mask was applied!\nBut in Jax, the same way of thinking results in the NonConcreteBooleanIndexError error I got.\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #ERROR HAPPENS HERE\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #WOULD HAPPEN HERE TOO IF THIS PART WAS FIRST\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI was considering using jnp.where to solve the issue, but the resulting pairwise_labels and pairwise_predictions have a different shape than what I expect (namely (n, n)) as seen in the code below:\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) # shape (n, n)\n\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) # shape (n, n)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n ,n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI fear that the differing shapes of pairwise_predictions and pairwise_labels after using jnp.where will result in a different loss than if I had just used the boolean mask as I would in np. There is also the fact that I get another error that happens later in the pipeline with the output ValueError: max() iterable argument is empty from line 143 in the pipeline.py file of TensorNeat. This is curiously circumvented by changing pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold to pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold, which probably also results in some loss that is incorrect.\nBelow is some code that should be enough to setup a minimal running example that is similar to my setup:\nfrom tensorneat import algorithm, genome, common\nfrom tensorneat.pipeline import Pipeline\nfrom tensorneat.genome.gene.node import DefaultNode\nfrom tensorneat.genome.gene.conn import DefaultConn\nfrom tensorneat.genome.operations import mutation\nimport jax, jax.numpy as jnp\nfrom tensorneat.problem import BaseProblem\n\ndef binary_cross_entropy(prediction, target):\n    return -(target * jnp.log(prediction) + (1 - target) * jnp.log(1 - prediction))\n\n# Define the custom Problem\nclass CustomProblem(BaseProblem):\n\n    jitable = True  # necessary\n\n    def __init__(self, inputs, labels, threshold):\n        self.inputs = jnp.array(inputs) #nb! already has shape (n, 768)\n        self.labels = jnp.array(labels).reshape((-1,1)) #nb! has shape (n), must be transformed to have shape (n, 1) \n        self.threshold = threshold\n\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold #this is the thing I actually want\n        #pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold #weird fix to circumvent ValueError: max() iterable argument is empty when using jnp.where for pairwise_labels and pairwise_predictions\n        print(pairs_to_keep.shape)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape)\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\n\n    @property\n    def input_shape(self):\n        # the input shape that the act_func expects\n        return (self.inputs.shape[1],)\n\n    @property\n    def output_shape(self):\n        # the output shape that the act_func returns\n        return (1,)\n\n    def show(self, state, randkey, act_func, params, *args, **kwargs):\n        # showcase the performance of one individual\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(state, params, self.inputs)\n\n        loss = jnp.mean(jnp.square(predict - self.labels))\n\n        n_elements = 5\n        if n_elements > len(self.inputs):\n            n_elements = len(self.inputs)\n\n        msg = f\"Looking at {n_elements} first elements of input\\n\"\n        for i in range(n_elements):\n            msg += f\"for input i: {i}, target: {self.labels[i]}, predict: {predict[i]}\\n\"\n        msg += f\"total loss: {loss}\\n\"\n        print(msg)\n\nalgorithm = algorithm.NEAT(\n    pop_size=10,\n    survival_threshold=0.2,\n    min_species_size=2,\n    compatibility_threshold=3.0,  \n    species_elitism=2,  \n    genome=genome.DefaultGenome(\n        num_inputs=768,\n        num_outputs=1,\n        max_nodes=769,  # must at least be same as inputs and outputs\n        max_conns=768,  # must be 768 connections for the network to be fully connected\n        output_transform=common.ACT.sigmoid,\n        mutation=mutation.DefaultMutation(\n            # no allowing adding or deleting nodes\n            node_add=0.0,\n            node_delete=0.0,\n            # set mutation rates for edges to 0.5\n            conn_add=0.5,\n            conn_delete=0.5,\n        ),\n        node_gene=DefaultNode(),\n        conn_gene=DefaultConn(),\n    ),\n)\n\n\nINPUTS = jax.random.uniform(jax.random.PRNGKey(0), (100, 768)) #the input data x\nLABELS = jax.random.uniform(jax.random.PRNGKey(0), (100)) #the annotated labels y\n\nproblem = CustomProblem(INPUTS, LABELS, 0.25)\n\nprint(\"Setting up pipeline and running it\")\nprint(\"-----------------------------------------------------------------------\")\npipeline = Pipeline(\n    algorithm,\n    problem,\n    generation_limit=1,\n    fitness_target=1,\n    seed=42,\n)\n\nstate = pipeline.setup()\n# run until termination\nstate, best = pipeline.auto_run(state)\n# show results\npipeline.show(state, best)",
        "answers": [
            "The solution I got from the authors of TensorNEAT was to update the evaluate() function to use jnp.nan instead of -jnp.inf in the first jnp.where() calls used on pairwise_labels and pairwise_predictions. I also had to make the loss take into consideration the nan values that would be present in the loss after running the bce. The new evaluate() function that has the same behavior as boolean indexing is pasted below.\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold\n\n        #finding only the labels to keep\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, jnp.nan) #use jnp.nan here\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n\n        #finding only the predictions to keep\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, jnp.nan) #use jnp.nan here\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # loss with shape (len(labels), len(labels)), we need to reduce it to a scalar\n        loss = jnp.mean(loss, where=~jnp.isnan(loss)) #only use number values in loss\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss        \n        return -loss",
            "Yes, the mask operation makes the shape of the resulting array dependent on the content of the array. And jax only supports static shapes. The workaround you propose looks reasonable, with using the value -inf as a placeholder. The missing part is ignoring the zero entries in the mean. This you could achieve by a custom “masked” mean function along the lines of:\nfrom jax import numpy as jnp\nfrom jax import random\nimport jax\n\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (4, 4))\n\nkey, subkey = random.split(key)\nmask = random.bernoulli(key, 0.5, (4, 4))\n\n@jax.jit\ndef masked_mean(x, mask):\n    return jnp.sum(jnp.where(mask, x, 0), axis=0) / jnp.sum(mask, axis=0)\n\n\nmasked_mean(x, mask)\nI have not checked other parts of the code in detail, but e.g. the statement jnp.where(pairwise_labels > 0, True, False) has no effect. And with the masked mean you might not need the placeholder values at all.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79423352/problems-when-boolean-indexing-in-jax-getting-nonconcretebooleanindexerror"
    },
    {
        "title": "How to use jax.vmap with a tuple of flax TrainStates as input?",
        "question": "I am setting up a Deep MARL framework and I need to assess my actor policies. Ideally, this would entail using jax.vmap over a tuple of actor flax TrainStates. I have tried the following:\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.linen.initializers import constant, orthogonal\nfrom flax.training.train_state import TrainState\nimport optax\nimport distrax\n\nclass PGActor_1(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 4\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(128, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0))(x)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nclass PGActor_2(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 2\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nstate= jnp.zeros((1, 5))\n\nnetwork_1 = PGActor_1()\nnetwork_1_init_rng = jax.random.PRNGKey(42)\nparams_1 = network_1.init(network_1_init_rng, state)\n\nnetwork_2 = PGActor_2()\nnetwork_2_init_rng = jax.random.PRNGKey(42)\nparams_2 = network_2.init(network_2_init_rng, state)\n\ntx = optax.chain(\noptax.clip_by_global_norm(1),\noptax.adam(lr=1e-3)\n)\nactor_trainstates= (\n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_1),             \n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_2)\n )\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(actor_trainstates)\nbut I recieve the following error:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nDoes anybody have any idea how to make this work?\nThank you in advance.",
        "answers": [
            "This is quite similar to other questions (e.g. Jax - vmap over batch of dataclasses). The key point is that JAX transformations like vmap require data in a struct of arrays pattern, whereas you are using an array of structs pattern.\nTo work directly with an array of structs pattern in JAX, you can use Python's built-in map function – due to JAX's asynchronous dispatch, the resulting operations will be executed in parallel where possible:\npis = map(lambda x: x.apply_fn(x.params, state), actor_trainstates)\nHowever, this doesn't take advantage of the automatic vectorization done by vmap. In order to do this, you can convert your data from an array of structs to a struct of arrays, although this requires that all entries have the same structure.\nFor compatible cases, the solution would look something like this, however it errors for your data:\ntrain_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(train_states_soa)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-36-da904fa40b9c> in <cell line: 0>()\n----> 1 train_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\n\nValueError: Dict key mismatch; expected keys: ['Dense_0', 'Dense_1', 'Dense_2']\nThe problem is that your two train states do not have matching structure, and so they cannot be transformed into a single struct of arrays. You can see the difference in structure by inspecting the params:\nprint(actor_trainstates[0].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1', 'Dense_2'])\nprint(actor_trainstates[1].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1'])\nThere is no way to use vmap in a context where your inputs have different structure, so you'll either have to change the problem to ensure the same structure, or stick with the map approach."
        ],
        "link": "https://stackoverflow.com/questions/79405049/how-to-use-jax-vmap-with-a-tuple-of-flax-trainstates-as-input"
    },
    {
        "title": "Serialization in JAX",
        "question": "What is the recommended way to do serialization/deserialization in JAX?\nIn the context of reinforcement learning my starting point in terms of data might be e.g. match replays that have to be pre-processed to obtain tuples of JAX arrays. This is a process that I would like to do just once, save to disk, then wrap around that a data loading interface like grain.DataLoader.\nBut I have no idea how to do the actual serialization.\nRight now I'm doing\ndef save_jax(path, x: jnp.array):\n    y = np.array(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.array(x)\n    return y\nIt works. My hope is there won't be any copies in wrapping jnp->np or the other way around, and then hopefully this will be mmapped.\nIs this approach bad for performance? What is a better way?",
        "answers": [
            "Both Jax and Numpy support the Python buffer protocol, allowing for zero-copy data sharing between the different types. That being said when using jnp.array or np.array, the default is to copy (see linked docs). So right now you do an unnecessary copy of the data. So I would suggest to use np.asarray instead (and the Jax equivalent), which only copies when needed. So your code would look like:\ndef save_jax(path, x: jnp.array):\n    y = np.asarray(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.asarray(x)\n    return y\nAside from the copy the native Numpy format might not be the best choice of format in neither I/O performance nor associated meta data. For a lightweight alternative you might want to look for example into safetensors.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79387670/serialization-in-jax"
    },
    {
        "title": "How to do jittable masked get?",
        "question": "How do I do a jax get from a masked index?\nThe code below works without jit.\nx = jnp.arange(25).reshape((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n    [1,2],\n])\ncoords_mask = jnp.array([True, True, False, True])\n\n@jax.jit\ndef masked_gather(x, coords, coords_mask):\n    coords_masked = coords[coords_mask]\n    return x.at[coords_masked[:, 0], coords_masked[:, 1]].get()\n\nmasked_gather(x, coords, coords_mask)\nFails with NonConcreteBooleanIndexError.\nShould return Array([ 7, 13,  7], dtype=int32)",
        "answers": [
            "There is no way to execute this function in a JIT-compatible way, because JAX does not support compilation of programs with dynamic shapes. In your case, the size of the returned array depends on the number of True elements in coords_mask, and so the shape is dynamic by definition.\nSee JAX Sharp Bits: Dynamic Shapes for more information.\nDepending on what you are doing with the resulting value, there are a number of available approaches to work around this: for example, if the shape is truly unknown, you could return an array padded with zeros; it might look something like this:\n@jax.jit\ndef masked_gather_padded(x, coords, coords_mask, fill_value=0):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='fill', fill_value=fill_value)\n  return result[order]\n\nmasked_gather_padded(x, coords, coords_mask)\n# Array([ 7, 13,  7,  0], dtype=int32)\nAlternatively, if the number of True entries in the mask is known a priori, you could modify the function to accept a static size argument and use that to construct an appropriate output. It might look something like this:\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['size'])\ndef masked_gather_with_size(x, coords, coords_mask, *, size):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='drop')\n  return result[order[:size]]\n\nmasked_gather_with_size(x, coords, coords_mask, size=3)\n# Array([ 7, 13,  7], dtype=int32)\nThe best approach will depend on your application."
        ],
        "link": "https://stackoverflow.com/questions/79375141/how-to-do-jittable-masked-get"
    },
    {
        "title": "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?",
        "question": "Is it possible to use vmap for auto-batching if your function isn't jittable?\nI have a function that's not jittable:\ndef testfunc(model, x1, x2, x2_mask):\n    ( ... non-jittable stuff with masks ... )\nI'm trying to wrap it in vmap so I can benefit from auto-batching as explained here.\nSo I do:\ntestfunc_batched = jax.vmap(testfunc, in_axes=(None, 0, 0, 0))\nThe intention is that in batched mode, each of x1, x2, and x2_mask will have an additional outter dimension, the batching dimension. The model shouldn't be treated differently in batched mode hence the None. Let me know if the syntax isn't right.\nI create batches of size one just to test, schematically:\nx1s = x1.reshape(1, ...)\nx2s = x2.reshape(1, ...)\nx2_masks = x2_mask.reshape(1, ...)\n\ntestfunc_batched(model, x1s, x2s, x2_masks)\nThe last line fails with ConcretizationTypeError.\nI've recently learned that stuff with masks makes functions not jittable. But does that mean that I also can't use vmap? Or am I doing something wrong?\n(There is further context in How to JIT code involving masked arrays without NonConcreteBooleanIndexError?, but you don't have to read that question to understand this one.)",
        "answers": [
            "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?\nNo. In general, functions which are incompatible with jit will also be incompatible with vmap, because both jit and vmap use the same JAX tracing mechanism to transform the program."
        ],
        "link": "https://stackoverflow.com/questions/79374152/is-it-possible-to-use-jax-vmap-for-auto-batching-if-your-function-isnt-jittable"
    },
    {
        "title": "Count onto 2D JAX coordinates of another 2D array",
        "question": "I have\nx = jnp.zeros((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\nI would like to count onto x how many times each of the individual (x,y) coordinates appear in coords. In other words, obtain the output:\nArray([[0., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI've tried x.at[coords].add(1) and this gives me:\nArray([[0., 0., 0., 0., 0.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI understand what it's doing, but not how to make it do the thing I want.\nThere's this related question[1], but I haven't been able to use it to solve my problem.\n[1] Update JAX array based on values in another array",
        "answers": [
            "For multiple indices, you should pass a tuple of index arrays:\nx = x.at[coords[:, 0], coords[:, 1]].add(1)\nprint(x)\n[[0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]",
            "The generalized operation is basically computing a histogram, especially when the coordinate arrays are float values. So depending on the context the code is used, the following alternative might communicate the intent a bit more clearly:\nfrom jax import numpy as jnp\n\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\n\nbins = jnp.arange(5 + 1) - 0.5 \nx, _ = jnp.histogramdd(coords, bins=(bins, bins))\nIt will also handle if coordinates are out of bounds. But I presume under the hood, it does the same operation as at[...].add(1). So I would not expect any relevant difference in performance."
        ],
        "link": "https://stackoverflow.com/questions/79370053/count-onto-2d-jax-coordinates-of-another-2d-array"
    },
    {
        "title": "Return a different class based on an optional flag in the arguments without factory",
        "question": "I am implementing a series of classes in Equinox to enable taking derivatives with respect to the class parameters. Most of the time, the user will be instantiating class A and using the fn function to generate some data, the details of which are unimportant. However, in cases where we are interested in gradients, it is beneficial to represent param_c in terms of a sigmoid function to ensure that it remains clamped in the range (0,1). However, I don't want the user to notice a difference in how the class behaves if they do this. As such, I implement another class A_sigmoid that has param_c as a property and use A_abstract to ensure that both classes inherit the fn method, which will call param_c in its logic. While I could simply have the user instantiate an A_sigmoid object with a _param_c_sigmoid instead of param_c I don't want to force the user to have to make this distinction. Rather, I would want them to pass in the same kwargs dictionary no matter the class and have conversion happen behind the scenes. I also wanted to make it so that when making a new A one could simply pass an optional flag to direct the program to use the sigmoid version of the code. To do so, I implemented the following MWE:\nclass A_abstract(eqx.Module):\n    param_a: jax.Array\n    param_b: jax.Array\n    param_c: eqx.AbstractVar[jax.Array]\n    \n    def fn(self,*args,**kwargs):\n        pass\n\nclass A_sigmoid(A_abstract):\n    _param_c_sigmoid: jax.Array\n\n    @property\n    def param_c(self):\n        return 1 / (1 + jnp.exp(-self._param_c_sigmoid))\n\nclass A(A_abstract):\n    param_c: jax.Array\n\n    def __new__(cls, **kwargs):\n        sigmoid_flag = kwargs.pop('use_sigmoid_c',False)\n        if sigmoid_flag == True:\n            param_c = kwargs.pop('param_c')\n            _param_c_sigmoid = jnp.log(param_c / (1 - param_c))\n            kwargs['_param_c_sigmoid'] = _param_c_sigmoid\n            instance = A_sigmoid.__new__(A_sigmoid)\n            instance.__init__(**kwargs)\n            print(type(instance))\n            return instance\n        else:\n            return super(A,cls).__new__(cls)\n\nclassA = A(param_a = 1.,param_b = 2.,param_c = 0.5,use_sigmoid_c=True)\nprint(type(classA))\nThe code correctly says that instance has type A_sigmoid when print is called in the __new__ method. However, when I print type(classA), it is of type A and has no attribute param_c, though it does have a value for _param_c_sigmoid. Why is this the case? Am I missing something in my use of __new__ that is causing this error? While I know that in principle a factory would be the best way to do this, there are other classes of types B, C, etc. that don't have this need for a sigmoid implementation and that I would like to behave exactly the same way as A to enable them to be easily swapped. Thus, I don't want some custom method to instantiate A that would be different from calling the default constructor on the other classes.\nI am running this on a Jupyter notebook with the following package versions:\nPython           : 3.12.4\nIPython          : 8.30.0\nipykernel        : 6.29.5\njupyter_client   : 8.6.3\njupyter_core     : 5.7.2",
        "answers": [
            "If you were using a normal class, what you did is perfectly reasonable:\nclass A_abstract:\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A_sigmoid'>\nHowever, eqx.Module includes a bunch of metaclass logic that overrides how __new__ works, and this seems to collide with the __new__ overrides that you're making. Notice here the only difference is that A_abstract inherits from eqx.Module, and the result is A rather than A_sigmoid:\nimport equinox as eqx\n\nclass A_abstract(eqx.Module):\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A'>\nI dug-in for a few minutes to try and find the exact cause of this change, but wasn't able to pin it down.\nIf you're trying to do metaprogramming during class construction, you'll have to modify it to work within the construction-time metaprogramming that equinox is already doing."
        ],
        "link": "https://stackoverflow.com/questions/79359839/return-a-different-class-based-on-an-optional-flag-in-the-arguments-without-fact"
    },
    {
        "title": "Trying to install an older version of Jax",
        "question": "Trying to add a specific version of jax and jaxlib\npip install -U jaxlib==0.4.10          \nERROR: Ignored the following yanked versions: 0.4.32\nERROR: Could not find a version that satisfies the requirement jaxlib==0.4.10 (from versions: 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.24, 0.4.25, 0.4.26, 0.4.27, 0.4.28, 0.4.29, 0.4.30, 0.4.31, 0.4.33, 0.4.34, 0.4.35, 0.4.36, 0.4.38)\nERROR: No matching distribution found for jaxlib==0.4.10\nLooks like my old app needs jax to be '<=0.4.10'\nNot sure how to move forward",
        "answers": [
            "From the error message you're seeing, I suspect you're using Python 3.12. The first jaxlib release to support Python 3.12 was v0.4.17. If you want to use an older jaxlib version, you'll have to install an older version of Python.\njaxlib v0.4.10 supports Python v3.8-3.11; one way to see this is to look at the available wheel files on PyPI for this version: https://pypi.org/project/jaxlib/0.4.10/#files"
        ],
        "link": "https://stackoverflow.com/questions/79333553/trying-to-install-an-older-version-of-jax"
    },
    {
        "title": "How Can I Use GPU to Accelerate Image Augmentation?",
        "question": "When setting up image augmentation pipelines using keras.layers.Random* or other augmentation or processing methods, we often integrate these pipelines with a data loader, such as the tf.data API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.\nTo address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?",
        "answers": [
            "We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in keras. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.\nClassification\nLet’s take a classification task as an example. If we use the tf.data API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.\nimport numpy as np\nfrom keras import layers\n\na = np.ones((4, 224, 224, 3)).astype(np.float32)\nb = np.ones((4, 2)).astype(np.float32)\n\naugmentation_layers = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.2),\n    ]\n)\n\ndataset = tf.data.Dataset.from_tensor_slices((a, b))\ndataset = dataset.batch(3, drop_remainder=True)\ndataset = dataset.map(\n    lambda x, y: (augmentation_layers(x), y), \n    num_parallel_calls=tf.data.AUTOTUNE\n)\nx.shape, y.shape\n(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))\nBut for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.\ninputs = keras.Input(shape=(224, 224, 3))\nprocessed = augmentation_layers(inputs)\nbackbone = keras.applications.EfficientNetB0(\n    include_top=True, pooling='avg'\n)(processed)\noutput = keras.layers.Dense(10)(backbone)\nmodel = keras.Model(inputs, output)\nmodel.count_params() / 1e6\n5.340581\nHere, we set the augmentation pipeline right after keras.Input. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.\nSegmentation\nI'll use this dataset for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using keras-3, which might allow for multi-backend support.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # torch, jax\n\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nkeras.__version__ # 3.4.1\n# ref https://keras.io/examples/vision/oxford_pets_image_segmentation/\n# u-net model\ndef get_model(img_size, num_classes, classifier_activation):\n    ...\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(\n        num_classes, \n        3, \n        activation=classifier_activation, \n        padding=\"same\", \n        dtype='float32'\n    )(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\nimg_size = (224, 224)\nnum_classes = 1\nclassifier_activation = 'sigmoid'\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nLet's define the augmentation pipelines.\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\")\n]\n\ndef augment_data(images, masks):\n    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = tf.cast(combined[..., 3:], tf.int32)\n    return images_augmented, masks_augmented\nLet’s define the tf.data API to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    \n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = tf.cast(image, tf.int32)\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.\n        \n    return image\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask  = read_image(mask_list, mask=True)\n    return image, mask\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE) \n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n    # Augmenting on CPU\n    dataset = dataset.map(\n        augment_data, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nprint(\"Train Dataset:\", train_dataset)\nTrain Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))>\nNow, let's compile it and run it.\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nmodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n\n%%time\nepochs = 2\nmodel.fit(\n    train_dataset, \n    epochs=epochs, \n)\nEpoch 1/2\n318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087\nEpoch 2/2\n318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338\nCPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s\nWall time: 1min 48s\nNext, we will remove the augmentation layers from the dataloader.\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE)\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nTo offload augmentation to the GPU, we’ll create a custom model class, override the train_step, and use the augment_data method that we defined earlier. Here's how to structure it:\nclass ExtendedModel(keras.Model):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def call(self, inputs):\n        return self.model(inputs)\n\n    def save(\n        self, filepath, \n        overwrite=True, \n        include_optimizer=True, \n        save_format=None, \n        add_loss=None, \n    ):\n        # Overriding this method will allow us to use the `ModelCheckpoint`\n        self.model.save(\n            filepath=filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            include_optimizer=include_optimizer,\n        )\nNow that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nemodel = ExtendedModel(model)\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nemodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n%%time\nepochs = 2\nemodel.fit(\n    train_dataset, \n    epochs=epochs, \n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            filepath='model.{epoch:02d}-{loss:.3f}.keras',\n            monitor='loss',\n            mode='min',\n            save_best_only=True\n        )\n    ]\n)\nEpoch 1/2\n318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748\nEpoch 2/2\n318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585\nCPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s\nWall time: 1min 29s\nSo, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around 18.35% improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.\nAs shown in the ExtendedModel class above, we override the save method, allowing the callbacks.ModelCheckpoint to save the full model. Inference can then be performed as shown below.\nloaded_model = keras.saving.load_model(\n    \"/kaggle/working/model.02-0.0585.keras\"\n)\nx, y = next(iter(train_dataset))\noutput = loaded_model.predict(x)\n1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step\nUpdate\nIn order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the augment_data that is used in ExtendedModel use the following backend agnostic keras.ops functions.\ndef augment_data(images, masks):\n    combined = keras.ops.concatenate(\n        [images, keras.ops.cast(masks, 'float32')], axis=-1\n    )\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')\n    return images_augmented, masks_augmented\nAdditionally, to make the pipeline flexible for all backend, we can update the ExtendedModel as follows. Now, this code can run with tensorflow, jax, and torch backends.\nclass ExtendedModel(keras.Model):\n    ...\n\n    def train_step(self, *args, **kwargs):\n        if keras.backend.backend() == \"jax\":\n            return self._jax_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"tensorflow\":\n            return self._tensorflow_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"torch\":\n            return self._torch_train_step(*args, **kwargs)\n\n    def _jax_train_step(self, state, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step(state, (x, y))\n\n    def _tensorflow_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def _torch_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    ..."
        ],
        "link": "https://stackoverflow.com/questions/79327723/how-can-i-use-gpu-to-accelerate-image-augmentation"
    },
    {
        "title": "jax and flax not playing nicely with each other",
        "question": "I want to implement a neural network with multiple LSTM gates stacked one after the other.I set the hidden states to 0, as suggested here. When I try to run the code, I get\nJaxTransformError: Jax transforms and Flax models cannot be mixed. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.JaxTransformError)\nWhen I try to replace jax.lax.scan by flax.linen.scan, it gives another error. Not quite sure how to proceed or what's actually going wrong here. Code attached below. Thanks!\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom typing import Sequence\n\n\nclass LSTMModel(nn.Module):\nlstm_hidden_size: int\nnum_lstm_layers: int\nlinear_layer_sizes: Sequence[int]\nmean_aggregation: bool\n\ndef initialize_carry(self, batch_size, feature_size=1):\n    \"\"\"Initialize carry states with zeros for all LSTM layers.\"\"\"\n    return [\n        (\n            # Hidden state (h)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n            # Cell state (c)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n        )\n        for _ in range(self.num_lstm_layers)\n    ]\n\n@nn.compact\ndef __call__(self, x, carry=None):\n    if carry is None:\n        raise ValueError(\n            \"Carry must be initialized explicitly using `initialize_carry`.\"\n        )\n\n    # Expand 2D input to 3D (if necessary)\n    if x.ndim == 2:\n        # [batch_size, sequence_length] -> [batch_size, sequence_length, 1]\n        x = jnp.expand_dims(x, axis=-1)\n\n    # Process through LSTM layers\n    for i in range(self.num_lstm_layers):\n        lstm_cell = nn.LSTMCell(\n            features=self.lstm_hidden_size, name=f'lstm_cell_{i}')\n\n        def step_fn(carry, xt):\n            new_carry, yt = lstm_cell(carry, xt)\n            return new_carry, yt\n\n        # Use lax.scan to process the sequence\n        carry[i], outputs = jax.lax.scan(step_fn, carry[i], x)\n        x = outputs  # Update x for the next layer\n\n    # Aggregate outputs\n    if self.mean_aggregation:\n        x = jnp.mean(x, axis=1)  # Average over the sequence\n    else:\n        x = x[:, -1, :]  # Use the last output\n\n    # Pass through linear layers\n    for size in self.linear_layer_sizes:\n        x = nn.Dense(features=size)(x)\n        x = nn.elu(x)\n\n    # Final output layer\n    x = nn.Dense(features=1)(x)\n    return x\n\n\n# Model hyperparameters\nlstm_hidden_size = 64\nnum_lstm_layers = 2\nlinear_layer_sizes = [32, 16]\nmean_aggregation = False\n\n# Initialize model\nmodel = LSTMModel(\n    lstm_hidden_size=lstm_hidden_size,\n    num_lstm_layers=num_lstm_layers,\n    linear_layer_sizes=linear_layer_sizes,\n    mean_aggregation=mean_aggregation\n)\n\n# Dummy input: batch of sequences with 10 timesteps\nkey = jax.random.PRNGKey(0)\n# [batch_size, sequence_length, feature_size]\ndummy_input = jax.random.normal(key, (32, 10, 1))\n\n# Initialize carry states\ncarry = model.initialize_carry(\n    batch_size=dummy_input.shape[0], feature_size=dummy_input.shape[-1])\n\n# Initialize parameters\nparams = model.init(key, dummy_input, carry)\n\n# Apply the model\noutputs = model.apply(params, dummy_input, carry)\n\n# Should print: [batch_size, 1]\nprint(\"Model output shape:\", outputs.shape)",
        "answers": [
            "Consider using nn.RNN to simplify your code:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  name=f'lstm_cell_{i}'\n)\noutputs = lstm(x)\nRNN will handle the carries for you. If you really want to handle the carries yourself you could use return_carry and initial_carry:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  return_carry=True, \n  name=f'lstm_cell_{i}'\n)\ncarry[i], outputs = lstm(x, initial_carry=carry[i])"
        ],
        "link": "https://stackoverflow.com/questions/79266328/jax-and-flax-not-playing-nicely-with-each-other"
    },
    {
        "title": "Efficiently custom array creation routines in JAX",
        "question": "I'm still getting a handle of best practices in jax. My broad question is the following:\nWhat are best practices for the implementation of custom array creation routines in jax?\nFor instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):\nimport numpy as np\nimport jax.numpy as jnp\n\ndef ones_at_col(shape_mat, idx):\n    idxs = jnp.arange(shape_mat[1])[None,:]\n    mat = jnp.where(idx==idxs, 1, 0)\n    mat = jnp.repeat(mat, shape_mat[0], axis=0)\n    return mat\n\nshape_mat = (5,10)\n\nprint(ones_at_col(shape_mat, 5))\n\n%timeit np.zeros(shape_mat)\n\n%timeit jnp.zeros(shape_mat)\n\n%timeit ones_at_col(shape_mat, 5)\nThe output is\n[[0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]]\n127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nMy function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.\nBut then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?\nIs there something better I can do or am I pushing the limits of what is possible in jax?",
        "answers": [
            "The best way to do this is probably something like this:\nmat = jnp.zeros(shape_mat).at[:, 5].set(1)\nRegarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.\nIf you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):\n@partial(jax.jit, static_argnames=['shape_mat', 'idx'])\ndef ones_at_col(shape_mat, idx):\n  return jnp.zeros(shape_mat).at[:, idx].set(1)\nYou could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.\nAnother side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects."
        ],
        "link": "https://stackoverflow.com/questions/79256001/efficiently-custom-array-creation-routines-in-jax"
    },
    {
        "title": "How to handle PRNG splitting in a jax.vmap context?",
        "question": "I have a function which simulates a stochastic differential equation. Currently, without stochastic noise, my invokation of simulating the process up to time t looks like this (and, yeah, I need to use jax):\ndef evolve(u, t):\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\n\ndef simulate(x, t):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt), u)\nNow, the pain comes with the noise. I'm a C++-guy who only occasionally needs to use Python for research/scientific work. And I really don't understand how I need (or should) implement PRNG splitting here. I guess I would change evolve to\ndef evolve(u, t, key):\n    noise = jax.random.multivariate_normal(key, jax.numpy.zeros(d), covariance_matrix, shape = (n,))\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\nBut that will not work properly I guess. If I got it right, I need to use jax.random.split to split the key. Cause if I don't, I end up with correlated samples. But how and where do I need to split?\nAlso: I guess I would need to modify simulate to def simulate(x, t, key). But then, should simulate also return the modified key?\nAnd to make it even more complicated: I actually wrap simulate into a batch_simulate function which uses jax.vmap to process a whole batch of x's and t's. How do I pass the PRNG to that batch_simulate function, how do I pass it (and broadcast it) to jax.vmap and what should batch_forward return? At first glance, it seems to me that it would take a single PRNG and split it into many (due to the vmap). But what does the caller of batch_forward do then ...\nCompletely lost on this. Any help is highly appreciated!",
        "answers": [
            "If I understand your setup correctly, you should make both evolve and simulate accept a key, and within simulate, use fold_in to generate unique keys for the loop:\ndef evolve(u, t, key):\n    ...\n\ndef simulate(x, t, key):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt, jax.random.fold_in(key, i)), u)\nThen if you want to vmap over simulate, you can split the key and map over it:\nx_batch = ...  # your batched x inputs\nt_batch = ...  # your batched t inputs\nkey_batch = jax.random.split(key, x_batch.shape[0])\n\nbatch_result = jax.vmap(simulate)(x_batch, t_batch, key_batch)"
        ],
        "link": "https://stackoverflow.com/questions/79238188/how-to-handle-prng-splitting-in-a-jax-vmap-context"
    },
    {
        "title": "Hello World for jaxtyping?",
        "question": "I can't find any instructions or tutorials for getting started with jaxtyping. I tried the simplest possible program and it fails to parse. I'm on Python 3.11. I don't see anything on GitHub jaxtyping project about an upper bound (lower bound is Python 3.9) and it looks like it's actively maintained (last commit was 8 hours ago). What step am I missing?\njaxtyping==0.2.36\nnumpy==2.1.3\ntorch==2.5.1\ntypeguard==4.4.1\n(It seems like numpy is required for some reason even though I'm not using it)\nfrom typeguard import typechecked\nfrom jaxtyping import Float\nfrom torch import Tensor\n\n\n@typechecked\ndef matmul(a: Float[Tensor, \"m n\"], b: Float[Tensor, \"n p\"]) -> Float[Tensor, \"m p\"]:\n    \"\"\"\n    Matrix multiplication of two 2D arrays.\n    \"\"\"\n    raise NotImplementedError(\"This function is not implemented yet.\")\n(venv) dspyz@dspyz-desktop:~/helloworld$ python matmul.py \nTraceback (most recent call last):\n  File \"/home/dspyz/helloworld/matmul.py\", line 6, in <module>\n    @typechecked\n     ^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 221, in typechecked\n    retval = instrument(target)\n             ^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 72, in instrument\n    instrumentor.visit(module_ast)\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 598, in visit_Module\n    self.generic_visit(node)\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 498, in generic_visit\n    node = super().generic_visit(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 494, in generic_visit\n    value = self.visit(value)\n            ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 672, in visit_FunctionDef\n    with self._use_memo(node):\n  File \"/usr/lib/python3.11/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 556, in _use_memo\n    new_memo.return_annotation = self._convert_annotation(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 582, in _convert_annotation\n    new_annotation = cast(expr, AnnotationTransformer(self).visit(annotation))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in visit_Subscript\n    [self.visit(item) for item in node.slice.elts],\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in <listcomp>\n    [self.visit(item) for item in node.slice.elts],\n     ^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 474, in visit_Constant\n    expression = ast.parse(node.value, mode=\"eval\")\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n    return compile(source, filename, mode, flags,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<unknown>\", line 1\n    m p\n      ^\nSyntaxError: invalid syntax",
        "answers": [
            "(jaxtyping author here)\nSadly this is a known bug in typeguard v4. It's been around forever and hasn't been fixed. (At a technical level: typeguard v4 attempts to load and reparse the source code of your function, but it doesn't properly parse all type annotations.)\nI use typeguard==2.13.3 myself, which seems to be pretty robust.\nEDIT: removed some other suggested workarounds. These turned out not to, well, work. For now I just recommend pinning to that earlier version of typeguard.",
            "You are running into the issue reported here: https://github.com/patrick-kidger/jaxtyping/issues/80\nYou can work around this by installing typeguard version 3.0, but given how long this bug has remained open without any real fix, I suspect the best conclusion is that jaxtyping should no longer be considered compatible with typeguard."
        ],
        "link": "https://stackoverflow.com/questions/79201839/hello-world-for-jaxtyping"
    },
    {
        "title": "Why is JAX's jit compilation slower on the second run in my example?",
        "question": "I am new to using JAX, and I’m still getting familiar with how it works. From what I understand, when using Just-In-Time (JIT) compilation (jax.jit), the first execution of a function might be slower due to the compilation overhead, but subsequent executions should be faster. However, I am seeing the opposite behavior.\nIn the following code snippet:\nfrom icecream import ic\nimport jax\nfrom time import time\nimport numpy as np\n\n\n@jax.jit\ndef my_function(x, y):\n    return x @ y\n\n\nvectorized_function = jax.vmap(my_function, in_axes=(0, None))\n\nshape = (1_000_000, 1_000)\n\nx = np.ones(shape)\ny = np.ones(shape[1])\n\nstart = time()\nvectorized_function(x, y)\nt_1 = time() - start\n\nstart = time()\nvectorized_function(x, y)\nt_2 = time() - start\n\nprint(f'{t_1 = }\\n{t_2 = }')\nI get the following results:\nt_1 = 13.106784582138062\nt_2 = 15.664098024368286\nAs you can see, the second run (t_2) is actually slower than the first one (t_1), which seems counterintuitive to me. I expected the second run to be faster due to JAX’s JIT caching.\nHas anyone encountered a similar situation or have any insights into why this might be happening?\nPS: I know I could have done x @ y directly without invoking vmap, but this is an easy example just to test its behaviour. My actual code is more complex, and the difference in runtime is even bigger (around 8x slower). I hope this simple example works similar.",
        "answers": [
            "For general tips on running JAX microbenchmarks effectively, see FAQ: Benchmarking JAX code.\nI cannot reproduce the timings from your snippet, but in your more complicated case, I suspect you are getting fooled by JAX's Asynchronous dispatch, which means that the timing method you're using will not actually reflect the time taken by the underlying computation. To address this, you can wrap your results in jax.block_until_ready:\nstart = time()\nvectorized_function(x, y).block_until_ready()\nt_1 = time() - start"
        ],
        "link": "https://stackoverflow.com/questions/79192549/why-is-jaxs-jit-compilation-slower-on-the-second-run-in-my-example"
    },
    {
        "title": "Restoring flax model checkpoints using orbax throws ValueError",
        "question": "The following code blocks are being utlized to save the train state of the model during training and to restore the state back into memory.\nfrom flax.training import orbax_utils\nimport orbax.checkpoint\n\ndirectory_gen_path = \"checkpoints_loc\"\norbax_checkpointer_gen = orbax.checkpoint.PyTreeCheckpointer()\ngen_options = orbax.checkpoint.CheckpointManagerOptions(save_interval_steps=5, create=True)\ngen_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n    directory_gen_path, orbax_checkpointer_gen, gen_options\n)\n\ndef save_model_checkpoints(step_, generator_state, generator_batch_stats):\n\n    gen_ckpt = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    save_args_gen = orbax_utils.save_args_from_target(gen_ckpt)\n    gen_checkpoint_manager.save(step_, gen_ckpt, save_kwargs={\"save_args\": save_args_gen})\n\ndef load_model_checkpoints(generator_state, generator_batch_stats):\n    gen_target = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    latest_step = gen_checkpoint_manager.latest_step()\n    gen_ckpt = gen_checkpoint_manager.restore(latest_step, items=gen_target)\n    generator_state = gen_ckpt[\"model\"]\n    generator_batch_stats = gen_ckpt[\"batch_stats\"]\n\n    return generator_state, generator_batch_stats\nThe training of the model was done on a GPU and loading the state onto GPU device works fine, however, when trying to load the model to cpu, the following error is being thrown by the orbax checkpoint manager's restore method\nValueError: SingleDeviceSharding with Device=cuda:0 was not found in jax.local_devices().\nI'm not quite sure what could be the reason, any thoughts folks?\nUpdate: Updated to the latest version of orbax-checkpoint, 0.8.0 traceback changed to the following error\nValueError: sharding passed to deserialization should be specified, concrete and an instance of `jax.sharding.Sharding`. Got None",
        "answers": [
            "What version of orbax.checkpoint are you using?\nIt looks like this issue was fixed in https://github.com/google/orbax/issues/678 – you should update to the most recent version of orbax-checkpoint, and try running your code again. If that doesn't work, I'd suggest reporting the problem at https://github.com/google/orbax/issues/new"
        ],
        "link": "https://stackoverflow.com/questions/79162665/restoring-flax-model-checkpoints-using-orbax-throws-valueerror"
    },
    {
        "title": "Storing and jax.vmap() over Pytrees",
        "question": "I've ran into an issue with Jax that will make me rewrite an entire 20000-line application if I don't solve it.\nI have a non-ML application which relies on pytrees to store data, and the pytrees are deep - about 6-7 layers of data storage (class1 stores class2, and that stores an array of class3 etc.)\nI've used python lists to store pytrees and hoped to vmap over them, but turns out jax can't vmap over lists.\n(So one solution is to rewrite literally every single dataclass to be a structured array and work from there, possibly putting all 6-7 layers of data into one mega-array)\nIs there a way to avoid the rewrite? Is there a way to store pytree classes in a vmappable state so that everything works as before?\nI have my classes marked with flax.struct.dataclass if that helps.",
        "answers": [
            "jax.vmap is designed to work with a struct-of-arrays pattern, and it sounds like you have an array-of-structs pattern. From your description, it sounds like you have a sequence of nested structs that look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom flax.struct import dataclass\n\n@dataclass\nclass Params:\n  x: jax.Array\n  y: jax.Array\n\n\n@dataclass\nclass AllParams:\n  p: list[Params]\n\n\nparams_list = [AllParams([Params(4, 2), Params(4, 3)]),\n               AllParams([Params(3, 5), Params(2, 4)]),\n               AllParams([Params(3, 2), Params(6, 3)])]\nThen you have a function that you want to apply to each element of the list; something like this:\ndef some_func(params):\n  a, b = params.p\n  return a.x * b.y - b.x * a.y\n\n[some_func(params) for params in params_list]\n[4, 2, -3]\nBut as you found, if you try to do this with vmap, you get an error:\njax.vmap(some_func)(params_list)\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nThe issue is that vmap operates separately over each entry of the list or pytree you pass to it, not over the elements of the list.\nTo address this, you can often transform your data structure from an array-of-structs into a struct-of-arrays, and then apply vmap over this. For example:\nparams_array = jax.tree.map(lambda *vals: jnp.array(vals), *params_list)\nprint(params_array)\nAllParams(p=[\n  Params(x=Array([4, 3, 3], dtype=int32), y=Array([2, 5, 2], dtype=int32)),\n  Params(x=Array([4, 2, 6], dtype=int32), y=Array([3, 4, 3], dtype=int32))\n])\nNotice that rather than a list of structures, this is now a single structure with the batching pushed all the way down to the leaves. This is the \"struct-of-arrays\" pattern that vmap is designed to work with, and so vmap will work correctly:\njax.vmap(some_func)(params_array)\nArray([ 4,  2, -3], dtype=int32)\nNow, this assumes that every dataclass in your list has identical structure: if not, then vmap will not be applicable, because by design it must map over computations with identical structure."
        ],
        "link": "https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees"
    },
    {
        "title": "JIT: partial or with static argnums? Non hashable input, but hashable partial",
        "question": "I am a bit lost on what exactly going on and what option to choose. Let's go trough an example:\nimport jax\nfrom functools import partial\nfrom typing import List\n\ndef dummy(a: int, b: List[str]):\n    return a + 1\nAs b argument is mutable, jitting with static argnames will be failed:\nj_dummy = jax.jit(dummy, static_argnames=['b'])\nj_dummy(2, ['kek'])\nValueError: Non-hashable static arguments are not supported\nHowever, if we do partial: jp_dummy = jax.jit(partial(dummy, b=['kek'])), we aim the goal. Somehow, partial object is indeed has __hash__ method, so we can check it with hash(partial(dummy, b=['kek'])).\nSo, I am a bit lost here: how I should proceed in a bigger picture? Should I produce partial functions with whatever arguments and then jit them or should I try to maintain my arguments hashable? What are situations when one approach is better than other? Is there any drawbacks?",
        "answers": [
            "When you use static_argnames, the static values passed to the function become part of the cache key, so if the value changes the function is re-compiled:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, s):\n  return x * len(s)\n\nf_jit = jax.jit(f, static_argnames=['s'])\n\nprint(f_jit(2, \"abc\"))  # 6\nprint(f_jit(2, \"abcd\"))  # 8\nThis is why the static arguments must be hashable: their hash is used as the JIT cache key.\nOn the other hand, when you wrap a static argument via closure, its value does not affect the cache key, and so it need not be hashable. But since it's not part of the cache key, if the global value changes, it does not trigger a recompilation and so you may get unexpected results:\nf_closure = jax.jit(lambda x: f(x, s))\n\ns = \"abc\"\nprint(f_closure(2))  # 6\ns = \"abcd\"\nprint(f_closure(2))  # 6\nFor this reason, explicit static arguments can be safer. In your case, it may be best to change your list into a tuple, as tuples are hashable and can be used as explicit static arguments."
        ],
        "link": "https://stackoverflow.com/questions/79114391/jit-partial-or-with-static-argnums-non-hashable-input-but-hashable-partial"
    },
    {
        "title": "precision of JAX",
        "question": "I have a question regarding the precision of float in JAX. For the following code,\nimport numpy as np\nimport jax.numpy as jnp\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is:','%.60f' % np.arctan(10))\n\njnp.arctan(10) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\n\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is:','%.60f' % np.arctan(10+1e-7))\n\njnp.arctan(10+1e-7) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\njnp gave identical results for arctan(x) for a small change of input variable (1e-7), but np did not. My question is how to let jax.numpy get the right number for a small change of x?\nAny comments are appreciated.",
        "answers": [
            "JAX defaults to float32 computation, which has a relative precision of about 1E-7. This means that your two inputs are effectively identical:\n>>> np.float32(10) == np.float32(10 + 1E-7)\nTrue\nIf you want 64-bit precision like NumPy, you can enable it as discussed at JAX sharp bits: double precision, and then the results will match to 64-bit precision:\nimport jax\njax.config.update('jax_enable_x64', True)\n\nimport jax.numpy as jnp\nimport numpy as np\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is: ','%.60f' % np.arctan(10))\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is: ','%.60f' % np.arctan(10+1e-7))\njnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\nnp.arctan(10) is:  1.471127674303734700345103192375972867012023925781250000000000\njnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\nnp.arctan(10+1e-7) is:  1.471127675293833592107262120407540351152420043945312500000000\n(but please note that even the 64-bit precision used by Python and NumPy is only accurate to about one part in 10^16, so most of the digits in the representation you printed are inaccurate compared to the true arctan value)."
        ],
        "link": "https://stackoverflow.com/questions/79098013/precision-of-jax"
    },
    {
        "title": "jax register_pytree_node_class and register_dataclass returns non consistent datatype: list and tuple accordingly",
        "question": "I am writing custom class, which is basically a wrapper around list, with custom setitem method. I would like this class participate in jax.jit code, so during that I found a following problem: during jitting List field converted to tuple. However, this is case only when using\nregister_pytree_node_class When use register_dataclas , then List keep being list.\nI simplify example to highlight only this problem.\nimport jax\nfrom jax.tree_util import register_dataclass\nfrom jax.tree_util import register_pytree_node_class\nfrom functools import partial\nfrom dataclasses import dataclass\nfrom typing import List\n\n@partial(register_dataclass,\n         data_fields=['data'],\n         meta_fields=['shift'])\n@dataclass\nclass DecoratorFlatten:\n    data: List[int]\n    shift: int = 5\n\n@register_pytree_node_class\n@dataclass\nclass CustomFlatten:\n    data: List[int]\n    shift: int = 5\n\n    def tree_flatten(self):\n            children = self.data\n            aux_data = self.shift\n            return (children, aux_data)\n    \n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        obj = object.__new__(cls)\n        obj.data = children\n        setattr(obj, 'shift', aux_data)\n        return obj\nNow let's call a simple as this function over instances of this two class:\n@jax.jit\ndef get_value(a):\n    return a.data\ndf = DecoratorFlatten([0,1,2])\ncf = CustomFlatten([0,1,3])\nget_value(df), get_value(cf)\nIn first case we get list as output, but in second tuple. I thought maybe this is because of my implementation of the tree_flatten method, however:\ncf.tree_flatten()\nLeads to ([0, 1, 3], 5) as desirable.",
        "answers": [
            "In tree_unflatten, children is a tuple, and you are assigning this directly to obj.data. If you want it to be a list, you should use obj.data = list(children)."
        ],
        "link": "https://stackoverflow.com/questions/79093341/jax-register-pytree-node-class-and-register-dataclass-returns-non-consistent-dat"
    },
    {
        "title": "Batched matrix multiplication with JAX on GPU faster with larger matrices",
        "question": "I'm trying to perform batched matrix multiplication with JAX on GPU, and noticed that it is ~3x faster to multiply shapes (1000, 1000, 3, 35) @ (1000, 1000, 35, 1) than it is to multiply (1000, 1000, 3, 25) @ (1000, 1000, 25, 1) with f64 and ~5x with f32.\nWhat explains this difference, considering that on cpu neither JAX or NumPy show this behaviour, and on GPU CuPy doesn't show this behaviour?\nI'm running this with JAX: 0.4.32 on an NVIDIA RTX A5000 (and get similar results on a Tesla T4), code to reproduce:\nimport numpy as np\nimport cupy as cp\nfrom cupyx.profiler import benchmark\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng()\n\nx = np.arange(5, 55, 5)\nGPU timings:\ndtype = cp.float64\ntimings_cp = []\nfor i in range(5, 55, 5):\n    a = cp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n    b = cp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n    timings_cp.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ndtype = jnp.float64\ntimings_jax_gpu = []\nwith jax.default_device(jax.devices('gpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n        b = jnp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_gpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.gpu_times.mean() for i in timings_cp], label=\"CuPy\")\nplt.plot(x, [i.gpu_times.mean() for i in timings_jax_gpu], label=\"JAX GPU\")\nplt.legend()\nTimings with those specific shapes:\ndtype = jnp.float64\nwith jax.default_device(jax.devices('gpu')[0]):\n    a = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\n    func = jax.jit(lambda a, b: a@b)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\n\n    a = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\nGives\nf64:\n0.01453789699935913\n0.004859122595310211\n\nf32:\n\n0.005860503035545349\n0.001209742688536644\nCPU timings:\ntimings_np = []\nfor i in range(5, 55, 5):\n    a = rng.random((1000, 1000, 3, i))\n    b = rng.random((1000, 1000, i, 1))\n    timings_np.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ntimings_jax_cpu = []\nwith jax.default_device(jax.devices('cpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)))\n        b = jnp.array(rng.random((1000, 1000, i, 1)))\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_cpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.cpu_times.mean() for i in timings_np], label=\"NumPy\")\nplt.plot(x, [i.cpu_times.mean() for i in timings_jax_cpu], label=\"JAX CPU\")\nplt.legend()",
        "answers": [
            "The difference seems to come from the compiler emitting a kLoop fusion for smaller sizes, and a kInput fusion for larger sizes. You can read about the effect of these in this source comment: https://github.com/openxla/xla/blob/e6b6e61b29cc439350a6ad2f9d39535cb06011e5/xla/hlo/ir/hlo_instruction.h#L639-L656\nThe compiler likely uses some heuristic to choose between the two, and it appears that this heuristic is suboptimal at the boundary for your particular problem. You can see this by outputting the compiled HLO for your operation:\na = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\nHloModule jit__lambda_, is_scheduled=true, entry_computation_layout={(f64[1000,1000,3,25]{3,2,1,0}, f64[1000,1000,25,1]{3,2,1,0})->f64[1000,1000,3,1]{3,2,1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=\"a02cbfe0fda9d44e2bd23462363b6cc0\"}\n\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.7: f64[1000,1000,3,25], param_1.6: f64[1000,1000,25,1]) -> f64[1000,1000,3] {\n  %param_0.7 = f64[1000,1000,3,25]{3,2,1,0} parameter(0)\n  %param_1.6 = f64[1000,1000,25,1]{3,2,1,0} parameter(1)\n  %bitcast.28.5 = f64[1000,1000,25]{2,1,0} bitcast(f64[1000,1000,25,1]{3,2,1,0} %param_1.6)\n  %broadcast.2.5 = f64[1000,1000,3,25]{3,2,1,0} broadcast(f64[1000,1000,25]{2,1,0} %bitcast.28.5), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  %multiply.2.3 = f64[1000,1000,3,25]{3,2,1,0} multiply(f64[1000,1000,3,25]{3,2,1,0} %param_0.7, f64[1000,1000,3,25]{3,2,1,0} %broadcast.2.5)\n  %constant_4 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,25]{3,2,1,0} %multiply.2.3, f64[] %constant_4), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,25], Arg_1.2.0: f64[1000,1000,25,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,25,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,25]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %loop_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,25]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,25,1]{3,2,1,0} %Arg_1.2.0), kind=kLoop, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %loop_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\na = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.5: f64[1000,1000,3,35], param_1.2: f64[1000,1000,35,1]) -> f64[1000,1000,3] {\n  %param_0.5 = f64[1000,1000,3,35]{3,2,1,0} parameter(0)\n  %param_1.2 = f64[1000,1000,35,1]{3,2,1,0} parameter(1)\n  %bitcast.28.3 = f64[1000,1000,35]{2,1,0} bitcast(f64[1000,1000,35,1]{3,2,1,0} %param_1.2)\n  %broadcast.2.3 = f64[1000,1000,3,35]{3,2,1,0} broadcast(f64[1000,1000,35]{2,1,0} %bitcast.28.3), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  %multiply.2.1 = f64[1000,1000,3,35]{3,2,1,0} multiply(f64[1000,1000,3,35]{3,2,1,0} %param_0.5, f64[1000,1000,3,35]{3,2,1,0} %broadcast.2.3)\n  %constant_3 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,35]{3,2,1,0} %multiply.2.1, f64[] %constant_3), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,35], Arg_1.2.0: f64[1000,1000,35,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,35,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,35]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %input_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,35]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,35,1]{3,2,1,0} %Arg_1.2.0), kind=kInput, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %input_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\nHere's a script to observe this compiler decision with respect to size:\nfor size in range(10, 55, 5):\n  a = jnp.array(rng.random((1000, 1000, 3, size)), dtype=dtype)\n  b = jnp.array(rng.random((1000, 1000, size, 1)), dtype=dtype)\n  hlo_text = jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text()\n  print(f\"{size=} {'kLoop' in hlo_text=}\")\nsize=10 'kLoop' in hlo_text=True\nsize=15 'kLoop' in hlo_text=True\nsize=20 'kLoop' in hlo_text=True\nsize=25 'kLoop' in hlo_text=True\nsize=30 'kLoop' in hlo_text=True\nsize=35 'kLoop' in hlo_text=False\nsize=40 'kLoop' in hlo_text=False\nsize=45 'kLoop' in hlo_text=False\nsize=50 'kLoop' in hlo_text=False\nI don't have any suggestion beyond perhaps reporting this at https://github.com/openxla/xla; it may be that the compiler heuristic for choosing to emit kLoop vs. kInput needs some additional logic."
        ],
        "link": "https://stackoverflow.com/questions/79085795/batched-matrix-multiplication-with-jax-on-gpu-faster-with-larger-matrices"
    },
    {
        "title": "Computing gradient using JAX of a function that outputs a list of arrays",
        "question": "I have a function which returns a list of arrays, and I need to find its derivative with respect to a single parameter. For instance, let's say we have\ndef fun(x):\n...\nreturn [a,b,c]\nwhere a,b,c and d are multi-dimensional arrays (for example, 2 by 2 by 2 real arrays). Now I want to obtain [da/dx, db/dx, dc/dx]. By db/dx I mean I want to obtain derivative of each element in the a:222 array with respect to x, so da/dx, db/dx, dc/dx are all 222 arrays.\nThis is me using JAX differentiation for the first time, and most of the examples I find online are about functions that has scalar output.\nFrom my search, I understand one way to find this is basically get the gradient of each scalar in all these arrays one at a time (probably making it faster using vmap). Is there any other way that is faster? I think JAX.jacobian might do the trick, but I am having hard time finding its documentation to see what does the function does exactly. Any help is very much appreciated.\nNow, I have tried JAX.jacobian with simple examples, and it does give me the answer that I expect. This assures me a bit, but I would like to find official documentation or assurance from others that is the right way to do it, and it is doing what I expect it.",
        "answers": [
            "You can use jax.jacobian for what you describe. Here is an example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  a = jnp.full((2, 2), 2) * x\n  b = jnp.full((2, 2), 3) * x\n  c = jnp.full((2, 2), 4) * x\n  return [a, b, c]\n\nda_dx, db_dx, dc_dx = jax.jacobian(f)(1.0)\n\nprint(da_dx)\n# [[2. 2.]\n#  [2. 2.]]\n\nprint(db_dx)\n# [[3. 3.]\n#  [3. 3.]]\n\nprint(dc_dx)\n# [[4. 4.]\n#  [4. 4.]]\njax.jacobian is an alias of jax.jacrev, and you can find the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.jacrev.html"
        ],
        "link": "https://stackoverflow.com/questions/79025241/computing-gradient-using-jax-of-a-function-that-outputs-a-list-of-arrays"
    },
    {
        "title": "Modifying multiple dimensions of Jax array simultaneously",
        "question": "When using the jax_array.at[idx] function, I wish to be able to set values at both a set of specified rows and columns within the jax_array to another jax_array containing values in the same shape. For example, given a 5x5 jax array, I might want to set the values, jax_array.at[[0,3],:][:,[1,2]] to some 2x2 array of values. However, I am coming across an issue where the _IndexUpdateRef' object is not subscriptable. I understand the idea of the error (and I get a similar one when using 2 chained .at[]s), but I want to know if there is anyway to achieve the desired functionality within 1 line.",
        "answers": [
            "JAX follows the indexing semantics of NumPy, and NumPy's indexing semantics allow you to do this via broadcasted arrays of indices (this is discussed in Integer array indexing in the NumPy docs).\nSo for example, you could do something like this:\nimport jax.numpy as jnp\n\nx = jnp.zeros((4, 6), dtype=int)\ny = jnp.array([[1, 2],\n               [3, 4]])\ni = jnp.array([0, 3])\nj = jnp.array([1, 2])\n\n# reshape indices so they broadcast \ni = i[:, jnp.newaxis]\nj = j[jnp.newaxis, :]\n\nx = x.at[i, j].set(y)\nprint(x)\n[[0 1 2 0 0 0]\n [0 0 0 0 0 0]\n [0 0 0 0 0 0]\n [0 3 4 0 0 0]]\nHere the i index has shape (2, 1), and the j index has shape (1, 2), and via broadcasting rules they index a 2x2 noncontiguous subgrid of the array x, which you can then set to the contents of y in a single statement."
        ],
        "link": "https://stackoverflow.com/questions/78985089/modifying-multiple-dimensions-of-jax-array-simultaneously"
    },
    {
        "title": "Mapping Over Arrays of Functions in JAX",
        "question": "What is the most performant, idiomatic way of mapping over arrays of functions in JAX?\nContext: This GitHub issue shows a way to apply vmap to several functions using lax.switch. The example is reproduced below:\nfrom jax import lax, vmap\nimport jax.numpy as jnp\n\ndef func1(x):\n  return 2 * x\n\ndef func2(x):\n  return -2 * x\n\ndef func3(x):\n  return 0 * x\n\nfunctions = [func1, func2, func3]\nindex = jnp.arange(len(functions))\nx = jnp.ones((3, 5))\n\nvmap_functions = vmap(lambda i, x: lax.switch(i, functions, x))\nvmap_functions(index, x)\n# DeviceArray([[ 2.,  2.,  2.,  2.,  2.],\n#              [-2., -2., -2., -2., -2.],\n#              [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)\nMy specific questions are:\nIs this (currently) the most idiomatic way of mapping over arrays of functions in JAX?\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)",
        "answers": [
            "For the kind of operation you're doing, where the functions are applied over full axes of an array in a way that's known statically, you'll probably get the best performance via a simple Python loop:\ndef map_functions(functions: list[Callable[[Array], Array], x: Array) -> Array:\n  assert len(functions) == x.shape[0]\n  return jnp.array([f(row) for f, row in zip(functions, x)])\nThe method based on switch is designed for the more general case where the structure of the indices is not known statically.\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)\nvmap of switch is implemented via select, which will compute the output of each function for the full input array before selecting just the pieces needed to construct the output, so if the functions are expensive to compute, it may lead to longer runtimes."
        ],
        "link": "https://stackoverflow.com/questions/78980521/mapping-over-arrays-of-functions-in-jax"
    },
    {
        "title": "JAX TypeError: 'Device' object is not callable",
        "question": "I found a piece of JAX codes from few years ago.\nimport jax\nimport jax.random as rand\n\ndevice_cpu = None\n\ndef do_on_cpu(f):\n    global device_cpu\n    if device_cpu is None:\n        device_cpu = jax.devices('cpu')[0]\n\n    def inner(*args, **kwargs):\n        with jax.default_device(device_cpu):\n            return f(*args, **kwargs)\n    return inner\n\nseed2key = do_on_cpu(rand.PRNGKey)\nseed2key.__doc__ = '''Same as `jax.random.PRNGKey`, but always produces the result on CPU.'''\nand I call it with:\nkey = seed2key(42)\nBut it results in TypeError:\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 14\n---> 14 key = seed2key(42)\n\nFile ~/bert-tokenizer-cantonese/lib/seed2key.py:12, in do_on_cpu.<locals>.inner(*args, **kwargs)\n     11 def inner(*args, **kwargs):\n---> 12     with jax.default_device(device_cpu):\n     13         return f(*args, **kwargs)\n\nTypeError: 'Device' object is not callable\nI think the function has breaking changes after version upgrade.\nCurrent versions:\njax 0.4.31\njaxlib 0.4.31\n(latest version at the moment of writing)\nHow can I change the codes to avoid the error? Thanks.",
        "answers": [
            "This code works fine in all recent versions of JAX: jax.default_device is a configuration function designed to be used as a context manager.\nI can reproduce the error you're seeing if I add this to the top of your script:\njax.default_device = jax.devices('cpu')[0]  # wrong!\nI suspect you inadvertently executed something similar to this at some point earlier in your notebook session. Try restarting your notebook runtime and rerunning just your valid code."
        ],
        "link": "https://stackoverflow.com/questions/78951225/jax-typeerror-device-object-is-not-callable"
    },
    {
        "title": "Using Jax Jit on a method as decorator versus applying jit function directly",
        "question": "I guess most people familiar with jax have seen this example in the documentation and know that it does not work:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit  # <---- How to do this correctly?\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\nc.calc(3)  \n3 workarounds are mentioned, but it appears that applying jit as a function directly, rather than a decorator works fine as well. That is, JAX does not complain about not knowing how to deal with the CustomClass type of self:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  # No decorator here !\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\n6 # works fine!\nAlthough not documented (which it maybe should be?), this appears to function identical to marking self as static via @partial(jax.jit, static_argnums=0), in that changing self does nothing for subsequent calls, i.e.:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\n6\n6 # no update\nSo I originally assumed that decorators in general might just deal with self as a static parameter when applying them directly. Because the method might be saved to another variable with a specific instance (copy) of self. As a sanity check, I checked if non-jit decorators indeed do this as well, but this appears not to be the case, as the below non-jit \"decorated\" function happily deals with changes to self:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\n\ncustom = CustomClass(2, True)\ndecorated_calc = decorator(custom.calc)\nprint(decorated_calc(3))\ncustom.mul = False\nprint(decorated_calc(3))\n6\n3\nI saw some other questions about applying decorators directly as functions versus decorator style (e.g. here and here), and there it is mentioned there is a slight difference in the two versions, but this should almost never matter. I am left wondering what it is about the jit decorator that makes these versions behave so differently, in that JAX.jit cán deal with the self type if not in decorated style. If anyone has an answer, that would be much appreciated.",
        "answers": [
            "Decorators have nothing to do with static arguments: static arguments are a concept specific to jax.jit.\nBacking up, you should keep in mind that whenever jax.jit compiles a function, it caches the compilation artifact based on several quantites, including:\nthe ID of the function or callable being compiled\nthe static attributes of any non-static arguments, such as shape and dtype\nthe hash of any arguments marked static via static_argnums or static_argnames\nthe value of any global configurations that would affect outputs\nWith this in mind, let's examine this snippet:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\nthe reason that jitted_calc doesn't update when you update attributes of c is because nothing related to the cache key has changed: (1) the function ID is the same, (2) the shape and dtype of the argument is unchanged, (3) there are no static arguments, (4) no global configurations have changed. Thus the previous cached compilation artifact (with the previous value of mul) is executed again. This is the primary reason I didn't mention this strategy in the doc you linked to: it's rarely the behavior that users would want.\nThis approach of wrapping the bound method in JIT is incidentally similar to wrapping the method definition with @partial(jit, static_argnums=0), but the details are not the same: in the static_argnums version, self is marked as a static argument, and so its hash becomes part of the JIT cache. The default __hash__ method for a class is simply based on the ID of the instance, and so changing c.mul does not change the hash, and does not trigger re-compilation. You can see an example of how to rectify this under Strategy 2 in the doc you linked to: basically, define appropriate __hash__ and __eq__ methods for the class:\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def __hash__(self):\n    return hash((self.x, self.mul))\n\n  def __eq__(self, other):\n    return (isinstance(other, CustomClass) and\n            (self.x, self.mul) == (other.x, other.mul))\nIn your last example, you define this:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\nThis code does not use jax.jit at all. The fact that changes to c.mul lead to changes in outputs has nothing to do with decorator syntax, but rather has to do with the fact that there is no JIT cache in play here.\nI hope that's all clear!"
        ],
        "link": "https://stackoverflow.com/questions/78918066/using-jax-jit-on-a-method-as-decorator-versus-applying-jit-function-directly"
    },
    {
        "title": "Zero length error of non-zero length array",
        "question": "I'm writing environment for rl agent training.\nMy env.step method takes as action array with length 3\n    def scan(self, f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n\n    def step_env(\n        self,\n        key: chex.PRNGKey,\n        state: EnvState,\n        action: Union[int, float, chex.Array],\n        params: EnvParams,\n    ) -> Tuple[chex.Array, EnvState, jnp.ndarray, jnp.ndarray, Dict[Any, Any]]:\n        \n        c_action = jnp.clip(action,\n                          params.min_action, \n                          params.max_action)\n        \n        _, m1 = self.scan(self.Rx, 0, action[0])\n        _, m2 = self.scan(self.Rx, 0, action[1])\n        _, m3 = self.scan(self.Rx, 0, action[2])\nI vectorize the env.step using and then call it\nobsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(rng_step,\n                                                                                          env_state,\n                                                                                          action,\n                                                                                          env_params)\nBut I got error\nCell In[9], line 65, in PCJ1_0.scan(self, f, init, xs, length)\n     63 ys = []\n     64 print(xs)\n---> 65 for x in xs:\n     66     carry, y = f(carry, x)\n     67     ys.append(y)\n\n    [... skipping hidden 1 frame]\n\nFile ~/anaconda3/envs/jax/lib/python3.10/site-packages/jax/_src/lax/lax.py:1592, in _iter(tracer)\n   1590 def _iter(tracer):\n   1591   if tracer.ndim == 0:\n-> 1592     raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n   1593   else:\n   1594     n = int(tracer.shape[0])\n\nTypeError: iteration over a 0-d array\nHow is it possible? If I plot the action array in the scan function I got array with length 5 (I vectored env.step for 5 envs), the length!=0\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0",
        "answers": [
            "When you print your value, it gives this:\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0\nHere float32[] tells you that this is a tracer with dtype float32 and shape []: that is, your array is zero-dimensional within the context of the vmapped function.\nThe purpose of vmap is to efficiently map a function over an axis of an array, so that within the function evaluation the array has one less dimension than it does outside the vmapped context. You can see that this way:\n>>> import jax\n\n>>> def f(x):\n...  print(f\"{x.shape=}\")\n...  print(f\"{x=}\")\n...\n>>> x = jax.numpy.arange(4.0)\n\n>>> f(x)\nx.shape=(4,)\nx=Array([0., 1., 2., 3.], dtype=float32)\n\n>>> jax.vmap(f)(x)\nx.shape=()\nx=Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0., 1., 2., 3.], dtype=float32)\n  batch_dim = 0\nIf you're passing a 1D input into your function and you want to manipulate the full 1D array within your function (instead of evaluating the function element-by-element), then it sounds like you should remove the vmap."
        ],
        "link": "https://stackoverflow.com/questions/78838517/zero-length-error-of-non-zero-length-array"
    },
    {
        "title": "Jax jitting of kd-tree code taking an intractably long amount of time",
        "question": "I've written myself into a corner with the following situation:\nI'm running an optimiser which requires smooth gradients to work, and I'm using Jax for automatic differentiation. Since this code is Jax jitted, this means that anything connected to it has to be Jax jit traceable.\nI need to interpolate a function to use with the optimiser, but can't use the Scipy library as it isn't compatable with Jax (there's a jax.scipy.interpolate.RegularGridInterpolator implementation, but this isn't smooth - it only supports linear and nearest neighbour interpolation).\nThis means that I'm having to write my own Jax-compatible smooth interpolator, which I'm basing off the Scipy RBFInterpolator code. The implementation of this is very nice - it uses a kd-tree to find the nearest neighbours of a queried point in space, and then uses these to construct a local interpolation. This means that I also need to write a Jax-compatable kd-tree class (the Scipy one also isn't compatible with Jax), which I've done.\nThe problem comes with jit-compiling the kd-tree code. I've written it in the 'standard way', using objects for the tree nodes with left and right node fields for the children. At the leaf nodes, these fields have None values to signify the absense of children.\nThe code runs and is functionally correct, however jit-compiling it takes a long time: 72 seconds for a tree of 64 coordinates, 131 seconds for 343 coordinates, ... and my intended dataset has over 14 million points. I think internally Jax is tracing every single possible path through the tree, which is why it's taking so long. The results are that it's blazingly quick: 0.0075s for kd-tree 10-point retrieval vs 0.4s for a brute force search over all of the points (for 343 points). These are the kind of speeds I'm hoping to obtain for use in the optimiser (without jitting it will be too slow). However it doesn't seem possible if the compilation times are going to continue to grow as experienced.\nI thought that the problem might lie in the structure of the tree, with lots of different objects to be stored, so have also implemented a kd-tree search algorithm where the tree is represented by a set of Jax-numpy arrays (e.g. coord, value, left and right; where each index corresponds to a point in the tree) and iteration rather than recursion is used to do the tree search (this was a challenge but it works!). However, converting this to work with jit (changing if-statements for jax.lax.cond) is going to be complicated, and before I start I was wondering if it's going to be worth it - surely I'll have the same problem: Jax will trace all branches of the tree until the 'null terminators' (-1 values in the left and right arrays) are reached, and it will still take a very long time to compile. I've been investigating structures like jax.lax.while_loop, in case they might help?\n(I've also written a hybrid of the two approaches, with an array-based tree and a recursion-based algorithm. In this case the tracing goes into an infinite loop, I think because of the fact that the null-terminator is -1 rather than None. But the arrays should be known statically (they don't change after construction, and belong to an object which is marked as a static input), so maybe the solution lies in this and I'm doing something wrong.)\nI was wondering if I'm doing anything which is obviously wrong (or if my understanding is wrong), and if there is anything I can do to speed it up? Is it just to be expected that the compile time would be so high when there are so many code paths to trace? I don't suppose I could even build the jitted function only once and then save it?\nI'm concerned that the only solution may be to rewrite the optimiser code so that it doesn't use Jax (e.g. if I hard-code the derivatives, and rewrite some of the code so that it operates on arrays directly instead of being vectorised across the inputs).\nThe code is available here: https://github.com/FluffyCodeMonster/jax_kd_tree\nAll three varieties described are given: the node-based tree with recursion, the array-based tree with iteration, and the array-based tree with recursion. The former works, but is very slow to jit compile as the number of points in the tree increases; the second also works, but is not written in a jit-able way yet. The last is written to be jitted, but can't jit compile as it gets into an infinite recursion.\nI really need to get this working urgently so that I can obtain the optimisation results.",
        "answers": [
            "All python-level control flow, including if statements, for and while loops, and recursion, is traced in full and flattened into a linear set of commands that is then sent to the compiler. If you are attempting a tree traversal via Python-level control flow, you're going to end up with very large programs that take a very long time to compile. This issue is discussed broadly at JAX sharp bits: control flow.\nIf you want to traverse a KD tree under JIT without the long compilation, you'll have to use an iterative approach with XLA control-flow operators such as jax.lax.fori_loop and jax.lax.while_loop.\nAlternatively, you might think about instead using jax.pure_callback in order to run neighbors queries using scipy on the host. There is some discussion of this at Exploring pure_callback. It's not super efficient—each call will incur some host synchronization and data movement overhead—but it can be a pretty effective solution for things like this, particularly if you're running on CPU."
        ],
        "link": "https://stackoverflow.com/questions/78791013/jax-jitting-of-kd-tree-code-taking-an-intractably-long-amount-of-time"
    },
    {
        "title": "Execution of conditional branches causing errors in Jax (kd-tree implementation)",
        "question": "I'm writing a kd-tree in Jax, and using custom written Node objects for the tree elements. Each Node is very simple, with a single data field (for holding numeric values) and left and right fields which are references to other Nodes. A leaf Node is identified as one for which the left and right fields are None.\nThe code performs conditional checks on the values of left and right as part of the tree traversal process - e.g. it will only try to traverse down the left or right branch of a node's subtree if it actually exists. Doing checks like if (current_node.left is not None) (or does it have to be jax.numpy.logical_not(current_node.left is None) in Jax - I've tried both?) was fine for this, but since converting the if statements to jax.lax.cond(...) I've been getting the error AttributeError: 'NoneType' object has no attribute 'left'.\nI think the situation might be like in the following minimum working example:\nimport jax\nimport jax.numpy as jnp\n\ndef my_func(val):\n    return 2*val\n\n@jax.jit\ndef test_fn(a):\n    return jax.lax.cond(a is not None,\n                lambda: my_func(a),\n                lambda: 0)\n\nprint(test_fn(2))       # Prints 4\n# in test_fn(), a has type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\nprint(test_fn(None))    # TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n# in test_fn(), a has type <class 'NoneType'>\nIn this code, if the Jax cond statement were a regular if statement, my_func() wouldn't even be called when a is None, and no error would be raised. To the best of my understanding, Jax tries to trace the function, meaning that all branches are executed, and this leads to my_func() being called with None (when a is None), causing the error. I believe a similar situation is arising in my tree code, where conditional branches are being executed even though .left and /or .right are None, and a traditional if statement wouldn't lead to execution of the code branches.\nIs my understanding correct, and what could I do about this issue? Strangely, the minimum working example code also has the problem when the @jax.jit decorator is omitted, suggesting that both branches are still being traced.\nAs a related point, is the tree structure 'baked into' the Jax/XLA code? I have noticed that when using larger trees the code takes longer to be jit-compiled, which makes me concerned that this might not be a valid approach with the very large number of points I need to represent (about 14,000,000). I would use the regular Scipy kd-tree implementation, but this isn't compatible with Jax unfortunately, and the rest of my code requires it. I might ask this as a separate question for clarity.",
        "answers": [
            "If you are using jax.lax.cond, the input must have a valid type for both branches. When a is None, the first branch is invalid because None * 2 results in an error.\nIn this case, the condition a is not None is known statically, so rather than using lax.cond you can use a regular if statement:\n@jax.jit\ndef test_fn(a):\n  return my_func(a) if a is not None else 0"
        ],
        "link": "https://stackoverflow.com/questions/78784486/execution-of-conditional-branches-causing-errors-in-jax-kd-tree-implementation"
    },
    {
        "title": "JAX/Equinox pipeline slows down after adding an integer argument to a loss function",
        "question": "I have the following training pipeline in JAX and Equinox. I want to pass a batch index to the loss function in order to apply different logic depending on index. Without batch index training loop works for about 15 sec, but if I pass an index, then it slows down for about an hour. Could you explain, why this happens? I'm new to JAX, sorry.\ndef fit_cv(model: eqx.Module, \n           dataloader: jdl.DataLoader, \n           optimizer: optax.GradientTransformation, \n           loss: tp.Callable, \n           n_steps: int = 1000):\n    \n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    dloss = eqx.filter_jit(eqx.filter_value_and_grad(loss))\n    \n    @eqx.filter_jit\n    def step(model, data, opt_state, batch_index):\n        loss_score, grads = dloss(model, data, batch_index)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state, loss_score\n    \n    loss_history = []\n    for batch_index, batch in tqdm(zip(range(n_steps), dataloader), total=n_steps):\n        if batch_index >= n_steps:\n            break\n        batch = batch[0] # dataloader returns tuple of size (1,)\n        model, opt_state, loss_score = step(model, batch, opt_state, batch_index)\n        loss_history.append(loss_score)\n    return model, loss_history\nLoss function has the following signature\ndef loss(self, model: eqx.Module, data: jnp.ndarray, batch_index: int):\nIn particular, I want to switch between two loss functions after N steps. So, probably, I need to know the concrete value of a batch index.\nSolution:\nTo use jax.lax.cond\n        condition = (batch_index // self.switch_steps) % 2 == 1\n        ...\n        loss_value = jax.lax.cond(\n            jnp.all(condition),\n            lambda: loss1(inputs),\n            lambda: loss2(inputs),\n        )\n        return loss_value",
        "answers": [
            "I suspect the issue is excessive recompilation. You are using filter_jit, which according to the docs has the following property:\nAll JAX and NumPy arrays are traced, and all other types are held static.\nEach time a static argument to a JIT-compiled function changes, it triggers a re-compilation. This means that if batch_index is a Python int, then each time you call your function with a new value, the function will be recompiled.\nAs a fix, I would recommend using regular old jax.jit, which requires you to explicitly specify static arguments, instead of the function trying to make the choice for you (potential surprises like this are one of the reasons why JAX has made this design choice - as the Zen of Python says, explicit is better than implicit). If you use jax.jit and don't mark batch_index as static, you shouldn't see this recompilation penalty.\nAlternatively, if you want to keep using filter_jit, then you could change your step call to this:\nstep(model, batch, opt_state, jnp.asarray(batch_index))\nWith this change, filter_jit will no longer decide to make the batch index static. Of course, either of these suggestions would require that that loss is compatible with dynamic batch_index, which can't be determined from the information included in your question."
        ],
        "link": "https://stackoverflow.com/questions/78775635/jax-equinox-pipeline-slows-down-after-adding-an-integer-argument-to-a-loss-funct"
    },
    {
        "title": "Dictionary indexing with Numpy/Jax",
        "question": "I'm writing an interpolation routine and have a dictionary which stores the function values at the fitting points. Ideally, the dictionary keys would be 2D Numpy arrays of the fitting point coordinates, np.array([x, y]), but since Numpy arrays aren't hashable these are converted to tuples for the keys.\n# fit_pt_coords: (n_pts, n_dims) array\n# fn_vals: (n_pts,) array\ndef fit(fit_pt_coords, fn_vals):\n    pt_map = {tuple(k): v for k, v in zip(fit_pt_coords, fn_vals)}\n    ...\nLater in the code I need to get the function values using coordinates as keys in order to do the interpolation fitting. I'd like this to be within @jax.jited code, but the coordinate values are of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>, which can't be converted to a tuple. I've tried other things, like creating a dictionary key as (x + y, x - y), but again this requires concrete values, and calling .item() results in an ConcretizationTypeError.\nAt the moment I've @jax.jited all of the code I can, and have just left this code un-jitted. It would be great if I could jit this code as well however. Are there any better ways to do the dictionary indexing (or better Jax-compatible data structures) which would allow all of the code to be jitted? I am new to Jax and still understading how it works, so I'm sure there must be better ways of doing it...",
        "answers": [
            "There is no way to use traced JAX values as dictionary keys. The problem is that the key values will not be known until runtime within the XLA compiler, and XLA has no dictionary-like data structure that such lookups can be lowered to.\nThere are imperfect solutions, such as keeping the dictionary on the host and using something like io_callback to do the dict lookups on host, but this approach comes with performance penalties that will likely make it impractical.\nUnfortunately, your best approach for doing this efficiently under JIT would probably be to switch to a different interpolation algorithm that doesn't depend on hash table lookups.",
            "I agree with @jakevdp that this might not be the best solution. Python is not the quickest when built-ins are looped over.\nPython can do anything... Except for-loops. We use numpy for that.\nMaybe a pandas.DataFrame with columns [\"x\", \"y\", \"v\"] would be a way to go.\nCan you not use scipy.interpolate's functions?"
        ],
        "link": "https://stackoverflow.com/questions/78767142/dictionary-indexing-with-numpy-jax"
    },
    {
        "title": "Taking derivatives with multiple inputs in JAX",
        "question": "I am trying to take first and second derivatives of functions in JAX however, my ways of doing that give me the wrong number or zeros. I have an array with two columns for each variable and two rows for each input\nimport jax.numpy as jnp\nimport jax\n\nrng = rng = jax.random.PRNGKey(1234)\narray = jax.random.normal(rng, (2,2))\nTwo test functions\ndef F1(arr):\n    return 1/arr\n\ndef F2(arr):\n    return jnp.array([arr[0]**2 + arr[1]**3])\nand two methods of taking first and second derivatives, one using jax.grad()\ndef dF_m1(arr, F):\n    return jax.grad(lambda arr: F(arr)[0])(arr)\n\ndef ddF_m1(arr, F, dF):\n    return jax.grad(lambda arr: dF(arr, F)[0])(arr)\nand another using jax.jacobian()\ndef dF_m2(arr, F):\n    jac = jax.jacobian(lambda arr: F(arr))(arr)\n    return jnp.diag(jac)\n\ndef ddF_m2(arr, F, dF):\n    hess = jax.jacobian(lambda arr: dF(arr, F))(arr)\n    return jnp.diag(hess)\nComputing the first and second derivative (and error) of each function using both methods gives the following\nexact_dF1  = (-1/array**2)\nexact_ddF1 = (2/array**3)\n\nprint(\"Function 1 using all grad()\")\ndF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)\nddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)\nprint(dF1_m1  - exact_dF1,\"\\n\")\nprint(ddF1_m1 - exact_ddF1,\"\\n\")\n\nprint(\"Function 1 using all jacobian()\")\ndF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)\nddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)\nprint(dF1_m2  - exact_dF1,\"\\n\")\nprint(ddF1_m2 - exact_ddF1,\"\\n\")\nOutput\nFunction 1 using all grad()\n[[ 0.         48.43877   ]\n [ 0.          0.62903005]] \n\n[[  0.        674.248    ]\n [  0.          0.9977852]] \n\nFunction 1 using all jacobian()\n[[0. 0.]\n [0. 0.]] \n\n[[0. 0.]\n [0. 0.]] \nand\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\nprint(\"Function 2 using all grad()\")\ndF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)\nddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)\nprint(dF2_m1  - exact_dF2,\"\\n\")\nprint(ddF2_m1 - exact_ddF2,\"\\n\")\n\nprint(\"Function 2 using all jacobian()\")\ndF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)\nddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)\nprint(dF2_m2  - exact_dF2,\"\\n\")\nprint(ddF2_m2 - exact_ddF2,\"\\n\")\nOutput\nFunction 2 using all grad()\n[[0. 0.]\n [0. 0.]] \n\n[[0.         0.86209416]\n [0.         7.5651155 ]] \n\nFunction 2 using all jacobian()\n[[ 0.         -0.10149619]\n [ 0.         -6.925739  ]] \n\n[[0.        2.8620942]\n [0.        9.565115 ]] \nI would prefer only to use jax.grad() for something like F1 but it seems right now that only jax.jacobian is working. The whole reason for this is that I need to calculate higher-order derivatives of a neural network with respect to its inputs. Thank you for any help.",
        "answers": [
            "Assuming exact_* is what you're attempting to compute, you're going about it in the wrong way. Your indexing within the differentiated functions (i.e. ...[0]) is removing some of the elements that you're trying to compute.\nWhat exact_dF1 and exact_ddF1 are computing is element-wise first and second derivatives for 2D inputs. You can compute this using either grad or jacobian by applying vmap twice (once for each input dimension). For example:\nexact_dF1  = (-1/array**2)\ngrad_dF1 = jax.vmap(jax.vmap(jax.grad(F1)))(array)\njac_dF1 = jax.vmap(jax.vmap(jax.jacobian(F1)))(array)\nprint(jnp.allclose(exact_dF1, grad_dF1))  # True\nprint(jnp.allclose(exact_dF1, jac_dF1))  # True\n\nexact_ddF1 = (2/array**3)\ngrad_ddF1 = jax.vmap(jax.vmap(jax.grad(jax.grad(F1))))(array)\njac_ddF1 = jax.vmap(jax.vmap(jax.jacobian(jax.jacobian(F1))))(array)\nprint(jnp.allclose(exact_ddF1, grad_ddF1))  # True\nprint(jnp.allclose(exact_ddF1, jac_ddF1))  # True\nWhat exact_dF2 and exact_ddF2 are computing is a row-wise jacobian and hessian of a 2D->1D mapping. By its nature, this is difficult to compute using jax.grad, which is meant for functions with scalar output, but you can compute it using the jacobian this way:\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\njac_dF2 = jax.vmap(jax.jacobian(lambda a: F2(a)[0]))(array)\njac_ddF2_full = jax.vmap(jax.jacobian(jax.jacobian(lambda a: F2(a)[0])))(array)\njac_ddF2 = jax.vmap(jnp.diagonal)(jac_ddF2_full)\nprint(jnp.allclose(exact_dF2, jac_dF2))  # True\nprint(jnp.allclose(exact_ddF2, jac_ddF2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax"
    },
    {
        "title": "weird shape when indexing a jax array",
        "question": "I am experiencing a weird issue when indexing a Jax array using a list. If I place a debugger in the middle of my code, I have the following:\nThis array are created by convering a numpy array.\nHowever, when I try this in a new instance of Python, I have the correct behavior: [\nWhat is it happening?",
        "answers": [
            "This is working as expected. JAX follows the semantics of NumPy indexing, and in the case of advanced indexing with multiple scalars and integer arrays separated by slices, the indexed dimensions are combined via broadcasting and moved to the front of the output array. You can read more about the details of this kind of indexing in the NumPy documentation: https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing. In particular:\nTwo cases of index combination need to be distinguished:\nThe advanced indices are separated by a slice, Ellipsis or newaxis. For example x[arr1, :, arr2].\nThe advanced indices are all next to each other. For example x[..., arr1, arr2, :] but not x[arr1, :, 1] since 1 is an advanced index in this regard.\nIn the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array\nThe code in your program falls under the first case, while the code in your separate interpreter falls under the second case. This is why you're seeing different results.\nHere's a concise example of this difference:\n>>> import numpy as np\n>>> x = np.zeros((3, 4, 5))\n\n>>> x[0, :, [1, 2]].shape  # size-2 dimension moved to front\n(2, 4)\n\n>>> x[:, 0, [1, 2]].shape  # size-2 dimension not moved to front\n(3, 2)"
        ],
        "link": "https://stackoverflow.com/questions/78741064/weird-shape-when-indexing-a-jax-array"
    },
    {
        "title": "Jax vmap with lax scan having different sequence length in batch dimension",
        "question": "I have this following code , where my sim_timestep is in batch I am not able to run this since the lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) ) requires the concrete array , but since I have vmapped the state_predictor function the sim_timestep is being as a tracedArray . Any help would be greatly appreciated . Thanks all\nfrom jax import random\nfrom jax import lax\nimport jax\nimport jax.numpy as jnp\nimport pdb\n\n\ndef fwd_dynamics(x_u, xs):\n    x0,uk =  x_u\n    Delta_T = 0.001\n    lwb = 1.2\n    psi0=x0[2][0]\n    v0= x0[3][0]\n    vdot0 = uk[0][0]\n    delta0 = uk[1][0]\n    thetadot0 = uk[2][0]\n        \n    xdot= jnp.asarray([[v0*jnp.cos(psi0) ],\n        [v0*jnp.sin(psi0)] ,\n        [v0*jnp.tan(delta0)/(lwb)],\n        [vdot0],\n        [thetadot0]])\n    x_next = x0 + xdot*Delta_T\n    return (x_next,uk), x_next  # (\"carryover\", \"accumulated\")\n\n\ndef state_predictor( xk,uk ,sim_timestep):\n    (x_next,_), _ = lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) )\n    return x_next\n\nlow = 0  # Adjust minimum value as needed\nhigh = 100  # Adjust maximum value as needed\nkey = jax.random.PRNGKey(44)\n\nsim_time = jax.random.randint(key, shape=(10, 1), minval=low, maxval=high)\n\nxk = jax.random.uniform(key, shape=(10,5, 1))\nuk = jax.random.uniform(key, shape=(10,2, 1))\n\nstate_predictor_vmap = jax.jit(jax.vmap(state_predictor,in_axes= 0 ,out_axes=0 ))\nx_next = state_predictor_vmap( xk,uk ,sim_time)\nprint(x_next.shape)\nI tried to solve it by above code , hoping to get alternative way to achieve the same functionality.",
        "answers": [
            "What you're asking to do is impossible: scan lengths must be static, and vmapped values are non-static by definition.\nWhat you can do instead is replace your scan with a fori_loop or a while_loop, and then the loop boundary does not need to be static. For example, if you implement your function this way and leave the rest of your code unchanged, it should work:\ndef state_predictor(xk, uk, sim_timestep):\n  body_fun = lambda i, x_u: fwd_dynamics(x_u, i)[0]\n  x_next, _ = lax.fori_loop(0, sim_timestep[0], body_fun, (xk, uk))\n  return x_next"
        ],
        "link": "https://stackoverflow.com/questions/78713478/jax-vmap-with-lax-scan-having-different-sequence-length-in-batch-dimension"
    },
    {
        "title": "Colab, Jax, and GPU: why does cell execution take 60 seconds when %%timeit says it only takes 70 ms?",
        "question": "As the basis for a project on fractals, I'm trying to use GPU computation on Google Colab using the Jax library.\nI'm using Mandelbrot on all accelerators as a model, and I'm encountering a problem.\nWhen I use the %%timeit command to measure how long it takes to calculate my GPU function (same as in the model notebook), the times are entirely reasonable, and in line with expected results -- 70 to 80 ms.\nBut actually running %%timeit takes something like a full minute. (By default, it runs the function 7 times in a row and reports the average -- but even that should take less than a second.)\nSimilarly, when I run the function in a cell and output the results (a 6 megapixel image), it takes around 60 seconds for the cell to finish -- to execute a function that supposedly only takes 70-80 ms.\nIt seems like something is producing a massive amount of overhead, that also seems to scale with the amount of computation -- e.g. when the function contains 1,000 iterative calculations %%timeit says it takes 71 ms while in reality it takes 60 seconds, but with just 20 iterations %%timeit says it takes 10 ms while in reality it takes about 10 seconds.\nI am pasting the code below, but here is a link to the Colab notebook itself -- anyone can make a copy, connect to a \"T4 GPU\" instance, and run it themselves to see.\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax\n\nassert len(jax.devices(\"gpu\")) == 1\n\ndef run_jax_kernel(c, fractal):\n    z = c\n    for i in range(1000):\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n    return fractal\n\nrun_jax_gpu_kernel = jax.jit(run_jax_kernel, backend=\"gpu\")\n\ndef run_jax_gpu(height, width):\n\n    mx = -0.69291874321833995150613818345974774914923989808007473759199\n    my = 0.36963080032727980808623018005116209090839988898368679237704\n    zw = 4 / 1e3\n\n    y, x = jax.numpy.ogrid[(my-zw/2):(my+zw/2):height*1j, (mx-zw/2):(mx+zw/2):width*1j]\n    c = x + y*1j\n    fractal = jax.numpy.full(c.shape, 1000, dtype=np.int32)\n    return np.asarray(run_jax_gpu_kernel(c, fractal).block_until_ready())\nTakes about a minute to produce an image:\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nax.imshow(run_jax_gpu(2000, 3000));\nTakes about a minute to report that the function only takes 70-80 ms to execute:\n%%timeit -o\nrun_jax_gpu(2000, 3000)",
        "answers": [
            "The first thing to realize is that %timeit will execute your code multiple times, and then return an average of the times for each run. The number of times it will execute is determined dynamically by the time of the first run.\nThe second thing to realize is that JAX code is just-in-time (JIT) compiled, meaning that on the first execution of any particular function, you will incur a one-time compilation cost. Many things affect compilation cost, but functions that use large for loops (say, 1000 or more repetitions) tend to compile very slowly, because JAX unrolls those loops before passing the operations to XLA for compilation, and XLA compilation scales approximately quadratically with the number of unrolled operations (there is some discussion of this at JAX Sharp Bits: Control Flow).\nPut these together, and you'll see why you're observing the timings that you are: under %timeit, your first run results in a very long compilation, and subsequent runs are very fast. The resulting average time is printed, and is very short compared to the first run, and to the overall time.\nWhen you run your code a single time to plot the results, you are mainly seeing the compilation time. Because it is not amortized away by multiple calls to your function, that compilation time is long.\nThe solution would be to avoid writing Python for loops in your function in order to avoid the long compilation time: one possibility would be to use lax.fori_loop, which allows you to write iterative computations without the huge compilation time penalty, though it will incur a runtime penalty on GPU compared to the for loop solution because the operations are executed sequentially rather than being parallelized by the compiler. In your case it might look like this:\ndef run_jax_kernel(c, fractal):\n    z = c\n    def body_fun(i, carry):\n        z, fractal = carry\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n        return (z, fractal)\n    z, fractal = jax.lax.fori_loop(0, 1000, body_fun, (z, fractal))\n    return fractal"
        ],
        "link": "https://stackoverflow.com/questions/78708817/colab-jax-and-gpu-why-does-cell-execution-take-60-seconds-when-timeit-says"
    },
    {
        "title": "Implementing a vectorized function over LinkedLists using Jax’s vmap function",
        "question": "Trying to implement a vectorized version of an algorithm (from computational geometry) using Jax. I have made the minimum working example using a LinkedList to particularly express my query (I am using a DCEL otherwise).\nThe idea is that this vectorized algorithm will be checking certain criteria over a DCEL. I have substituted this “criteria checking procedure” with a simple summation algorithm for the sake simplicity.\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\n\nclass Node: \n  \n    # Constructor to initialize the node object \n    def __init__(self, data): \n        self.data = data \n        self.next = None\n\nclass LinkedList: \n  \n    def __init__(self): \n        self.head = None\n  \n    def push(self, new_data): \n        new_node = Node(new_data) \n        new_node.next = self.head \n        self.head = new_node \n\n    def printList(self): \n        temp = self.head \n        while(temp): \n            print (temp.data,end=\" \") \n            temp = temp.next\n\ndef summate(list) :\n    prev = None\n    current = list.head\n    sum = 0\n    while(current is not None): \n        sum += current.data\n        next = current.next\n        current = next\n    return sum\n\nlist1 = LinkedList() \nlist1.push(20) \nlist1.push(4) \nlist1.push(15) \nlist1.push(85) \n\nlist2 = LinkedList() \nlist2.push(19)\nlist2.push(13)\nlist2.push(2)\nlist2.push(13)\n\n#list(map(summate, ([list1, list2])))\n\nvmap(summate)(jnp.array([list1, list2]))\nI get the following error.\n TypeError: Value '<__main__.LinkedList object at 0x1193799d0>' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.\nThe objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion. I have implemented what I want in basic Python, but I want to do it in Jax as there is a larger probabilistic function which I will be using this subprocedure for (it’s a Markov Chain).\nIt might be the case that I am completely unable to work over such data structures over Jax as the error suggests that only numeric types are supported. Can I use pytrees in some way to mitigate this constraint?\nIt will be tempting to suggest I use a simple list from jnp, but I am using Linkedlist just as an example of a simple(st) data structure. As mentioned earlier, am actually working over a DCEL.\nPS : the Linkedlist code was taken from GeeksForGeeks, as I wanted to come up with a minimum working example quickly.",
        "answers": [
            "The objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion.\nThis goal is not feasible using JAX. You could register your class as a custom Pytree to make it work with JAX functions (see Extending pytrees), but this won't mean you can vectorize an operation over a list of such objects.\nJAX transformations like vmap and jit work for data stored with a struct-of-arrays pattern (e.g. a single LinkedList object containing arrays that represent multiple batched linked lists) not an array-of-structs pattern (e.g. a list of multiple LinkedList objects).\nFurther, the algorithm you're using, based on a while loop, is not compatible with JAX transformations (See JAX sharp bits: control flow), and the dynamically sized tree of nodes will not fit into the static shape constraints of JAX programs.\nI'd love to point you in the right direction, but I think you either need to give up on using JAX, or give up on using dynamic linked lists. You won't be able to do both."
        ],
        "link": "https://stackoverflow.com/questions/78677115/implementing-a-vectorized-function-over-linkedlists-using-jax-s-vmap-function"
    },
    {
        "title": "Simplest equivalent implementation of numpy.ma.notmasked_edges() for use in JAX",
        "question": "I have a square numpy.ndarray and a numpy boolean mask of the same shape. I want to find the first element in each row of the array that is not masked.\nMy code currently relies on numpy.ma.notmasked_edges(), which does exactly what I need. However, I now need to migrate my code to JAX, which has not implemented numpy.ma within jax.numpy.\nWhat would be the simplest way to find the index of the first unmasked element in each row, calling only numpy functions that have been implemented in JAX (which exclude numpy.ma)?\nThe code I'm trying to reproduce is something like:\nimport numpy as np\nmy_array = np.random.rand(5,5)\nmask = (my_array < 0.5)\nmy_masked_array = np.ma.masked_array(my_array, mask=mask)\nnp.ma.notmasked_edges(my_masked_array, axis=1)[0]\nI'm sure there are many ways to do this, but I'm looking for the least unwieldy way.",
        "answers": [
            "Here's a JAX implementation of nonmasked_edges, which takes a boolean mask and returns the same indices returned by the numpy.ma function:\nimport jax.numpy as jnp\n\ndef notmasked_edges(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  alltrue = mask.all(axis=axis).ravel()\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind)[~alltrue] for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel()[~alltrue])\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel()[~alltrue])\n  \n  return [tuple(first), tuple(last)]\nThis will not be compatible with JIT, because the size of the output arrays depend on the values of the mask (rows which have no unmasked value are left out).\nIf you want a JIT-compatible version, you can remove the [~alltrue] indexing, and the first/last index will be returned for rows that have no unmasked value:\ndef notmasked_edges_v2(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind) for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel())\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel())\n\n  return [tuple(first), tuple(last)]\nHere's an example:\nimport numpy as np\nmask = np.array([[True, False, False, True],\n                 [False, False, True, True],\n                 [True, True, True, True]])\n\narr = np.ma.masked_array(np.ones_like(mask), mask=mask)\nprint(np.ma.notmasked_edges(arr, axis=1))\n# [(array([0, 1]), array([1, 0])), (array([0, 1]), array([2, 1]))]\n\nprint(notmasked_edges(mask, axis=1))\n# [(Array([0, 1], dtype=int32), Array([1, 0], dtype=int32)),\n#  (Array([0, 1], dtype=int32), Array([2, 1], dtype=int32))]\n\nprint(notmasked_edges_v2(mask, axis=1))\n# [(Array([0, 1, 2], dtype=int32), Array([1, 0, 0], dtype=int32)),\n#  (Array([0, 1, 2], dtype=int32), Array([2, 1, 3], dtype=int32))]"
        ],
        "link": "https://stackoverflow.com/questions/78660344/simplest-equivalent-implementation-of-numpy-ma-notmasked-edges-for-use-in-jax"
    },
    {
        "title": "Why is Flax Linear layer not identical to matrix multiplication?",
        "question": "Due to the novelty of Flax, NNX, and JAX, there’s not a lot of resources available. I’m running into the following peculiarity:\nx = jnp.random.normal((1,512), key=KEY)\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = layer.kernel@x.squeeze() + layer.bias\nprint(y1==y2) # returns all False\nMy understanding is that matrix multiplication should be identical to a linear / fully connected layer. The discrepancy demonstrated here hinders the inspection of certain behavior (and the implementation of invertible dense layers using jnp.tensorsolve).\nDoes anyone know what causes this discrepancy?",
        "answers": [
            "The matmul should be transposed; also floating point equality checks should be done via approximate rather than exact comparison, because different ways of computing the same result may lead to different floating point rounding errors:\nimport jax\nfrom flax import nnx\n\nKEY = jax.random.key(0)\nx = jax.random.normal(KEY, (1,512))\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = x @ layer.kernel + layer.bias\nprint(jax.numpy.allclose(y1, y2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78659890/why-is-flax-linear-layer-not-identical-to-matrix-multiplication"
    },
    {
        "title": "Using JAX ndarray.at apply(ufunc) with arguments",
        "question": "Can arguments be passed to a jax.numpy.ufunc within a jax.numpy.ndarray.at call?\nThe following is an attempt to replicate jax.numpy.ndarray.at[...].add(...)\nimport jax.numpy as jnp\n\ndef myadd(a,b=1):\n    return a+b\n\numyadd = jnp.frompyfunc(myadd,2,1,identity=0)\n\nx = jnp.arange(4)\n\n# call jnp.add(x,x)\nx.at[:].add(x)\n# [0 2 4 6]\n\n# call umyadd.at\numyadd.at(x, np.arange(x.size), x, inplace=False)\n# [0 2 4 6]\n\n# Default b=1 (can b be passed here?)\nx.at[:].apply(umyadd)\n# [1 2 3 4]",
        "answers": [
            "arr.at[...].apply() only accepts unary functions that map a scalar to a scalar. So you could pass b via closure, as long as it's a scalar; for example:\nx.at[:].apply(lambda a: umyadd(a, 2))\n# [2, 3, 4, 5]\nBut there is no way to pass b=jnp.arange(4) within apply(), because then the applied function no longer maps a scalar to a scalar."
        ],
        "link": "https://stackoverflow.com/questions/78642505/using-jax-ndarray-at-applyufunc-with-arguments"
    },
    {
        "title": "how to log activation values using jax",
        "question": "I am following a jax tutorial that trains mnist using mlp network. I am trying to add an additional code that saves the activation patterns at every layer except the last. Here is the modified code:\nfrom collections import defaultdict\n# this is my activation pattern logger\nclass ActivationLogger:\n    def __init__(self, epoch):\n       self.reset(epoch)\n\n    def __call__(self, layer, activations):\n        D = activations.shape[0]\n        for i in range(D):\n            self.activations[(layer, i)].append(\n                    jax.lax.stop_gradient(activations[i]))\n\n    def reset(self, epoch):\n        self.epoch = epoch\n        self.activations = defaultdict(list)\n\nactivation_logger = ActivationLogger(epoch=1)\n\n...\n\ndef predict(params, image):\n    # per-example predictions\n    activations = image\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        activation_logger(l+1, activations) # <- this was added\n\n    final_w, final_b = params[-1]\n    logits = jnp.dot(final_w, activations) + final_b\n    return logits - logsumexp(logits)\n\nbatched_predict = jax.vmap(\n        predict, \n        in_axes=(None, 0), \n        out_axes=0)\n\n@jax.jit\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\nWhen I run my training code, I keep getting the following error message:\nTracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[].\nThis BatchTracer with object id 7541955024 was created on line:\n  /var/folders/km/3nj8tmq56s16dsgc9_63530r0000gn/T/ipykernel_73750/3494160804.py:12:20 (ActivationLogger.__call__)\nAny suggestions on how to fix this?",
        "answers": [
            "Python functions in JAX code are executed at trace-time, not runtime, and so as written you're not logging concrete runtime values, but rather their abstract trace-time representations.\nIf you want to log runtime values, the best tool is probably jax.debug.callback; for info on using this, I'd suggest starting with External Callbacks in JAX.\nUsing it in your case would look something like this:\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        jax.debug.callback(activation_logger, l+1, activations)\nFor more background on JAX's execution model, and why your function didn't work as expected when executed directly a trace-time, a good place to start is How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78611094/how-to-log-activation-values-using-jax"
    },
    {
        "title": "jax complaining about static start/stop/step",
        "question": "Here is a very simple computation in jax which errors out with complaints about static indices:\ndef get_slice(ar, k, I):\n  return ar[i:i+k]\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2,3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-32-6c60650ce6b7> in <cell line: 1>()\n----> 1 vec_get_slice(arr, 2, jnp.arange(3))\n\n    [... skipping hidden 3 frame]\n\n4 frames\n<ipython-input-29-9528369725c2> in get_slice(ar, k, i)\n      1 def get_slice(ar, k, i):\n----> 2   return ar[i:i+k]\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/array.py in __getitem__(self, idx)\n    346           return out\n    347 \n--> 348     return lax_numpy._rewriting_take(self, idx)\n    349 \n    350   def __iter__(self):\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _rewriting_take(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4602 \n   4603   treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape)\n-> 4604   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n   4605                  unique_indices, mode, fill_value)\n   4606 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4611             unique_indices, mode, fill_value):\n   4612   idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n-> 4613   indexer = _index_to_gather(shape(arr), idx)  # shared with _scatter_update\n   4614   y = arr\n   4615 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _index_to_gather(x_shape, idx, normalize_indices)\n   4854                \"dynamic_update_slice (JAX does not support dynamically sized \"\n   4855                \"arrays within JIT compiled functions).\")\n-> 4856         raise IndexError(msg)\n   4857 \n   4858       start, step, slice_size = _preprocess_slice(i, x_shape[x_axis])\n\nHorrible error output below. I am obviously missing something simple, but what?\n\n\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0, 1, 2], dtype=int32)\n  batch_dim = 0, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([2, 3, 4], dtype=int32)\n  batch_dim = 0, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).",
        "answers": [
            "Indices passed to slices in JAX must be static. Values that are mapped over in vmap are not static: because you're mapping over the start indices, your indices are not static and you see this error.\nThere is good news though: the size of your subarray is controlled by k, which is unmapped in your code and therefore static; it's only the location of the slice (given by I) that is dynamic. This is exactly the situation that jax.lax.dynamic_slicewas designed for, and so you can rewrite your code like this:\nimport jax\nimport jax.numpy as jnp\n\ndef get_slice(ar, k, I):\n  return jax.lax.dynamic_slice(ar, (I,), (k,))\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2, 3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n# Array([[1, 2],\n#        [2, 3],\n#        [3, 4]], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78588301/jax-complaining-about-static-start-stop-step"
    },
    {
        "title": "TypeError: unhashable type: 'ArrayImpl' when trying to use Equinox module with jax.lax.scan",
        "question": "I'm new to Equinox and JAX but wanted to use them to simulate a dynamical system.\nBut when I pass my system model as an Equinox module to jax.lax.scan I get the unhashable type error in the title. I understand that jax expects the function argument to be a pure function but I thought an Equinox Module would emulate that.\nHere is a test script to reproduce the error\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\n\n\nclass EqxModel(eqx.Module):\n    A: jax.Array\n    B: jax.Array\n    C: jax.Array\n    D: jax.Array\n\n    def __call__(self, states, inputs):\n        x = states.reshape(-1, 1)\n        u = inputs.reshape(-1, 1)\n        x_next = self.A @ x + self.B @ u\n        y = self.C @ x + self.D @ u\n        return x_next.reshape(-1), y.reshape(-1)\n\n\ndef simulate(model, inputs, x0):\n    xk = x0\n    outputs = []\n    for uk in inputs:\n        xk, yk = model(xk, uk)\n        outputs.append(yk)\n    outputs = jnp.stack(outputs)\n    return xk, outputs\n\n\nA = jnp.array([[0.7, 1.0], [0.0, 1.0]])\nB = jnp.array([[0.0], [1.0]])\nC = jnp.array([[0.3, 0.0]])\nD = jnp.array([[0.0]])\nmodel = EqxModel(A, B, C, D)\n\n# Test simulation\ninputs = jnp.array([[0.0], [1.0], [1.0], [1.0]])\nx0 = jnp.zeros(2)\nxk, outputs = simulate(model, inputs, x0)\nassert jnp.allclose(xk, jnp.array([2.7, 3.0]))\nassert jnp.allclose(outputs, jnp.array([[0.0], [0.0], [0.0], [0.3]]))\n\n# This raises TypeError\nxk, outputs = jax.lax.scan(model, x0, inputs)\nWhat is unhashable type: 'ArrayImpl' referring to? Is it the arrays A, B, C, and D? In this model, these matrices are parameters and therefore should be static for the duration of the simulation.\nI just found this issue thread that might be related:\nlax.scan for equinox Modules",
        "answers": [
            "Owen Lockwood (lockwo) has provided an explanation and answer in this issue thread, which I will re-iterate below.\nI believe your issue is happening because jax tries to hash the function you are scanning over, but it can't hash the arrays that are in the module. There are probably a number of things that you could do to solve this, the simplest being to just curry the model, e.g. xk, outputs = jax.lax.scan(lambda carry, y: model(carry, y), x0, inputs) works fine\nOr, re-written in terms of the variable names I am using:\nxk, outputs = jax.lax.scan(lambda xk, uk: model(xk, uk), x0, inputs)"
        ],
        "link": "https://stackoverflow.com/questions/78583009/typeerror-unhashable-type-arrayimpl-when-trying-to-use-equinox-module-with-j"
    },
    {
        "title": "Multiplying chains of matrices in JAX",
        "question": "Suppose I have a vector of parameters p which parameterizes a set of matrices A_1(p), A_2(p),...,A_N(p). I have a computation in which for some list of indices q of length M, I have to compute A_{q_M} * ... * A_{q_2} * A_{q_1} * v for several different q s. Each q has a different length, but crucially doesn't change! What changes, and what I wish to take gradients against is p.\nI'm trying to figure out how to convert this to performant JAX. One way to do it is to have some large matrix Q which contains all the different qs on each row, padded out with identity matrices such that each multiplication chain is the same length, and then scan over a function that switch es between N different functions doing matrix-vector multiplications by A_n(p).\nHowever -- I don't particularly like the idea of this padding. Also, since Q here is fixed, is there potentially a smarter way to do this? The distribution of lengths of q s has a very long tail, so Q will be dominated by padding.\nEDIT: Here's a (edit 2: functional) minimal example\nsigma0 = jnp.eye(2)\nsigmax = jnp.array([[0, 1], [1, 0]])\nsigmay = jnp.array([[0, -1j], [1j, 0]])\nsigmaz = jnp.array([[1, 0], [0, -1]])\nsigma = jnp.array([sigmax, sigmay, sigmaz])\n\ndef gates_func(params):\n    theta = params[\"theta\"]\n    epsilon = params[\"epsilon\"]\n\n    n = jnp.array([jnp.cos(theta), 0, jnp.sin(theta)])\n    omega = jnp.pi / 2 * (1 + epsilon)\n    X90 = expm(-1j * omega * jnp.einsum(\"i,ijk->jk\", n, sigma) / 2)\n\n    return {\n        \"Z90\": expm(-1j * jnp.pi / 2 * sigmaz / 2),\n        \"X90\": X90\n    }\n\ndef multiply_out(params):\n    gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n\n    gates = gates_func(params)\n    out = jnp.zeros(len(gate_lists)) \n    \n    for i, gate_list in enumerate(gate_lists):\n        init = jnp.array([1.0,0.0], dtype=jnp.complex128)\n        for g in gate_list:\n            init = gates[g] @ init\n        out = out.at[i].set(jnp.abs(init[0]))\n\n    return out\n\nparams = dict(theta=-0.0, epsilon=0.001)\nmultiply_out(params)",
        "answers": [
            "The main issue here is that JAX does not support string inputs. But you can use NumPy to manipulate string arrays and turn them into integer categorical arrays that can then be used by jax.jit and jax.vmap. The solution might look something like this:\nimport numpy as np\n\ndef gates_func_int(params, gate_list_vals):\n  g = gates_func(params)\n  identity = jnp.eye(*list(g.values())[0].shape)\n  return jnp.stack([g.get(val, identity) for val in gate_list_vals])\n\n@jax.jit\ndef multiply_out_2(params):\n  # compile-time pre-processing\n  gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n  max_size = max(map(len, gate_lists))\n  gate_array = np.array([gates + [''] * (max_size - len(gates))\n                        for gates in gate_lists])\n  gate_list_vals, gate_list_ints = np.unique(gate_array, return_inverse=True)\n  gate_list_ints = gate_list_ints.reshape(gate_array.shape)\n\n  # runtime computation\n  gates = gates_func_int(params, gate_list_vals)[gate_list_ints]\n  initial = jnp.array([[1.0],[0.0]], dtype=jnp.complex128)\n  return jax.vmap(lambda g: jnp.abs(jnp.linalg.multi_dot([*g, initial]))[0])(gates).ravel()\n\nmultiply_out_2(params)"
        ],
        "link": "https://stackoverflow.com/questions/78562406/multiplying-chains-of-matrices-in-jax"
    },
    {
        "title": "How to set a new learning rate manually in optax optimizer?",
        "question": "I have the following optimizer being create using optax:\ndef create_optimizer(learning_rate=6.25e-2, beta1=0.4, beta2=0.999,\n                     eps=2e-4, centered=False):\n\n  Returns:\n    An optax optimizer.\n  \"\"\"\n \n    return optax.adam(learning_rate, b1=beta1, b2=beta2, eps=eps)\nHow during training update this learning rate manually?\nI couldn't find any documentation about that.",
        "answers": [
            "Disclaimer. Usually, you would use a schedule to adapt the learning rate during training. This answer provides a solution to obtain direct control over the learning rate.\nIn general, you can put any optimizer's hyperparmeters (such as the learning rate) into the optimizer's state and then directly mutate the state. Moving the hyperparameters into the state is necessary as optax optimizers are pure functions. Especially, the only way to dynamically change their behaviour is by changing their input.\nSetup. I am using a stochastic gradient descent optimizer to highlight the effect of the learning rate on the update suggested by the optimizer.\nimport jax.numpy as jnp\nimport optax\n\n# Define example parameters and gradients.\nparams, grads = jnp.array([0.0, 0.0]), jnp.array([1.0, 2.0])\n\n# Ensure the learning rate is part of the optimizer's state.\nopt = optax.inject_hyperparams(optax.sgd)(learning_rate=1e-2)\nopt_state = opt.init(params)\nUpdate computation.\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.01, -0.02], dtype=float32)\nDirectly setting the learning rate.\nopt_state.hyperparams['learning_rate'] = 3e-4\nSame update computation as before (with new learning rate).\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.0003, -0.0006], dtype=float32)\nSee this discussion for more information."
        ],
        "link": "https://stackoverflow.com/questions/78527164/how-to-set-a-new-learning-rate-manually-in-optax-optimizer"
    },
    {
        "title": "Why JAX is considering same list as different data structure depending on appending a new array inside function?",
        "question": "I am very new to JAX. Please excuse me if this something obvious or I am making some stupid mistake. I am trying to implement a function which does the following. All these functions will be called from other JIT-ed function. So, removing JIT may not be possible.\nget_elements function takes a JAX array( call it state (1D)). Looks at each element in it and calls a function get_condition.\nget_condition returns a tuple depending on the element at the given position of state. The tuple may be (1,0),(0,1) or (0,0)\nHere I want to call update_state only if the tuple received from get_conn is (0,1) or (1,0). In that case update_state_vec will get called and add a new vector of same length as state will get appended to the list.\nBut, I couldn't make jax.lax.cond work here. So, I tried to call update_state for each case, but I want the list to remain unchanged if the codition is (0,0).\nIn update_state_vec, no_update_state should return the same array\nthat it receives withourt appending anything\nHere, is the entire code:\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import lax\nimport copy\nfrom copy import deepcopy\n\nimport numpy as np\n\n\ndef get_condition(state, x, y):\n   L = (jnp.sqrt(len(jnp.asarray(state)))).astype(int)\n   state = jnp.reshape(state, (L,L), order=\"F\")\n   s1 = state[x, y]\n\n   branches = [lambda : (0,1), lambda : (1,0), lambda : (0,0)]\n   conditions = jnp.array([s1==2, s1==4, True])\n   result = lax.switch(jnp.argmax(conditions), branches)\n   return tuple(x for x in result)\n\n\n\n\ndef update_state_vec(state, x, y, condition, list_scattered_states):\n   L = (jnp.sqrt(len(state))).astype(int)   \n   def update_state_4(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(4)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n   def update_state_2(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(2)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n\n   def no_update_state (list_scattered_states):\n       #state1 = jnp.ravel(state, order=\"F\")\n       #list_scattered_states.append(jnp.ravel(state, order=\"F\"))\n       #This doesn't work---------------------------------\n       return list_scattered_states\n\n\n\n   conditions = jnp.array([condition == (1, 0), condition == (0, 1), condition == (0, 0)])\n   print(conditions)\n   branches = [update_state_4, update_state_2,no_update_state]\n\n   return(lax.switch(jnp.argmax(conditions), branches, operand=list_scattered_states))\n           \n\n\ndef get_elements(state):\n\n   L = (jnp.sqrt(len(state))).astype(int)\n   list_scattered_states = []\n   for x in range(L):\n       for y in range(L):\n           condition=get_condition(state, x, y)\n           print(condition)\n           list_scattered_states = update_state_vec(state, x, y, condition, list_scattered_states)\n\n\n   return list_scattered_states\nWe can take an example input as follows,\narr=jnp.asarray([2., 1., 3., 4., 1., 2., 3., 4., 4., 1., 2., 3., 4., 2., 1., 3.])\nget_elements(arr)\nI get an error message as below:\n    print(conditions)\n 41 branches = [update_state_4, update_state_2,no_update_state]\n ---> 43 return(lax.switch(jnp.argmax(conditions), branches, \n operand=list_scattered_states))\n TypeError: branch 0 and 2 outputs must have same type structure, got PyTreeDef([*]) \n and PyTreeDef([]).\nSo, the error is coming from the face that no_update_state is returning something that doesn't match with return type of update_state_4 or update_state_2. I am quite clueless at this point. Any help will be much appreciated.",
        "answers": [
            "The root of the issue here is that under transformations like jit, vmap, switch, etc. JAX requires the shape of outputs to be known statically, i.e. at compile time (see JAX sharp bits: dynamic shapes). In your case, the functions you are passing to switch return outputs of different shapes, and since jnp.argmax(conditions) is not known at compile time, there's no way for the compiler to know what memory to allocate for the result of this function.\nSince you're not JIT-compiling or otherwise transforming your code, the easiest way to address this would be to replace the lax.switch statement with this:\n  if condition == (1, 0):\n    list_scattered_states = update_state_4(list_scattered_states)\n  elif condition == (0, 1):\n    list_scattered_states = update_state_2(list_scattered_states)\n  return list_scattered_states\nIf you do want your function to be compatible with jit or other JAX transformations, you'll have to re-write the logic so that the size of list_scattered_states remains constant, e.g. by padding it to the expected size from the beginning."
        ],
        "link": "https://stackoverflow.com/questions/78512663/why-jax-is-considering-same-list-as-different-data-structure-depending-on-append"
    },
    {
        "title": "Jax dynamic slicing tracer array",
        "question": "To make this brief: I wrote the following codes:\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef body_func(carry,x):\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nbut got\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nHere's the full situation: I'm trying to develop a model to do phase recognition tasks, and I would like to normalize my logits phase by phase using jax. For example, suppose I have the phase labels and logits:\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\nI would like to normalize the first 4 elements in logits where in the phase labels they all belong to phase 0. Then the next 4 elements, because in the phase labels they all belong to phase 1. So the normalized logits should look like:\nnormalized_logits=[0,0.33,0.66,1.0,0,0.33,0.66,1.0,0,0.33,0.66,1.0]\nHere's what tried:\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef min_max_normalization(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndef body_func(carry,x):\n    jax.debug.print(\"carry is {}\",carry)\n    jax.debug.print(\"x is {}\",x)\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    print(min_max_normalization(jax.lax.dynamic_slice(arr, [start_idx], [jax.lax.tie_in(x, x-start_idx+1)])))\n    print(jax.lax.dynamic_slice(arr, [x+1], [jax.lax.tie_in(x, len(arr)-x-1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nBasically, this is a debug version, the actual return value should concatenate three dynamically sliced array together. But I'm getting the error below:\nTraceback (most recent call last):\n  File \"/path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 250, in scan\n    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n                                                                ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 236, in _create_jaxpr\n    jaxpr, consts, out_tree = _initial_style_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 64, in _initial_style_jaxpr\n    jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 58, in _initial_style_open_jaxpr\n    jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2155, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/slicing.py\", line 110, in dynamic_slice\n    static_sizes = core.canonicalize_shape(slice_sizes)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/core.py\", line 2086, in canonicalize_shape\n    raise _invalid_shape_error(shape, context)\njax._src.traceback_util.UnfilteredStackTrace: TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /Users/wuhaoyang/Documents/Research/Project_Surgical_Robot/Code/SSM_Med/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nThe reason why I'm not simply using a for loop is that I'm later going to wrap this function into another one that uses jit compile, so I want to do this with pure jax API. Any help is appreciated, please tell me if you need more information.",
        "answers": [
            "JAX arrays used in transformations like jit, vmap, and scan must always be statically-shaped (see Sharp bits: Dynamic Shapes for some discussion of this).\ndynamic_slice allows you to slice a static length at a dynamic position, while you're trying to use it to slice a dynamic length at a static position, and thus you're seeing this concretization error.\nTo solve your problem, I would avoid scan and instead use JAX's segment_min and segment_max functions to compute the output in a vectorized rather than iterative manner:\nimport jax\nimport jax.numpy as jnp\n\nlabels = jnp.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits = jnp.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\nl_min = jax.ops.segment_min(logits, labels)[labels]\nl_max = jax.ops.segment_max(logits, labels)[labels]\n\nnormalized_logits = (logits - l_min) / (l_max - l_min)\nprint(normalized_logits)\n# [0.         0.33333334 0.6666667  1.         0.         0.33333334\n#  0.6666667  1.         0.         0.33333334 0.6666667  1.        ]\nIf you want this to be compatible with jit and other transformations, you'll need to pass a static num_segments argument to your segment reductions to specify an upper-bound for the number of segments present:\nl_min = jax.ops.segment_min(logits, labels, num_segments=3)[labels]\nl_max = jax.ops.segment_max(logits, labels, num_segments=3)[labels]"
        ],
        "link": "https://stackoverflow.com/questions/78496911/jax-dynamic-slicing-tracer-array"
    },
    {
        "title": "Finite basis physics-informed neural networks (FBPINNs) JAX problem",
        "question": "I am trying to modify Ben Moseley's code available on github https://github.com/benmoseley/FBPINNs. My intention is to insert a vector of values into the loss fn that is dependent on x y coordinates, and I need the original vector Z to be interpolated as a function of x and y, and then the values at the same coordinates with which the algorithm samples x and y are extracted, so that the values match. The problem I have encountered is that within loss fn I cannot use libraries other than JAX and to my knowledge there are no functions within JAX to interpolate in 2D.\nI'm trying to get around the problem in every way but I'm not succeeding, one of my ideas was to extrapolate the x,y points sampled by the algorithm but I'm not succeeding, the code is really very articulated. Would anyone be able to give me any advice/help on this?\nThere would be the function jax.scipy.ndimage.map_coordinates but it doesn't work properly and the points it extrapolates are meaningless.",
        "answers": [
            "If linear or nearest-neighbor interpolation is sufficient, you may be able to do what you need with jax.scipy.interpolate.RegularGridInterpolator\nIf you need something more sophisticated, like spline interpolation, there is nothing included in jax itself. That said, you may be able to find downstream implementations that work for you. One I came across that might be worth trying is in the jax_cosmo project: https://jax-cosmo.readthedocs.io/en/latest/_modules/jax_cosmo/scipy/interpolate.html."
        ],
        "link": "https://stackoverflow.com/questions/78494686/finite-basis-physics-informed-neural-networks-fbpinns-jax-problem"
    },
    {
        "title": "Why is custom pytree 'aux_data' traced after jax.jit() for jnp.array but not for np.array?",
        "question": "I am trying to understand how pytrees work and registered my own class as a pytree. I noticed that if the aux_data in the pytree is a jax.numpy.ndarray the auxilliary data is subsequently traced and returned as a Traced<ShapedArray(...)>.... However, if the aux_data is a numpy.ndarray (i.e. not JAX array), then it is not traced and returns an array from a jit tranformed function.\nNow, I am aware of the tracing that happens during the jax.jit() transformation, but I do not understand why, on the level of pytrees, this results in the behaviour described above.\nHere is an example to reproduce this behaviour (multiplying both the aux_data and the tree leaves by two, which may be a problem in itself after JIT transformation...?). I have used the custom pytree implementations of accepted libraries (equinox and simple_pytree) for comparison, and they all give the same result, so that I am very sure that this is not a bug but a feature that I am trying to understand.\nimport jax\nfrom jax.tree_util import tree_structure, tree_leaves\nimport numpy as np\n\ndef get_pytree_impl(base):\n    if base == \"equinox\":\n        import equinox as eqx\n        Module = eqx.Module\n        static_field = eqx.static_field\n    elif base == \"simple_pytree\":\n        from simple_pytree import Pytree, static_field\n        Module = Pytree\n    elif base == \"dataclasses\":\n        from dataclasses import dataclass, field\n        @dataclass\n        class Module():\n            pass\n        static_field = field\n    \n    class PytreeImpl(Module):\n        x: jax.numpy.ndarray\n        y: jax.numpy.ndarray = static_field()\n\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n\n    if base == 'dataclasses':\n        from jax.tree_util import register_pytree_node\n        \n        def flatten(ptree):\n            return ((ptree.x,), ptree.y)\n        \n        def unflatten(aux_data, children):\n            return PytreeImpl(*children, aux_data)\n\n        register_pytree_node(PytreeImpl, flatten, unflatten)\n        \n    return PytreeImpl\n\ndef times_two(ptree):\n    return type(ptree)(ptree.x*2, ptree.y*2)\n\ntimes_two_jitted = jax.jit(times_two)\n\nbases = ['dataclasses', 'equinox', 'simple_pytree']\nfor base in bases:\n    print(\"========  \" + base + \"  ========\")\n    for lib_name, array_lib in zip(['jnp', 'np'], [jax.numpy, np]):\n        print(\"====  \" + lib_name)\n        PytreeImpl = get_pytree_impl(base)\n        x = jax.numpy.array([1,2])\n        y = array_lib.array([3,4])\n        input_tree = PytreeImpl(x, y)\n        for tag, pytree in zip([\"input\", \"no_jit\", \"jit\"],[input_tree, times_two(input_tree), times_two_jitted(input_tree)]):\n            print(f' {tag}:')\n            print(f'\\t Structure: {tree_structure(pytree)}')\n            print(f'\\t Leaves: {tree_leaves(pytree)}')\nThis produces the follwing, where dataclasses is my naive custom implementation of a pytree:\n========  dataclasses  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  equinox  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([3, 4], dtype=int32),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([6, 8], dtype=int32),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>,)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([3, 4]),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  simple_pytree  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([3, 4], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([6, 8], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>, '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([3, 4]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\nI ran this example using Python 3.12.1 with equinox 0.11.4 jax 0.4.28 jaxlib 0.4.28 simple-pytree 0.1.5",
        "answers": [
            "From the JAX docs:\nWhen defining unflattening functions, in general children should contain all the dynamic elements of the data structure (arrays, dynamic scalars, and pytrees), while aux_data should contain all the static elements that will be rolled into the treedef structure.\naux_data in a pytree flattening must contain static elements, and static elements must be hashable and immutable. Neither np.ndarray nor jax.Array satisfy this, so they should not be included in aux_data. If you do include such values in aux_data, you'll get unsupported, poorly-defined behavior.\nWith that background: the answer to your question of why you're seeing the results you're seeing is that you are defining your pytrees incorrectly. If you define aux_data to only contain static (i.e. hashable and immutable) attributes, you will no longer see this behavior."
        ],
        "link": "https://stackoverflow.com/questions/78485445/why-is-custom-pytree-aux-data-traced-after-jax-jit-for-jnp-array-but-not-for"
    },
    {
        "title": "What is an efficient method to calculate multiple \"offset-traces\" in JAX?",
        "question": "Given a matrix m with shape (n, n), I need to compute the series of \"offset traces\" [np.trace(m, offset=i) for i in range(q)] in JAX. For my application, n >> q, and q is a static parameter.\nThe obvious JAX approach using vmap does not work, possibly because although the trace has fixed output size, each offset diagonal has a different length?\nI came up with two other approaches using JAX which work but are about 100x slower than NumPy. get_traces_jax_1 is the more efficient of the two. But it does a lot of extra work when I only need a few diagonals, and I don't think that extra work gets compiled away.\nIs there a more efficient way to do this in JAX with similar performance to NumPy? I want to use JAX because:\nI need to vmap this across many matrices;\nIt is part of a larger algorithm, other parts of which are significantly sped up by JAX jit.\nBelow are the methods I explored and timings on my computer.\nimport numpy as np\nfrom numpy import random\nimport jax\njax.config.update(\"jax_enable_x64\", True) # default is float32\nfrom jax import numpy as jnp\nfrom functools import partial\n\nn, q = 1000, 5\n\n# check the methods produce the same result\ndef distance(u, v):\n    return jnp.max(jnp.abs(u - v))\n\n# numpy - this is what I want\ndef get_traces_np(mat, q):\n    return np.array([np.trace(mat, offset=i) for i in range(q)])\n\n# jax\n# !! This does not work\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_broken(mat, q):\n    return jax.vmap(lambda i: jnp.trace(mat, offset=i))(jnp.arange(q)) # !! does not work\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_0(mat, q):\n    return jnp.array([jnp.trace(mat, offset=i) for i in range(q)])\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_1(mat, q):\n    n = mat.shape[0]\n    padded = jnp.pad(mat, ((0, 0), (0, n-1)), 'constant')\n    shifts = jax.vmap(lambda v, i: jnp.roll(v, -i))(padded, jnp.arange(n))[:, :n]\n    return jnp.sum(shifts, axis=0)[:q]\n\nmat = random.uniform(size=(n, n))\n# Check they produce the same result and precompile\nd0 = distance(get_traces_np(mat, q), get_traces_jax_0(mat, q))\nd1 = distance(get_traces_np(mat, q), get_traces_jax_1(mat, q))\nprint(f'Errors: {d0}, {d1}')\n\nmat = jnp.array(mat)\nprint('Numpy:')\n%timeit get_traces_np(mat, q) # 7.43 microseconds\nprint('Jax 0:')\n%timeit get_traces_jax_0(mat, q) # 4.82ms\nprint('Jax 1:')\n%timeit get_traces_jax_1(mat, q) # 1.22ms",
        "answers": [
            "vmapping jnp.trace across offset doesn't work because as currently implemented, the offset parameter of jnp.trace must be static, and vmapped parameters are not static. You could address this by constructing your own version of the trace function that does not require a static parameter; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef dynamic_trace(x, offset):\n  assert x.ndim == 2\n  i = jnp.arange(x.shape[0])[:, None]\n  j = jnp.arange(x.shape[1])\n  return jnp.sum(jnp.where(i + offset == j, x, 0))\n\nx = jnp.arange(12).reshape(3, 4)\n\noffset = jnp.arange(-2, 3)\n\njax.vmap(dynamic_trace, in_axes=(None, 0))(x, offset)\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\n\njnp.array([jnp.trace(x, int(o)) for o in offset])\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\nIn terms of benchmark comparisons, you should first make sure you're measuring what you think you're measuring. For example, your benchmark is written in such a way that it doesn't just measure runtime, but also device transfer and JIT compilation costs. It also ignores potentially confounding effects of asynchronous dispatch. For a discussion of these issues, see JAX FAQ: Benchmarking JAX code.\nEven accounting for this, however, I would not expect JAX to be faster than NumPy here for the reasons discussed at JAX FAQ: Is JAX Faster than NumPy?. Namely, this benchmark is doing a short sequence of relatively small array operations on CPU: this is a regime in which NumPy will always beat JAX due to its lower per-operation dispatch overhead. But not to worry: when this operation is used as part of a larger JIT-compiled function, those dispatch costs are amortized away."
        ],
        "link": "https://stackoverflow.com/questions/78479150/what-is-an-efficient-method-to-calculate-multiple-offset-traces-in-jax"
    },
    {
        "title": "Iterators in jit JAX functions",
        "question": "I'm new to JAX and reading the docs i found that jitted functions should not contain iterators (section on pure functions)\nand they bring this example:\nimport jax.numpy as jnp\nimport jax.lax as lax\nfrom jax import jit\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\ntrying to fiddling with it a little bit in order to see if i can get directly an error instead of undefined behaviour i wrote\n@jit\ndef f(x, arr):\n    for i in range(10):\n        x += arr[i]\n    return x\n\n@jit\ndef f1(x, arr):\n    it = iter(arr)\n    for i in range(10):\n        x += next(it)\n    return x\n\nprint(f(0,array)) # 45 as expected\nprint(f1(0,array)) # still 45 \nIs it a \"chance\" that the jitted function f1() now shows the correct behaviour?",
        "answers": [
            "Your code works because of the way that JAX's tracing model works. When JAX's tracing encounters Python control flow, like for loops, the loop is fully evaluated at trace-time (There's some exploration of this in JAX Sharp Bits: Control Flow).\nBecause of this, your use of an iterator in this context is fine, because every iteration is evaluated at trace-time, and so next(it) is re-evaluated at every iteration.\nIn contrast, when using lax.fori_loop, next(iterator) is only executed a single time and its output is treated as a trace-time constant that will not change during the runtime iterations."
        ],
        "link": "https://stackoverflow.com/questions/78403517/iterators-in-jit-jax-functions"
    },
    {
        "title": "how to vmap over multiple Dense instances in flax model? trying to avoid looping over a list of Dense instances",
        "question": "from jax import random,vmap\nfrom jax import numpy as jnp\nimport pprint\n\ndef f(s,layers,do,dx):\n    x = jnp.zeros((do,dx))\n    for i,layer in enumerate(layers):\n        x=x.at[i].set( layer( s[i] ) )\n    return x\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layers = [ nn.Dense( self.dx, use_bias=False )\n                        for _ in range(self.do) ]\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None,None))(s,self.layers,self.do,self.dx)\n        return x\n\nif __name__ == '__main__':\n    seed = 123\n    key = random.PRNGKey( seed )\n    key,subkey = random.split( key )\n    outer_batches = 4\n    s_observations = 5 # AKA the inner batch\n    x_features = 2\n    s_features = 3\n    s_shape = (outer_batches,s_observations, s_features)\n    s = random.uniform( subkey, s_shape )\n\n    key,subkey = random.split( key )    \n    model = net(x_features,s_observations)\n    p = model.init( subkey, s )\n    x = model.apply( p, s )    \n\n    params = p['params']\n    pkernels = jnp.array([params[key]['kernel'] for key in params.keys()])\n    x_=jnp.zeros((outer_batches,s_observations,x_features))\n    \n    g = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\n    \n    x_=g(s,pkernels)\n    print('s shape:',s.shape)\n    print('p shape:',pkernels.shape)\n    print('x shape:',x.shape)\n    print('x_ shape:',x_.shape)\n    print('sum of difference:',jnp.sum(x-x_))\nHi. I need some \"batch-specific\" parameters in my model. Here, there is an \"inner batch\" of length do such that there is a flax.linen.Dense instance for each element in that batch. The outer batch just passes multiple data instances into those layers. I accomplish this by creating a list of flax.linen.Dense instances in the setup method. Then in the __call__ method, I iterate over those layers to fill up an array. This iteration is encapsulated in a function f, and that function is wrapped in jax.vmap.\nI have also included some equivalent logic written as matrix multiplication (see the function g) to make it explicit what operation I was hoping to capture with this class.\nI would like to replace the for-loop in the __call__ method with a call to jax.vmap. I ofc get an error when I pass a list to vmap, and I ofc get an error when I try to put multiple Dense instances in a jax array. Is there an alternative to using a list to contain my multiple Dense instances? A constraint is that I should be able to create an arbitrary number of Dense instances at the time of model initialization.",
        "answers": [
            "vmap can be used to map a single function over batches of data. You are attempting to use it to map multiple functions over batches of data, which it cannot do.\nUpdated answer based on updated question:\nSince each layer is identical aside from the parameters fit to the input data, it sounds like what you want is to map a single dense layer against a batch of data. It might look something like this:\nkeys = vmap(random.fold_in, in_axes=(None, 0))(subkey, jnp.arange(s_observations))\nmodel = nn.Dense(x_features, use_bias=False)\np = vmap(model.init, in_axes=(0, 1))(keys, s)\nx = vmap(model.apply, in_axes=(0, 1), out_axes=1)(p, s)\n\npkernels = p['params']['kernel']\ng = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\nx_=g(s,pkernels)\n\nprint('sum of difference:',jnp.sum(x-x_))\n# sum of difference: 0.0\nPrevious answer\nIn general, the fix would be to define a single parameterized layer that you can pass to vmap. In the example you gave, every layer is identical, and so to achieve the result you're looking for you could write something like this:\ndef f(s,layer,dx):\n  return layer(s)\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layer = nn.Dense( self.dx, use_bias=False )\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None))(s,self.layer,self.dx)\n        return x\nIf you had different parameterization per layer, then you could achieve this within vmap by passing those parameters to vmap as well."
        ],
        "link": "https://stackoverflow.com/questions/78385261/how-to-vmap-over-multiple-dense-instances-in-flax-model-trying-to-avoid-looping"
    },
    {
        "title": "How to restore a orbax checkpoint with jax/flax?",
        "question": "I saved a orbax checkpoint with the code below:\ncheck_options = ocp.CheckpointManagerOptions(max_to_keep=5, create=True)\ncheck_path = Path(os.getcwd(), out_dir, 'checkpoint')\ncheckpoint_manager = ocp.CheckpointManager(check_path, options=check_options, item_names=('state', 'metadata'))\ncheckpoint_manager.save(\n                    step=iter_num,\n                    args=ocp.args.Composite(\n                        state=ocp.args.StandardSave(state),\n                        metadata=ocp.args.JsonSave((model_args, iter_num, best_val_loss, losses['val'].item(), config))))\nWhen I try to resume from the saved checkpoints, I used the code below to recover the state variable:\nstate, lr_schedule = init_train_state(model, params['params'], learning_rate, weight_decay, beta1, beta2, decay_lr, warmup_iters, \n                     lr_decay_iters, min_lr)  # Here state is the initialied state variable with type Train_state.\nstate = checkpoint_manager.restore(checkpoint_manager.latest_step(), items={'state': state})\nBut when I try to use the recovered state in the training loop, I got this error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:584, in shaped_abstractify(x)\n    583 try:\n--> 584   return _shaped_abstractify_handlers[type(x)](x)\n    585 except KeyError:\n\nKeyError: <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'>\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 37\n     34 if iter_num == 0 and eval_only:\n     35     break\n---> 37 state, loss = train_step(state, get_batch('train'))\n     39 # timing and logging\n     40 t1 = time.time()\n\n    [... skipping hidden 6 frame]\n\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:575, in _shaped_abstractify_slow(x)\n    573   dtype = dtypes.canonicalize_dtype(x.dtype, allow_extended_dtype=True)\n    574 else:\n--> 575   raise TypeError(\n    576       f\"Cannot interpret value of type {type(x)} as an abstract array; it \"\n    577       \"does not have a dtype attribute\")\n    578 return core.ShapedArray(np.shape(x), dtype, weak_type=weak_type,\n    579                         named_shape=named_shape)\n\nTypeError: Cannot interpret value of type <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'> as an abstract array; it does not have a dtype attribute\nSo, how should I correctly recover the state checkpoint and use it in the training loop?\nThanks!",
        "answers": [
            "You're mixing the old and new APIs in a way that is not allowed. Apologies that an error to that effect is not being raised, I can look into that.\nYour saving is correct, but I'd recommend that it look more like the following:\nwith ocp.CheckpointManager(path, options=options, item_names=('state', 'metadata')) as mngr:\n  mngr.save(\n      step, \n      args=ocp.args.Composite(\n          state=ocp.args.StandardSave(state),\n          metadata=ocp.args.JsonSave(...),\n      )\n  )\nWhen restoring, you're currently using items which is part of the old API, and the usage is inconsistent with the CheckpointManager's definition, which is done based on the new API.\nitem_names and args are hallmarks of the new API.\nYou should do:\nwith ocp.CheckpointManager(...) as mngr:\n  mngr.restore(\n      mngr.latest_step(), \n      args=ocp.args.Composite(\n          state=ocp.args.StandardRestore(abstract_state),\n      )\n  )\nLet me know if there's any unexpected issues with that."
        ],
        "link": "https://stackoverflow.com/questions/78376465/how-to-restore-a-orbax-checkpoint-with-jax-flax"
    },
    {
        "title": "acme error - AttributeError: module 'jax' has no attribute 'linear_util'",
        "question": "I am using acme framework to run some experiments, and I installed acme based on documentation. However, I have attribute error that raised likely from JAX, HAIKU, and when I looked into github issue, there was no solution given at this time. Can anyone take a look what package dependecy caused this issue?\nmy venv spec:\nhere is my venv spec\ndm-acme                      0.4.0\ndm-control                   0.0.364896371\ndm-env                       1.6\ndm-haiku                     0.0.10\ndm-launchpad                 0.5.0\ndm-reverb                    0.7.0\ndm-tree                      0.1.8\nacme                         2.10.0\ndm-acme                      0.4.0\njax                          0.4.26\njaxlib                       0.4.26+cuda12.cudnn89\npython -V                    Python 3.9.5\nerror details:\nFile \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in from acme.agents.jax import dqn File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/init.py\", line 18, in from acme.agents.jax.dqn.actor import behavior_policy File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in from acme.agents.jax import actor_core as actor_core_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in from acme.jax import networks as networks_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/init.py\", line 18, in from acme.jax.networks.atari import AtariTorso File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/atari.py\", line 29, in from acme.jax.networks import base File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/base.py\", line 24, in import haiku as hk File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/init.py\", line 20, in from haiku import experimental File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/experimental/init.py\", line 34, in from haiku._src.dot import abstract_to_dot File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/_src/dot.py\", line 163, in @jax.linear_util.transformation File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 54, in getattr raise AttributeError(f\"module {module!r} has no attribute {name!r}\") AttributeError: module 'jax' has no attribute 'linear_util'\nseems it raised from haiku and JAX, how this can be fixed? any quick thoughts?\nupdated attempt\nbased on @jakevdp suggestion, I reinstalled jax, jaxlib, but now I am getting this error again:\nTraceback (most recent call last):\n  File \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in <module>\n    from acme.agents.jax import dqn\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/__init__.py\", line 18, in <module>\n    from acme.agents.jax.dqn.actor import behavior_policy\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in <module>\n    from acme.agents.jax import actor_core as actor_core_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in <module>\n    from acme.jax import networks as networks_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/__init__.py\", line 45, in <module>\n    from acme.jax.networks.multiplexers import CriticMultiplexer\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/multiplexers.py\", line 20, in <module>\n    from acme.jax import utils\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/utils.py\", line 190, in <module>\n    devices: Optional[Sequence[jax.xla.Device]] = None,\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 53, in getattr\n    raise AttributeError(f\"module {module!r} has no attribute {name!r}\")\nAttributeError: module 'jax' has no attribute 'xla'\nhere is my pip freeze list on this public gist: acme pip list\nI looked into this github issue: jax xla attribute issue\n@jakevdp, any updated comment or possible workaround for this jax.xla issue? thanks",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt sounds like you have too new a JAX version for the framework code you are using. I'd try installing an older version; e.g.\npip install --upgrade \"jax[cuda12_pip]<0.4.24\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSee JAX installation for more installation options.\nIf you're hoping to update the framework code for compatibility with more recent JAX versions, you might find replacements for previous functionality in jax.extend.linear_util."
        ],
        "link": "https://stackoverflow.com/questions/78372618/acme-error-attributeerror-module-jax-has-no-attribute-linear-util"
    },
    {
        "title": "How can we cast a `ctypes.POINTER(ctypes.c_float)` to `int`? [duplicate]",
        "question": "This question already has answers here:\nGet the memory address pointed to by a ctypes pointer (2 answers)\nClosed last year.\nI think this is a simple task, but I could not find a solution on the web to this. I have a external C++ library, which I'm using in my Python code, returning a ctypes.POINTER(ctypes.c_float) to me. I want to pass an array of these pointers to a jax.vmap function. The problem is that jax does not accept the ctypes.POINTER(ctypes.c_float) type. So, can I somehow cast this pointer to an ordinary int. Technically, this is clearly possible. But how do I do this in Python?\nHere is an example:\nlib = ctypes.cdll.LoadLibrary(lib_path)\nlib.foo.argtypes = None\nlib.foo.restype = ctypes.POINTER(ctypes.c_float)\n\nbar = jax.vmap(lambda : dummy lib.foo())(jax.numpy.empty(16))\n\nx = jax.numpy.empty(16, 256, 256, 1)\ny = jax.vmap(lib.bar, in_axes = (0, 1))(x, bar)\nSo, I want to invoke lib.foo 16-times so that I have an array bar containing all the pointers. Then I want to invoke another library function lib.bar which expects bar together with another (batched) parameter x.\nThe problem is that jax claims that ctypes.POINTER(ctypes.c_float) is not a valid jax type. This is why I think the solution is to cast the pointers to ints and store those ints in bar instead.",
        "answers": [
            "Listing:\n[SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer) - a common pitfall when working with CTypes (calling functions)\n[Python.Docs]: ctypes - A foreign function library for Python\nHere's a piece of code exemplifying how to handle pointers and their addresses. The trick is to use ctypes.addressof (documented in the 2nd URL).\ncode00.py:\n#!/usr/bin/env python\n\nimport ctypes as cts\nimport sys\n\n\nCType = cts.c_float\nCTypePtr = cts.POINTER(CType)\n\n\ndef ctype_pointer(seq):  # Helper\n    CTypeArr = (CType * len(seq))\n    ctype_arr = CTypeArr(*seq)\n    return cts.cast(ctype_arr, CTypePtr)\n\n\ndef pointer_elements(addr, count):  # Helper\n    return tuple(CType.from_address(addr + i * cts.sizeof(CType)).value for i in range(count))\n\n\ndef main(*argv):\n    seq = (2.718182, -3.141593, 1.618034, -0.618034, 0)\n    ptr = ctype_pointer(seq)\n    print(f\"Pointer: {ptr}\")\n    print(f\"\\nPointer elements: {tuple(ptr[i] for i in range(len(seq)))}\")  # Check if pointer has correct data\n    ptr_addr = cts.addressof(ptr.contents)  # @TODO - cfati: Straightforward\n    print(f\"\\nAddress: {ptr_addr} (0x{ptr_addr:016X})\\nElements from address: {pointer_elements(ptr_addr, len(seq))}\")\n    ptr_addr0 = cts.cast(ptr, cts.c_void_p).value  # @TODO - cfati: Alternative\n    print(f\"\\nAddresses match: {ptr_addr == ptr_addr0}\")\n\n\nif __name__ == \"__main__\":\n    print(\n        \"Python {:s} {:03d}bit on {:s}\\n\".format(\n            \" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n            64 if sys.maxsize > 0x100000000 else 32,\n            sys.platform,\n        )\n    )\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\nNotes:\nAlthough it adds a bit of complexity, I introduced the CType \"layer\" to show that it should work with any type, not just float (as long as the values in the sequence are of that type)\nThe only truly relevant lines are those marked with @TODO\nOutput:\n(py_pc064_03.08_test0_lancer) [cfati@cfati-5510-0:/mnt/e/Work/Dev/StackExchange/StackOverflow/q078366208]> python ./code00.py \nPython 3.8.19 (default, Apr  6 2024, 17:58:10) [GCC 11.4.0] 064bit on linux\n\nPointer: <__main__.LP_c_float object at 0x7203e97e7d40>\n\nPointer elements: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddress: 125361127594576 (0x00007203E97A9A50)\nElements from address: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddresses match: True\n\nDone."
        ],
        "link": "https://stackoverflow.com/questions/78366208/how-can-we-cast-a-ctypes-pointerctypes-c-float-to-int"
    },
    {
        "title": "Simultaneously going over different kinds of data with Keras training",
        "question": "In a regression task I'm given the following data:\nInput vectors with a known label. MSE loss should be used between the precidtion and the label.\nPairs of input vectors without a label, for which it is known that the model should give similar results. MSE loss should be used between the two predictions.\nWhat's the right way to fit a Keras model with these two kinds of data simultaneously?\nIdeally, I'd like the train loop to iterate the two kinds in an interleaved way - a superivsed (1) batch and then a self-supervised (2) batch, then supervised again etc.\nIf it matters, I'm using the Jax backend. Keras version 3.2.1.",
        "answers": [
            "I eventually found a trick that solved it for my case without too many customizations.\nBut if you do need to pass different kinds of data for training, I don't think there's an easy answer as for today.\nIt should be possible though to write your own training loop, and use any structure that you want for the data and labels. In this case you might also want to use the trainer pattern, implementing a custom version of keras.src.backend.jax.trainer.JAXTrainer."
        ],
        "link": "https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training"
    },
    {
        "title": "Flax neural network with nans in the outputs",
        "question": "I am training a neural network using Flax. My training data has a significant number of nans in the outputs. I want to ignore these and only use the non-nan values for training. To achieve this, I have tried to use jnp.nanmean to compute the losses, i.e.:\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    return jnp.nanmean((pred - targets) ** 2)\n\ndef train_step(state, inputs, targets):\n    loss, grads = jax.value_and_grad(nanloss)(state.params, inputs, targets)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\nHowever, after one training step the loss is nan.\nIs what I am trying to achieve possible? If so, how can I fix this?",
        "answers": [
            "I suspect you are hitting the issue discussed here: JAX FAQ: gradients contain NaN where using where. You've handled the NaNs in the computation itself, but they're still sneaking into the gradient due to how autodiff is implemented.\nIf this is in fact the issue, you can fix this by filtering the values before computing the loss; for example like this:\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    mask = jnp.isnan(pred) | jnp.isnan(targets)\n    pred = jnp.where(mask, 0, pred)\n    targets = jnp.where(mask, 0, targets)\n    return jnp.mean((pred - targets) ** 2, where=~mask)"
        ],
        "link": "https://stackoverflow.com/questions/78332120/flax-neural-network-with-nans-in-the-outputs"
    },
    {
        "title": "Jax ValueError: Incompatible shapes for broadcasting: shapes",
        "question": "I'm trying to write a weighted cross-entropy loss to train my model with Jax. However, I think there are some issues with my input dimension. Here are my codes:\nimport jax.numpy as np\nfrom functools import partial\nimport jax\n\n@partial(np.vectorize, signature=\"(c),(),()->()\")\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[0])\n    return -np.sum(weights* logits*one_hot_label)\n\nlogits=np.array([[1,2,3,4,5,6,7],[2,3,4,5,6,7,8]])\nlabels=np.array([1,2])\nweights=np.array([1,2,3,4,5,6,7])\nprint(weighted_cross_entropy_loss(logits,label,weights))\nHere are my error messages:\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 147, in broadcast_shapes\n    return _broadcast_shapes_cached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 284, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 277, in cached\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 153, in _broadcast_shapes_cached\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/PATH/test.py\", line 15, in <module>\n    print(weighted_cross_entropy_loss(a,label,weights))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 274, in wrapped\n    broadcast_shape, dim_sizes = _parse_input_dimensions(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 123, in _parse_input_dimensions\n    broadcast_shape = lax.broadcast_shapes(*shapes)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 149, in broadcast_shapes\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\nI'm expecting a single number that represents the cross-entropy loss between logits and labels.\nI'm fairly new to this, can somebody tell me what is going on? Any help is appreciated.",
        "answers": [
            "label is length 2, and weights is length 7, which means they cannot be broadcast together.\nIt's not clear to me from your question what your expected outcome was, but you can read more about how broadcasting works in NumPy (and in JAX, which implements NumPy's semantics) at https://numpy.org/doc/stable/user/basics.broadcasting.html.\nEdit: it looks like this is the operation you were aiming for:\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[1])\n    return -np.sum(weights * logits * one_hot_label)\nSince you want a single scalar output, I don't think vectorize is the right mechanism to use here."
        ],
        "link": "https://stackoverflow.com/questions/78323919/jax-valueerror-incompatible-shapes-for-broadcasting-shapes"
    },
    {
        "title": "Stable diffusion: AttributeError: module 'jax.random' has no attribute 'KeyArray'",
        "question": "When I run the stable diffusion on colab https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\nwith no modification, it fails on the line\nfrom diffusers import StableDiffusionPipeline \nThe error log is\nAttributeError: module 'jax.random' has no attribute 'KeyArray'\nHow can I fix this or any clue ?\nThe import should work, the ipynb should run with no error.",
        "answers": [
            "jax.random.KeyArray was deprecated in JAX v0.4.16 and removed in JAX v0.4.24. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier.\nYou can install JAX v0.4.23 with GPU support like this:\npip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nor, if you prefer targeting a local CUDA installation, like this:\npip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nFor more information on GPU installation, see JAX Installation: NVIDIA GPU.\nFrom the colab tutorial, update the second segment into:\n!pip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n!pip install diffusers==0.11.1\n!pip install transformers scipy ftfy accelerate",
            "# Change this\n# !pip install diffusers==0.11.1\n\n# To just\n!pip install diffusers \nIf you've already run pip install in your Colab runtime, you'll need to either disconnect and open a new runtime (my recommendation) or use  --upgrade.\nDiffusers v0.11.1 is now over 18 months old, and the notebook works with current v0.29.0 without any other changes. Instead of using an old version of diffusers, requiring an old version of jax, we can use the latest versions.",
            "In the end, we need to downgrade the jax, Try each from the lateset to ealier, and luckily it works for\njax==0.4.23 jaxlib==0.4.23"
        ],
        "link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray"
    },
    {
        "title": "AttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'",
        "question": "I'm trying to run a model written in jax, https://github.com/lindermanlab/S5. However, I ran into some error that says\n   Traceback (most recent call last):\n  File \"/Path/run_train.py\", line 101, in <module>\n    train(parser.parse_args())\n  File \"/Path/train.py\", line 144, in train\n    state = create_train_state(model_cls,\n  File \"/Path/train_helpers.py\", line 135, in create_train_state\n    params = variables[\"params\"].unfreeze()\nAttributeError: 'dict' object has no attribute 'unfreeze'\nI tried to replicate this error by\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\n\nmodel = nn.Dense(features=3)\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1, 2)))\nparams_unfrozen = flax.traverse_util.unfreeze(params)\nAnd the error reads:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'\nI'm using:\nflax 0.7.4\njax 0.4.13\njaxlib 0.4.13+cuda12.cudnn89\nI think this is an issue relating to the version of flax, but does anyone know what exactly is going on? Any help is appreciated. Let me know if you need any further information",
        "answers": [
            "unfreeze is a method of Flax's FrozenDict class: (See FrozenDict.unfreeze). It appears that you have passed a Python dict where a FrozenDict is expected.\nTo fix this, you should ensure that variables['params'] is a FrozenDict, not a dict.\nRegarding the error in your attempted replication: flax.traverse_util does not define an unfreeze function, but this seems unrelated to the original problem."
        ],
        "link": "https://stackoverflow.com/questions/78256559/attributeerror-module-flax-traverse-util-has-no-attribute-unfreeze"
    },
    {
        "title": "jax: How do we solve the error: pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0?",
        "question": "I'm trying to run this simple introduction to score-based generative modeling. The code is using flax.optim, which seems to be moved to optax meanwhile (https://flax.readthedocs.io/en/latest/guides/converting_and_upgrading/optax_update_guide.html).\nI've made a copy of the colab code with the changes I think needed to be made (I'm only unsure how I need to replace optimizer = flax.jax_utils.replicate(optimizer)).\nNow, in the training section, I get the error\npmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nat the line loss, params, opt_state = train_step_fn(step_rng, x, params, opt_state). This obviously comes from the return jax.pmap(step_fn, axis_name='device') in the \"Define the loss function\" section.\nHow can I fix this error? I've googled it, but have no idea what's going wrong here.",
        "answers": [
            "This happens because you are passing a scalar argument to a pmapped function. For example:\nimport jax\nfunc = lambda x: x ** 2\npfunc = jax.pmap(func)\n\npfunc(1.0)\n# ValueError: pmap was requested to map its argument along axis 0, which implies\n# that its rank should be at least 1, but is only 0 (its shape is ())\nIf you want to operate on a scalar, you should use the function without wrapping it in pmap:\nfunc(1.0)\n# 1.0\nAlternatively, if you want to use pmap, you should operate on an array whose leading dimension matches the number of devices:\nnum_devices = len(jax.devices())\nx = jax.numpy.arange(num_devices)\npfunc(x)\n# Array([ 0,  1,  4,  9, 16, 25, 36, 49], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78244620/jax-how-do-we-solve-the-error-pmap-was-requested-to-map-its-argument-along-axi"
    },
    {
        "title": "what are the numbers in the operation names when profiling an application",
        "question": "What are the numbers in \"fusion_2\", \"fusion_4\"? Where do they come from? Thank you!",
        "answers": [
            "These numbers exist to de-duplicate the names of generated HLO operations. The first fusion operation created by the compiler is called fusion, the next is fusion_2, then fusion_3, and so on.\nNote that the order of creation does not necessarily match the order of execution."
        ],
        "link": "https://stackoverflow.com/questions/78236312/what-are-the-numbers-in-the-operation-names-when-profiling-an-application"
    },
    {
        "title": "Cannot import name 'linear_util' from 'jax'",
        "question": "I'm trying to reproduce the experiments of the S5 model, https://github.com/lindermanlab/S5, but I encountered some issues when solving the environment. When I'm running the shell script./run_lra_cifar.sh, I get the following error\nTraceback (most recent call last):\n  File \"/Path/S5/run_train.py\", line 3, in <module>\n    from s5.train import train\n  File \"/Path/S5/s5/train.py\", line 7, in <module>\n    from .train_helpers import create_train_state, reduce_lr_on_plateau,\\\n  File \"/Path/train_helpers.py\", line 6, in <module>\n    from flax.training import train_state\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py\", line 19, in <module>\n    from . import core\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py\", line 15, in <module>\n    from .axes_scan import broadcast\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py\", line 22, in <module>\n    from jax import linear_util as lu\nImportError: cannot import name 'linear_util' from 'jax' (/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py)\nI'm running this on an RTX4090 and my CUDA version is 11.8. My jax version is 0.4.25 and jaxlib version is 0.4.25+cuda11.cudnn86\nI first tried to install the dependencies using the author's\npip install -r requirements_gpu.txt\nHowever, this doesn't seem to work in my case since I can't evenimport jax. So I installed jax according to the instructions on https://jax.readthedocs.io/en/latest/installation.html by typing\npip install --upgrade pip\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo far I've tried:\nUsing a older GPU(3060 and 2070)\nDowngrading python to 3.9\nDoes anyone know what could be wrong? Any help is appreciated",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt appears that flax is the source of the linear_util import, meaning that you are using an older flax version with a newer jax version.\nTo fix your issue, you'll either need to install an older version of JAX which still has jax.linear_util, or update to a newer version of flax which is compatible with more recent JAX versions."
        ],
        "link": "https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax"
    },
    {
        "title": "Numpyro AR(1) mean switching model sampling incongrouencies",
        "question": "I'm trying to estimate an AR(1) process y with a switching mean according to a latent state S =0,1 that evolves as a markov process with fixed transition probabilities (as in here). In short, it takes the form:\ny_t - mu_{0/1} = phi * (y_{t-1} - mu_{0/1})+ epsilon_t\nwhere mu_0 would be used if state_t = 0 and mu_1 if state_t =1. I'm using jax/numpyro with DiscreteHMCGibbs (although normal NUTS with latent state enumeration yields the same result) but I can't seem to have the sampler work properly. From all diagnostics I run, it seems that all hyperparameters are stuck at initialization value, and summary returns accordingly with all std==0. Here below I have a MWE that reproduces my problem. Is there an obvious mistake I am making in the implementation?\nMWE:\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import MCMC, NUTS,DiscreteHMCGibbs\nfrom jax import random, pure_callback\nimport jax\nimport numpy as np\n\ndef generate_synthetic_data(T=100, mu=[0, 5], phi=0.5, sigma=1.0, p=np.array([[0.95, 0.05], [0.1, 0.9]])):\n    states = np.zeros(T, dtype=np.int32)\n    y = np.zeros(T)\n    current_state = np.random.choice([0, 1], p=[0.5, 0.5])\n    states[0] = current_state\n    y[0] = np.random.normal(mu[current_state], sigma)\n\n    for t in range(1, T):\n        current_state = np.random.choice([0, 1], p=p[current_state,:])\n        states[t] = current_state\n        y[t] = np.random.normal(mu[current_state] + phi * (y[t-1] - mu[current_state]), sigma)\n\n    return y, states\n\n\ndef mean_switching_AR1_model(y):\n    T = len(y)\n    phi = numpyro.sample('phi', dist.Normal(0, 1))\n    sigma = numpyro.sample('sigma', dist.Exponential(1))\n    \n    \n    with numpyro.plate('state_plate', 2):\n        mu = numpyro.sample('mu', dist.Normal(0, 5))\n        p = numpyro.sample('p', dist.Dirichlet(jnp.ones(2)))\n\n    probs_init = numpyro.sample('probs_init', dist.Dirichlet(jnp.ones(2)))\n    s_0 = numpyro.sample('s_0', dist.Categorical(probs_init))\n\n    def transition_fn(carry, y_t):\n        prev_state = carry\n        state_probs = p[prev_state]\n        state = numpyro.sample('state', dist.Categorical(state_probs))\n\n        mu_state = mu[state]\n        y_mean = mu_state + phi * (y_t - mu_state)\n        y_next = numpyro.sample('y_next', dist.Normal(y_mean, sigma), obs=y_t)\n        return state, (state, y_next)\n\n    _ , (signal, y)=scan(transition_fn, s_0, y[:-1], length=T-1)\n    return (signal, y)\n\n# Synthetic data generation\nT = 1000\nmu_true = [0, 3]\nphi_true = 0.5\nsigma_true = 0.25\ntransition_matrix_true = np.array([[0.95, 0.05], [0.1, 0.9]])\ny, states_true = generate_synthetic_data(T, mu=mu_true, phi=phi_true, sigma=sigma_true, p=transition_matrix_true)\n\n\nrng_key = random.PRNGKey(0)\nnuts_kernel = NUTS(mean_switching_AR1_model)\ngibbs_kernel = DiscreteHMCGibbs(nuts_kernel, modified=True)\n\n# Run MCMC\nmcmc = MCMC(gibbs_kernel, num_samples=1000, num_warmup=1000)\nmcmc.run(rng_key, y=y)\nmcmc.print_summary()",
        "answers": [
            "So it turns out there was indeed a pretty obvious mistake in the sense that I was not correctly carrying down y_{t-1} as part of the state variables. The following corrected transition functions yield the intended result without problems.\ndef transition_fn(carry, y_curr):\n    prev_state, y_prev = carry\n    state_probs = p[prev_state]\n    state = numpyro.sample('state', dist.Categorical(state_probs))\n\n    mu_state = mu[state]\n    y_mean = mu_state + phi * (y_prev - mu_state)\n    y_curr = numpyro.sample('y_curr', dist.Normal(y_mean, sigma), obs=y_curr)\n    return (state, y_curr), (state, y_curr)\n\n_, (signal, y) = scan(transition_fn, (s_0, y[0]), y[1:], length=T-1)"
        ],
        "link": "https://stackoverflow.com/questions/78209454/numpyro-ar1-mean-switching-model-sampling-incongrouencies"
    },
    {
        "title": "Slow JAX Optimization with ScipyBoundedMinimize and Optax - Seeking Speedup Strategies",
        "question": "I'm working on optimizing a model in jax that involves fitting a large observational dataset (4800 data points) with a complex model containing interpolation. The current optimization process using jaxopt.ScipyBoundedMinimize takes around 30 seconds for 100 iterations, with most of the time spent seemingly during or before the first iteration starts. You can find the relevant code snippet below. you can find the necessary data for the relevant code at the following link.\nnecessary data (idc, sg and cpcs)\nimport jax.numpy as jnp\nimport time as ela_time\nfrom jaxopt import ScipyBoundedMinimize\nimport optax\nimport jax\nimport pickle\n\n\nfile1 = open('idc.pkl', 'rb')\nidc = pickle.load(file1)\nfile1.close()\n\nfile2 = open('sg.pkl', 'rb')\nsg = pickle.load(file2)\nfile2.close()\n\nfile3 = open('cpcs.pkl', 'rb')\ncpcs = pickle.load(file3)\nfile3.close()\n\n\ndef model(fssc, fssh, time, rv, amp):\n\n    fssp = 1.0 - (fssc + fssh)\n\n    ivis = cpcs['common'][time]['ivis']\n    areas = cpcs['common'][time]['areas']\n    mus = cpcs['common'][time]['mus']\n\n    vels = idc['vels'].copy()\n\n    ldfs_phot = cpcs['line'][time]['ldfs_phot']\n    ldfs_cool = cpcs['line'][time]['ldfs_cool']\n    ldfs_hot = cpcs['line'][time]['ldfs_hot']\n\n    lps_phot = cpcs['line'][time]['lps_phot']\n    lps_cool = cpcs['line'][time]['lps_cool']\n    lps_hot = cpcs['line'][time]['lps_hot']\n\n    lis_phot = cpcs['line'][time]['lis_phot']\n    lis_cool = cpcs['line'][time]['lis_cool']\n    lis_hot = cpcs['line'][time]['lis_hot']\n\n    coeffs_phot = lis_phot * ldfs_phot * areas * mus\n    wgt_phot = coeffs_phot * fssp[ivis]\n    wgtn_phot = jnp.sum(wgt_phot)\n\n    coeffs_cool = lis_cool * ldfs_cool * areas * mus\n    wgt_cool = coeffs_cool * fssc[ivis]\n    wgtn_cool = jnp.sum(wgt_cool)\n\n    coeffs_hot = lis_hot * ldfs_hot * areas * mus\n    wgt_hot = coeffs_hot * fssh[ivis]\n    wgtn_hot = jnp.sum(wgt_hot)\n\n    prf = jnp.sum(wgt_phot[:, None] * lps_phot + wgt_cool[:, None] * lps_cool + wgt_hot[:, None] * lps_hot, axis=0)\n    prf /= wgtn_phot + wgtn_cool + wgtn_hot\n\n    prf = jnp.interp(vels, vels + rv, prf)\n\n    prf = prf + amp\n\n    avg = jnp.mean(prf)\n\n    prf = prf / avg\n\n    return prf\n\n\ndef loss(x0s, lmbd):\n\n    noes = sg['noes']\n\n    noo = len(idc['times'])\n\n    fssc = x0s[:noes]\n    fssh = x0s[noes: 2 * noes]\n    fssp = 1.0 - (fssc + fssh)\n    rv = x0s[2 * noes: 2 * noes + noo]\n    amp = x0s[2 * noes + noo: 2 * noes + 2 * noo]\n\n    chisq = 0\n    for i, itime in enumerate(idc['times']):\n        oprf = idc['data'][itime]['prf']\n        oprf_errs = idc['data'][itime]['errs']\n\n        nop = len(oprf)\n\n        sprf = model(fssc=fssc, fssh=fssh, time=itime, rv=rv[i], amp=amp[i])\n\n        chisq += jnp.sum(((oprf - sprf) / oprf_errs) ** 2) / (noo * nop)\n\n    wp = sg['grid_areas'] / jnp.max(sg['grid_areas'])\n\n    mem = jnp.sum(wp * (fssc * jnp.log(fssc / 1e-5) + fssh * jnp.log(fssh / 1e-5) +\n                    (1.0 - fssp) * jnp.log((1.0 - fssp) / (1.0 - 1e-5)))) / sg['noes']\n\n    ftot = chisq + lmbd * mem\n\n    return ftot\n\n\nif __name__ == '__main__':\n\n    # idc: a dictionary containing observational data (150 x 32)\n    # sg and cpcs: dictionaries with related coefficients\n\n    noes = sg['noes']\n    lmbd = 1.0\n    maxiter = 1000\n    tol = 1e-5\n\n    fss = jnp.ones(2 * noes) * 1e-5\n    x0s = jnp.hstack((fss, jnp.zeros(len(idc['times']) * 2)))\n\n    minx0s = [1e-5] * (2 * noes) + [-jnp.inf] * len(idc['times']) * 2\n    maxx0s = [1.0 - 1e-5] * (2 * noes) + [jnp.inf] * len(idc['times']) * 2\n\n    bounds = (minx0s, maxx0s)\n\n    start = ela_time.time()\n\n    optimizer = ScipyBoundedMinimize(fun=loss, maxiter=maxiter, tol=tol, method='L-BFGS-B',\n                                 options={'disp': True})\n    x0s, info = optimizer.run(x0s, bounds,  lmbd)\n\n    # optimizer = optax.adam(learning_rate=0.1)\n    # optimizer_state = optimizer.init(x0s)\n    #\n    # for i in range(1, maxiter + 1):\n    #\n    #     print('ITERATION -->', i)\n    #\n    #     gradients = jax.grad(loss)(x0s, lmbd)\n    #     updates, optimizer_state = optimizer.update(gradients, optimizer_state, x0s)\n    #     x0s = optax.apply_updates(x0s, updates)\n    #     x0s = jnp.clip(x0s, jnp.array(minx0s), jnp.array(maxx0s))\n    #     print('Objective function: {:.3E}'.format(loss(x0s, lmbd)))\n\n    end = ela_time.time()\n\n    print(end - start)   # total elapsed time: ~30 seconds\nHere's a breakdown of the relevant aspects:\nNumber of free parameters (x0s): 5263\nData: Observational data stored in idc dictionary (4800 data points)\nModel: Defined in model function, also utilizes interpolation\nOptimization methods tried:\njaxopt.ScipyBoundedMinimize with L-BFGS-B method (slow ~30 seconds, with most of the time spent during or just before the first iteration)\noptax.adam (too slow ~200 seconds)\nAttempted parallelization: I attempted to parallelize optax.adam, yet due to the inherent nature of the modeling, I couldn't succeed as the x0s couldn't be divided. (assuming I understood parallelization correctly)\nQuestions:\nWhat are potential reasons for the slowness before or during the first iteration in ScipyBoundedMinimize ?\nAre there alternative optimization algorithms in jax that might be faster for my scenario (large number of free parameters and data points, complex model with interpolation)?\nDid I misunderstand parallelization with optax.adam? Are there any strategies for potential parallelization in this case?\nAre there any code optimizations within the provided snippet that could improve performance (e.g., vectorization)?\nAdditional Information:\nHardware: Intel® Core™ i7-9750H CPU @ 2.60GHz × 12, 16 GiB RAM (laptop)\nSoftware: OS Ubuntu 22.04, Python 3.10.12, JAX 0.4.25, optax 0.2.1\nI'd appreciate any insights or suggestions to improve the optimization performance.",
        "answers": [
            "JAX code is Just-in-time (JIT) compiled, meaning that the long duration of the first step is likely related to compilation costs. The longer your code is, the more time it will take to compile.\nOne common issue leading to long compile times is the use of Python control flow such as for loops. JAX's tracing machinery essentially flattens out these loops (see JAX Sharp Bits: Control Flow). In your case, you loop over 4800 entries in your data structure, and thus are creating a very long and inefficient program.\nThe typical solution in a case like this is to rewrite your program using jax.vmap. Like most JAX constructs, this works best with a struct-of-arrays pattern rather than the array-of-structs pattern used in your data. So the first step to using vmap is to restructure your data in a way that JAX can use; it might look something like this:\nitimes = jnp.arange(len(idc['times']))\nprf = jnp.array([idc['data'][i]['prf'] for i in itimes])\nerrs = jnp.array([idc['data'][i]['errs'] for i in itimes])\n\nsprf = jax.vmap(model, in_axes=[None, None, 0, 0, 0])(fssc, fssh, itimes, rv, amp)\nchi2 = jnp.sum((oprf - sprf) / oprf_errs) ** 2) / len(times) / sprf.shape[1]\nThis will not work directly: you'll also have to restructure the data used by your model function into the struct-of-arrays style, but hopefully this gives you the general idea.\nNote also that this assumes that every entry of idc['data'][i]['prf'] and idc['data'][i]['errs'] has the same shape. If that's not the case, then I'm afraid your problem is not particularly well-suited to JAX's SPMD programming model, and there's not an easy way to work around the need for long compilations."
        ],
        "link": "https://stackoverflow.com/questions/78174997/slow-jax-optimization-with-scipyboundedminimize-and-optax-seeking-speedup-stra"
    },
    {
        "title": "Equivalent of `jax.lax.cond` for multiple boolean conditions",
        "question": "Currently jax.lax.cond works for one boolean condition. Is there a way to extend it to multiple boolean conditions?\nAs an example, below is an untraceable function:\ndef func(x):\n    if x < 0: return x\n    elif (x >= 0) & (x < 1): return 2*x\n    else: return 3*x\nHow to write this function in JAX in a traceable way?",
        "answers": [
            "One compact way to write something like this is using jnp.select:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef func(x):\n  return jnp.select([x < 0, x < 1], [x, 2 * x], default=3 * x)\n\nx = jnp.array([-0.5, 0.5, 1.5])\nprint(func(x))\n# [-0.5  1.   4.5]"
        ],
        "link": "https://stackoverflow.com/questions/78122820/equivalent-of-jax-lax-cond-for-multiple-boolean-conditions"
    },
    {
        "title": "\"The truth value of an array with more than one element is ambiguous\" when trying to train a new JAX+Equinox model a second time",
        "question": "TL;DR: I create a new instance of my equinox.Module model and fit it using Optax. Everything works fine. When I create a new instance of the same model and try to fit it from scratch, using the same code, same initial values, same everything, I get:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n...somewhere deep in Optax code. My code doesn't compare any arrays. The error message doesn't show where exactly the comparison happens. What's wrong?\nCode\n# 1. Import dependencies.\nimport jax; jax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as np, jax.random as rnd, equinox as eqx\nimport optax\n\n# 2. Define loss function. I'm fairly confident this is correct.\ndef npdf(x, var):\n    return np.exp(-0.5 * x**2 / var) / np.sqrt(2 * np.pi * var)\n\ndef mixpdf(x, ps, vars):\n    return ps.dot(npdf(x, vars))\n\ndef loss(model, series):\n    weights, condvars = model(series)\n    return -jax.vmap(\n        lambda x, vars: np.log(mixpdf(x, weights, vars))\n    )(series[1:], condvars[:-1]).mean()\n\n# 3. Define recurrent neural network.\nclass RNNCell(eqx.Module):\n    bias: np.ndarray\n    Wx: np.ndarray\n    Wh: np.ndarray\n    def __init__(self, ncomp: int, n_in: int=1, *, key: np.ndarray):\n        k1, k2, k3 = rnd.split(key, 3)\n        self.bias = rnd.uniform(k1, (ncomp, ))\n        self.Wx = rnd.uniform(k2, (ncomp, n_in))\n        self.Wh = 0.9 * rnd.uniform(k3, (ncomp, ))\n\n    def __call__(self, vars_prev, obs):\n        vars_new = self.bias + self.Wx @ obs + self.Wh * vars_prev\n        return vars_new, vars_new\n\nclass RNN(eqx.Module):\n    cell: RNNCell\n    logits: np.ndarray\n    vars0: np.ndarray = eqx.field(static=True)\n\n    def __init__(self, vars0: np.ndarray, n_in=1, *, key: np.ndarray):\n        self.vars0 = np.array(vars0)\n        K = len(self.vars0)\n        self.cell = RNNCell(K, n_in, key=key)\n        self.logits = np.zeros(K)\n\n    def __call__(self, series: np.ndarray):\n        _, hist = jax.lax.scan(self.cell.__call__, self.vars0, series**2)\n        return jax.nn.softmax(self.logits), abs(hist)\n\n    def condvar(self, series):\n        weights, variances = self(series)\n        return variances @ weights\n\n    def predict(self, series: np.ndarray):\n        return self.condvar(series).flatten()[-1]\n\n# 4. Training/fitting code.\ndef fit(model, logret, nepochs: int, optimizer, loss):\n    loss_and_grad = eqx.filter_value_and_grad(loss)\n    \n    @eqx.filter_jit\n    def make_step(model, opt_state):\n        loss_val, grads = loss_and_grad(model, logret)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss_val, model, opt_state\n\n    opt_state = optimizer.init(model)\n    for epoch in range(nepochs):\n        loss_val, model, opt_state = make_step(model, opt_state)\n    print(\"Works!\")\n    return model\n\ndef experiment():\n    series = rnd.normal(rnd.PRNGKey(8), (100, 1))\n    model = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\n    return fit(model, series, 100, optax.adam(0.01), loss)\n\n# 5. Run the exact same code twice.\nexperiment() # 1st call, works\nexperiment() # 2nd call, error\nError message\n> python my_RNN.py\nWorks!\nTraceback (most recent call last):\n  File \"/Users/forcebru/test/my_RNN.py\", line 75, in <module>\n    experiment() # 2nd call, error\n    ^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 72, in experiment\n    return fit(model, series, 100, optax.adam(0.01), loss)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 65, in fit\n    loss_val, model, opt_state = make_step(model, opt_state)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 206, in __call__\n    return self._call(False, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_module.py\", line 935, in __call__\n    return self.__func__(self.__self__, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 200, in _call\n    out = self._cached(dynamic_donate, dynamic_nodonate, static)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 495, in common_infer_params\n    jaxpr, consts, out_shardings, out_layouts_flat, attrs_tracked = _pjit_jaxpr(\n                                                                    ^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1150, in _pjit_jaxpr\n    jaxpr, final_consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n                                                   ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 350, in memoized_fun\n    ans = call(fun, *args)\n          ^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1089, in _create_pjit_jaxpr\n    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/profiler.py\", line 336, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2314, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2336, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 49, in fun_wrapped\n    out = fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 59, in make_step\n    updates, opt_state = optimizer.update(grads, opt_state)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/combine.py\", line 59, in update_fn\n    updates, new_s = fn(updates, s, params, **extra_args)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/base.py\", line 337, in update\n    return tx.update(updates, state, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/transform.py\", line 369, in update_fn\n    mu_hat = bias_correction(mu, b1, count_inc)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 491, in common_infer_params\n    canonicalized_in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 4, in __eq__\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 745, in __bool__\n    check_bool_conversion(self)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 662, in check_bool_conversion\n    raise ValueError(\"The truth value of an array with more than one element is \"\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nProblem\nThe error message says File \"<string>\", line 4, in __eq__, which doesn't help.\nIt refers to the line mu_hat = bias_correction(mu, b1, count_inc) in Optax code, but as far as I understand, it doesn't compare any arrays.\nIt also refers to JAX code that's supposedly responsible for JIT compilation, but this seems outside my control.\nIs there a bug in my model definition (RNNCell or RNN)? Did I implement the training loop wrong? I basically copied it straight from Equinox docs, so it should be fine. Why does it work when I call experiment() the first time, but not the second?",
        "answers": [
            "It appears this is a bug in equinox. The function _process_in_axis_resources is decorated in functools.lru_cache, meaning that all inputs are checked for equality with arguments from the previous call. On the second run, this triggers a call to equinox.Module.__eq__, which raises the error. You can see this problem by doing the equality check directly:\nmodel = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel2 = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel == model2\n# ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nI would suggest reporting this bug at https://github.com/patrick-kidger/equinox/issues\nYou could probably work around this issue by not storing a numpy array (vars0) as a static attribute. I suspect that equinox assumes that all static attributes are hashable, and numpy arrays are not.\nEdit: I just checked, and changing this:\nvars0: np.ndarray = eqx.field(static=True)\nto this:\nvars0: np.ndarray\nresolves the issue.\nEdit 2: Indeed it looks like static fields in equinox must be hashable, so this is not an equinox bug but rather a usage error (see the discussion at https://github.com/patrick-kidger/equinox/issues/154#issuecomment-1561735995). You might try storing vars0 as a tuple (which is hashable) rather than an array (which isn't)."
        ],
        "link": "https://stackoverflow.com/questions/78074623/the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous-when-tryin"
    },
    {
        "title": "Jax scan with dynamic number of iterations",
        "question": "I wanted to perform a scan with a dynamic number of iterations. To accomplish that, I want to recompile the function each time when iters_to_do changes.\nTo avoid a huge slowdown, I'll be using a recompilation_cache but that's beside the point.\nHowever, when I mark the argument in @partial(jax.jit) I'm still obtaining a concretization error:\n@partial(jax.jit, static_argnums=(3))\ndef iterate_for_steps(self,\n                        interim_thought: Array, \n                        mask: Array,\n                        iters_to_do: int, \n                        input_arr: Array, \n                        key: PRNGKeyArray) -> Array:\n\n    # These are constants\n    input_arr = input_arr.astype(jnp.bfloat16)\n    interim_thought = interim_thought.astype(jnp.bfloat16)\n    \n    def body_fun(i: int, thought: Array) -> Array:\n        latent = jnp.concatenate([thought, input_arr], axis=-1).astype(jnp.bfloat16)\n        latent = self.main_block(latent, input_arr, mask, key).astype(jnp.bfloat16)\n        latent = jax.vmap(self.post_ln)(latent).astype(jnp.bfloat16)  # LN to keep scales tidy\n\n        return latent\n    \n    iters_to_do = iters_to_do.astype(int).item()\n    final_val = jax.lax.scan(body_fun, interim_thought, xs=None, length=iters_to_do)\n    \n    return final_val\nFull traceback is here.\nI've tried marking multiple arguments with @partial but to no avail.\nI'm not sure how to approach debugging this - with a python debugger, I'm getting no help apart from the fact that its definitely a tracer.\nMRE\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\ninit = jnp.ones((5,))\niterations = jnp.array([1, 2, 3])\n\n@partial(jax.jit, static_argnums=(0,))\ndef iterate_for_steps(iters: int):\n    def body_fun(carry):\n        return carry * 2\n    \n    iters = iters.astype(int)\n    output = jax.lax.scan(body_fun, init, xs=None, length=iters)\n    \n    return output\n\nprint(jax.vmap(iterate_for_steps)(iterations))",
        "answers": [
            "First of all, the number of iterations in a scan must be static. If you want something similar to scan that allows a dynamic number of iterations, you can take a look at while_loop.\nRegarding your code: in isolation, your fix of marking iters_to_do as static using static_argnums is probably roughly the right idea, so long as you are passing a static int in this position when you call the function.\nBut the fact that you are calling the astype array method in your function (in iters_to_do.astype(int).item()) and getting a ConcretizationError rather than an AttributeError makes me think that the error you linked to is not coming from the code as pasted in your question.\nTo help address this discrepancy, I'd suggest trying to construct a minimal reproducible example of the problem you're having. Without that, any answer to your question is going to require too much guesswork regarding what code you're actually executing.",
            "One can use equinox's (internal as of right now) while_loop implementation which would also be able to handle a dynamic amount of iterations with checkpointing to reduce memory usage.\nNote that this can be used as a drop-in replacement to jax's native while_loop. One can also use equinox's eqx.internal.scan if they wish to leverage similar checkpointing with scan."
        ],
        "link": "https://stackoverflow.com/questions/78070050/jax-scan-with-dynamic-number-of-iterations"
    },
    {
        "title": "How to train a model using gradient descent with multioutput (vector-valued) loss function in JAX?",
        "question": "I am trying to train a model that has two outputs with gradient descent. My cost function therefore returns two errors. What is the typical way to deal with this problem?\nI've seen mentions here and there of this problem, but I haven't come up with a satisfactory solution.\nThis is a toy example that reproduces my problem:\nfrom jax import jit, random, grad\nimport optax\n\n\n@jit\ndef my_model(forz, params):\n    a, b = params\n\n    a_vect = a + forz**b\n    b_vect = b + forz**a\n\n    return a_vect, b_vect*50.\n\n\n@jit\ndef rmse(predictions, targets):\n\n    rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n    return rmse\n\n\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a, loss_b\n\n\ngrad_myloss = jit(grad(my_loss, argnums=1))\n\n# synthetic true data\nkey = random.PRNGKey(758493)\nforz = random.uniform(key, shape=(1000,))\n\ntrue_params = [8.9, 6.6]\ntrue_a, true_b = my_model(forz, true_params)\n\n# Train\nmodel_params = random.uniform(key, shape=(2,))\noptimizer = optax.adabelief(1e-1)\nopt_state = optimizer.init(model_params)\n\nfor i in range(1000):\n\n    grads = grad_myloss(forz, model_params, true_a, true_b)  # this fails\n    updates, opt_state = optimizer.update(grads, opt_state)\n    model_params = optax.apply_updates(model_params, updates)\nI understand that either the two errors has to be somehow aggregated to a single one implementing some kind of normalization to the losses (my output vectors have non-comparable units),\n@jit\ndef normalized_rmse(predictions, targets):\n   std_dev_targets = jnp.std(targets)\n   rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n   return rmse/std_dev_targets\n\n\n@jit\ndef my_loss_single(forz, params, true_a, true_b):\n\n   sim_a, sim_b = my_model(forz, params)\n\n   loss_a = normalized_rmse(sim_a, true_a)\n   loss_b = normalized_rmse(sim_b, true_b)\n\n   return jnp.sqrt((loss_a ** 2) + (loss_b * 2)) \nor I should use the Jacobian matrix (jacrev) somehow?",
        "answers": [
            "optax, like most optimization frameworks, is only able to optimize a single-valued loss function. You should decide what single-valued loss makes sense for your particular problem. A good option given the RMS form of your individual losses might be the square sum:\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a ** 2 + loss_b ** 2\nWith this change, your code executes without an error."
        ],
        "link": "https://stackoverflow.com/questions/78044014/how-to-train-a-model-using-gradient-descent-with-multioutput-vector-valued-los"
    },
    {
        "title": "Using Orbax to checkpoint flax `TrainState` with new `CheckpointManager` API",
        "question": "Context\nThe Flax docs describe how to checkpoint a flax.training.train_state.TrainState with orbax. In a nutshell, you set up a orbax.checkpoint.CheckpointManager which keeps track of checkpoints. Next, you use the CheckpointManager to save the state to disk. Summarising the code snippets from the Flax docs:\nimport orbax\n\n# <-- Code building an empty and a full chkpt. -->.\nabstract_chkpt = ...\nchkpt = ...\n\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\nsave_args = orbax_utils.save_args_from_target(ckpt)\n\noptions = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_manager = orbax.checkpoint.CheckpointManager(\n    '/tmp/flax_ckpt/orbax/managed', orbax_checkpointer, options)\n\n# Save and restore a checkpoint.\ncheckpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args})\ncheckpoint_manager.restore(1, items=abstract_ckpt)\nThe notebook provided by the Flax docs does what I want: periodically track TrainState, which can then be restored. However, when executing the code provided by the Flax docs warn that this orbax checkpoint API is deprecated:\nWARNING:absl:Configured CheckpointManager using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\nThe link indicated by the error message gives some pointers how to use the new orbax.checkpoint.CheckpointManager.\nQuestion\nHow do I save and restore a Flax TrainState with the new orbax.checkpoint.CheckpointManager API?\nHere is my failed attempt (based on the Orbax migration instructions) at saving and restoring a trivial flax.training.train_state.TrainState:\nimport orbax.checkpoint as obc\nfrom flax.training.train_state import TrainState\n\nabstract_ckpt = TrainState(step=0, apply_fn=lambda _: None, params={}, tx={}, opt_state={})\nckpt = abstract_ckpt.replace(step=1)\n\n# Set up the checkpointer.\noptions = obc.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_dir = obc.test_utils.create_empty('/tmp/checkpoint_manager')\ncheckpoint_manager = obc.CheckpointManager(checkpoint_dir, options=options)\nsave_args = obc.args.StandardSave(abstract_ckpt)\n\n# Do actual checkpointing.\ncheckpoint_manager.save(1, ckpt, args=save_args)\n\n# Restore checkpoint.\nrestore_args = obc.args.StandardRestore(abstract_ckpt)\nrestored_ckpt = checkpoint_manager.restore(1, args=restore_args)\n\n# Verify if it is correctly restored.\nassert ckpt.step == restored_ckpt.step  # AssertionError\nMy guess would be that the problem relates to save_args, but I haven't managed to pinpoint the problem and figure out a fix. Any suggestions how to correctly restore the checkpoint using the new CheckpointManager API?",
        "answers": [
            "You created save_args = ocp.args.StandardSave(abstract_ckpt) instead of save_args = ocp.args.StandardSave(ckpt), so you're just saving the wrong thing.\nAlso note that checkpoint_dir = ocp.test_utils.create_empty('/tmp/checkpoint_manager') is a bit unnecessary - it's just a test utility for deleting a directory if it already exists - makes running our colabs a bit easier. Probably you shouldn't need to use it in real life, as the create option in CheckpointManager will create the directory for you."
        ],
        "link": "https://stackoverflow.com/questions/78033458/using-orbax-to-checkpoint-flax-trainstate-with-new-checkpointmanager-api"
    },
    {
        "title": "JAX jax.grad on simple function that takes an array: `ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected`",
        "question": "I'm trying to implement this function and use JAX to automatically build the gradient function:\n$f(x) = \\sum\\limits_{k=1}^{n-1} [100 (x_{k+1} - x_k^2)^2 + (1 - x_k)^2]$\n(sorry, I don't know how to format math on stackoverflow. Some sister sites allow TeX, but apparently this site does not?)\nimport jax\nimport jax.numpy as jnp\n\n# x is an array, which does not handle type hints well.\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].item()\n        xip1 = x[i].item()\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\n\n\n# with n=2.\ndef rosenbrock2(x: any) -> float:\n    return rosenbrock(2, x)\n\n\ngrad_rosenbrock2 = jax.grad(rosenbrock2)\n\nx = jnp.array([-1.2, 1], dtype=jnp.float32).reshape(2,1)\n\n# this line fails with the error given below\ngrad_rosenbrock2(x)\nThis last line results in:\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[1].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nI'm trying to follow the docs, and I'm confused. This is my first time using JAX or Autograd, can someone help me resolve this? Thanks!",
        "answers": [
            "The problem is that the .item() method attempts to convert an array to a static Python scalar, and since you have traced arrays within your grad transformation, conversion to a static value is not possible.\nWhat you need here is to convert a size-1 array to a scalar array, which you can do using .reshape(()):\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].reshape(())\n        xip1 = x[i].reshape(())\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\nFor more background on jax transformations and traced arrays, I'd recommend How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78030853/jax-jax-grad-on-simple-function-that-takes-an-array-concretizationtypeerror-a"
    },
    {
        "title": "How to return last index with jnp.where in jit function",
        "question": "Say I have two arrays:\nz = jnp.array([[5.55751118],\n              [5.18212974],\n              [4.35981727],\n              [3.4559711 ],\n              [3.35750248],\n              [2.65199945],\n              [2.02298999],\n              [1.59444971],\n              [0.80865185],\n              [0.77579791]])\n\nz1 = jnp.array([[ 1.58559484],\n               [ 3.79094097],\n               [-0.52712522],\n               [-1.0178286 ],\n               [-3.51076985],\n               [ 1.30108161],\n               [-1.29824303],\n               [-0.19209007],\n               [ 0.37451138],\n               [-2.33619987]])\nI would like to start at the first row in array z and find where in the second matrix a second value is within a threshold of this value.\nexample without @jit: I would like to return the last index of array z1. Value should be -3.51x\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nWhen using JIT (as needed in a larger scale model)\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nthis error is produced:\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThe error occurred while tracing the function test at /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:9 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:bool[10,1] = ge b c\n    from line /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:11:10 (test)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
        "answers": [
            "The problem is that jnp.where returns a dynamically-sized array, and JAX transformations like jit are not compatible with dynamically-sized arrays (See JAX Sharp Bits: Dynamic Shapes). You can pass a size argument to jnp.where to make the result statically sized. Since we don't know how many elements will be returned, we can choose the maximum possible number of returned elements, which is idx.shape[0]. Since the result will be padded with zeros, the maximum index will give what you're looking for:\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx, size=idx.shape[0])[0].max()]\n    return val\ntest()"
        ],
        "link": "https://stackoverflow.com/questions/78030406/how-to-return-last-index-with-jnp-where-in-jit-function"
    },
    {
        "title": "Can jax.vmap() do a hstack()?",
        "question": "As the title says, I currently manually hstack() the first axis of a 3D array returned by jax.vmap(). In my code, the copy operation in hstack() is a currently a speed bottleneck. Can I avoid this by instructing jax.vmap() to do this directly?\nHere is a simplified example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(a, b, c):\n  return jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n\ndef arr(m, n):\n  return jnp.arange(m*n).reshape((m, n))\n\nm = 3\n\na = arr(m, 2)\nb = arr(m, 5)\nc = arr(m, 7)\n\nfv = jax.vmap(f)\n\nvmap_output = fv(a, b, c)\ndesired_output = jnp.hstack(fv(a, b, c))\n\nprint(vmap_output)\nprint(desired_output)\nThis yields:\n# vmap() output\n[[[  1.  10.]\n  [ 21.   0.]]\n\n [[  5.  35.]\n  [ 70.   0.]]\n\n [[  9.  60.]\n  [119.   0.]]]\n# Desired output\n[[  1.  10.   5.  35.   9.  60.]\n [ 21.   0.  70.   0. 119.   0.]]\nIf this is not possible, I would resort to pre-allocating an array and simply writing to the columns manually, but I hope to avoid this. Thanks for any clue!\nUpdate from @jakevdp's answer\nAlright, it isn't possible. So I resort to writing to the columns, but this fails as well:\ndef g(output, idx, a, b, c):\n  block = jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n  jax.lax.dynamic_update_slice_in_dim(output, block, idx*2, axis=1)\n\n# Defined above: jax, jnp, m, a, b, c\n\ng_output = jnp.zeros((2, 2*m))\nidxs = jnp.arange(m)\n\ngv = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n\ngv(g_output, idxs, a, b, c)\n\nprint(g_output)\nThis yields:\n[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\nSo writing to g_output in the function g is not retained. Is there a way around this?",
        "answers": [
            "No, vmap does not have any built-in capability to stack outputs differently than the batching semantics would imply. But if you're interested in fusing the hstack operation with the vmap operation to the extent possible, you could do so by wrapping it in jit. For example:\n@jax.jit\ndef do_the_thing(a, b, c):\n  return jnp.hstack(fv(a, b, c))\n\nprint(do_the_thing(a, b, c))\nEdit: responding to your edited question: the reason the result is all zeros is because your function doesn't do anything: it returns None, so there's no way for it to affect the input array called g_output. JAX requires pure functions so side-effecting code like what you wrote above is not compatible. If you wanted to replace the hstack with an indexed update, you could do something like this:\ni = jnp.arange(2).reshape(1, 2, 1)\nj = jnp.arange(6).reshape(3, 1, 2)\ng_output = jnp.zeros((2, 2*m)).at[i, j].set(fv(a, b, c))\nbut a nontrivial scatter operation like this will not typically be faster than a simple reshape, especially if you're running on an accelerator like GPU.\nIf your arrays are large enough that reshapes are costly, you might find that a more direct implementation is better; for example:\n@jax.jit\ndef g(a, b, c):\n  output = jnp.zeros((2, 6))\n  output = output.at[0, 0::2].set(a.sum(1))\n  output = output.at[0, 1::2].set(b.sum(1))\n  output = output.at[1, 0::2].set(c.sum(1))\n  return output\n\ng_output = g(a, b, c)"
        ],
        "link": "https://stackoverflow.com/questions/78027629/can-jax-vmap-do-a-hstack"
    },
    {
        "title": "How can I use PyTorch 2.2 with Google Colab TPUs?",
        "question": "I'm having trouble getting PyTorch 2.2 running with TPUs on Google Colab. I'm getting an error about a JAX bug, but I'm confused about this because I'm not doing anything with JAX.\nMy setup process is very simple:\n!pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html\nAnd then\nimport torch\nimport torch_xla.core.xla_model as xm\nwhich gives the error\n/usr/local/lib/python3.10/dist-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')\n This a JAX bug; please report an issue at https://github.com/google/jax/issues\n  _warn(f\"cloud_tpu_init failed: {repr(exc)}\\n This a JAX bug; please report \"\n/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nThen trying\nt1 = torch.tensor(100, device=xm.xla_device())\nt2 = torch.tensor(200, device=xm.xla_device())\nprint(t1 + t2)\ngives the error\n2 frames\n/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py in xla_device(n, devkind)\n    121 \n    122   if n is None:\n--> 123     return torch.device(torch_xla._XLAC._xla_get_default_device())\n    124 \n    125   devices = xm.get_xla_supported_devices(devkind=devkind)\n\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: No ba16c7433 device found.",
        "answers": [
            "Colab currently only provides an older generation of TPUs which is not compatible with recent JAX or PyTorch releases. It’s possible that may change in the future, but I don’t know of any official timeline of when that might happen. In the meantime, you can access recent-generation TPUs via Kaggle or Google Cloud."
        ],
        "link": "https://stackoverflow.com/questions/78014487/how-can-i-use-pytorch-2-2-with-google-colab-tpus"
    },
    {
        "title": "Does jax save the jaxpr of jit compiled functions?",
        "question": "Consider the following example:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n    \nprint(test(jnp.ones(8,)))\nprint(test(jnp.ones(3,)))\nThe output is\n1\n-1\nHowever, I thought that on the first call jax compiles a function to use in subsequent calls. Shouldn't this then give the output 1 and 1, because jax traces through an if and does not use a conditional here? In the jaxpr of the first call is no conditional:\n{ lambda ; a:f32[8]. let\n    b:i32[] = pjit[name=test jaxpr={ lambda ; c:f32[8]. let  in (1,) }] a\n  in (b,) }\nSo how exactly does this work under the hood. Is the jaxpr unique for every call. Does jax only reuse jaxprs if the shape matches? Does jax recompile functions if the shape is different?",
        "answers": [
            "JAX does cache the jaxpr and compiled artifact for each compatible call of the function. This compatibility is determined via the cache key, which contains the shape and dtype of array arguments, as well as the hash of any static arguments and some additional information such as global flags that may affect the computation. Any time something in the cache key changes, it results in a new tracing & compilation of the function. You can see this by printing the _cache_size() of the compiled function. For example:\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n\nx8 = jnp.ones(8)\nx3 = jnp.ones(3)\n\nprint(test._cache_size())  # no calls yet, so no cache\n# 0\n\ntest(x8)\nprint(test._cache_size())  # first call caches the jaxpr\n# 1\n\ntest(x8)\nprint(test._cache_size())  # repeated call, so size doesn't change\n# 1\n\ntest(x3)\nprint(test._cache_size())  # new call, so size increases\n# 2\n\ntest(x8)\nprint(test._cache_size())  # repeated call -> size doesn't change\n# 2\nBy keeping track of these static attributes, jit-compiled functions can change their output based on static attributes, but still avoid recompilation for compatible inputs."
        ],
        "link": "https://stackoverflow.com/questions/78011718/does-jax-save-the-jaxpr-of-jit-compiled-functions"
    },
    {
        "title": "multivariate derivatives in jax - efficiency question",
        "question": "I have the following code which computes derivatives of the function:\nimport jax\nimport jax.numpy as jnp\n\n\ndef f(x):\n    return jnp.prod(x)\n\n\ndf1 = jax.grad(f)\ndf2 = jax.jacobian(df1)\ndf3 = jax.jacobian(df2)\nWith this, all the partial derivatives are available, for example (with vmap additionally):\nx = jnp.array([[ 1.,  2.,  3.,  4.,  5.],\n               [ 6.,  7.,  8.,  9., 10.],\n               [11., 12., 13., 14., 15.],\n               [16., 17., 18., 19., 20.],\n               [21., 22., 23., 24., 25.],\n               [26., 27., 28., 29., 30.]])\ndf3_x0_x2_x4 = jax.vmap(df3)(x)[:, 0, 2, 4]\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nThe question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?",
        "answers": [
            "The question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?\nEssentially, you're asking for a way to compute sparse Hessians and Jacobians; JAX does not have general support for this (see previous issue threads; e.g https://github.com/google/jax/issues/1032).\nEdit\nIn this particular case, though, since you're effectively computing the gradient/jaacobian with respect to a single element per derivative pass, you can do better by just applying the JVP to a single one-hot vector in each transformation. For example:\ndef deriv(f, x, v):\n  return jax.jvp(f, [x], [v])[1]\n\ndef one_hot(i):\n  return jnp.zeros(x.shape[1]).at[i].set(1)\n\ndf_x0 = lambda x: deriv(f, x, one_hot(0))\ndf2_x0_x2 = lambda x: deriv(df_x0, x, one_hot(2))\ndf3_x0_x2_x4 = lambda x: deriv(df2_x0_x2, x, one_hot(4))\nprint(jax.vmap(df3_x0_x2_x4)(x))\n# [  8.  63. 168. 323. 528. 783.]\nPrevious answer\nIf you're willing to relax your \"leaving f with a single argument\" criterion, you could do something like this:\ndef f(*x):\n  return jnp.prod(jnp.asarray(x))\n\ndf1 = jax.grad(f, argnums=4)\ndf2 = jax.jacobian(df1, argnums=2)\ndf3 = jax.jacobian(df2, argnums=0)\n\ndf3_x0_x2_x4 = jax.vmap(df3)(*(x.T))\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nHere rather than computing all gradients and slicing out the result, you are only computing the gradients with respect to the specific three elements you are interested in."
        ],
        "link": "https://stackoverflow.com/questions/78001753/multivariate-derivatives-in-jax-efficiency-question"
    },
    {
        "title": "JAX `custom_vjp` for functions with multiple outputs",
        "question": "In the JAX documentation, custom derivatives for functions with a single output are covered. I'm wondering how to implement custom derivatives for functions with multiple outputs such as this one?\n# want to define custom derivative of out_2 with respect to *args\ndef test_func(*args, **kwargs):\n    ...\n    return out_1, out_2",
        "answers": [
            "You can define custom derivatives for functions with any number of inputs and outputs: just add the appropriate number of elements to the primals and tangents tuples in the custom_jvp rule. For example:\nimport jax\nimport jax.numpy as jnp\n\n@jax.custom_jvp\ndef f(x, y):\n  return x * y, x / y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primals_out = f(x, y)\n  tangents_out = (x_dot * y + y_dot * x, \n                  x_dot / y - y_dot * x / y ** 2)\n  return primals_out, tangents_out\n\nx = jnp.float32(0.5)\ny = jnp.float32(2.0)\n\njax.jacobian(f, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))\nComparing this with the result computed using the standard non-custom derivative rule for the same function shows that the results are equivalent:\ndef f2(x, y):\n  return x * y, x / y\n\njax.jacobian(f2, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))"
        ],
        "link": "https://stackoverflow.com/questions/77952302/jax-custom-vjp-for-functions-with-multiple-outputs"
    },
    {
        "title": "Can't calculate matrix exponential in python",
        "question": "I want to calculate:\nfrom jax.scipy.linalg import expm\nimport jax.numpy as jnp\nfrom functools import lru_cache, reduce\n\nnum_qubits=2\ntheta = jnp.asarray(np.pi*np.random.random((15,2,2,2,2,2,2,2,2)))\n\ndef pauli_matrix(num_qubits):\n    _pauli_matrices = jnp.array(\n    [[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]]\n    )\n    return reduce(jnp.kron, (_pauli_matrices for _ in range(num_qubits)))[1:]\n\ndef SpecialUnitary(num_qubits,theta):\n    assert theta.shape[0] == 15\n    A = jnp.tensordot(theta, pauli_matrix(num_qubits), axes=[[0], [0]])\n    print(f'{A.shape= }{pauli_matrix(num_qubits).shape=}{theta.shape=}')\n    return expm(1j*A/2)\n\nSpecialUnitary(num_qubits,theta)\nShapes: A.shape= (2, 2, 2, 2, 2, 2, 2, 2, 4, 4)pauli_matrix(num_qubits).shape=(15, 4, 4)theta.shape=(15, 2, 2, 2, 2, 2, 2, 2, 2) Error: ValueError: expected A to be a square matrix\nI'm stuck because the documentation says that the expm is calculated on the last two axes, which must be square, which is done.",
        "answers": [
            "Batched expm is supported in recent JAX versions, and you should find that this works fine in JAX v0.4.7 or newer:\nimport jax.numpy as jnp\nimport jax.scipy.linalg\n\nX = jnp.arange(128.0).reshape(2, 2, 2, 4, 4)\n\nresult = jax.scipy.linalg.expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)\nIf for some reason you must use an older JAX version, you can work around this by using jax.numpy.vectorize. For example:\nexpm = jnp.vectorize(jax.scipy.linalg.expm, signature='(n,n)->(n,n)')\n\nresult = expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)"
        ],
        "link": "https://stackoverflow.com/questions/77946266/cant-calculate-matrix-exponential-in-python"
    },
    {
        "title": "Jax Implementation of function similar to Torch's 'Scatter'",
        "question": "For graph learning purposes, I am trying to implement a global sum batching function, that takes as inputs batched graph representations 'x' of size (n x d) and a corresponding vector of batches (n x 1). I then want to compute the sum over all graph representations for each batch. Here is a graphical representation: torch's scatter function\nThis is my current attempt:\ndef global_sum_pool(x, batch):\n    graph_reps = []\n    i = 0\n    n = jnp.max(batch)\n    while True:\n        ind = jnp.where(batch == i, True, False).reshape(-1, 1)\n        ind = jnp.tile(ind, x.shape[1])\n        x_ind = jnp.where(ind == True, x, 0.0)\n        graph_reps.append(jnp.sum(x_ind, axis=0))\n        if i == n:\n            break\n        i += 1\n    return jnp.array(graph_reps)\nI get the following exception on the line if i == n:\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function make_step at /venvs/jax_env/lib/python3.11/site-packages/equinox/_jit.py:37 for jit. \nI understand this is due to the fact that at compile time, Jax does not a priori know the max value of the 'batch' array and hence cannot allocate memory. Does anyone know a workaround or different implementation?",
        "answers": [
            "Rather than implementing this via a for loop, you should use JAX's built-in scatter operator. The most convenient interface for this is the Array.at syntax. If I understand your goal correctly, it might look something like this:\nimport jax.numpy as jnp\nimport numpy as np\n\n# Generate some data\nnum_batches = 4\nn = 10\nd = 3\nx = np.random.randn(n, d)\nind = np.random.randint(low=0, high=num_batches, size=(n,))\n\n#Compute the result with jax.lax.scatter\nresult = jnp.zeros((num_batches, d)).at[ind].add(x)\nprint(result.shape)\n# (4, 3)"
        ],
        "link": "https://stackoverflow.com/questions/77932146/jax-implementation-of-function-similar-to-torchs-scatter"
    },
    {
        "title": "JAX `vjp` fails for vmapped function with `custom_vjp`",
        "question": "Below is an example where a function with a custom-defined vector-Jacobian product (custom_vjp) is vmapped. For a simple function like this, invoking vjp fails:\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef test_func(f: Callable[..., float],\n              R: Array\n              ) -> float:\n\n    return f(jnp.dot(R, R))\n\n\ndef test_func_fwd(f, primal):\n\n    primal_out = test_func(f, primal)\n    residual = 2. * primal * primal_out\n    return primal_out, residual\n\n\ndef test_func_bwd(f, residual, cotangent):\n\n    cotangent_out = residual * cotangent\n    return (cotangent_out, )\n\n\ntest_func.defvjp(test_func_fwd, test_func_bwd)\n\ntest_func = vmap(test_func, in_axes=(None, 0))\n\n\nif __name__ == \"__main__\":\n\n    def f(x):\n        return x\n\n    # vjp\n    primal, f_vjp = vjp(partial(test_func, f),\n                        jnp.ones((10, 3))\n                        )\n\n    cotangent = jnp.ones(10)\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[0].shape)\nThe error message says:\nValueError: Shape of cotangent input to vjp pullback function (10,) must be the same as the shape of corresponding primal input (10, 3).\nHere, I think the error message is misleading, because the cotangent input should have the same shape as the primal output, which should be (10, ) in this case. Still, it's not clear to me why this error occurs.",
        "answers": [
            "The problem is that in test_func_fwd, you recursively call test_func, but you've overwritten test_func in the global namespace with its vmapped version. If you leave the original test_func unchanged in the global namespace, your code will work as expected:\n...\n\ntest_func_mapped = vmap(test_func, in_axes=(None, 0))\n\n... \n\nprimal, f_vjp = vjp(partial(test_func_mapped, f),\n                    jnp.ones((10, 3))\n                    )"
        ],
        "link": "https://stackoverflow.com/questions/77930920/jax-vjp-fails-for-vmapped-function-with-custom-vjp"
    },
    {
        "title": "JAX `vjp` does not recognize cotangent argument with `custom_vjp`",
        "question": "I have a JAX function cart_deriv() which takes another function f and returns the Cartesian derivative of f, implemented as follows:\n@partial(custom_vjp, nondiff_argnums=0)\ndef cart_deriv(f: Callable[..., float],\n               l: int,\n               R: Array\n               ) -> Array:\n\n    df = lambda R: f(l, jnp.dot(R, R))\n\n    for i in range(l):\n        df = jacrev(df)\n\n    return df(R)\n\n\ndef cart_deriv_fwd(f, l, primal):\n\n    primal_out = cart_deriv(f, l, primal)\n    residual = cart_deriv(f, l+1, primal)  ## just a test\n\n    return primal_out, residual\n\n\ndef cart_deriv_bwd(f, residual, cotangent):\n\n    cotangent_out = jnp.ones(3)  ## just a test\n\n    return (None, cotangent_out)\n\n\ncart_deriv.defvjp(cart_deriv_fwd, cart_deriv_bwd)\n\n\n\nif __name__ == \"__main__\":\n\n    def test_func(l, r2):\n        return l + r2\n\n    primal_out, f_vjp = vjp(cart_deriv, \n                            jax.tree_util.Partial(test_func),\n                            2,\n                            jnp.array([1., 2., 3.])\n                            )\n\n    cotangent = jnp.ones((3, 3))\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[1].shape)\nHowever this code produces the error:\nTypeError: cart_deriv_bwd() missing 1 required positional argument: 'cotangent'\nI have checked that the syntax agrees with that in the documentation. I'm wondering why the argument cotangent is not recognized by vjp, and how to fix this error?",
        "answers": [
            "The issue is that nondiff_argnums is expected to be a sequence:\n@partial(custom_vjp, nondiff_argnums=(0,))\nWith this properly defined, it's better to avoid wrapping the function in Partial, and just pass it as a static argument by closing over it in the vjp call:\nprimal_out, f_vjp = vjp(partial(cart_deriv, test_func),\n                        2,\n                        jnp.array([1., 2., 3.])\n                        )\ncotangent_out = f_vjp(jnp.ones((3, 3)))\n\nprint(*cotangent_out)\n# (b'',) [1. 1. 1.]"
        ],
        "link": "https://stackoverflow.com/questions/77924142/jax-vjp-does-not-recognize-cotangent-argument-with-custom-vjp"
    },
    {
        "title": "How to implement the next function (the use of Dynamic Shapes) in JAX?",
        "question": "I have a simple function that takes an jax Array as input, searches for the first occurrence of 1, and replaces it with another jax Array (specified as a second input):\nrules_int = [\n    jnp.array([0,0]),\n    jnp.array([1,1,1]),\n]\n# Even with the same size of inputs, the sizes of outputs can be different\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1)[0]\n    if index.size == 0:\n        return arr\n    index = index[0]\n    new_arr = jnp.concatenate([arr[:index], rules_int[action], arr[index+1:]])\n    return new_arr\n\nreplace_first_one(jnp.array([1]), 0)\n# result is Array([0, 0], dtype=int32)\nBut when I use vmap a get an exception:\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])  # Corresponding actions for each array\n\n# Vectorize the function\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nresult = vectorized_replace_first_one(batch_arr, batch_actions)\nindex = jnp.where(arr == 1)[0] The size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations. This BatchTracer with object id 140260750414512 was created on line:\nI read on JAX docs:\nJAX code used within transforms like jax.jit, jax.vmap, jax.grad, etc. requires all output arrays and intermediate arrays to have static shape: that is, the shape cannot depend on values within other arrays.\nPlease suggest how to make it work?\nIdeally, these rules should be applied recursively until there are no rules to apply. (string rewriting system)",
        "answers": [
            "As written, it is impossible to do this with vmap because the output of your function has a shape that depends on the value of action, and so the output would have to be a ragged array, which JAX does not support (see JAX Sharp Bits: Dynamic Shapes).\nTo make the function compatible with vmap, you'll have to adjust it so that it has static shape semantics: in particular, every entry of rules_int must have the same length, and you cannot return arr alone in cases where arr doesn't have any 1 entries. Making these changes and adjusting the logic to avoid dynamically-shaped intermediates, you could write something like this:\nimport jax\n\nrules_int = jnp.array([\n    [0,0],\n    [1,1],\n])\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1, size=1)[0][0]\n    arr_to_insert = rules_int[action]\n    output_size = len(arr) - 1 + len(arr_to_insert)\n    new_arr = jnp.where(jnp.arange(output_size) < index,\n                        jnp.concatenate([arr[:-1], arr_to_insert]),\n                        jnp.concatenate([arr_to_insert, arr[1:]]))\n    return jax.lax.dynamic_update_slice(new_arr, arr_to_insert, (index,))\n\nreplace_first_one(jnp.array([1]), 0)\n# Array([0, 0], dtype=int32)\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])\n\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nvectorized_replace_first_one(batch_arr, batch_actions)\n# Array([[0, 0, 4, 5, 1],\n#        [6, 1, 1, 8, 1]], dtype=int32)\nIf adjusting the semantics of your function in this way to avoid dynamic shapes is not viable given your use-case, then your use-case is unfortunately not compatible with vmap or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77915540/how-to-implement-the-next-function-the-use-of-dynamic-shapes-in-jax"
    },
    {
        "title": "Vectorizing power of `jax.grad`",
        "question": "I'm trying to vectorize the following \"power-of-grad\" function so that it accepts multiple orders: (see here)\ndef grad_pow(f, order, argnum):\n\n    for i in jnp.arange(order):\n        f = grad(f, argnums=argnum)\n\n    return f\nThis function produces the following error after applying vmap on the argument order:\njax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nIt arose in the jnp.arange argument 'stop'\nI have tried writing a static version of grad_pow using jax.lax.cond and jax.lax.scan, following the logic here:\ndef static_grad_pow(f, order, argnum):\n\n    order_max = 3  ## maximum order\n\n    def grad_pow(f, i):\n        return cond(i <= order, grad(f, argnum), f), None\n\n    return scan(grad_pow, f, jnp.arange(order_max+1))[0]\n\n\nif __name__ == \"__main__\":\n\n    test_func = lambda x: jnp.exp(-2*x)\n    test_func_grad_pow = static_grad_pow(jax.tree_util.Partial(test_func), 1, 0)\n    print(test_func_grad_pow(1.))\nNevertheless, this solution still produces an error:\n    return cond(i <= order, grad(f, argnum), f), None\nTypeError: differentiating with respect to argnums=0 requires at least 1 positional arguments to be passed by the caller, but got only 0 positional arguments.\nJust wondering how this issue can be resolved?",
        "answers": [
            "The fundamental issue with your question is that a vmapped function cannot return a function, it can only return arrays. All other details aside, that precludes any possibility of writing a valid function that does what you intend.\nThere are alternatives: for example, rather than attempting to create a function that will return a function, you could instead create a function that accepts arguments and applies that function to those arguments.\nIn that case, you'll run into another issue: if n is traced, there is no way to apply grad n times. JAX transformations like grad are evaluated at trace-time, and traced values like n are not available until runtime. One way to work around this is to pre-define all the functions you're interested in, and to use lax.switch to choose between them at runtime. The result would look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=[0], static_argnames=['argnum', 'max_order'])\ndef apply_multi_grad(f, order, *args, argnum=0, max_order=10):\n  funcs = [f]\n  for i in range(max_order):\n    funcs.append(jax.grad(funcs[-1], argnum))\n  return jax.lax.switch(order, funcs, *args)\n\n\norder = jnp.arange(3)\nx = jnp.ones(3)\nf = jnp.sin\n\nprint(jax.vmap(apply_multi_grad, in_axes=(None, 0, 0))(f, order, x))\n# [ 0.84147096  0.5403023  -0.84147096]\n\n# Compare by doing it manually:\nprint(jnp.array([f(x[0]), jax.grad(f)(x[1]), jax.grad(jax.grad(f))(x[2])]))\n# [ 0.84147096  0.5403023  -0.84147096]"
        ],
        "link": "https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad"
    },
    {
        "title": "jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed",
        "question": "I am trying to run multiple sbx programs (that use JAX) concurrently using joblib. Here is my program -\n'''\nFor installation please do -\npip install gym\npip install sbx-rl\npip install mujoco\npip install shimmy\n'''\nfrom joblib import Parallel, delayed\n\nimport gym\nfrom sbx import SAC\n\n# from stable_baselines3 import SAC\ndef train():\n\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env, verbose=1)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\ndef train_model():\n\n    train()\n\n\n\nif __name__ == '__main__':\n    Parallel(n_jobs=10)(delayed(train)() for i in range(3))\nThis is the error that I am getting -\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n2024-01-30 11:19:12.354168: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:\nINTERNAL: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory\n2024-01-30 11:19:12.354264: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\njoblib.externals.loky.process_executor._RemoteTraceback: \n\"\"\"\njax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/work/LAS/usr/tbd/5_test.py\", line 23, in my_func\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 109, in __init__\n    self._setup_model()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 126, in _setup_model\n    self.key = self.policy.build(self.key, self.lr_schedule, self.qf_learning_rate)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/policies.py\", line 143, in build\n    key, actor_key, qf_key, dropout_key = jax.random.split(key, 4)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 303, in split\n    return _return_prng_keys(wrapped, _split(typed_key, num))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 289, in _split\n    return prng.random_split(key, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 769, in random_split\n    return random_split_p.bind(keys, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 444, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 447, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 935, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 781, in random_split_impl\n    base_arr = random_split_impl_base(\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 787, in random_split_impl_base\n    return split(base_arr)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 786, in <lambda>\n    split = iterated_vmap_unary(keys_ndim, lambda k: impl.split(k, shape))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 1291, in threefry_split\n    return _threefry_split(key, shape)\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/work/LAS/usr/tbd/5_test.py\", line 27, in <module>\n    Parallel(n_jobs=3)(delayed(my_func)() for i in range(3))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1952, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\n    yield from self._retrieve()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1699, in _retrieve\n    self._raise_error_fast()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1734, in _raise_error_fast\n    error_job.get_result(self.timeout)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 736, in get_result\n    return self._return_or_raise()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 754, in _return_or_raise\n    raise self._result\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\nI am using a 40 GB GPU (a100-pcie). Therefore I doubt that my GPU is running out of memory. Please let me know if any clarification is needed.\nEdit 1: This is how I call my program - export XLA_PYTHON_CLIENT_PREALLOCATE=false && python 5_test.py (The name of my program is 5_test.py)",
        "answers": [
            "It appears you are using multiple processes targeting the same GPU. In each process, JAX will attempt to reserve 75% of the available GPU memory (see GPU memory allocation), so attempting this with two or more processes will exhaust the available memory.\nYou could fix this by turning off pre-allocation as mentioned in that doc, by setting the environment variables XLA_PYTHON_CLIENT_PREALLOCATE=false or XLA_PYTHON_CLIENT_MEM_FRACTION=.XX (with .XX set to .08 or something suitable), but I suspect the end result will be less efficient than if you had just run your full program from a single JAX process: multiple host processes targeting a single GPU device concurrently will just compete with each other for resources and lead to suboptimal results."
        ],
        "link": "https://stackoverflow.com/questions/77908236/jaxlib-xla-extension-xlaruntimeerror-internal-failed-to-execute-xla-runtime-ex"
    },
    {
        "title": "JAX `grad` error for function with `jax.lax.switch` and compound boolean conditions",
        "question": "I have encountered a scenario where applying jax.grad to a function with jax.lax.switch and compound boolean conditions yields jax.errors.TracerBoolConversionError. A minimal program to reproduce this behavior is the following:\nfrom jax.lax import switch\nimport jax.numpy as jnp\nfrom jax import grad\n\nfunc_0 = lambda x: jnp.where(0. < x < 1., x, 0.)\nfunc_1 = lambda x: jnp.where(0. < x < 1., x, 1.)\n\nfunc_list = [func_0, func_1]\n\nfunc = lambda index, x: switch(index, func_list, x)\n\ndf = grad(func, argnums=1)(1, 2.)\nprint(df)\nThe error is the following:\nTraceback (most recent call last):\n  File \"***/grad_test.py\", line 12, in <module>\n    df = grad(func, argnums=1)(1, 0.5)\n  File \"***/grad_test.py\", line 10, in <lambda>\n    func = lambda index, x: switch(index, func_list, x)\n  File \"***/grad_test.py\", line 5, in <lambda>\n    func_0 = lambda x: jnp.where(0 < x < 1., x, 0.)\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function <lambda> at ***/grad_test.py:5 for switch. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\nHowever, if the boolean condition is changed to a single condition (for example, x < 1), then no error occurs. I'm wondering if this could be a bug, or otherwise, how the original program should be changed.",
        "answers": [
            "You cannot use chained inequalities with JAX or NumPy arrays. Instead of 0 < x < 1, you should write (0 < x) & (x < 1) (note that due to operator precedence, the parentheses are not optional here)."
        ],
        "link": "https://stackoverflow.com/questions/77900299/jax-grad-error-for-function-with-jax-lax-switch-and-compound-boolean-conditi"
    },
    {
        "title": "Getting derivatives of NNs according to its inputs by batches in JAX",
        "question": "There is a neural network that takes as an input a two variables: net(x, t), where x is usually d-dim, and t is a scalar. The NN outputs a vector of length d. x and t might be batches, so x is of shape (b, d), and t is (b, 1), and the output is (b,d). I need to find\nderivative d out/dt of the NN output. It should be d dim vector (or (batch, d));\nderivative d out/dx of the NN\ngradient of divergence of the NN output according to x, it still should be (batch, d) vector\nSince the NN doesn’t output a scalar, I don’t think Jax grad would help here. I know how to do what I described in torch, but I’m totally new to JAX. I’d really appreciate your help with this question!\nThere is an example:\nimport jaxlib\nimport jax\nfrom jax import numpy as jnp\nimport flax.linen as nn\nfrom flax.training import train_state\n\n\n\nclass NN(nn.Module):\n    hid_dim : int # Number of hidden neurons\n    output_dim : int # Number of output neurons\n\n    @nn.compact  \n    def __call__(self, x, t):\n        out = jnp.hstack((x, t))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.Dense(features=self.output_dim)(out)\n        return out\n\nd = 3\nbatch_size = 10\nnet = NN(hid_dim=100, output_dim=d)\n\nrng_nn, rng_inp1, rng_inp2 = jax.random.split(jax.random.PRNGKey(100), 3)\ninp_x = jax.random.normal(rng_inp1, (1, d)) # batch, d\ninp_t = jax.random.normal(rng_inp2, (1, 1))\nparams_net = net.init(rng_nn, inp_x, inp_t)\n\nx = jax.random.normal(rng_inp2, (batch_size, d)) # batch, d\nt = jax.random.normal(rng_inp1, (batxh_size, 1))\n\nout_net = net.apply(params_net, x, t)\n\noptimizer = optax.adam(1e-3)\n\nmodel_state = train_state.TrainState.create(apply_fn=net.apply,\n                                            params= params_net,\n                                            tx=optimizer)\nI'd like to calculate an $L_2$ loss based on some derivatives of the NN's outputs according to its inputs. For example, I'd like to have d f/dx or d f/dt where f is the NN. ALso the gradient of the divergence by x. I assume it'd be something like\ndef find_derivatives(net, params, X, t):\n    d_dt = lambda net, params, X, t: jax.jvp(lambda time: net(params, X, t), (t, ), (jnp.ones_like(t), ))\n    d_dx = lambda net, params, X, t: jax.jvp(lambda X: net(params, X, t), (Xs_all, ), (jnp.ones_like(X), ))\n    out_f, df_dt = d_dt(net.apply, params, X, t)\n\n    d_ddx = lambda net, params, X, t: d_dx(lambda params, X, t: d_dx(net, params, X, t)[1], params, X, t)\n    df_dx, df_ddx = d_ddx(net.apply, params, X, t)\n    \n    return out_f, df_dt, df_dx, df_ddx\n\n\nout_f, df_dt, df_dx, df_ddx = find_derivatives(net, params_net, x, t)",
        "answers": [
            "I would avoid using jax.jvp here, because this is meant as a lower-level API. You can use jax.jacobian to compute the Jacobian (since your function has multiple outputs), and vmap for batching. For example:\ndf_dx = jax.vmap(\n    jax.jacobian(net.apply, argnums=1),\n    in_axes=(None, 0, 0)\n  )(params_net, x, t)\nprint(df_dx.shape)  # (10, 3, 3)\n\ndf_dt = jax.vmap(\n    jax.jacobian(net.apply, argnums=2),\n    in_axes=(None,0, 0)\n  )(params_net, x, t).reshape(10, 3)\nprint(df_dt.shape)  # (10, 3)\nHere df_dx is the batch-wise Jacobian of the 3-dimensional output vector with respect to the 3-dimensional x input vector, and df_dt is the batch-wise gradient of the 3-dimensional output vector with respect to the input t."
        ],
        "link": "https://stackoverflow.com/questions/77897419/getting-derivatives-of-nns-according-to-its-inputs-by-batches-in-jax"
    },
    {
        "title": "`jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].`",
        "question": "I am trying to perform multiprocessing to parallelize my program (that uses JAX) using pmap. I am a newbie with JAX and realize that maybe pmap isn't the right approach. The documentation here, said that pmap can express SPMD programs (which is the case here) and therefore I decided to use it.\nHere's my program. I am basically trying to run a reinforcement learning program (that uses JAX too) twice, using parallel execution -\n'''\nFor installation please do -\npip install gym\npip install sbx\npip install mujoco\npip install shimmy\n'''\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\nimport gym\nfrom sbx import SAC\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda _: my_func())(jnp.arange(2))\nI get the following error -\n(tbd) thoma@thoma-Lenovo-Legion-5-15IMH05H:~/PycharmProjects/tbd$ python new.py\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 678, in parallel_callable\n    pmap_computation = lower_parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 825, in lower_parallel_callable\n    jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 748, in stage_parallel_callable\n    jaxpr, out_sharded_avals, consts = pe.trace_to_jaxpr_final(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2233, in trace_to_jaxpr_final\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/core.py\", line 605, in __array__\n    raise TracerArrayConversionError(self)\njax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n   0% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/700,000  [ 0:00:00 < -:--:-- , ? it/s ]Exception ignored in: <function tqdm.__del__ at 0x7fa875eb8af0>\nTraceback (most recent call last):\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/std.py\", line 1149, in __del__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/rich.py\", line 120, in close\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1177, in __exit__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1163, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/live.py\", line 155, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1137, in line\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1674, in print\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1535, in _collect_renderables\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/protocol.py\", line 28, in rich_cast\nImportError: sys.meta_path is None, Python is likely shutting down\nBasically I am trying to replace the parallelization performed by joblib with JAX. Here's my original program that I am changing -\nfrom joblib import Parallel, delayed\nimport gym\nimport os\nfrom sbx import SAC\nimport multiprocessing\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\n\nParallel(n_jobs=2)(delayed(my_func)() for i in range(2))",
        "answers": [
            "The problem is here:\n File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\nThe sbx package is calling np.array on the inputs – this tells me that sbx is built on NumPy, not on JAX. JAX transformations like pmap are not compatible with NumPy functions, they require functions written with JAX operations. Unless sbx is substantially re-designed, you won't be able to use it with pmap, vmap, jit, grad, or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77892458/jax-errors-tracerarrayconversionerror-the-numpy-ndarray-conversion-method-ar"
    },
    {
        "title": "How to use JAX pmap with CPU cores",
        "question": "I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible - Here's my code -\nimport jax.numpy as jnp\nimport os\nfrom jax import pmap\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\nHere's the error -\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 682, in parallel_callable\n    pmap_executable = pmap_computation.compile()\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 923, in compile\n    executable = UnloadedPmapExecutable.from_hlo(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 993, in from_hlo\n    raise ValueError(msg.format(shards.num_global_shards,\njax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\nValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\nBased on this and this discussion, I did this os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8', but it doesn't seem to work.\nEdit 1:\nI tried this but it still doesn't work -\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\n\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)",
        "answers": [
            "XLA flags are read when JAX is imported, so you need to set them before importing JAX if you want the flags to have an effect.\nYou should also make sure you're in a clean runtime (i.e. not using a Jupyter kernel where you have previously imported jax).\nAdditionally, keep in mind that --xla_force_host_platform_device_count=8 only affects the host (CPU) device count, so the code as written above won't work if you're using GPU-enabled JAX with a single GPU device. If this is the case, you can force pmap to run on the non-default CPU devices using the devices argument:\nout = pmap(lambda x: x ** 2, devices=jax.devices('cpu')(jnp.arange(8))"
        ],
        "link": "https://stackoverflow.com/questions/77889712/how-to-use-jax-pmap-with-cpu-cores"
    },
    {
        "title": "Unexpected behavior of JAX `vmap` for multiple arguments",
        "question": "I have found that vmap in JAX does not behave as expected when applied to multiple arguments. For example, consider the function below:\ndef f1(x, y, z):\n    f = x[:, None, None] * z[None, None, :] + y[None, :, None]\n    return f\nFor x = jnp.arange(7), y = jnp.arange(5), z = jnp.arange(3), the output of this function has shape (7, 5, 3). However, for the vmap version below:\n@partial(vmap, in_axes=(None, 0, 0), out_axes=(1, 2))\ndef f2(x, y, z):\n    f = x*z + y\n    return f\nIt outputs this error:\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 5: axis 0 of argument y of type int32[5];\n  * one axis had size 3: axis 0 of argument z of type int32[3]\nCould someone kindly explain what's behind this error?",
        "answers": [
            "The semantics of vmap are that it does a single batching operation along one or more arrays. When you specify in_axes=(None, 0, 0), the meaning is \"map simultaneously along the leading dimension of y and z\": the error you're seeing is telling you that the leading dimensions of y and z have different sizes, and so they are not compatible for batching.\nYour function f1 essentially uses broadcasting to encode three batching operations, so to replicate that logic with vmap you'll need three applications of vmap. You can express that as follows:\n@partial(vmap, in_axes=(0, None, None))\n@partial(vmap, in_axes=(None, 0, None))\n@partial(vmap, in_axes=(None, None, 0))\ndef f2(x, y, z):\n    f = x*z + y\n    return f"
        ],
        "link": "https://stackoverflow.com/questions/77886057/unexpected-behavior-of-jax-vmap-for-multiple-arguments"
    },
    {
        "title": "How is it possible that jax vmap returns not iterable?",
        "question": "import jax\nimport pgx\nfrom jax import vmap, jit\nimport jax.numpy as jnp\n\nenv = pgx.make(\"tic_tac_toe\")\nkey = jax.random.PRNGKey(42)\n\nstates = jax.jit(vmap(env.init))(jax.random.split(key, 4))\ntype(states)\nstates has a type pgx.tic_tac_toe.State. I was expecting an Iterable object with a size 4. Somehow iterable results are inside pgx.tic_tac_toe.State.\nCan you please explain how is it possible that jax vmap returns not iterable?\nHow to force vmap to return the next result:\nstates = [env.init(key) for key in jax.random.split(key, 4)]\nNote, this code works as expected:\ndef square(x):\n    return x ** 2\ninputs = jnp.array([1, 2, 3, 4])\nresult = jax.vmap(square)(inputs)\nprint(result) # list object",
        "answers": [
            "Can you please explain how is it possible that jax vmap returns not iterable?\nWhen passed a non-array object, vmap will map the leading axes of each array in its flattened pytree representation. You can see the shapes in the flattened object here:\nprint([arr.shape for arr in jax.tree_util.tree_flatten(states)[0]])\n# [(4,), (4, 3, 3, 2), (4, 2), (4,), (4,), (4, 9), (4,), (4,), (4, 9)]\nThis is an example of the struct-of-arrays pattern used by vmap, where it sounds like you were expececting an array-of-structs pattern.\nHow to force vmap to return the next result\nIf you wanted to convert this output into the list of state objects you were expecting, you could do so using utilities in jax.tree_util:\nleaves, treedef = jax.tree_util.tree_flatten(states)\nstates_list = [treedef.unflatten(leaf) for leaf in zip(*leaves)]\nprint(len(states_list))\n# 4\nThat said, it appears that pgx is built to work natively with the original struct-of-arrays pattern, so you may find that you won't actually need this unstacked version in practice."
        ],
        "link": "https://stackoverflow.com/questions/77881821/how-is-it-possible-that-jax-vmap-returns-not-iterable"
    },
    {
        "title": "Why do I get different values from jnp.round and np.round?",
        "question": "I'm writing tests for some jax code and using np.testing.assert_array...-type functions and came across this difference in values that I didn't expect:\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\na = jnp.array([-0.78073686, -0.7908204 ,  2.174842])\nb = np.array(a, dtype='float32')\nassert_array_equal(a, b)\n\nprint(a.round(2), a.dtype)\nprint(b.round(2), b.dtype)\nOutput:\n[-0.78       -0.78999996  2.1699998 ] float32\n[-0.78 -0.79  2.17] float32\nTest:\nassert_array_equal(a.round(2), b.round(2))\nOutput:\nAssertionError: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 2.3841858e-07\nMax relative difference: 1.0987031e-07\n x: array([-0.78, -0.79,  2.17], dtype=float32)\n y: array([-0.78, -0.79,  2.17], dtype=float32)\nFootnote:\nI get exactly the same results if I define b as follows, so it's not a problem with the conversion of the array from jax to numpy:\nb = np.array([-0.78073686, -0.7908204 ,  2.174842], dtype='float32')",
        "answers": [
            "This is an example of a general property of floating point computations: two different ways of expressing the same computation will not always produce bitwise-equivalent outputs (see e.g. Is floating point math broken?).\nJAX and NumPy use identical implementations for x.round(2); essentially it is round_to_int(x * 100) / 100 (compare the JAX implementation and the NumPy implementation).\nThe difference is that JAX jit-compiles jnp.round by default. When you disable compilation and perform these operations in sequence, the results are identical:\nimport jax\nwith jax.disable_jit():\n  assert_array_equal(a.round(2), b.round(2))  # passes!\nBut JAX's JIT optimizes the implementation by fusing some operations – this leads to faster computation but in general you should not expect the result to be bitwise-equivalent to the unoptimized version.\nTo address this, whenever you are comparing floating point values, you should avoid exact equality checks in favor of checks that take this floating point roundoff error into account. For example:\nnp.testing.assert_allclose(a.round(2), b.round(2), rtol=1E-6)  # passes!"
        ],
        "link": "https://stackoverflow.com/questions/77868226/why-do-i-get-different-values-from-jnp-round-and-np-round"
    },
    {
        "title": "finding the maximum of a function using jax",
        "question": "I have a function which I would like to find its maximum by optimizing two of its variables using Jax.\nThe current code that I have currently, which does not work, reads\nimport jax.numpy as jnp\nimport jax \nimport scipy\nimport numpy as np\n\ndef temp_func(x,y,z):\n    tmp = x + jnp.dot( jnp.power(y,3), jnp.tanh(z) )\n    return -tmp\ndef obj_func(xy, z):\n    x,y = xy[:2], xy[2:].reshape(2,2)\n    return jnp.sum(temp_func(jnp.array(x),jnp.array(y),z))\n\ngrad_tmp = jax.grad(obj_func, argnums=0) # x,y\n\nxy = jnp.concatenate([np.random.rand(2), np.random.rand(2*2) ])\nz= jnp.array( np.random.rand(2,2) )\nprint(obj_func(xy,z))\n\nresult = scipy.optimize.minimize(obj_func,\n                                 xy,\n                                 args=(z,),\n                                 method='L-BFGS-B',\n                                 jac=grad_tmp\n                                )\nWith this code, I get the error ValueError: failed in converting 7th argument g' of _lbfgsb.setulb to C/Fortran array` Do you have any suggestions to resolve the issue?",
        "answers": [
            "You might think about using the jax version of scipy.optimize.minimize, which will automatically compute and use the derivative:\nimport jax.scipy.optimize\nresult = jax.scipy.optimize.minimize(obj_func, xy, args=(z,), method='BFGS')\nThat said, the results in either case are not going to be very meaningful, because your objective function is linearly decreasing in x and y, so it will be minimized when x, y → ∞"
        ],
        "link": "https://stackoverflow.com/questions/77860052/finding-the-maximum-of-a-function-using-jax"
    },
    {
        "title": "Custom JVP and VJP for higher order functions in JAX",
        "question": "I find custom automatic differentiation capabilities (JVP, VJP) very useful in JAX, but am having a hard time applying it to higher order functions. A minimal example of this sort is as follows: given a higher order function:\ndef parent_func(x):\n    def child_func(y):\n        return x**2 * y\n    return child_func\nI would like to define custom gradients of child_func with respect to x and y. What would be the correct syntax to achieve this?",
        "answers": [
            "Gradients in JAX are defined with respect to a function’s explicit inputs. Your child_func does not take x as an explicit input, so you cannot directly differentiate child_func with respect to x. However, you could do so indirectly by calling it from another function that takes x. For example:\ndef func_to_differentiate(x, y):\n  child_func = parent_func(x)\n  return child_func(y)\n\njax.grad(func_to_differentiate, argnums=0)(1.0, 1.0)  # 2.0\nThen if you wish, you could define standard custom derivative rules for func_to_differentiate."
        ],
        "link": "https://stackoverflow.com/questions/77859418/custom-jvp-and-vjp-for-higher-order-functions-in-jax"
    },
    {
        "title": "JAX python C callbacks",
        "question": "Numba allows to create C-callbacks directly in python with the @cfunc-decorator ( https://numba.pydata.org/numba-doc/0.42.0/user/cfunc.html ):\n@cfunc(\"float64(float64)\") \ndef square(x):\n    return x**2\nTo clarify, the resulting function is a pure C-function, which can then be called directly from C-code.\nIs there an equivalent functionality available in JAX ( https://jax.readthedocs.io/en/latest/# )?\nI have been searching for a while but couldn't find anything. I would appreciate any tips.",
        "answers": [
            "No, JAX doesn't provide any API similar to Numba's cfunc."
        ],
        "link": "https://stackoverflow.com/questions/77855169/jax-python-c-callbacks"
    },
    {
        "title": "How to loop a random number of times in jax with jit compilation?",
        "question": "I am using jax in python, and I want to loop over some code for a random number of times. This is part of a function which is jit compiled later. I have a small example below which should explain what I want to do.\nnum_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\narr = []\nfor i in range(num_iters):\n  arr += [i*i]\nThis works without any error and gives arr=[0,1,4] at the end of the loop (with the fixed seed of 0 that we're using in PRNGKey).\nHowever, if this is part of a jit-compiled function:\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nI get a TracerIntegerConversionError on num_iters. The function works fine without the jit decorator. How to get this to work with jit? I basically just want to construct the list arr whose length depends on a random number. Alternatively, I can also use a list with the maximum possible size, but then I'd have to loop over it a random number of times.\nFurther context\nIt's possible to make it not throw an error using a numpy random number generator instead:\n@jax.jit\ndef do_stuff(start):\n  np_rng = np.random.default_rng()\n  num_iters = np_rng.integers(1, 10)\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nHowever, this is not what I want. There is a jax rng which is passed to my function which I wish to use to generate num_iters. Otherwise, arr always has the same length since the numpy seed is fixed to what was available at jit-compile time, and I always get the same result without any randomness. However, if I use that rng key as seed for numpy (like np.random.default_rng(seed=rng[0])) it again gives the following error:\nTypeError: SeedSequence expects int or sequence of ints for entropy not Traced<ShapedArray(uint32[])>with<DynamicJaxprTrace(level=1/0)>",
        "answers": [
            "You could use jax.lax.fori_loop for this:\nimport jax\n\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  return jax.lax.fori_loop(0, num_iters, lambda i, val: val + i * i, start)\n\nprint(do_stuff(10))\n# 15",
            "Jax complains in this case, because you try to use a traced value as a static integer. See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError for more information.\nAs one possible solution you could pass the num_iters as an argument to do_stuff, declare it as static and create the keys outside, along the lines of:\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(1,))\ndef do_stuff(start, num_iters):    \n  arr = []\n  \n  for i in range(num_iters):\n    arr += [i*i]\n  \n  for value in arr:\n    start += value\n  \n  return start\n\nkey = jax.random.PRNGKey(238)\n\nfor _ in range(4):\n  key, _ = jax.random.split(key)\n  num_iters = int(jax.random.randint(key, (1,), 1, 10))\n  print(do_stuff(0, num_iters))\nWhich prints:\n5\n0\n140\n30\nOther alternative solutions are proposed in the link I listed above.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/77844256/how-to-loop-a-random-number-of-times-in-jax-with-jit-compilation"
    },
    {
        "title": "Update JAX array based on values in another array",
        "question": "I have a Jax array X like this:\n[[[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]]\nHow do I set the values of this array to 1, whose indices are given by array Y:\n[[[1 2]\n [1 2]]\n\n [[0 2]\n [0 1]]\n\n [[1 0]\n [1 0]]]\nDesired output:\n([[[0., 1., 1.],\n        [0., 1., 1.]],\n\n       [[1., 0., 1.],\n        [1., 1., 0.]],\n\n       [[1., 1., 0.],\n        [1., 1., 0.]]]",
        "answers": [
            "There are a couple ways to approach this. First let's define the arrays:\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.zeros((3, 2, 3))\nindices = jnp.array([[[1, 2],\n                      [1, 2]],\n                     [[0, 2],\n                      [0, 1]],\n                     [[1, 0],\n                      [1, 0]]])\nOne way to do this is to use typical numpy-style broadcasting of indices. It might look like this:\ni = jnp.arange(3).reshape(3, 1, 1)\nj = jnp.arange(2).reshape(2, 1)\nx = x.at[i, j, indices].set(1)\nprint(x)\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]\nAnother option is to use a double-vmap transformation to compute the batched indices:\nf = jax.vmap(jax.vmap(lambda x, i: x.at[i].set(1)))\nprint(f(x, indices))\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]"
        ],
        "link": "https://stackoverflow.com/questions/77799930/update-jax-array-based-on-values-in-another-array"
    },
    {
        "title": "Occurence of NaN in softmax & JIT issues",
        "question": "I'm trying to implement the Transformer architecture from scratch using Jax. I find three issues while training:\njax.disable_jit() does not remove implicit jit compilations.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nclass SelfAttention(eqx.Module):\n    def __call__(self, query, key, value, mask):\n        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n        scaled_dot_prod = mask + scaled_dot_prod\n        return (jax.nn.softmax(scaled_dot_prod) @ value)\n\ndef create_mask(arr):\n    return jnp.where(arr == 0, np.NINF, 0)\n\ndef loss(model, X, y, X_mask, y_mask, labels):\n    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n    y_pred = jnp.where(labels==0, 0, jnp.take(y_pred, labels, axis=-1))\n    count = jnp.count_nonzero(y_pred)\n    return -jnp.sum(y_pred)/count\n\nwith jax.disable_jit():\n    for e in range(EPOCHS):\n        total_loss = 0\n        num_batches = 0\n        total_tokens = 0\n        for i, (Xbt, ybt, labelbt) in enumerate(dataloader(Xtr, ytr, SEQ_LEN)):\n            total_tokens += len([token for seq in labelbt for token in list(filter(lambda x: x!=0, seq))])\n            Xbt, ybt, labelbt = [jnp.array(x) for x in (Xbt, ybt, labelbt)]\n            Xmask, ymask = [create_mask(x) for x in (Xbt, ybt)]\n\n            model, opt_state, batch_loss = step(model, opt_state, Xbt, ybt, Xmask, ymask, labelbt)\n            total_loss += batch_loss\n            num_batches += 1\n\n            if num_batches % 20 == 0:\n                print(f\"Batches trained: {num_batches} | Avg. Batch loss: {total_loss/num_batches}\")\n\n        epoch_loss = total_loss / num_batches\n        print(f\"Epoch {e} | loss: {epoch_loss}\")\nError:\ndef _softmax_deprecated(\n    478     x: ArrayLike,\n    479     axis: Optional[Union[int, tuple[int, ...]]] = -1,\n    480     where: Optional[ArrayLike] = None,\n    481     initial: Optional[ArrayLike] = None) -> Array:\n    482   x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n--> 483   unnormalized = jnp.exp(x - lax.stop_gradient(x_max))\n    484   result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)\n    485   if where is not None:\n\nFloatingPointError: invalid value (nan) encountered in jit(sub)\nThe above problem is encountered only after crossing 200 batches of training. I haven't checked that skipping the specific batch where the error occurs. Maybe I sohuld inspect if some specific inputs are the resason to this error.\nBut I can't find the answer to the above 3 questions :(",
        "answers": [
            "To answer your questions:\njax.disable_jit() does not remove implicit jit compilations.\nIf this is true, it is a bug and you should report it on the JAX issue tracker. It's unclear from your question what makes you believe this is the case.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nBecause _softmax_deprecated is the old default algorithm, that will someday be deprecated but the deprecation has not happened yet. See https://github.com/google/jax/pull/15677 for details. To use the newer algorithm, you can set the jax_softmax_custom_jvp=True configuration.\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nYou didn't include enough code to reproduce your issue (next time, try to add a minimal reproducible example to allow others to answer your question without guesswork). But it would be worth setting jax_softmax_custom_jvp=True to see if that addresses your issue. The pull request linked above has details."
        ],
        "link": "https://stackoverflow.com/questions/77677455/occurence-of-nan-in-softmax-jit-issues"
    },
    {
        "title": "Why does installing JAX with Docker create such a large image?",
        "question": "I am trying to pip install JAX using Docker and I found that using it just blows up the size of Docker image. The size of image currently is 4.82 GB.\nI made sure to bypass caching while installing packages by doing --no-cache-dir. While that did reduce the size, the size is still unreasonable huge.\nHere is my Dockerfile -\nFROM ubuntu:22.04\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    libosmesa6-dev \\\n    sudo \\\n    wget \\\n    curl \\\n    unzip \\\n    gcc \\\n    g++\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\nRUN mkdir -p ~/miniconda3\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nRUN bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nRUN rm -rf ~/miniconda3/miniconda.sh\nRUN ~/miniconda3/bin/conda init bash\nRUN conda init\n\nRUN pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nThis is how I built it -\ndocker build -t tbd_jax .\nWhen I do docker images, I get this -\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\ntbd_jax      latest    812292e2264e   7 minutes ago    4.82GB\nAfter doing docker history --no-trunc tbd_jax:latest -\nSIZE      COMMENT\nsha256:812292e2264e4340b7715956824055d7409f9546f8dfa54ccad1da056febf300   8 minutes ago    RUN |1 PATH=/root/miniconda3/bin:/root/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin /bin/sh -c pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # buildkit   3.54GB    buildkit.dockerfile.v0\nIs there something I can do to reduce the size? I am a bit of a Docker and Linux newbie so pardon my slowness.",
        "answers": [
            "Note that jax[cuda12_pip] installs all the cuda drivers listed here:\n'cuda12_pip': [\n  ...\n  \"nvidia-cublas-cu12>=12.2.5.6\",\n  \"nvidia-cuda-cupti-cu12>=12.2.142\",\n  \"nvidia-cuda-nvcc-cu12>=12.2.140\",\n  \"nvidia-cuda-runtime-cu12>=12.2.140\",\n  \"nvidia-cudnn-cu12>=8.9\",\n  \"nvidia-cufft-cu12>=11.0.8.103\",\n  \"nvidia-cusolver-cu12>=11.5.2\",\n  \"nvidia-cusparse-cu12>=12.1.2.141\",\n  \"nvidia-nccl-cu12>=2.18.3\",\nThese nvidia driver packages are quite large: for example the nvidia_cublas_cu12 wheel is over 400MB, and nvidia-cudnn-cu12 is over 700MB. You may be able to do better by setting up your docker image with system-native CUDA & CUDNN drivers, installed via apt. You can find a description of the requirements here. You can also use NVIDIA's pre-defined GPU containers, as mentioned here."
        ],
        "link": "https://stackoverflow.com/questions/77669055/why-does-installing-jax-with-docker-create-such-a-large-image"
    },
    {
        "title": "How to understand and debug memory usage with JAX?",
        "question": "I am new to JAX and trying to learn use it for running some code on a GPU. In my example I want to search for regular grids in a point cloud (for indexing X-ray diffraction data).\nWith test_mats[4_000_000,3,3] the memory usage seems to be 15 MB. But with test_mats[5_000_000,3,3] I get an error about it wanting to allocate 19 GB.\nI can't tell whether this is a glitch in JAX, or because I am doing something wrong. My example code and output are below. I guess the problem is that it wants to create a temporary array of (N, 3, gvec.shape[1]) before doing the reduction, but I don't know how to see the memory profile for what happens inside the jitted/vmapped function.\nimport sys\nimport os\nimport jax\nimport jax.random\nimport jax.profiler\n\nprint('jax.version.__version__',jax.version.__version__)\n\nimport scipy.spatial.transform\nimport numpy as np\n\n# (3,N) integer grid spot positions\nhkls = np.mgrid[-3:4, -3:4, -3:4].reshape(3,-1)\n\nUmat = scipy.spatial.transform.Rotation.random( 10, random_state=42 ).as_matrix()\na0 = 10.13\ngvec = np.swapaxes( Umat.dot(hkls)/a0, 0, 1 ).reshape(3,-1)\n\ndef count_indexed_peaks_hkl( ubi, gve, tol ):\n    \"\"\" See how many gve this ubi can account for \"\"\"\n    hkl_real = ubi.dot( gve )\n    hkl_int = jax.numpy.round( hkl_real )\n    drlv2 = ((hkl_real - hkl_int)**2).sum(axis=0)\n    npks = jax.numpy.where( drlv2 < tol*tol, 1, 0 ).sum()\n    return npks\n\ndef testsize( N ):\n    print(\"Testing size\",N)\n    jfunc = jax.vmap( jax.jit(count_indexed_peaks_hkl), in_axes=(0,None,None))\n    key = jax.random.PRNGKey(0)\n    test_mats = jax.random.orthogonal(key, 3, (N,) )*a0\n    dev_gvec = jax.device_put( gvec )\n    scores = jfunc( test_mats, gvec, 0.01 )\n    jax.profiler.save_device_memory_profile(f\"memory_{N}.prof\")\n    os.system(f\"~/go/bin/pprof -top {sys.executable} memory_{N}.prof\")\n\ntestsize(400000)\ntestsize(500000)\nOutput is:\ngpu4-03:~/Notebooks/JAXFits % python mem.py \njax.version.__version__ 0.4.16\nTesting size 400000\nFile: python\nType: space\nShowing nodes accounting for 15.26MB, 99.44% of 15.35MB total\nDropped 25 nodes (cum <= 0.08MB)\n      flat  flat%   sum%        cum   cum%\n   15.26MB 99.44% 99.44%    15.26MB 99.44%  __call__\n         0     0% 99.44%    15.35MB   100%  [python]\n         0     0% 99.44%     1.53MB 10.00%  _pjit_batcher\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl_python\n         0     0% 99.44%    15.30MB 99.70%  _python_pjit_helper\n         0     0% 99.44%    15.35MB   100%  bind\n         0     0% 99.44%    15.35MB   100%  bind_with_trace\n         0     0% 99.44%    15.30MB 99.70%  cache_miss\n         0     0% 99.44%    15.30MB 99.70%  call_impl_cache_miss\n         0     0% 99.44%     1.53MB 10.00%  call_wrapped\n         0     0% 99.44%    13.74MB 89.51%  deferring_binary_op\n         0     0% 99.44%    15.35MB   100%  process_primitive\n         0     0% 99.44%    15.30MB 99.70%  reraise_with_filtered_traceback\n         0     0% 99.44%    15.35MB   100%  testsize\n         0     0% 99.44%     1.53MB 10.00%  vmap_f\n         0     0% 99.44%    15.31MB 99.74%  wrapper\nTesting size 500000\n2023-12-14 10:26:23.630474: W external/tsl/tsl/framework/bfc_allocator.cc:296] Allocator\n(GPU_0_bfc) ran out of memory trying to allocate 19.18GiB with freed_by_count=0. The caller\nindicates that this is not a failure, but this may mean that there could be performance \ngains if more memory were available.\nTraceback (most recent call last):\n  File \"~/Notebooks/JAXFits/mem.py\", line 38, in <module>\n    testsize(500000)\n  File \"~/Notebooks/JAXFits/mem.py\", line 33, in testsize\n    scores = jfunc( test_mats, gvec, 0.01 )\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to \nallocate 20596777216 bytes.\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following \nexception. Set JAX_TRACEBACK_FILTERING=off to include these.```",
        "answers": [
            "The vmapped function is attempting to create an intermediate array of shape [N, 3, 3430]. For N=400_000, with float32 this amounts to 15GB, and for N=500_000 this amounts to 19GB.\nYour best option in this situation is probably to split your computation into sequentially-executed batches using lax.map or similar. Unfortunately there's not currently any automatic way to do that kind of chunked vmao, but there is a relevant feature request at https://github.com/google/jax/issues/11319, and there are some useful suggestions in that thread."
        ],
        "link": "https://stackoverflow.com/questions/77659069/how-to-understand-and-debug-memory-usage-with-jax"
    },
    {
        "title": "JAX dynamic slice inside of control flow function",
        "question": "I would like to do dynamic slicing inside of lax.while_loop() using a variable carried over, getting an error as below. I know in the case of a simple function, I can pass the variable as a static value, using partial , but how can I handle the case in which the variable (in my case length) is carried over?\nnew_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got (0, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThis is how I coded. This code is just to illustrate the problem. What I would like to do is to extract a part of u and do some operations. Thank you.\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import lax\nfrom functools import partial\nimport jax\n\nu = jnp.array([[1,2,3,4,5],[0,0,0,0,0]])\n\ndef body_fun(carry):\n    length, sum_u = carry\n    new_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n    #new_u = lax.dynamic_slice(u,(0,0),(2,4))\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n\ndef cond_fun(carry):\n    length, sum_u = carry\n    keep_condition = sum_u < 5\n    return keep_condition\n\ninit_carry = (5,10)\nout = lax.while_loop(cond_fun, body_fun, init_carry)\nprint(out)",
        "answers": [
            "The problem is that you are attempting to construct a dynamically-shaped array, and JAX does not support dynamically-shaped arrays (length is a dynamic variable in your loop). See JAX Sharp Bits: Dynamic Shapes for more.\nA typical strategy in these cases is to use a statically-sized array while masking out a dynamic range of values; in your case, you could use a value of 0 for the masked values so that they don't contribute to the sum. It might look like this:\ndef body_fun(carry):\n    length, sum_u = carry\n    idx = jnp.arange(u.shape[1])\n    new_u = jnp.where(idx < length, u, 0)\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n(Side-note: it seems like you were using dynamic_slice in hopes that you could generate dynamic array shapes, but the dynamic in dynamic_slice refers to the dynamic offset, not a dynamic size)."
        ],
        "link": "https://stackoverflow.com/questions/77603954/jax-dynamic-slice-inside-of-control-flow-function"
    },
    {
        "title": "Efficient copying of an ensemble in JAX",
        "question": "I have an ensemble of models and want to assign the same parameters to each of the models. Both the models' parameters as well as the new parameters have the same underlying structure. Currently I use the following approach that uses a for-loop.\nimport jax\nimport jax.numpy as jnp\n\nmodel1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nmodel2 = [\n    [jnp.asarray([2]), jnp.asarray([3, 4])],\n    [jnp.asarray([5]), jnp.asarray([6, 7])],\n]\n\nmodels = [model1, model2]\n\nparams = [\n    [jnp.asarray([3]), jnp.asarray([4, 5])],\n    [jnp.asarray([6]), jnp.asarray([7, 8])],\n]\n\nmodels = [jax.tree_map(jnp.copy, params) for _ in range(len(models))]\nIs there a more efficient way in JAX to assign the parameters from params to each model in models?",
        "answers": [
            "Since JAX arrays are immutable, there's no need to copy the parameter arrays, and you could achieve the same result like this:\nmodels = len(models) * [params]"
        ],
        "link": "https://stackoverflow.com/questions/77595758/efficient-copying-of-an-ensemble-in-jax"
    },
    {
        "title": "Parallelize inference of ensemble",
        "question": "I used the this tutorial from JAX to create an ensemble of networks. Currently I compute the loss of each network in a for-loop which I would like to avoid:\nfor params in ensemble_params:\n    loss = mse_loss(params, inputs=x, targets=y)\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\nHere ensemble_params is a list of pytrees (lists of tuples holding JAX parameter arrays). The parameter structure of each network is the same.\nI tried to get rid of the for-loop by applying jax.vmap:\nensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\nHowever, I keep getting the following error message which I do not understand.\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (8 of them) had size 3, e.g. axis 0 of argument params[0][0][0] of type float32[3,2];\n  * some axes (8 of them) had size 4, e.g. axis 0 of argument params[0][1][0] of type float32[4,3]\nHere is a minimal reproducible example:\nimport jax\nfrom jax import Array\nfrom jax import random\nimport jax.numpy as jnp\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits\n\nbatched_predict = jax.vmap(predict, in_axes=(None, 0))\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\n\nif __name__ == \"__main__\":\n\n    num_models = 4\n    dim_in = 2\n    dim_out = 4\n    layer_dims = [dim_in, 3, dim_out]\n    batch_size = 2\n\n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble_params = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    key_x, key_y = random.split(key)\n    x = random.normal(key=key_x, shape=(batch_size, dim_in))\n    y = random.normal(key=key_y, shape=(batch_size, dim_out))\n\n    for params in ensemble_params:\n        loss = mse_loss(params, inputs=x, targets=y)\n        print(f\"{loss = }\")\n\n    ensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\n    losses = ensemble_loss(ensemble_params, x, y)\n    print(f\"{losses = }\")  # Same losses expected as above.",
        "answers": [
            "The main issue here is that vmap maps over arrays, not over lists.\nYou are passing a list of parameter objects, expecting vmap to map over the elements of that list. However, the semantics of vmap are that it maps over the first axis of each tree leaf in the argument, and the leaves in your argument differ in their leading axis.\nTo fix this, instead of passing a list of parameter objects containing unbatched arrays, you need to pass a single parameter object containing batched arrays; in other words you need a struct-of-arrays pattern rather than a list-of-structs pattern.\nIn your case, you can create your batched ensemble parameters this way:\nensemble_params = jax.tree_map(lambda *args: jnp.stack(args), *ensemble_params)\nIf you pass this to the ensemble_loss function, you get the expected output:\nlosses = Array([3.762451 , 4.39846  , 4.1425314, 6.045669 ], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77581033/parallelize-inference-of-ensemble"
    },
    {
        "title": "Weighted sum of pytrees in JAX",
        "question": "I have a pytree represented by a list of lists holding parameter tuples. The sub-lists all have the same structure (see example).\nNow I would like to create a weighted sum so that the resulting pytree has the same structure as one of the sub-lists. The weights for each sub-list are stored in a separate array / list.\nSo far I have the following code that seems to works but requires several steps and for-loop that I would like avoid for performance reasons.\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n]\n\nlist_2 = [\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n]\n\nlist_3 = [\n    [jnp.asarray([[7, 1], [4, 4]]), jnp.asarray([6, 2])],\n    [jnp.asarray([[6, 4], [3, 7]]), jnp.asarray([7, 3])],\n]\n\nweights = [1, 2, 3] \npytree = [list_1, list_2, list_3]\n\nweighted_pytree = [jax.tree_map(lambda tree: weight * tree, tree) for weight, tree in zip(weights, pytree)]\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *weighted_pytree)",
        "answers": [
            "I think this will do what you have in mind:\ndef wsum(*args, weights=weights):\n  return jnp.asarray(weights) @ jnp.asarray(args)\n\nreduced = jax.tree_util.tree_map(wsum, *pytree)\nFor the edited question, where tree elements have more general shapes, you can define wsum like this instead:\ndef wsum(*args, weights=weights):\n  return sum(weight * arg for weight, arg in zip(weights, args))"
        ],
        "link": "https://stackoverflow.com/questions/77550969/weighted-sum-of-pytrees-in-jax"
    },
    {
        "title": "Reduce list of lists in JAX",
        "question": "I have a list holding many lists of the same structure (Usually, there are much more than two sub-lists inside the list, the example shows two lists for the sake of simplicity). I would like to create the sum or product over all sub-lists so that the resulting list has the same structure as one of the sub-lists. So far I tried the following using the tree_reduce method but I get errors that I don't understand.\nI could need some guidance on how to use tree_reduce() in such a case.\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nlist_2 = [\n    [jnp.asarray([7]), jnp.asarray([8, 9])],\n    [jnp.asarray([10]), jnp.asarray([11, 12])],\n]\n    \nlist_of_lists = [list_1, list_2]\n   \nreduced = jax.tree_util.tree_reduce(lambda x, y: x + y, list_of_lists, 0, is_leaf=True)\n    \n# Expected\n# reduced = [\n#     [jnp.asarray([8]), jnp.asarray([10, 12])],\n#     [jnp.asarray([14]), jnp.asarray([16, 18])],\n# ]",
        "answers": [
            "You can do this with tree_map of a sum over the splatted list:\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *list_of_lists)\nprint(reduced)\n[[Array([8], dtype=int32), Array([10, 12], dtype=int32)],\n [Array([14], dtype=int32), Array([16, 18], dtype=int32)]]"
        ],
        "link": "https://stackoverflow.com/questions/77548225/reduce-list-of-lists-in-jax"
    },
    {
        "title": "Add noise to parameters of ensemble in JAX",
        "question": "I use the following code to create parameters for an ensemble of models stored as list of lists holding tuples of weights and biases. How do I efficiently add random noise to all parameters of the ensemble with JAX? I tried to use tree_map() but run into many errors probably caused by the nested structure.\nCould you please provide guidance on how to use tree_map() in this case or point to other methods that JAX provides for such a case?\nfrom jax import Array\nfrom jax import random\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = 0 * random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = 0 * random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\nif __name__ == \"__main__\":\n    num_models = 2\n    layer_dims = [2, 3, 4]\n    \n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    # Add noise to ensemble.",
        "answers": [
            "Here's one way you could do this:\nfrom jax import tree_util\nleaves, tree = tree_util.tree_flatten(ensemble)\nkey, *subkeys = random.split(key, len(leaves) + 1)\nsubkeys = tree_util.tree_unflatten(tree, subkeys)\n\ndef add_noise(val, key, eps=0.1):\n  return val + eps * random.normal(key, val.shape)\n\nensemble_with_noise = tree_util.tree_map(add_noise, ensemble, subkeys)\nEssentially, you create a tree of subkeys with the same structure as the tree of parameters, then use tree_map to apply the noise function to the tree.",
            "Another possiblity is to use ravel_pytree to first flatten the parameters into a single vector, then add noise, then unravel the noised vector back to the original tree structure:\nimport jax\nimport jax.flatten_util\n\nvector, unravel = jax.flatten_util.ravel_pytree(ensemble)\nnoisy_vector = vector + jax.random.normal(key, vector.shape)\nnoisy_ensemble = unravel(vector)"
        ],
        "link": "https://stackoverflow.com/questions/77539206/add-noise-to-parameters-of-ensemble-in-jax"
    },
    {
        "title": "Jax vmap limit memory",
        "question": "I'm wondering if there is a good way to limit the memory usage for Jax's VMAP function? Equivalently, to vmap in batches at a time if that makes sense?\nIn my specific use case, I have a set of images and I'd like to calculate the affinity between each pair of images; so ~order((num_imgs)^2 * (img shape)) bytes of memory used all at once if I'm understanding vmap correctly (which gets huge since in my real example I have 10,000 100x100 images).\nA basic example is:\ndef affininty_matrix_ex(n_arrays=10, img_size=5, key=jax.random.PRNGKey(0), gamma=jnp.array([0.5])):\n    arr_of_imgs = jax.random.normal(jax.random.PRNGKey(0), (n_arrays, img_size, img_size))\n    arr_of_indices = jnp.arange(n_arrays)\n    inds_1, inds_2 = zip(*combinations(arr_of_indices, 2))\n    v_cPA = jax.vmap(calcPairAffinity2, (0, 0, None, None), 0)\n    affinities = v_cPA(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma)\n    print()\n    print(jax.make_jaxpr(v_cPA)(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma))\n    \n    affinities = affinities.reshape(-1)\n    \n    arr = jnp.zeros((n_arrays, n_arrays), dtype=jnp.float16)\n    arr = arr.at[jnp.triu_indices(arr.shape[0], k=1)].set(affinities)\n    arr = arr + arr.T\n    arr = arr + jnp.identity(n_arrays, dtype=jnp.float16)\n    \n    return arr\n\n\ndef calcPairAffinity2(ind1, ind2, imgs, gamma):\n    #Returns a jnp array of 1 float, jnp.sum adds all elements together\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\nI suppose I could just say something like \"only feed into vmap X pairs at a time, and loop through n_chunks = n_arrays/X, appending each groups results to a list\" but that doesn't seem to be ideal. My understanding is vmap does not like generators, not sure if that would be an alternative way around the issue.",
        "answers": [
            "Edit, Aug 13 2024\nAs of JAX version 0.4.31, what you're asking for is possible using the batch_size argument of lax.map. For an iterable of size N, this will perform a scan with N // batch_size steps, and within each step will vmap the function over the batch. lax.map has less flexible semantics than jax.vmap, but for the simplest cases they look relatively similar. Here's an example using your calcPairAffinity function:\nFor example\nimport jax\nimport jax.numpy as jnp\n\ndef calcPairAffinity(ind1, ind2, imgs, gamma=0.5):\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\n\nimgs = jax.random.normal(jax.random.key(0), (100, 5, 5))\ninds = jnp.arange(imgs.shape[0])\ninds1, inds2 = map(jnp.ravel, jnp.meshgrid(inds, inds))\n\ndef f(inds):\n  return calcPairAffinity(*inds, imgs, 0.5)\n\n\nresult_vmap = jax.vmap(f)((inds1, inds2))\nresult_batched = jax.lax.map(f, (inds1, inds2), batch_size=1000)\nassert jnp.allclose(result_vmap, result_batched)\nOriginal answer\nThis is a frequent request, but unfortunately there's not yet (as of JAX version 0.4.20) any built-in utility to do chunked/batched vmap (xmap does have some functionality along these lines, but is experimental/incomplete and I wouldn't recommend relying on it).\nAdding chunking to vmap is tracked in https://github.com/google/jax/issues/11319, and there's some code there that does a limited version of what you have in mind. Hopefully something like what you describe will be possible with JAX's built-in vmap soon. In the meantime, you might think about applying vmap to chunks manually in the way you describe in your question."
        ],
        "link": "https://stackoverflow.com/questions/77527847/jax-vmap-limit-memory"
    },
    {
        "title": "JAX grad: derivate with respect an specific variable in a matrix",
        "question": "I am using Jax to do the grad of a matrix. For example I have a function f(A) where A is a matrix like A = \\[\\[a,b\\], \\[c,d\\]\\]. I want to just do the grad of f(A) for a,c and d (more specific for the lower-triangular part). How can I do that? also for a general NxN matrix not just the 2x2.\nI tried to convert the regular grad in a lower-triangular, but I am not sure if that is the same of if the output is correct.",
        "answers": [
            "JAX does not offer any way to take the gradient with respect to individual matrix elements. There are two ways you could proceed; first, you could take the gradient with respect to the entire array and extract the elements you're interested in; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(A):\n  return (A ** 2).sum()\n\nA = jnp.array([[1.0, 2.0], [3.0, 4.0]])\ndf_dA = jax.grad(f)(A)\nprint(df_dA[0, 0], df_dA[0, 1], df_dA[1, 2])\n2.0 4.0 8.0\nAlternatively, you could split the entries of the array into individual function arguments, and then use argnums to take the gradient with respect to just the ones you're interested in:\ndef f(a, b, c, d):\n  A = jnp.array([[a, b], [c, d]])\n  return (A ** 2).sum()\n\ndf_da, df_db, df_dc = jax.grad(f, argnums=(0, 1, 2))(1.0, 2.0, 3.0, 4.0)\nprint(df_da, df_db, df_dc)\n2.0 4.0 8.0\nIn general you'll probably find the first approach to be both easier to use in practice, and also more efficient. It does have some wasted computation, but sticking with vectorized computations will generally be a net win, especially if you're running on accelerators like GPU or TPU."
        ],
        "link": "https://stackoverflow.com/questions/77517357/jax-grad-derivate-with-respect-an-specific-variable-in-a-matrix"
    },
    {
        "title": "How to implement nested for loops with branches efficiently in JAX",
        "question": "I am wanting to reimplement a function in jax that loops over a 2d array and modifies the output array at an index that is not necessarily the same as the current iterating index based on conditions. Currently I am implementing this via repeated use of jnp.where for the conditions separately, but the function is ~4x slower than the numba implementation on cpu, on gpu it is ~10x faster - which I suspect is due to the fact that I am iterating over the whole array again for every condition.\nThe numba implementation is as follows:\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport numba as nb\n\nrng = np.random.default_rng()\n\n\n@nb.njit\ndef raytrace_np(ir, dx, dy):\n    assert ir.ndim == 2\n    n, m = ir.shape\n    assert ir.shape == dx.shape == dy.shape\n    output = np.zeros_like(ir)\n\n    for i in range(ir.shape[0]):\n        for j in range(ir.shape[1]):\n            dx_ij = dx[i, j]\n            dy_ij = dy[i, j]\n            \n            dxf_ij = np.floor(dx_ij)\n            dyf_ij = np.floor(dy_ij)\n\n            ir_ij = ir[i, j]\n            index0 = i + int(dyf_ij)\n            index1 = j + int(dxf_ij)\n\n            if 0 <= index0 <= n - 1 and 0 <= index1 <= m - 1:\n                output[index0, index1] += (\n                    ir_ij * (1 - (dx_ij - dxf_ij)) * (1 - (dy_ij - dyf_ij))\n                )\n            if 0 <= index0 <= n - 1 and 0 <= index1 + 1 <= m - 1:\n                output[index0, index1 + 1] += (\n                    ir_ij * (dx_ij - dxf_ij) * (1 - (dy_ij - dyf_ij))\n                )\n            if 0 <= index0 + 1 <= n - 1 and 0 <= index1 <= m - 1:\n                output[index0 + 1, index1] += (\n                    ir_ij * (1 - (dx_ij - dxf_ij)) * (dy_ij - dyf_ij)\n                )\n            if 0 <= index0 + 1 <= n - 1 and 0 <= index1 + 1 <= m - 1:\n                output[index0 + 1, index1 + 1] += (\n                    ir_ij * (dx_ij - dxf_ij) * (dy_ij - dyf_ij)\n                )\n    return output\nand my current jax reimplementation is:\n@jax.jit\ndef raytrace_jax(ir, dx, dy):\n    assert ir.ndim == 2\n    n, m = ir.shape\n    assert ir.shape == dx.shape == dy.shape\n\n    output = jnp.zeros_like(ir)\n\n    dxfloor = jnp.floor(dx)\n    dyfloor = jnp.floor(dy)\n    \n    dxfloor_int = dxfloor.astype(jnp.int64)\n    dyfloor_int = dyfloor.astype(jnp.int64)\n    \n    meshyfloor = dyfloor_int + jnp.arange(n)[:, None]\n    meshxfloor = dxfloor_int + jnp.arange(m)[None]\n\n    validx = (meshxfloor >= 0) & (meshxfloor <= m - 1)\n    validy = (meshyfloor >= 0) & (meshyfloor <= n - 1)\n    validx2 = (meshxfloor + 1 >= 0) & (meshxfloor + 1 <= m - 1)\n    validy2 = (meshyfloor + 1 >= 0) & (meshyfloor + 1 <= n - 1)\n\n    validxy = validx & validy\n    validx2y = validx2 & validy\n    validxy2 = validx & validy2\n    validx2y2 = validx2 & validy2\n    \n    dx_dxfloor = dx - dxfloor\n    dy_dyfloor = dy - dyfloor\n\n    output = output.at[\n        jnp.where(validxy, meshyfloor, 0), jnp.where(validxy, meshxfloor, 0)\n    ].add(\n        jnp.where(validxy, ir * (1 - dx_dxfloor) * (1 - dy_dyfloor), 0)\n    )\n    output = output.at[\n        jnp.where(validx2y, meshyfloor, 0),\n        jnp.where(validx2y, meshxfloor + 1, 0),\n    ].add(jnp.where(validx2y, ir * dx_dxfloor * (1 - dy_dyfloor), 0))\n    output = output.at[\n        jnp.where(validxy2, meshyfloor + 1, 0),\n        jnp.where(validxy2, meshxfloor, 0),\n    ].add(jnp.where(validxy2, ir * (1 - dx_dxfloor) * dy_dyfloor, 0))\n    output = output.at[\n        jnp.where(validx2y2, meshyfloor + 1, 0),\n        jnp.where(validx2y2, meshxfloor + 1, 0),\n    ].add(jnp.where(validx2y2, ir * dx_dxfloor * dy_dyfloor, 0))\n    return output\nTest and timings:\nshape = 2000, 2000\nir = rng.random(shape)\ndx = (rng.random(shape) - 0.5) * 5\ndy = (rng.random(shape) - 0.5) * 5\n\n_raytrace_np = raytrace_np(ir, dx, dy)\n_raytrace_jax = raytrace_jax(ir, dx, dy).block_until_ready()\n\nassert np.allclose(_raytrace_np, _raytrace_jax)\n\n%timeit raytrace_np(ir, dx, dy)\n%timeit raytrace_jax(ir, dx, dy).block_until_ready()\nOutput:\n14.3 ms ± 84.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n62.9 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nSo is there a way to implement this algorithm in jax with performance more comparable to the numba implementation?",
        "answers": [
            "The way you implemented it in JAX is pretty close to what I'd recommend. Yes, it's 3x slower than a custom Numba implementation on CPU, but I think for an operation like this, that is to be expected.\nThe operation you defined applies specific logic to each individual entry of the array – that is precisely the computational regime that Numba is designed for, and precisely the kind of computation that CPUs were designed for: it's not surprising that with Numba on CPU your computation is very fast.\nI suspect the reason you used Numba rather than NumPy here is that NumPy is not designed for this sort of algorithm: it is an array-oriented language, not an array-element-oriented language. JAX/XLA is more similar to NumPy than to Numba: it is an array-oriented language; it encodes operations across whole arrays at once, rather than choosing a different computation per-element.\nThe benefit of this array-oriented computing model becomes really apparent when you move away from CPU and run the code on an accelerator like a GPU or TPU: this hardware is specifically designed for vectorized array operations, which is why you found that the same, array-oriented code was 10x faster on GPU."
        ],
        "link": "https://stackoverflow.com/questions/77459386/how-to-implement-nested-for-loops-with-branches-efficiently-in-jax"
    },
    {
        "title": "JAX @jit for nested class method",
        "question": "I am trying to use @jit with nested function, having a problem. I have a class One that take in another class Plant with a method func. I would like to call this method jitted func from One. I think that I followed the FAQ of JAX, \"How to use jit with methods?\" section. https://jax.readthedocs.io/en/latest/faq.html#how-to-use-jit-with-methods However, I encountered an error saying that TypeError: One.__init__() got multiple values for argument 'plant'. Would anyone tell me how to solve this?\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np\nfrom functools import partial\nfrom jax import tree_util\n\nclass One:\n    def __init__(self, plant,x):\n        self.plant = plant\n        self.x = x\n    \n    @jit\n    def call_plant_func(self,y):\n        out = self.plant.func(y) + self.x\n        return out\n    \n    def _tree_flatten(self):\n        children = (self.x,)  # arrays / dynamic values\n        aux_data = {'plant':self.plant}  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        import pdb; pdb.set_trace();\n        return cls(*children, **aux_data)\n        \ntree_util.register_pytree_node(One,\n                               One._tree_flatten,\n                               One._tree_unflatten)    \n    \nclass Plant:\n    def __init__(self, z,kk):\n        self.z =z\n    \n    @jit\n    def func(self,y):\n        y = y + self.z\n        return y\n    \n    def _tree_flatten(self):\n        children = (self.z,)  # arrays / dynamic values\n        aux_data = None # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, children):\n        return cls(*children)\n   \ntree_util.register_pytree_node(Plant,\n                               Plant._tree_flatten,\n                               Plant._tree_unflatten)\n\nplant = Plant(5,2)\none = One(plant,2)\nprint(one.call_plant_func(10))\nThe last line gives me an error described above.",
        "answers": [
            "You have issues in the tree_flatten and tree_unflatten code in both classes.\nOne._tree_flatten treats plant as static data, but it is not: it is a pytree that has non-static elements.\nOne._tree_unflatten instantiates One with arguments in the wrong order, leading to the error you're seeing\nPlant.__init__ does nothing with the kk argument.\nPlant._tree_unflatten is missing the aux_data argument, and fails to pass the kk argument to Plant.__init__\nWith these issues fixed, your code executes without error:\nclass One:\n    def __init__(self, plant,x):\n        self.plant = plant\n        self.x = x\n    \n    @jit\n    def call_plant_func(self,y):\n        out = self.plant.func(y) + self.x\n        return out\n    \n    def _tree_flatten(self):\n        children = (self.plant, self.x)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n        \ntree_util.register_pytree_node(One,\n                               One._tree_flatten,\n                               One._tree_unflatten)    \n    \nclass Plant:\n    def __init__(self, z, kk):\n        self.kk = kk\n        self.z =z\n    \n    @jit\n    def func(self, y):\n        y = y + self.z\n        return y\n    \n    def _tree_flatten(self):\n        children = (self.z, self.kk)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n   \ntree_util.register_pytree_node(Plant,\n                               Plant._tree_flatten,\n                               Plant._tree_unflatten)\n\nplant = Plant(5,2)\none = One(plant,2)\nprint(one.call_plant_func(10))"
        ],
        "link": "https://stackoverflow.com/questions/77439217/jax-jit-for-nested-class-method"
    },
    {
        "title": "JAX vmap JIT behind the scenes? [closed]",
        "question": "Closed. This question needs to be more focused. It is not currently accepting answers.\nWant to improve this question? Guide the asker to update the question so it focuses on a single, specific problem. Narrowing the question will help others answer the question concisely. You may edit the question if you feel you can improve it yourself. If edited, the question will be reviewed and might be reopened.\nClosed 25 days ago.\nImprove this question\nI'm trying to vmap a function. My understanding of vmap is essentially anywhere I would write a ~for loop/list comprehension I should instead consider vmapping. I have a few points of confusion:\nDoes vmap need fixed sizes for everything through the function(s) being vmapped?\nDoes vmap try to JIT my function behind the scenes? (Wondering bc. 1 is a behavior I expect from JIT, I didn't expect it from vmap but I don't really know vmap).\nIf vmap is jit-ing something, how would one use something like a static-arguments with vmap?\nWhat is the best practice for dealing with ~extraneous information (eg if some outputs are sized a and some sized b, do you just make an array sized max(a,b) then ~ignore the extra values?)\nThe reason I'm asking is that it seems like vmap, like JIT, runs into all sorts of ConcretizationTypeError and seems (not 100% clear yet) to need constant sized items for everything. I associate this behavior with any function I'm trying to Jit, but not necessarily any function I write in Jax.",
        "answers": [
            "Does vmap need fixed sizes for everything through the function(s) being vmapped?\nyes – vmap, like all JAX transformations, requires any arrays defined in the function to have static shapes.\nDoes vmap try to JIT my function behind the scenes? (Wondering bc. 1 is a behavior I expect from JIT, I didn't expect it from vmap but I don't really know vmap).\nNo, vmap does not jit-compile a function by default, although you can always compose both if you wish (e.g. jit(vmap(f)))\nIf vmap is jit-ing something, how would one use something like a static-arguments with vmap?\nAs mentioned, vmap is unrelated to jit, but an analogy of jit static_argnums is passing None to in_axes, which will keep the argument unmapped and therefore static within the transformation.\nWhat is the best practice for dealing with ~extraneous information (eg if some outputs are sized a and some sized b, do you just make an array sized max(a,b) then ~ignore the extra values?)",
            "A section of my code now looks like:\nvmaped_f = jax.vmap(my_func, parallel_axes, 0)\nn_batches = int(num_items / batch_size)\nif num_items % batch_size != 0:\n    n_batches += 1 #Round up\n    \nall_vals = []\nfor i in range(n_batches):\n    top = min([num_items, (i+1)*batch_size])\n    batch_inds = jnp.arange(i*batch_size, top)\n    batch_inds_1, batch_inds_2 = jnp.array(inds_1)[batch_inds], \\\n                                 jnp.array(inds_2)[batch_inds]\n    f_vals = vmaped_f(batch_inds_1, batch_inds2, other_relevant_inputs)\n    all_vals.extend(f_vals.tolist())\nThe vmap'd function basically takes in all of my data, and the indices of that data to use (which will be constant sized except for potentially the last batch, so only need to jit compile 2x if you'd want to jit it)."
        ],
        "link": "https://stackoverflow.com/questions/77427904/jax-vmap-jit-behind-the-scenes"
    },
    {
        "title": "Idiomatic ways to handle errors in JAX jitted functions",
        "question": "As the title states, I'd like to know what idiomatic methods are available to raise exceptions or handle errors in JAX jitted functions. The functional nature of JAX makes it unclear how to accomplish this.\nThe closest official documentation I could find is the jax.experimental.checkify module, but this wasn't very clear and seemed incomplete.\nThis Github comment claims that Python exceptions can be raised by using jax.debug.callback() and jax.lax.cond() functions. I attempted to do this, but an error is thrown during compilation. A minimum working example is below:\nimport jax\nfrom jax import jit\n\ndef _raise(ex):\n    raise ex\n\n\n@jit\ndef error_if_positive(x):\n    jax.lax.cond(\n        x > 0,\n        lambda : jax.debug.callback(_raise, ValueError(\"x is positive\")),\n        lambda : None,\n    )\n\nif __name__ == \"__main__\":\n\n    error_if_positive(-1)\nThe abbreviated error statement:\nTypeError: Value ValueError('x is positive') with type <class 'ValueError'> is not a valid JAX type",
        "answers": [
            "You can use callbacks to raise errors, for example:\nimport jax\nfrom jax import jit\n\ndef _raise_if_positive(x):\n  if x > 0:\n    raise ValueError(\"x is positive\")\n\n@jit\ndef error_if_positive(x):\n  jax.debug.callback(_raise_if_positive, x)\n\nif __name__ == \"__main__\":\n  error_if_positive(-1)  # no error\n  error_if_positive(1)\n  # XlaRuntimeError: INTERNAL: Generated function failed: CpuCallback error: ValueError: x is positive\nThe reason your approach didn't work is becuase your error is raised at trace-time rather than at runtime, and both branches of the cond will always be traced."
        ],
        "link": "https://stackoverflow.com/questions/77381356/idiomatic-ways-to-handle-errors-in-jax-jitted-functions"
    },
    {
        "title": "Convert for loop to jax.lax.scan",
        "question": "How does one convert the following (to accelerate compiling)? The for loop version works with jax.jit,\nimport functools\nimport jax\nimport jax.numpy as jnp\n\n@functools.partial(jax.jit, static_argnums=0)\ndef func(n):\n\n    p = 1\n    x = jnp.arange(8)\n    y = jnp.zeros((n,))\n\n    for idx in range(n):\n        y = y.at[idx].set(jnp.sum(x[::p]))\n        p = 2*p\n\n    return y\n\nfunc(2)\n# >> Array([28., 12.], dtype=float32)\nbut will return static start/stop/step errors when using scan\nimport numpy as np\n\ndef body(p, xi):\n\n    y = jnp.sum(x[::p])\n\n    p = 2*p\n\n    return p, y\n\nx = jnp.arange(8)\n\njax.lax.scan(body, 1, np.arange(2))\n# >> IndexError: Array slice indices must have static start/stop/step ...",
        "answers": [
            "The issue here is that within scan, the p variable represents a dynamic value, meaning that x[::p] is a dynamically-sized array, so the operation is not allowed in JAX transformations (see JAX sharp bits: dynamic shapes).\nOften in such cases it's possible to replace approaches using dynamically-shaped intermediates with other approaches that compute the same thing using only use static arrays; in this case one thing you might do is replace this problematic line:\njnp.sum(x[::p])\nwith this, which does the same sum using only statically-sized arrays:\njnp.sum(x, where=jnp.arange(len(x)) % p == 0)\nUsing this idea, here's a version of your original function that uses scan:\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnums=0)\ndef func_scan(n):\n    p = 1\n    x = jnp.arange(8)\n    y = jnp.zeros((n,))\n\n    def body(carry, _):\n      idx, y, p = carry\n      y = y.at[idx].set(jnp.sum(x, where=jnp.arange(len(x)) % p == 0))\n      return (idx + 1, y, 2 * p), None\n\n    (i, y, p), _ = jax.lax.scan(body, (0, y, p), xs=None, length=n)\n    return y\n\nfunc_scan(2)\n# Array([28., 12.], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77364000/convert-for-loop-to-jax-lax-scan"
    },
    {
        "title": "Why matrix multiplication results with JAX are different if the data is sharded differently on the GPU",
        "question": "I am running a tutorial on muatrix multiplication with JAX with data sharded in different ways across multiple GPUs. I found not only the computation time is different for different way of sharding, the results are also slightly different.\nHere are my observations:\nResults of method 1 is exactly the same as method 0; but different from method\nComputation speed is the fastest in method 1, then method 2, then method 0.\nCan anyone help me understand these two observations? One additional question is: if the way of sharding is so important, will mainstream machine learning algorithms have ways to deal with it automatically so that different way of sharding won't give different models?\nMethod 0: Perform the matrix multiplication on the same GPU device (just use 1 device).\nMethod 1:\nx = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\ny = jax.device_put(x, sharding.reshape(4, 2).replicate(1))\nz = jax.device_put(x, sharding.reshape(4, 2).replicate(0))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)\nMethod 2:\nx = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\ny = jax.device_put(x, sharding.reshape(4, 2))\nz = jax.device_put(x, sharding.reshape(4, 2))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)",
        "answers": [
            "Regarding your observation of differing results: this is to be expected with floating point operations. Every time you do a floating point operation, it accumulates a small amount of error, and when you express the \"same\" floating point computation in different ways, the errors accumulate differently.\nHere's an example of this using NumPy:\nimport numpy as np\nx = np.random.rand(10000).astype('float32')\nx_reversed = x[::-1]\nnp.dot(x, x) == np.dot(x_reversed, x_reversed)\n# False\nIf we were dealing with real numbers, we'd expect these two to be identical. But because we're representing our computation with floating point values, the two approaches return slightly different results. This is similar to the situation in your question: different sharding layouts lead to different ordering of the dot product accumulations, which leads to slightly different results.\nRegarding your observation about computation speed: the results seem reasonable. Method 0 is the slowest because it only uses a single device, and method 1 is faster than method 2 because pre-replicating the data means that less data movement is required during the actual computation."
        ],
        "link": "https://stackoverflow.com/questions/77362635/why-matrix-multiplication-results-with-jax-are-different-if-the-data-is-sharded"
    },
    {
        "title": "vectorized minimization and root finding in jax",
        "question": "I have a family of functions parameterized by args\nf(x, args)\nand want to determine the minimum of f over x for N = 1000 values of args. I have access to both the function and its derivative. My first attempt was to loop through the different values of args and use a scipy.optimizer at each iteration, but it takes too long. I believe the operations can be sped up with vectorization. My next attempt was to use jax.vmap inside a jax.scipy.optimize.minimize or jaxopt.ScipyMinimize, but I can't seem to pass more than one value for args.\nAlternatively, I can code my own vectorized optimization method, e.g. bisection, where by vectorized I mean doing operations on arrays for a fixed number of iterations and not stopping early if one of the optimization problems has reached a certain error tolerance level early. I was hoping to use some optimized off-shelf algorithm.\nI was hoping to use some already optimized, off-the-shelf algorithm if an implementation is available in jax.this thread is related, but the args are not changing.",
        "answers": [
            "You can define a function to find the minimum given particular args, and then wrap it in jax.vmap to automatically vectorize it. For example:\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy import optimize\n\ndef f(x, args):\n  a, b = args\n  return jnp.sum(a + (x - b) ** 2)\n\ndef find_min(a, b):\n  x0 = jnp.array([1.0])\n  args = (a, b)\n  return optimize.minimize(f, x0, (args,), method=\"BFGS\")\n\na_grid, b_grid = jnp.meshgrid(jnp.arange(5.0), jnp.arange(5.0))\n\nresults = jax.vmap(find_min)(a_grid.ravel(), b_grid.ravel())\n\nprint(results.success)\n# [ True  True  True  True  True  True  True  True  True  True  True  True\n#   True  True  True  True  True  True  True  True  True  True  True  True\n#   True]\n\nprint(results.x.T)\n# [[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n#   3. 3. 3. 3. 3. 4. 4. 4. 4. 4.]]"
        ],
        "link": "https://stackoverflow.com/questions/77359908/vectorized-minimization-and-root-finding-in-jax"
    },
    {
        "title": "Cross dimensional segmented operation",
        "question": "Say you have the following a array\n>>> a = np.arange(27).reshape((3,3,3))\n>>> a\narray([[[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8]],\n\n       [[ 9, 10, 11],\n        [12, 13, 14],\n        [15, 16, 17]],\n\n       [[18, 19, 20],\n        [21, 22, 23],\n        [24, 25, 26]]], dtype=int64)\nAnd m, an array that specifies segment ids\n>>> m = np.linspace(start=0, stop=6, num=27).astype(int).reshape(a.shape)\n>>> m\narray([[[0, 0, 0],\n        [0, 0, 1],\n        [1, 1, 1]],\n\n       [[2, 2, 2],\n        [2, 3, 3],\n        [3, 3, 3]],\n\n       [[4, 4, 4],\n        [4, 5, 5],\n        [5, 5, 6]]])\nWhen using JAX and wishing to perform, say, a sum over the scalars in a that share the same id in m, we can rely on jax.ops.segment_sum.\n>>> jax.ops.segment_sum(data=a.ravel(), segment_ids=m.ravel())\nArray([10, 26, 42, 75, 78, 94, 26], dtype=int64)\nNote that I had to resort to numpy.ndarray.ravel since ~.segment_sum assumes m to indicate the segments of data along its leading axis.\nQ1 : Can you confirm there is no better approach, either with or without JAX ?\nQ2 : How would one then build n, an array that results from the replacement of the ids with the just-performed sums ? Note that I am not interested in non-vectorized approaches such as numpy.where.\n>>> n\narray([[[10, 10, 10],\n        [10, 10, 26],\n        [26, 26, 26]],\n\n       [[42, 42, 42],\n        [42, 75, 75],\n        [75, 75, 75]],\n\n       [[78, 78, 78],\n        [78, 94, 94],\n        [94, 94, 26]]], dtype=int64)",
        "answers": [
            "Use np.bincount with a as the weights parameter:\ns = np.bincount(m.ravel(), weights = a.ravel())\ns\nOut[]: array([10., 26., 42., 75., 78., 94., 26.])\nAnd to put the values back in the array:\nn = s[m]\nn\nOut[]: \narray([[[10., 10., 10.],\n        [10., 10., 26.],\n        [26., 26., 26.]],\n\n       [[42., 42., 42.],\n        [42., 75., 75.],\n        [75., 75., 75.]],\n\n       [[78., 78., 78.],\n        [78., 94., 94.],\n        [94., 94., 26.]]])",
            "The segment_sum operation is somewhat more specialized than what you're asking about. In the case you describe, I would use ndarray.at directly:\nsums = jnp.zeros(m.max() + 1).at[m].add(a)\nprint(sums[m])\n[[[10. 10. 10.]\n  [10. 10. 26.]\n  [26. 26. 26.]]\n\n [[42. 42. 42.]\n  [42. 75. 75.]\n  [75. 75. 75.]]\n\n [[78. 78. 78.]\n  [78. 94. 94.]\n  [94. 94. 26.]]]\nThis will also work when the segments are non-adjacent."
        ],
        "link": "https://stackoverflow.com/questions/77344277/cross-dimensional-segmented-operation"
    },
    {
        "title": "Why does jnp.einsum produce a different result from manual looping?",
        "question": "Let's say I want to compute an inner product along the last dimension of two matrices\na = jax.random.normal(jax.random.PRNGKey(0), shape=(64,16), dtype=jnp.float32)\nb = jax.random.normal(jax.random.PRNGKey(1), shape=(64,16), dtype=jnp.float32)\nI can do it with jnp.einsum:\ninner_prod1 = jnp.einsum('i d, j d -> i j', a, b)\nor manually call jnp.dot in a loop:\ninner_prod2 = jnp.zeros((64,64))\nfor i1 in range(64):\n  for i2 in range(64):\n    inner_prod2 = inner_prod2.at[i1, i2].set(jnp.dot(a[i1], b[i2]))\nprint(jnp.amax(inner_prod1 - inner_prod2)) # 0.03830552\nThis is quite a large difference between the two, even if they are mathematically equivalent. What gives?",
        "answers": [
            "All operations in floating point accumulate rounding errors, so in general when you express the same operation in two different ways, you should expect the results to not be bitwise-equivalent.\nThe magnitude of the difference you're seeing is larger than is typical for float32 precision; it makes me think you're probably running your code on TPU, where matrix multiplication is done at lower-precision by default. You can adjust this using the default_matmul_precision configuration; for example like this:\nwith jax.default_matmul_precision('float32'):\n  inner_prod1 = jnp.einsum('i d, j d -> i j', a, b)\n  inner_prod2 = jnp.zeros((64,64))\n  for i1 in range(64):\n    for i2 in range(64):\n      inner_prod2 = inner_prod2.at[i1, i2].set(jnp.dot(a[i1], b[i2]))\nIf you do the computation this way, I suspect you'll probably see a smaller difference more typical of float32 computations, on order 1E-6 or so."
        ],
        "link": "https://stackoverflow.com/questions/77334401/why-does-jnp-einsum-produce-a-different-result-from-manual-looping"
    },
    {
        "title": "how to use jit with control flow and arrays without slowing down",
        "question": "import jax\nfrom jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n\narray1 = np.random.normal(size=(1000,1000))\ndef first():\n  for i in range(1000):\n    for j in range(1000):\n      if array1[i,j] >= 0:\n        array1[i,j] = 1\n      else:\n        array1[i,j] = -1\n# %timeit first()\n\nfrom functools import partial\nkey = jax.random.PRNGKey(seed=0)\narray2 = jax.random.normal(key, shape=(1000,1000))\n\n@partial(jit, static_argnums=(0,1,2))\ndef f( i,j):\n  r = jax.lax.cond(array2[i,j] >= 0, lambda x: 1, lambda x: -1, None)\n  # if array2[i,j] >= 0:\n  # # if i == j:\n  #   array2.at[i,j].set(1)\n  # else: array2.at[i,j].set(-1)\n  array2.at[i,j].set(r)\n\n# f_jit = jit(f, static_argnums=(0,1))\ndef second():\n  for i in range(1000):\n    for j in range(1000):\n      # jax.lax.cond(array2[i,j]>=0, lambda x: True, lambda x: False, None)\n      f(i,j)\n%timeit second()\nI have two functions: first and second. I want second to run as fast as (or faster) first does. first function is a function using numpy. second function uses jax. What is the best way to implement first function using jax in this case? jax.lax.cond significantly slows down the process I think.\nI left the comments on purpose to show what I've tried.",
        "answers": [
            "The reason first runs relatively quickly is because it does 1,000,000 numpy array operations, and numpy has been optimized for fast per-operation dispatch.\nThe reason second runs relatively slowly is because it does 1,000,000 JAX array operations, and JAX has not been optimized for fast per-operation dispatch.\nFor some general background on this, see JAX FAQ: Is JAX faster than NumPy?.\nBut if you're asking about the fastest way to accomplish what you're doing, in both NumPy and JAX the answer would be to avoid writing loops. Here is the equivalent, making the computation pure rather than in-place for ease of comparison (your original second function actually does nothing, because array.at[i].set() does not operate in-place):\ndef first_fast(array):\n  return np.where(array >= 0, 1, 0)\n\ndef second_fast(array):\n  return jnp.where(array >= 0, 1, 0)\nIn general, if you find yourself writing loops over array values in NumPy or in JAX, you can expect that the resulting code will be slow. In both NumPy and JAX, there's almost always a better way to compute the result using built-in vectorized operations.\nIf you're interested in further benchmarks between JAX and NumPy, be sure to read FAQ: Benchmarking JAX Code to ensure that you're comparing the right things."
        ],
        "link": "https://stackoverflow.com/questions/77333949/how-to-use-jit-with-control-flow-and-arrays-without-slowing-down"
    },
    {
        "title": "How to get get the index position of a value with jit?",
        "question": "What would be a workaround of this code in jitted function?\nj = indices.index(list(neighbor)) where neighbor is, for example, (2,3), indices = [[1,2], [4,5], ...]\nOther alternatives like partial didn't work. One issue when using partial is that indices is not hashable so can't use partial function.",
        "answers": [
            "list.index is a Python function that will not work within JIT if the contents of the list are traced values. I would recommend converting your lists to arrays, and do something like this:\nimport jax\nimport jax.numpy as jnp\n\nindices = jnp.array([[1, 2], [4, 5], [3, 6], [2, 3], [5, 7]])\nneighbor = jnp.array([2, 3])\n\n@jax.jit\ndef get_index(indices, neighbor):\n  return jnp.where((indices == neighbor).all(-1), size=1)[0]\n\nidx = get_index(indices, neighbor)\nprint(idx)\n# [3]"
        ],
        "link": "https://stackoverflow.com/questions/77328302/how-to-get-get-the-index-position-of-a-value-with-jit"
    },
    {
        "title": "What is the difference between xmap vs pmap as the parallel processing model?",
        "question": "Can anyone help me understand the similarities and differences between the xmap and pmap in JAX? Read through the documentation multiple times, but still cannot understand the new concepts in the tutorials.\nI am specifically interested in if there are any examples for how to convert pmap train setup to xmap train setup.\nIt seems to me that: mesh+xmap can do similar things as pmap. Am I understanding correctly?",
        "answers": [
            "pmap is a simple parallelizing transform that only supports distribution of data over a single leading axis. It is deprecated, and will likely be removed in a future version of JAX.\nxmap is a generalization of pmap that allows for more flexible parallelization over multiple named axes. It has always been experimental, and will likely be removed in a future version of JAX.\nThe best way to do parallel computation in JAX going forward is either implicitly via sharded inputs into jit, or explicitly via shard_map. Unfortunately, neither of these approaches is very well documented at the moment; there is some information at Distributed Arrays and Automatic Parallelization and shard_map for simple per-device code but both are written more for developers than for end-users. That said, more comprehensive docs for the newer parallelism models in JAX are currently in progress, and should be on the website soon."
        ],
        "link": "https://stackoverflow.com/questions/77275923/what-is-the-difference-between-xmap-vs-pmap-as-the-parallel-processing-model"
    },
    {
        "title": "aggregate calculation vmap jax",
        "question": "I'm trying to implement fast routine to calculate array of energies and find the smallest calculated value and its index. Here is my code that is working fine:\n@jit\ndef findMinEnergy(x):\n  def calcEnergy(a):\n    return a*a  # very simplified body, it is actually 15 lines of code\n  energies = vmap(calcEnergy, in_axes=(0))(x)\n  idx = energies.argmin(axis=0)\n  minenrgy = energies[idx]\n  return idx, minenrgy\nI wonder if it is possible to not use the (separate) argmin call, but return the min calculated energy value and it's index from the vmap (similar like other aggregate functions work, e.g. jax.sum)? I hope that it could be more efficient.",
        "answers": [
            "If you JIT-compile your current approach, you should find that it's as efficient as doing something more sophisticated.\nLooking at the implementation of argmin, you'll see that it computes both the value and the index before returning only the index: https://github.com/google/jax/blob/jax-v0.4.18/jax/_src/lax/lax.py#L3892-L3914\nIf you want, you could follow this implementation and define a function using lax.reduce that returns both these values in a single pass:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef min_and_argmin_onepass(x):\n  # This only works for 1D float arrays, but you could generalize it.\n  assert x.ndim == 1\n  assert jnp.issubdtype(x.dtype, jnp.floating)\n  def reducer(op_val_index, acc_val_index):\n    op_val, op_index = op_val_index\n    acc_val, acc_index = acc_val_index\n    pick_op_val = (op_val < acc_val) | jnp.isnan(op_val)\n    pick_op_index = pick_op_val | ((op_val == acc_val) & (op_index < acc_index))\n    return (jnp.where(pick_op_val, op_val, acc_val),\n            jnp.where(pick_op_index, op_index, acc_index))\n  indices = jnp.arange(len(x))\n  return jax.lax.reduce((x, indices), (jnp.inf, 0), reducer, (0,))\nTesting this, we see it matches the output of the less sophisticated approach:\n@jax.jit\ndef min_and_argmin(x):\n  i = jnp.argmin(x)\n  return x[i], i\n\nx = jax.random.uniform(jax.random.key(0), (1000000,))\nprint(min_and_argmin_onepass(x))\n# (Array(9.536743e-07, dtype=float32), Array(24430, dtype=int32))\nprint(min_and_argmin(x))\n# (Array(9.536743e-07, dtype=float32), Array(24430, dtype=int32))\nIf you compare the runtime of the two, you'll see comparable runtimes:\n%timeit jax.block_until_ready(min_and_argmin_onepass(x))\n# 2.17 ms ± 68.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%timeit jax.block_until_ready(min_and_argmin(x))\n# 2.07 ms ± 66.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nThe jax.jit decorator here means that the compiler optimizes the sequence of operations in the less sophisticated approach, and the result is that you don't gain much advantage from trying to express things more cleverly. Given this, I think your best option is to stick with your original code rather than trying to out-optimize the XLA compiler.",
            "Assuming that by efficient you mean not having to keep a large array (energies) in memory, you can just stack the individual values of idx and minenergy into a single array within calcEnergy and return a (2,) array to vmap instead of an (N,) array. It's not pretty as you'll (presumably) have to cast both values to the same dtype, but it should work fine."
        ],
        "link": "https://stackoverflow.com/questions/77254706/aggregate-calculation-vmap-jax"
    },
    {
        "title": "How to use FLAX LSTM in 2023",
        "question": "I am wondering if anyone here knows how to get FLAX LSTM layers to work in 2023. I have tried some of the code snippets on the actual Flax documentation, such as:\nhttps://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.scan.html\nand, the first example provided there,\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\n\nclass LSTM(nn.Module):\n  features: int\n\n  @nn.compact\n  def __call__(self, x):\n    ScanLSTM = nn.scan(\n      nn.LSTMCell, variable_broadcast=\"params\",\n      split_rngs={\"params\": False}, in_axes=1, out_axes=1)\n\n    lstm = ScanLSTM(self.features)\n    input_shape =  x[:, 0].shape\n    carry = lstm.initialize_carry(jax.random.key(0), input_shape)\n    carry, x = lstm(carry, x)\n    return x\n\nx = jnp.ones((4, 12, 7))\nmodule = LSTM(features=32)\ny, variables = module.init_with_output(jax.random.key(0), x)\nthrows an error. I have looked for other examples but it seems they have changed their API at some point in 2023, so what I could find online wasn't working anymore.\nIn short, what I am looking for is a simple example on how to pass a time series into an LSTM in FLAX.\nThank you for your help.",
        "answers": [
            "The snippet you provided runs correctly with the most recent version of flax (version 0.7.4). If you're using an older version of flax, you should change jax.random.key to jax.random.PRNGKey. For some information about this JAX PRNG key change, see JEP 9263: Typed Keys and Pluggable PRNGs."
        ],
        "link": "https://stackoverflow.com/questions/77222395/how-to-use-flax-lstm-in-2023"
    },
    {
        "title": "jax PRNG key error with tfp normal distribution",
        "question": "I am trying to run the following piece of Pyhon code from the Ubuntu WSL from Windows\nimport tensorflow_probability as tfp; tfp = tfp.substrates.jax\ntfd = tfp.distributions\ndist = tfd.Normal(loc=0., scale=3.)\ndist.cdf(1.)\ndist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\ndist.prob([0, 1.5])\ndist.sample([3])\nI am getting the following error\n> AttributeError                            Traceback (most recent call\n> last) Cell In[45], line 4\n>       2 tfd = tfp.distributions\n>       3 dist = tfd.Normal(loc=0., scale=3.)\n> ----> 4 dist.cdf(1.)\n>       5 dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])\n>       6 dist.prob([0, 1.5])\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1429,\n> in Distribution.cdf(self, value, name, **kwargs)    1411 def cdf(self,\n> value, name='cdf', **kwargs):    1412   \"\"\"Cumulative distribution\n> function.    1413     1414   Given random variable `X`, the cumulative\n> distribution function `cdf` is:    (...)    1427       values of type\n> `self.dtype`.    1428   \"\"\"\n> -> 1429   return self._call_cdf(value, name, **kwargs)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1405,\n> in Distribution._call_cdf(self, value, name, **kwargs)    1403 with\n> self._name_and_control_scope(name, value, kwargs):    1404   if\n> hasattr(self, '_cdf'):\n> -> 1405     return self._cdf(value, **kwargs)    1406   if hasattr(self, '_log_cdf'):    1407     return\n> tf.exp(self._log_cdf(value, **kwargs))\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/distributions/normal.py:195,\n> in Normal._cdf(self, x)\n>     194 def _cdf(self, x):\n> --> 195   return special_math.ndtr(self._z(x))\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/internal/special_math.py:136,\n> in ndtr(x, name)\n>     132 if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n>     133   raise TypeError(\n>     134       \"x.dtype=%s is not handled, see docstring for supported types.\"\n>     135       % x.dtype)\n> --> 136 return _ndtr(x)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/substrates/jax/internal/special_math.py:141,\n> in _ndtr(x)\n>     139 def _ndtr(x):\n>     140   \"\"\"Implements ndtr core logic.\"\"\"\n> --> 141   half_sqrt_2 = tf.constant(\n>     142       0.5 * np.sqrt(2.), dtype=x.dtype, name=\"half_sqrt_2\")\n>     143   half = tf.constant(0.5, x.dtype)\n>     144   one = tf.constant(1., x.dtype)\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:117,\n> in _constant(value, dtype, shape, name)\n>     116 def _constant(value, dtype=None, shape=None, name='Const'):  # pylint: disable=unused-argument\n> --> 117   x = convert_to_tensor(value, dtype=dtype)\n>     118   if shape is None:\n>     119     return x\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:167,\n> in _convert_to_tensor(value, dtype, dtype_hint, name)\n>     164     pass\n>     166 if ret is None:\n> --> 167   ret = conversion_func(value, dtype=dtype)\n>     168 return ret\n> \n> File\n> ~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/jax/ops.py:222,\n> in _default_convert_to_tensor(value, dtype)\n>     218 \"\"\"Default tensor conversion function for array, bool, int, float, and complex.\"\"\"\n>     219 if JAX_MODE:\n>     220   # TODO(b/223267515): We shouldn't need to specialize here.\n>     221   if hasattr(value, 'dtype') and jax.dtypes.issubdtype(\n> --> 222       value.dtype, jax.dtypes.prng_key\n>     223   ):\n>     224     return value\n>     225   if isinstance(value, (list, tuple)) and value:\n> \n> AttributeError: module 'jax.dtypes' has no attribute 'prng_key'\nThe installed packages are\n> Package                  Version             \n------------------------ --------------------\nabsl-py                  2.0.0               \nAmbit-Stochastics        1.0.6               \nanyio                    3.6.2               \nargon2-cffi              21.3.0              \nargon2-cffi-bindings     21.2.0              \nasttokens                2.2.1               \nattrs                    19.3.0              \nAutomat                  0.8.0               \nBabel                    2.11.0              \nbackcall                 0.2.0               \nbeautifulsoup4           4.11.2              \nbleach                   6.0.0               \nblinker                  1.4                 \ncertifi                  2019.11.28          \ncffi                     1.15.1              \nchardet                  3.0.4               \ncharset-normalizer       3.0.1               \nClick                    7.0                 \ncloud-init               23.2.1              \ncloudpickle              2.2.1               \ncolorama                 0.4.3               \ncomm                     0.1.2               \ncommand-not-found        0.3                 \nconfigobj                5.0.6               \nconstantly               15.1.0              \ncryptography             2.8                 \ncycler                   0.11.0              \ndbus-python              1.2.16              \ndebugpy                  1.6.6               \ndecorator                5.1.1               \ndefusedxml               0.7.1               \ndistlib                  0.3.6               \ndistro                   1.4.0               \ndistro-info              0.23ubuntu1         \ndm-tree                  0.1.8               \nentrypoints              0.3                 \nenv                      0.1.0               \nexecuting                1.2.0               \nfastjsonschema           2.16.2              \nfilelock                 3.9.0               \ngast                     0.5.4               \nhttplib2                 0.14.0              \nhyperlink                19.0.0              \nidna                     2.8                 \nimportlib-metadata       6.0.0               \nimportlib-resources      5.10.2              \nincremental              16.10.1             \nipykernel                6.20.2              \nipython                  8.9.0               \nipython-genutils         0.2.0               \nipywidgets               8.1.1               \njax                      0.4.13              \njaxlib                   0.4.13              \njedi                     0.18.2              \nJinja2                   3.1.2               \njson5                    0.9.11              \njsonpatch                1.22                \njsonpointer              2.0                 \njsonschema               4.17.3              \njupyter                  1.0.0               \njupyter-client           8.0.2               \njupyter-console          6.6.3               \njupyter-core             5.2.0               \njupyter-events           0.6.3               \njupyter-server           2.2.0               \njupyter-server-terminals 0.4.4               \njupyterlab               3.5.3               \njupyterlab-pygments      0.2.2               \njupyterlab-server        2.19.0              \njupyterlab-widgets       3.0.9               \nkeyring                  18.0.1              \nkiwisolver               1.3.2               \nlanguage-selector        0.1                 \nlaunchpadlib             1.10.13             \nlazr.restfulclient       0.14.2              \nlazr.uri                 1.0.3               \nMarkupSafe               2.1.2               \nmatplotlib               3.4.3               \nmatplotlib-inline        0.1.6               \nmistune                  2.0.4               \nml-dtypes                0.2.0               \nmore-itertools           4.2.0               \nnbclassic                0.5.1               \nnbclient                 0.7.2               \nnbconvert                7.2.9               \nnbformat                 5.7.3               \nnest-asyncio             1.5.6               \nnetifaces                0.10.4              \nnotebook                 6.5.2               \nnotebook-shim            0.2.2               \nnumpy                    1.21.3              \noauthlib                 3.1.0               \nopt-einsum               3.3.0               \npackaging                23.0                \npandas                   1.3.4               \npandocfilters            1.5.0               \nparso                    0.8.3               \npexpect                  4.6.0               \npickleshare              0.7.5               \nPillow                   8.4.0               \npip                      20.0.2              \npkgutil-resolve-name     1.3.10              \nplatformdirs             2.6.2               \nprometheus-client        0.16.0              \nprompt-toolkit           3.0.36              \npsutil                   5.9.4               \nptyprocess               0.7.0               \npure-eval                0.2.2               \npyasn1                   0.4.2               \npyasn1-modules           0.2.1               \npycparser                2.21                \nPygments                 2.14.0              \nPyGObject                3.36.0              \nPyHamcrest               1.9.0               \nPyJWT                    1.7.1               \npymacaroons              0.13.0              \nPyNaCl                   1.3.0               \npyOpenSSL                19.0.0              \npyparsing                3.0.4               \npyrsistent               0.15.5              \npyserial                 3.4                 \npython-apt               2.0.1+ubuntu0.20.4.1\npython-dateutil          2.8.2               \npython-debian            0.1.36+ubuntu1.1    \npython-json-logger       2.0.4               \npytz                     2021.3              \nPyYAML                   5.3.1               \npyzmq                    25.0.0              \nqtconsole                5.4.4               \nQtPy                     2.4.0               \nrequests                 2.28.2              \nrequests-unixsocket      0.2.0               \nrfc3339-validator        0.1.4               \nrfc3986-validator        0.1.1               \nscipy                    1.10.1              \nSecretStorage            2.3.1               \nSend2Trash               1.8.0               \nservice-identity         18.1.0              \nsetuptools               45.2.0              \nsimplejson               3.16.0              \nsix                      1.14.0              \nsniffio                  1.3.0               \nsos                      4.4                 \nsoupsieve                2.3.2.post1         \nssh-import-id            5.10                \nstack-data               0.6.2               \nsystemd-python           234                 \nterminado                0.17.1              \ntfp-nightly              0.22.0.dev20231002  \ntinycss2                 1.2.1               \ntomli                    2.0.1               \ntornado                  6.2                 \ntraitlets                5.9.0               \nTwisted                  18.9.0              \ntyping-extensions        4.5.0               \nubuntu-advantage-tools   8001                \nufw                      0.36                \nunattended-upgrades      0.1                 \nurllib3                  1.25.8              \nvirtualenv               20.17.1             \nwadllib                  1.3.3               \nwcwidth                  0.2.6               \nwebencodings             0.5.1               \nwebsocket-client         1.5.0               \nwheel                    0.34.2              \nwidgetsnbextension       4.0.9               \nzipp                     1.0.0               \nzope.interface           4.7.1     \nIf I change the Normal distribution to the Gamma distribution, I no longer get an error. Any ideas why this might be? The code snippet is taken directly from the tensorflow website. thanks!",
        "answers": [
            "jax.dtypes.prng_key was added in JAX version 0.4.14. You should update JAX to a newer version (0.4.14 or later), or if that is not possible, downgrade tensorflow_probability to an older version (0.21.0 or older should be sufficient for this issue)."
        ],
        "link": "https://stackoverflow.com/questions/77221932/jax-prng-key-error-with-tfp-normal-distribution"
    },
    {
        "title": "Should models be trained using fori_loop?",
        "question": "When optimizing weights and biases of a model, does it make sense to replace:\nfor _ in range(epochs):\n    w, b = step(w, b)\nWith:\nw, b = lax.fori_loop(0, epochs, lambda wb: step(wb[0], wb[1]), (w, b))\nIf I understand correctly, this means that the entire training process can then be a single compiled JAX function (takes in training data, outputs optimized weights and biases).\nIs this a standard approach? What are the tradeoffs to consider?",
        "answers": [
            "It's fine to train your model with fori_loop, particularly for simple models. It may be slightly faster, but in general XLA won't fuse operations across different loop steps. It's also not possible to return early within a fori_loop when you reach a certain loss threshold (though you could do that with while_loop if you wish).\nFor more complicated models, you often will want to do some sort of I/O at every step (e.g. loading new training data, logging fit parameters, etc.) While this is possible to do within fori_loop via jax.experimental.io_callback, it is somewhat less convenient than doing it directly from the host within a Python for loop, so in general users tend to use for loops for their training iterations."
        ],
        "link": "https://stackoverflow.com/questions/77217387/should-models-be-trained-using-fori-loop"
    },
    {
        "title": "Does equinox (jax) do no batch dim broadcasting and expects you to use vmap instead?",
        "question": "https://docs.kidger.site/equinox/api/nn/mlp/#equinox.nn.MLP\nThe only way I was able to use MLP is like this\nimport jax\nimport equinox as eqx\nimport numpy as np\n\n\njax.vmap(eqx.nn.MLP(in_size=12, out_size=4, width_size=6, depth=5, key=key))(np.random.randn(5, 12)\nIs this the intended usage? It differs a bit from other frameworks then. But maybe safer.",
        "answers": [
            "Yup, this is intended!\nEvery layer in eqx.nn acts on a single batch element, and you can apply them to batches by calling jax.vmap, exactly as you're doing.\nSee also this FAQ entry: https://docs.kidger.site/equinox/faq/#how-do-i-input-higher-order-tensors-eg-with-batch-dimensions-into-my-model\nI hope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/77090293/does-equinox-jax-do-no-batch-dim-broadcasting-and-expects-you-to-use-vmap-inst"
    },
    {
        "title": "How to select between different function based on a value of a parameter in flax?",
        "question": "I am iterating through each head and applying either f1 or f2 function depending on the value of the parameter self.alpha.\nI only want to evaluate either function f1 or f2 not both and then select output of one based on conditional.\n        def f1 (x):\n            print('f1')\n            return x/x.shape[2]\n        def f2 (x):\n            print('f2')\n            temp = nn.relu(x)\n            return temp/(jnp.sum(temp,axis=-1,keepdims=True) + 1e-5)\n        \n        def choose_attention(alpha, x):\n            return jax.lax.cond(alpha[0, 0, 0,0],f2,f1,operand=x)\n        \n        results = []\n        func = [f1,f2]\n        for i in range(self.alpha.shape[1]):\n            print(i)\n            alpha_i = self.alpha[:, i:i+1, :, :]\n            x_i = attn_weights[:, i:i+1, :, :]\n            result_i = jax.lax.switch(self.alpha[0,0,0,0].astype(int),func,x_i)\n            results.append(result_i)\n\n        final_result = jnp.concatenate(results, axis=1)\nMy print statements read like 0 f1 f2 1 2 3 4 5 6 7 8 9 10 11",
        "answers": [
            "jax.lax.switch does what you want: it chooses between two different functions based on a runtime value. Your use of print statements is misleading you: Python print runs at trace-time rather than runtime, and all code will be traced even if it is not eventually executed.\nFor some background on how to think about the execution model of JAX programs, I would suggest How to think in JAX.\nSide note: for better performance, I would also suggest avoiding using Python for loops to loop through array values, and instead express your algorithm using either Numpy-style explicit vectorization, or using jax.vmap to automatically vectorize your code."
        ],
        "link": "https://stackoverflow.com/questions/77083595/how-to-select-between-different-function-based-on-a-value-of-a-parameter-in-flax"
    },
    {
        "title": "jax.errors.UnexpectedTracerError only when using jax.debug.breakpoint()",
        "question": "My jax code runs fine but when I try to insert a breakpoint with jax.debug.breakpoint I get the error: jax.errors.UnexpectedTracerError.\nI would expect this error to show up also without setting a breakpoint.\nIs this intended behavior or is something weird happening? When using jax_checking_leaks none of the reported tracers seem to actually be leaked.",
        "answers": [
            "There is currently a bug in jax.debug.breakpoint that can lead to spurious tracer leaks in some situations: see https://github.com/google/jax/issues/16732.\nThere's not any easy workaround at the moment, unfortunately, but hopefully the issue will be addressed soon."
        ],
        "link": "https://stackoverflow.com/questions/77067644/jax-errors-unexpectedtracererror-only-when-using-jax-debug-breakpoint"
    },
    {
        "title": "JAX produces memory error for simple program on GPU",
        "question": "I installed JAX (pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html) and even for a simple code like\na = jnp.array([1,2,3])\na.dot(a)\nI get the following error:\n2023-09-08 10:12:55.791658: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:445] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n2023-09-08 10:12:55.791696: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:449] Memory usage: 8058437632 bytes free, 8513978368 bytes total.\nIt looks like a memory issue. I tried the tips mentioned here\nhttps://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\nbut to no success.\nThis is the result of nvidia-smi on my system:\nNVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   64C    P0    37W /  N/A |    364MiB /  8119MiB |      2%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\nIt seems strange to me that there seems to be a out of memory issue when it is just a single array.\nAny ideas?",
        "answers": [
            "I don't believe this is a memory issue; rather it looks like you have a mismatch between your CUDA and CUDNN versions.\nOne way to ensure your CUDA versions are compatible is to use the pip-based installation (see JAX pip installation: GPU (CUDA, installed via pip, easier)). This should ensure that you install mutually-compatible CUDA, CUDNN, and jaxlib versions on your system. Installing JAX using pip-installed CUDA looks something like this:\n$ pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nIt looks like this may be the approach you used; if so, you should check that your system path (e.g. LD_LIBRARY_PATH) is not pre-empting the pip-installed CUDA with a local version. There is some relevant discussion at https://github.com/google/jax/issues/17497.\nIf you want to use a local CUDA installation, you can follow JAX pip installation: GPU (CUDA, installed locally, harder), but then it is up to you to ensure that your CUDA, CUDNN, and jaxlib versions are mutually compatible. Installing JAX using local CUDA looks something like this:\n$ pip install --upgrade \"jax[cuda12_local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nbut if you use this approach, be sure to read the details at the above link."
        ],
        "link": "https://stackoverflow.com/questions/77065313/jax-produces-memory-error-for-simple-program-on-gpu"
    },
    {
        "title": "Reason to return updated stack along with top element from pop() method",
        "question": "This:\n  def pop(self) -> tuple[Any, Stack]:\n    \"\"\"Pops from the stack, returning an (elem, updated stack) pair.\"\"\"\nWhat is the reason behind returning updated stack along with top element from pop() method?",
        "answers": [
            "From the docstring of the Stack class: \"A bounded functional stack implementation.\"\nA functional implementation generally means that pure functions are used. The pop function implements two pure function properties:\nThe output does not change when called multiple times with the same input.\nThe function does not mutate the stack, ie it does not introduce side effects.\nThis is why it must return a (mutated) copy of the input Stack instead of modifying it."
        ],
        "link": "https://stackoverflow.com/questions/77057836/reason-to-return-updated-stack-along-with-top-element-from-pop-method"
    },
    {
        "title": "Parallelize with JAX over all GPU cores",
        "question": "In order to minimize the function x^2+y^2, I tried to implement the Adam optimizer from scratch with JAX:\n@jax.jit\ndef fit(X, batches, params=[0.001, 0.9, 0.99, 1e-8]):\n    global fun\n\n    # batches is array containing n batches\n\n    @jax.jit\n    def adam_update(X, t, grads, m, v, alpha, b1, b2, epsilon):\n        m = b1*m + (1-b1)*grads\n        v = b2*v + (1-b2)*grads**2\n        m_hat = m / (1-b1**t)\n        v_hat = v / (1-b2**t)\n        X = X - alpha*(1-b2**t)**(1/2)*m_hat/(1-b1**t)/((v_hat)**(1/2)+epsilon)\n        return [X, m, v]\n\n    dim=jnp.shape(X)[0]\n\n    params = jnp.array(params)\n    alpha = params[0]\n    b1=params[1]\n    b2=params[2]\n    epsilon=params[3]\n    \n    adam_update = jax.jit(partial(adam_update, alpha=alpha, b1=b1, b2=b2, epsilon=epsilon))\n\n    m=jnp.zeros(dim)\n    v=jnp.zeros(dim)\n\n    for t, batch in enumerate(batches):\n        fun_ = jax.jit(partial(fun, batch=batch))\n        grads = jax.grad(fun_)(X)\n        X, m, v = adam_update(X, t+1, grads, m, v)\n    return X\nWith JAX I could parallelize this simply with jax.pmap, however it would only be parallelized over the 8 GPUs, instead over all GPU cores. Is there a way too parallelize this code over all cores?\nCan it be that all cores of one GPU are miraculously used upon using @jax.jit. Also, why does it need 200 seconds for compiling for 1000 iterations, while the optax-Adam optimizer does not take so long too compile?",
        "answers": [
            "Can it be that all cores of one GPU are miraculously used upon using @jax.jit\nIn general, yes. For computations on a single device, the XLA GPU compiler will use all available cores of the GPU to complete a computation.\nAlso, why does it need 200 seconds for compiling for 1000 iterations, while the optax-Adam optimizer does not take so long too compile?\nThis is because you are JIT-compiling a Python for-loop. Python loops within JIT are unrolled by JAX into a linear program (see JAX Sharp Bits: Control Flow), and compilation time grows with the size of the program.\nBy contrast, the optax quick-start recommends JIT-compiling the step function, but does not JIT-compile the fitting loop. This would lead to much faster compilation times than the pattern used in your code, where the full for-loop is within a JIT-compiled function."
        ],
        "link": "https://stackoverflow.com/questions/77036442/parallelize-with-jax-over-all-gpu-cores"
    },
    {
        "title": "JAX update breaks working code of linear algebra solver",
        "question": "I am dealing with an issue related to an update of jax. I have a library which is supposed to solve a system of linear equations using the bicgstab algorithm.\nThe solver is implemented as follows:\ndef bicgstabsolver(A, b, eps):\n  '''Returns the loop initialization and iteration functions.'''\n\n  def init(z, b, x0):\n    '''Forms the args that will be used to update stuff.'''\n\n    x = x0\n    r = b - A(x, z)\n    rstrich = r\n    v = vecfield.zeros(b.shape)\n    p = vecfield.zeros(b.shape)\n    alpha = 1\n    rho = 1\n    omega = 1\n\n    term_err = eps * vecfield.norm(b)\n\n    return x, r, rstrich, v, p, alpha, rho, omega, term_err\n    \n  @jax.jit\n  def iter(x, r, rstrich, v, p, alpha, rho, omega, z):\n    '''Run the iteration loop `n` times.'''\n    rhoold = rho\n    rho = vecfield.dot(vecfield.conj(rstrich),r)\n    beta = (rho / rhoold) * (alpha / omega)\n    p = r + beta * (p - omega * v)\n    v = A(p,z)\n    alpha = rho / vecfield.dot(vecfield.conj(rstrich),v)\n    h = x + alpha * p\n    s = r - alpha * v\n    t = A(s,z)\n    omega = vecfield.dot(vecfield.conj(t),s) / vecfield.dot(vecfield.conj(t),t)\n    x = h + omega * s\n    r = s - omega * t\n    err = vecfield.norm(r)\n    return x, r, rstrich, v, p, alpha, rho, omega, err\n\n  return init, iter\nThe implementation of the VecField class:\nimport jax.numpy as np\nfrom typing import Any, NamedTuple\n\n\nclass VecField(NamedTuple):\n  '''Represents a 3-tuple of arrays.'''\n  x: Any\n  y: Any\n  z: Any\n\n  @property\n  def shape(self):\n    assert self.x.shape == self.y.shape == self.z.shape\n    return self.x.shape\n\n  @property\n  def dtype(self):\n    assert self.x.dtype == self.y.dtype == self.z.dtype\n    return self.x.dtype\n\n  def as_array(self):\n    return VecField(*(np.array(a) for a in self))\n\n  def __add__(x, y):\n    return VecField(*(a + b for a, b in zip(x, y)))\n\n  def __sub__(x, y):\n    return VecField(*(a - b for a, b in zip(x, y)))\n\n  def __mul__(x, y):\n    return VecField(*(a * b for a, b in zip(x, y)))\n\n  def __rmul__(y, x):\n    return VecField(*(x * b for b in y))\n\n\ndef zeros(shape):\n  return VecField(*(np.zeros(shape, np.complex128) for _ in range(3)))\n\ndef ones(shape):\n  return VecField(*(np.ones(shape, np.complex128) for _ in range(3)))\n\n# TODO: Check if this hack is still necessary to obtain good performance.\ndef dot(x, y):\n  z = VecField(*(a * b for a, b in zip(x, y)))\n  return sum(np.sum(np.real(c)) + 1j * np.sum(np.imag(c)) for c in z)\n\n\ndef norm(x):\n  return np.sqrt(sum(np.square(np.linalg.norm(a)) for a in x))\n\n\ndef conj(x):\n  return VecField(*(np.conj(a) for a in x))\n\n\ndef real(x):\n  return VecField(*(np.real(a) for a in x))\n\n\ndef from_tuple(x):\n  return VecField(*(np.reshape(a, (1, 1) + a.shape) for a in x))\n\n\ndef to_tuple(x):\n  return tuple(np.reshape(a, a.shape[2:]) for a in x)\nThe code is running perfectly fine using jax and jaxlib version 0.3.10. However, if I update jax to 0.4.13 it stops working with a cryptic error:\nFile \"***\", line 66, in iter\n    p = r + beta * (p - omega * v)\n  File \"***/python3.8/site-packages/jax/_src/numpy/array_methods.py\", line 791, in op\n    return getattr(self.aval, f\"_{name}\")(self, *args)\n  File \"***/python3.8/site-packages/jax/_src/numpy/array_methods.py\", line 260, in deferring_binary_op\n    raise TypeError(f\"unsupported operand type(s) for {opchar}: \"\njax._src.traceback_util.UnfilteredStackTrace: TypeError: unsupported operand type(s) for *: 'DynamicJaxprTracer' and 'VecField'\nI have no clue so far how to migrate this code to be compatible with the newer version of jax. Probably I'm missing something very obvious. Any help would be greatly appreciated!",
        "answers": [
            "It looks like JAX's array __mul__ methods are raising a TypeError on unsupported input rather than returning NotImplemented, which means that omega * v is not correctly dispatching to v.__rmul__().\nThis is a bug in JAX: I would suggest reporting this in a new issue at http://github.com/google/jax/issues/\nIn the meantime, you should be able to work around this by making sure that every time you operate between a VecField by a JAX array, the VecField appears on the left of the operand; e.g. change this:\np = r + beta * (p - omega * v)\nto this:\np = (p - v * omega) * beta + r\nEdit: it looks like the bug was introduced in https://github.com/google/jax/pull/11234 (meaning it's present in all JAX versions 0.3.14 and newer) and only affects subtypes of builtin collections (which includes NamedTuple).\nEdit 2: this has been fixed in https://github.com/google/jax/pull/17406, which should be part of a future JAX 0.4.16 release."
        ],
        "link": "https://stackoverflow.com/questions/77024323/jax-update-breaks-working-code-of-linear-algebra-solver"
    },
    {
        "title": "How to extract chunks of a 2D numpy array that has been flattened",
        "question": "I would like to know the best way of extacting chunks of elements from a 2D numpy array that has been flattened. See example python code below which hopefully explains what I want to do a little better.\nimport numpy as np\n\nnx = 5\nnz = 7\nnumGPs = nx*nz\n\nGPs_matrix = np.arange(numGPs).reshape((nx,nz), order='F')\nav = np.zeros_like(GPs_matrix)\nav[1:-1,1:-1] = (GPs_matrix[1:-1,2:] + GPs_matrix[1:-1,:-2] + GPs_matrix[2:,1:-1] + GPs_matrix[:-2,1:-1])/4\n# How to do the above if GPs is flattened as per below?\nGPs_flat = GPs_matrix.reshape(-1, order='F')\n# One (very clunky) way is to do the following\ncor = GPs_matrix[1:-1,1:-1].reshape(-1, order='F')\nbtm = GPs_matrix[1:-1,:-2].reshape(-1, order='F')\ntop = GPs_matrix[1:-1,2:].reshape(-1, order='F')\nlft = GPs_matrix[:-2,1:-1].reshape(-1, order='F')\nrgt = GPs_matrix[2:,1:-1].reshape(-1, order='F')\nav_flat = np.zeros_like(GPs_flat)\nav_flat[cor] = (GPs_flat[top] + GPs_flat[btm] + GPs_flat[rgt] + GPs_flat[lft])/4\n# Check\nprint(av.reshape(-1, order='F'))\nprint(av_flat)\n# Is there a better way?\n# I see np.r_() may be useful but I don't know how to best use it. I assume the below attempt can be improved upon\nsl_cor = np.r_[nx+1:2*nx-1,2*nx+1:3*nx-1,3*nx+1:4*nx-1,4*nx+1:5*nx-1,5*nx+1:6*nx-1] # Should match cor above\n# Check\nprint(sl_cor)\nprint(cor)\n# The below also works but I would like to avoid using loops if possible\nprint(np.r_[*[np.arange(1,nx-1)+nx*j for j in range(1,nz-1)]])\nEssentially, I am trying to solve the possion equation in two spatial dimensions. It is convenient to set up the problem in the form of a 2D array as the location of the elements in the array corresponds to the position of the grid points in the cartesian mesh. Ultimately I will be using an iterate solver in jax to solve the linear system (e.g. bicgstab) which requires a linear operator function as input. Therefore, the function needs to return a vector and loops are not efficient.",
        "answers": [
            "First off, it looks like what you're attempting to compute is a convolution. In general I'd avoid doing a convolution by hand, and instead use something like scipy.signal.convolve2d. Here's the equivalent for your case:\nfrom scipy.signal import convolve2d\nkernel = np.array(([[0, 1, 0],\n                    [1, 0, 1],\n                    [0, 1, 0]])) / 4\nav = np.zeros_like(GPs_matrix)\nav[1:-1, 1:-1] = convolve2d(GPs_matrix, kernel, mode='valid').astype(av.dtype)\nIn the flattened case, your best approach is probably going to be to reshape the 1D input, perform the 2D convolution, and then flatten the output. For example:\ndef eval_1D(vec):\n  mat = vec.reshape(nx, nz, order='F')\n  kernel = np.array(([[0, 1, 0],\n                      [1, 0, 1],\n                      [0, 1, 0]])) / 4\n  av = np.zeros_like(mat)\n  av[1:-1, 1:-1] = convolve2d(mat, kernel, mode='valid').astype(av.dtype)\n  return av.ravel(order='F')\n\nprint(eval_1D(GPs_matrix.ravel(order='F')))\n# [ 0  0  0  0  0  0  6  7  8  0  0 11 12 13  0  0 16 17\n#  18  0  0 21 22 23  0  0 26 27 28  0  0  0  0  0  0]"
        ],
        "link": "https://stackoverflow.com/questions/76952780/how-to-extract-chunks-of-a-2d-numpy-array-that-has-been-flattened"
    },
    {
        "title": "Migration from haiku: Alternative to Haiku's PRNGSequence?",
        "question": "I am writing a Markov chain Monte Carlo simulation in JAX which involves a large series of sampling steps. I currently rely on haiku's PRNGSequence to do the pseudo random number generator key bookkeeping:\nimport haiku as hk\n\ndef step(key, context):\n  key_seq = hk.PRNGSequence(key)\n  x1 = sampler(next(key_seq), context_1)\n  ...\n  xn = other_sampler(next(key_seq), context_n)\nQuestion:\nSince Haiku has been discontinued, I am looking for an alternative to PRNGSequence.\nI find the standard JAX approach:\ndef step(key, context):\n  key, subkey = jax.random.split(key)\n  x1 = sampler(subkey, context_1)\n  ...\n  key, subkey = jax.random.split(key)\n  xn = other_sampler(subkey, context_n)\nunsatisfactory on two accounts:\nVery error prone: It is easy to slip up and re-use a key. This is especially problematic in MCMC simulations, which are sensitive to these biases and very difficult to debug.\nIt is quite bulky: I need to roughly double the size of my code to split keys.\nAny suggestions how to mitigate these problems?\nThanks!\nHylke",
        "answers": [
            "If all you need is a simple class that locally handles splitting keys for you, why not define it yourself? You could create a suitable one in a few lines – for example:\nimport jax\n\nclass PRNGSequence:\n  def __init__(self, key):\n    self._key = key\n  def __next__(self):\n    self._key, key = jax.random.split(self._key)\n    return key\n\ndef step(key):\n  key_seq = PRNGSequence(key)\n  print(jax.random.uniform(next(key_seq)))\n  print(jax.random.uniform(next(key_seq)))\n\nstep(jax.random.PRNGKey(0))\n# 0.10536897\n# 0.2787192\nAs always, though, you have to be careful about this kind of hidden state when you're using JAX transformations like jit: see JAX Sharp Bits: Pure Functions for information on this.",
            "I recently came across this nice one-liner alternative:\nimport itertools\n\nkey_seq = map(jax.random.key, itertools.count())\nkey = next(key_seq)\nwhich uses the infinite iterator count."
        ],
        "link": "https://stackoverflow.com/questions/76912173/migration-from-haiku-alternative-to-haikus-prngsequence"
    },
    {
        "title": "how does the bind() function work in JAX when making a new primitive?",
        "question": "While I am checking and studying the jax code, I see lots of bind() usage. I wanted to know how it works and what's its functionality. Here is one of the example.\ndef test_unimplemented_interpreter_rules(self):\n    foo_p = core.Primitive('foo')\n    def foo(x):\n      return foo_p.bind(x)",
        "answers": [
            "You can think of bind() as essentially equivalent to calling the function represented by the primitive. But because JAX primitives are designed to work with its abstract evaluation and transformation framework, binding a primitive is a bit more complicated than just calling a function, because the meaning of binding a primitive changes depending on the context. For example:\nin normal, non-transformed code, it will result in a call to the primitive's impl rule\nduring general abstract evaluation like jax.make_jaxpr or jax.eval_shape, it will call the primitive's abstract evaluation rule\ninside jax.vmap, it will result in a call to the primitive's batching rule\ninside jax.grad, it will result in a call to the primitive's autodiff-related rules (jvp and/or transpose)\nIf you want to understand more about the details of JAX primitives and how they interact with JAX tracing, abstract evaluation, and transformations, a good resource is Autodidax: JAX core from scratch, which walks through creating a simplified version of JAX from the ground up."
        ],
        "link": "https://stackoverflow.com/questions/76908334/how-does-the-bind-function-work-in-jax-when-making-a-new-primitive"
    },
    {
        "title": "Installing jaxlib for cuda 11.8",
        "question": "I'm trying to install jax and jaxlib on my Ubuntu 18 with python 3.8 for snerg (https://github.com/google-research/google-research/tree/master/snerg). Unfortunately when I try to install jax and jaxlib for Cuda 11.8 with the following command :\npip install --upgrade jax jaxlib==0.1.69+cuda118 -f https://storage.googleapis.com/jax-releases/jax_releases.html \nI get the following error:\nERROR: Ignored the following versions that require a different python version: 0.4.14 Requires-Python >=3.9\nERROR: Could not find a version that satisfies the requirement jaxlib==0.1.69+cuda118 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15, 0.3.18, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13)\nERROR: No matching distribution found for jaxlib==0.1.69+cuda118\nWould appreciate any help. Thanks",
        "answers": [
            "Follow the following instructions which are primarily obtained from the source:\nUninstall previous versions (if any):\n$ pip uninstall jax jaxlib jaxtyping -y\nUpgrade your pip:\n$ pip install --upgrade pip\nFind out which CUDA is already installed on your machine:\n$ nvidia-smi\n\nThu Jan  4 11:24:58 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A1000 6GB Lap...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   58C    P0              12W /  35W |      8MiB /  6144MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      3219      G   /usr/lib/xorg/Xorg                            4MiB |\n+---------------------------------------------------------------------------------------+\nDepending on the CUDA version of your machine( wheels only available on linux ), run EITHER of the following:\n# CUDA 12.X installation\n$ pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n#### OR ####\n\n# CUDA 11.X installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nTo double check if you have have successfully configured the gpu:\n$ python -c \"import jax; print(f'Jax backend: {jax.default_backend()}')\"\nJax backend: gpu ",
            "jaxlib version 0.1.69 is quite old (it was released in July 2021) CUDA 11.8 was released over a year later, in September 2022. Thus I would not expect there to be pre-built binaries for jaxlib version 0.1.69 targeting CUDA 11.8.\nIf possible, your best bet would be to install a newer version of jaxlib, one which has builds targeting CUDA 11.8. The current jaxlib+CUDA GPU installation instructions can be found here.\nIf for some reason you absolutely need this very old jaxlib version, you'll probably first have to install an older CUDA version on your system. The CUDA jaxlib installation instructions from jaxlib 0.1.69 can be found here: it looks like it was built to target CUDA 10.1-10.2, 11.0, or 11.1-11.3."
        ],
        "link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8"
    },
    {
        "title": "Is it possible to obtain values from jax traced arrays with dynamicjaxprtrace level larger than 1 using any of the callback functions?",
        "question": "So I have a program that have multiple functions with its own jax calls and here is the main function:\n@partial(jax.jit, static_argnames=(\"numberOfVoxels\",))    \ndef process_valid_voxels(numberOfVoxels, voxelPositions, voxelLikelihoods, ps, t, M, tmp):\n   \n    func = lambda tmp_val: process_voxel(tmp_val, voxelPositions, voxelLikelihoods, ps, t, M, tmp)\n    ys, likelihoods = jax.vmap(func)(jnp.arange(numberOfVoxels))\n   \n    return ys, likelihoods\nThis is the output of ys and likelihoods:\n(Pdb) ys\nTraced<ShapedArray(int32[3700,3,1])>with<DynamicJaxprTrace(level=3/0)>`\nlikelihoods\nTraced<ShapedArray(float32[3700,7,1])>with<DynamicJaxprTrace(level=3/0)>\nI want to get values from traced arrays ys, likelihoods so that I can modify them. I have tried using the io_callback function:\ndef callback1(x):\n    return jax.experimental.io_callback(process_voxel, x , x)\na = callback1(jnp.arange(numberOfVoxels))\nbut the output is the same except for the shape of the array:\nTraced<ShapedArray(int32[3700])>with<DynamicJaxprTrace(level=3/0)>",
        "answers": [
            "This is similar to one of JAX's FAQs: How can I convert a tracer to a numpy array?. That answer mentions callbacks, which you use above, but I think you have the wrong mental model of what io_callback does.\nWhen you run transformed JAX code, there are essentially two stages of execution:\nTracing happens within the Python runtime, using abstract representations of the arrays (tracers) to extract the sequence of operations implied by your code. During tracing in most cases, array values are not available by design.\nExecution happens within the XLA runtime once tracing has encoded the sequence of operations to be run. Array values are available during XLA execution, but this is not a Python runtime, and so Python debugging, printing, etc. is not available.\nYour question amounts to \"How can I access array values from stage 2 by inserting breakpoints into the runtime during stage 1\" The answer is: you can't!\nBut you can use callbacks and jax debugging tools to encode an instruction to tell XLA to pause execution and pass its values to some callback function during stage 2 execution and let you interact with array values from within Python.\nOne way to do so might look like this:\n@partial(jax.jit, static_argnames=(\"numberOfVoxels\",))    \ndef process_valid_voxels(numberOfVoxels, voxelPositions, voxelLikelihoods, ps, t, M, tmp):\n    func = lambda tmp_val: process_voxel(tmp_val, voxelPositions, voxelLikelihoods, ps, t, M, tmp)\n    ys, likelihoods = jax.vmap(func)(jnp.arange(numberOfVoxels))\n    jax.debug.breakpoint()\n    return ys, likelihoods\nThis will tell XLA during stage 2 to pause execution, call back to a Python-side debugging tool, and let you interact with the values there.\nFor more intuition on the mental model of JAX program execution, you may find How to think in JAX useful."
        ],
        "link": "https://stackoverflow.com/questions/76827852/is-it-possible-to-obtain-values-from-jax-traced-arrays-with-dynamicjaxprtrace-le"
    },
    {
        "title": "Computing the gradient of a batched function using JAX",
        "question": "I would need to compute the gradient of a batched function using JAX. The following is a minimal example of what I would like to do:\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.expand_dims(jnp.linspace(-1, 1, 20), axis=1)\n\nu = lambda x: jnp.sin(jnp.pi * x)\nux = jax.vmap(jax.grad(u))\n\nplt.plot(x, u(x))\nplt.plot(x, ux(x))  # Use vx instead of ux\nplt.show()\nI have tried a variety of ways of making this work using vmap, but I don't seem to be able to get the code to run without removing the batch dimension in the input x. I have seen some workarounds using the Jacobian but this doesn't seem natural as the given is a scalar function of a single variable.\nIn the end u will be a neural network (implemented in Flax) that I need to differentiate with respect to the input (not the parameters of the network), so I cannot remove the batch dimension.",
        "answers": [
            "To ensure the kernel (u) returns a scalar value, so that jax.grad makes sense, the batched dimension also needs to be mapped over.\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.expand_dims(jnp.linspace(-1, 1, 20), axis=1)\n\nu = lambda x: jnp.sin(jnp.pi * x)\nux = jax.vmap(jax.vmap(jax.grad(u)))\n# ux = lambda x : jax.lax.map(jax.vmap(jax.grad(u)), x) # sequential version\n# ux = lambda x : jax.vmap(jax.grad(u))(x.reshape(-1)).reshape(x.shape) # flattened map version\n\nplt.plot(x, u(x))\nplt.plot(x, ux(x))  # Use vx instead of ux\nplt.show()\nWhich composition of maps to use depends on what's happening in the batched dimension."
        ],
        "link": "https://stackoverflow.com/questions/76791201/computing-the-gradient-of-a-batched-function-using-jax"
    },
    {
        "title": "Does Haiku cache parameters when combined with jax.vmap?",
        "question": "I have a haiku Module with a call function as follows\nclass MyModule(hk.Module):\n    __call__(self, x):\n        A = hk.get_parameter(\"A\", shape=[self.Ashape], init=A_init)\n        B = hk.get_parameter(\"B\", shape=[self.Bshape], init=B_init)\n        C = self.demanding_computation(A, B)\n\n        res = easy_computation(C, x)\n        return res\nI use this module via\ndef _forward(x):\n    module = MyModule()\n    return module(x)\n\n\nforward = hk.without_apply_rng(hk.transform(_forward))\nx_test = jnp.ones(1)\nparams = forward.init(jax.random.PRNGKey(42), x_test)\nf = jax.vmap(forward.apply, in_axes=(None, 0))\nThen I apply f with the same params to many different x. Is the demanding_computation (that is not depending on x) then cached within the jax.vmap call? If not, what is the correct pattern to separate these computations and get demanding_computation cached?\nI have tried to test this by adding a print statement from jax.experimental.host_callback:\n    def demanding_computation(self, A, B):\n        C = compute(A, B)\n        id_print(C)\n        return C\nand it indeed only printed once. Is that sufficient evidence that this computation is actually cached or is only the printing omitted in subsequent iterations?",
        "answers": [
            "demanding_computation will only be called once, but not because of caching.\nvmap doesn't loop over the batched axes, it replaces the operations with vectorized versions (e.g. scalar additions become vector additions). Since demanding_computation doesn't involve inputs with batch axes it won't be modified by this use of vmap. (Even if it did, it would still only be run once, it would just be a vectorized version)."
        ],
        "link": "https://stackoverflow.com/questions/76788819/does-haiku-cache-parameters-when-combined-with-jax-vmap"
    },
    {
        "title": "How to compute the number of gradient evaluations in Jax.Scipy.minimize.optimize?",
        "question": "I wish to obtain the total number of gradient evaluations during optimization using jax.scipy.optimize.minimize. How can I do so?",
        "answers": [
            "You can find this in the njev (number of Jacobian evaluations) attribute of the output of minimize. For example:\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\n\ndef f(x):\n  return jnp.sum(x ** 2)\n\nout = minimize(f, jnp.array([1., 2.]), method=\"BFGS\")\nprint(out.njev)\n3\nYou can find a list of the information available in the optimization output in the documentation for OptimizeResults."
        ],
        "link": "https://stackoverflow.com/questions/76721987/how-to-compute-the-number-of-gradient-evaluations-in-jax-scipy-minimize-optimize"
    },
    {
        "title": "pytorch and jax networks give different accuracy with same settings",
        "question": "I have pytorch code which performs with more than 95% accuracy. The code essentially implements a feedforward neural network using PyTorch to classify the digits dataset. It trains the model using the Adam optimizer and computes the cross-entropy loss, and then evaluates the model's performance on the test set by calculating the accuracy.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the digits dataset\ndigits = load_digits()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, test_size=0.2, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert the data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Define the FFN model\nclass FFN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, output_size):\n        super(FFN, self).__init__()\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_sizes)):\n            if i == 0:\n                self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[i]))\n            else:\n                self.hidden_layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n            self.hidden_layers.append(nn.ReLU())\n        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n\n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.output_layer(x)\n        return x\n\n# Define the training parameters\ninput_size = X_train.shape[1]\nhidden_sizes = [64, 32]  # Modify the hidden layer sizes as per your requirement\noutput_size = len(torch.unique(y_train_tensor))\nlearning_rate = 0.001\nnum_epochs = 200\nbatch_size = len(X_train)  # Set batch size to the size of the training dataset\n\n# Create the FFN model\nmodel = FFN(input_size, hidden_sizes, output_size)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n\n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\n# Evaluate the model on the test set\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs.data, 1)\n    for j in range(len(predicted)):\n        print(predicted[j], y_test_tensor[j])\n    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0) * 100\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\nAlso I have the equivalent jax code, with performs with less than 10% of accuracy\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, random, value_and_grad\nfrom jax.scipy.special import logsumexp\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom jax.example_libraries.optimizers import adam, momentum, sgd, nesterov, adagrad, rmsprop\nfrom jax import nn as jnn\n\n\n# Load the digits dataset\ndigits = load_digits()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n\n# Reshape the target variables\ny_train_reshaped = jnp.reshape(y_train, (-1, 1))\ny_test_reshaped = jnp.reshape(y_test, (-1, 1))\n\nX_train_reshaped = jnp.reshape(X_train, (-1, 1))\nX_test_reshaped = jnp.reshape(X_test, (-1, 1))\n#print(np.shape(X_train),np.shape(y_train_reshaped),np.shape(y_train))\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_reshaped)\ny_test_scaled = scaler.transform(y_test_reshaped)\n\n# Convert the data to JAX arrays\nX_train_array = jnp.array(X_train, dtype=jnp.float32)\ny_train_array = jnp.array(y_train_reshaped, dtype=jnp.int32)\nX_test_array = jnp.array(X_test, dtype=jnp.float32)\ny_test_array = jnp.array(y_test_reshaped, dtype=jnp.int32)\n\n# Define the FFN model\ndef init_params(rng_key):\n    sizes = [X_train_array.shape[1]] + hidden_sizes + [output_size]\n    keys = random.split(rng_key, len(sizes))\n    params = []\n    for i in range(1, len(sizes)):\n        params.append((random.normal(keys[i], (sizes[i-1], sizes[i])), \n                       random.normal(keys[i], (sizes[i],))))\n    return params\n\ndef forward(params, x):\n    for w, b in params[:-1]:\n        x = jnp.dot(x, w) + b\n        x = jax.nn.relu(x)\n    w, b = params[-1]\n    x = jnp.dot(x, w) + b\n    return x\n\ndef softmax(logits):\n    logsumexp_logits = logsumexp(logits, axis=1, keepdims=True)\n    return jnp.exp(logits - logsumexp_logits)\n\ndef cross_entropy_loss(logits, labels):\n    log_probs = logits - logsumexp(logits, axis=1, keepdims=True)\n    return -jnp.mean(jnp.sum(log_probs * labels, axis=1))\n\n# Define the training parameters\ninput_size = X_train_array.shape[1]\nhidden_sizes = [64, 32]  # Modify the hidden layer sizes as per your requirement\noutput_size = len(jnp.unique(y_train_array))\nlearning_rate = 0.001\nnum_epochs = 200\nbatch_size = len(X_train_array)  # Set batch size to the size of the training dataset\n# Create the FFN model\nrng_key = random.PRNGKey(0)\nparams = init_params(rng_key)\n\n# Define the loss function\ndef loss_fn(params, x, y):\n    logits = forward(params, x)\n    probs = softmax(logits)\n    labels = jax.nn.one_hot(y, output_size)\n    return cross_entropy_loss(logits, labels)\n\n# Create the optimizer\nopt_init, opt_update, get_params = adam(learning_rate)\nopt_state = opt_init(params)\n\n# Define the update step\n@jit\ndef update(params, x, y, opt_state):\n    grads = grad(loss_fn)(params, x, y)\n    return opt_update(0, grads, opt_state)\n\n# Train the model\nfor epoch in range(num_epochs):\n    perm = random.permutation(rng_key, len(X_train_array))\n    for i in range(0, len(X_train_array), batch_size):\n        batch_idx = perm[i:i+batch_size]\n        X_batch = X_train_array[batch_idx]\n        y_batch = y_train_array[batch_idx]\n        params = get_params(opt_state)\n        opt_state = update(params, X_batch, y_batch, opt_state)\n\n    if (epoch + 1) % 10 == 0:\n        params = get_params(opt_state)\n        loss = loss_fn(params, X_train_array, y_train_array)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n\n# Evaluate the model on the test set\nparams = get_params(opt_state)\nlogits = forward(params, X_test_array)\npredicted = jnp.argmax(logits, axis=1)\n\nfor j in range(len(predicted)):\n    print(predicted[j], y_test_array[j])\n\naccuracy = jnp.mean(predicted == y_test_array) * 100\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\nI dont understand why the jax code performs poorly. Could you please help me in underding the bug in the jax code.",
        "answers": [
            "There are 2 probles in your jax code that are, actually, in data processing:\nYour data are not scaled. If you look at your X_train_array definition, it is the jax version of X_train, that is the raw data. Please consider using:\n# Scale the features\nscaler = StandardScaler().fit(X_train)  # No need to flat it!\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert the data to JAX arrays\nX_train_array = jnp.array(X_train, dtype=jnp.float32)\ny_train_array = jnp.array(y_train_reshaped, dtype=jnp.int32)\nX_test_array = jnp.array(X_test, dtype=jnp.float32)\ny_test_array = jnp.array(y_test_reshaped, dtype=jnp.int32)\nYour labels are of shape (N, 1) before one-hot encoding. After one-hot encoding it is (N, 1, n_out) while your predictions are of shape (N, n_out) so when you make your loss computation the two arrays are cast in (N, n_out, n_out) with repetitions and your loss is wrong. You can solve it very simply by remove the 1 in the reshape:\n# Reshape the target variables\ny_train_reshaped = jnp.reshape(y_train, (-1,))\ny_test_reshaped = jnp.reshape(y_test, (-1,))\nI tested your code with 300 epochs and lr=0.01 and I got an accuracy of 90% in test (and the loss decreased to 0.0001)"
        ],
        "link": "https://stackoverflow.com/questions/76703681/pytorch-and-jax-networks-give-different-accuracy-with-same-settings"
    },
    {
        "title": "How to apply constraints to optimisation in jax.scipy.optimize.minimize",
        "question": "I am trying to minimize the function so that the value of x that minimizes it remains between 0 and 1. One possible walk around is using sigmoid transformation. Is there any other solution associated with jax in such cases? Here is a minimal example:\nimport jax\nfrom jax.scipy.optimize import minimize\nimport jax.numpy as jnp\nrng = jax.random.PRNGKey(0)\n\ndef gen_num(shape):\n  return jax.random.uniform(rng,shape)\n\nshape = (10,) \nmu = gen_num(shape)*2+3\nlog_sigma = gen_num(shape)*2\nc = gen_num(shape)\nx1 = gen_num((10,10))*4 +5\n\ndef kl_loss(x1, mu, log_sigma, c):\n    return -(log_sigma* c[:, jnp.newaxis]).mean() + jnp.log(jnp.std(x1))\n\nkl_value = lambda x: kl_loss(x1, mu, log_sigma, x)\nres = minimize(kl_value, c, method='BFGS', tol=1e-5)\nans = res.x",
        "answers": [
            "jax.scipy.optimize.minimize is quite limited. The JAX team recommends jaxopt instead; it has information on constrained optimization here.\nIf you can edit your question here to expand your example snippet into a full minimal reproducible example, I could add an example of how to adapt your code for use with jaxopt.",
            "The only supported method in Jax is \"BFGS\". As you might see it doens't support hard constraint. However passing the constraint in the objective function can do the trick. For instance you can artificially increase the gradient outside [0, 1]:\ndef objective_function(x: jnp.array, mu: float, log_sigma: float, x1: jnp.array):\n  \"\"\"Objective function.\"\"\"\n  kl_value = kl_loss(x1, mu, log_sigma, x)\n  constraint = 10 * (1 - jnp.where(0 <= x <= 1)) * (x - 0.5)**2\n  return k1_value + constraint\n\nres = minimize(kl_value, c, args=(mu, log_sigma, x1), method='BFGS', tol=1e-5, )\nIn this case the gradient outside the domain is proportional to 10*(x-0.5) which may be sufficient to maintain the value in the domain while having no impact inside the domain.\nFinally you can try to fit the factor \"10\" to have a stable training and an efficient constraint.\nThis trick worked for me when I tried to implement a SVM in jax with jax.scipy.optimize.minimize."
        ],
        "link": "https://stackoverflow.com/questions/76695661/how-to-apply-constraints-to-optimisation-in-jax-scipy-optimize-minimize"
    },
    {
        "title": "Jaxlib 0.4.7 can't be installed or built in my OSX 10.13, Python 3.9.13",
        "question": "I've been trying to upgrade my jaxlib but it is impossible. Jax is fine. None of the wheels in the google repository work for me. I suppose it is because I'm using OSX 10.13?\nRuntimeError: jaxlib is version 0.3.10, but this version of jax requires version >= 0.4.7.\nWhen I attempt to install Jaxlib through pip, it throws this:\n$ pip install --upgrade jaxlib==0.4.7 -f https://storage.googleapis.com/jax-releases/jax_releases.html\nLooking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\nERROR: Could not find a version that satisfies the requirement jaxlib==0.4.7 (from versions: 0.1.60, 0.1.63, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10)\nERROR: No matching distribution found for jaxlib==0.4.7\nI am using the latest version of pip, but also tried downgrading pip. The problem still occurs. Please help me solve this issue.\nI tried installing through pip, building my own jaxlib as the site's instructions (https://jax.readthedocs.io/en/latest/developer.html) and installing through the google repositories. I also downgraded pip, with no luck.\nMy objective is to have Jaxlib 0.4.7 running in my computer.",
        "answers": [
            "Recent jaxlib releases require OSX version 10.14 or newer (see https://github.com/google/jax/blob/main/CHANGELOG.md#jaxlib-0314-june-27-2022).\nYour options are either to update your OSX to a more recent version, to build jaxlib yourself (this can be tricky; see building from source), or to use an older jaxlib release where your OS version is supported."
        ],
        "link": "https://stackoverflow.com/questions/76665537/jaxlib-0-4-7-cant-be-installed-or-built-in-my-osx-10-13-python-3-9-13"
    },
    {
        "title": "How to make a function a valid jax type?",
        "question": "When I pass an object created using the following function function into a jax.lax.scan function:\ndef logdensity_create(model, centeredness = None, varname = None):\n    if centeredness is not None:\n        model = reparam(model, config={varname: LocScaleReparam(centered= centeredness)})\n          \n    init_params, potential_fn_gen, *_ = initialize_model(jax.random.PRNGKey(0),model,dynamic_args=True)\n    logdensity = lambda position: -potential_fn_gen()(position)\n    initial_position = init_params.z\n    return (logdensity, initial_position)\nI get the following error (on passing the logdensity to an iterative function created using jax.lax.scan):\nTypeError: Value .logdensity_create.. at 0x13fca7d80> with type  is not a valid JAX type\nHow can I resolve this error?",
        "answers": [
            "I would probably do this via jax.tree_util.Partial, which wraps callables in a PyTree for compatibility with jit and other transformations:\nlogdensity = jax.tree_util.Partial(lambda position: -potential_fn_gen()(position))"
        ],
        "link": "https://stackoverflow.com/questions/76655153/how-to-make-a-function-a-valid-jax-type"
    },
    {
        "title": "How to slice jax arrays using jax tracer?",
        "question": "I am trying to modify a code base to create a subarray using an existing array and indices in the form of Jax tracer. When I try to pass these Jax tracers directly for indices. I get the following error:\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\nWhat is a possible workaround/ solution for this?",
        "answers": [
            "There are two main workarounds here that may be applicable depending on your problem: using static indices, or using dynamic_slice.\nQuick background: one constraint of arrays used in JAX transformations like jit, vmap, etc. is that they must be statically shaped (see JAX Sharp Bits: Dynamic Shapes for some discussion of this).\nWith that in mind, a function like f below will always fail, because i and j are non-static variables and so the shape of the returned array cannot be known at compile time:\n@jit\ndef f(x, i, j):\n  return x[i:j]\nOne workaround for this is to make i and j static arguments in jit, so that the shape of the returned array will be static:\n@partial(jit, static_argnames=['i', 'j'])\ndef f(x, i, j):\n  return x[i:j]\nThat's the only possible workaround to use jit in such a situation, because of the static shape constraint.\nAnother flavor of slicing problem that can lead to the same error might look like this:\n@jit\ndef f(x, i):\n  return x[i:i + 5]\nThis will also result in a non-static index error. It could be fixed as above by marking i as static, but there is more information here: assuming that 0 <= i < len(x) - 5 holds, we know that the shape of the output array is (5,). This is a case where jax.lax.dynamic_slice is applicable (when you have a fixed slice size at a dynamic location):\n@jit\ndef f(x, i):\n  return jax.lax.dynamic_slice(x, (i,), (5,))\nNote that this will have different semantics than x[i:i + 5] in cases where the slice overruns the bounds of the array, but in most cases of interest it is equivalent.\nThere are other examples where neither of these two workarounds are applicable, for example when your program logic is predicated on creating dynamic-length arrays. In these cases, there is no easy work-around, and your best bet is to either (1) re-write your algorithm in terms of static array shapes, perhaps using padded array representations, or (2) not use JAX."
        ],
        "link": "https://stackoverflow.com/questions/76626143/how-to-slice-jax-arrays-using-jax-tracer"
    },
    {
        "title": "Selecting all elements of subsets if at least one element is selected (JAX)",
        "question": "I have the following problem, for which I cannot manage to write a solution in JAX that is jittable and efficient.\nI have a set of elements. Some of these elements are included (on the basis of a condition, which is not important now). The included elements are denoted by 1, the not-included elements by 0. For example, the array arr = jnp.array([1, 0, 0, 0, 0, 0]) indicates that I have 6 elements, the first of them included based on my condition.\nThese elements are grouped into subsets. I have a second array that indicates where each subset starts in the first array arr. For example, the array subsets = jnp.array([0, 2]) indicates that the first subset starts at position 0 and the second subset starts at position 2.\nNow, if one element is included based on arr, I would like to include all the elements in the same subset. In this example, the output should then be [1, 1, 0, 0, 0, 0].\nI have tried with a jax.lax.fori_loop, but it is slow.\n@jax.jit                                                                        \ndef select_subsets(arr, subsets):                 \n    new_arr = arr.copy()            \n    n_resid = subsets.shape[0]    \n    indices = jnp.arange(arr.shape[0])       \n                                                                                    \n    def func(i, new_arr):    \n        start = subsets[i]    \n        stop = subsets[i+1]    \n        arr_sliced = jnp.where((indices >= start) & (indices < stop), arr, 0.0)    \n        sum_ = jnp.sum(arr_sliced)    \n        new_arr = jnp.where(sum_ > 0.5, jnp.where((indices >= start) & (indices < stop), 1, new_arr), new_arr)    \n        return new_arr                          \n                                                 \n    new_arr = jax.lax.fori_loop(0, n_resid-1, func, new_arr)    \n    return new_arr\nthis function works if I use a subsets with the last element equal to the number of elements in arr, subsets = jnp.array([0, 2, 6]).\nI then thought about writing a vectorized version (using jax.numpy operations), but I cannot manage to do it.\nIs there a JAX guru that can help me with this?\nThanks a lot!",
        "answers": [
            "Here's a vectorized version. It instantiates a mask with shape len(subsets) x len(arr), which might be undesirable depending on how big those values are.\n           \n@jax.jit                                                                             \ndef vectorized_select_subsets(arr, subsets):                         \n    l, = arr.shape               \n                               \n    indices = jnp.arange(l)[None, :]\n                \n    # Broadcast to mask of shape (n_subsets, input_length)\n    subset_masks = (\n        (indices >= subsets[:-1, None])\n        & (indices < subsets[1:, None])        \n    )                                                           \n                                                    \n    # Shape (n_subsets,) array indicating whether each subset is included\n    include_subset = jnp.any(subset_masks & arr[None, :], axis=1)          \n                                                          \n    # Reduce down columns \n    result = jnp.any(subset_masks & include_subset[:, None], axis=0).astype(jnp.int32)   \n    return result\nI timed this against the loop-based version on an array with length 512 and 32 subsets:\nLoop: 6254.647 it/s\nVectorized: 37940.335 it/s"
        ],
        "link": "https://stackoverflow.com/questions/76617821/selecting-all-elements-of-subsets-if-at-least-one-element-is-selected-jax"
    },
    {
        "title": "What are the tradeoffs between jax.lax.map and jax.vmap?",
        "question": "This Github issue hints that there are tradeoffs in performance / memory / compilation time when choosing between jax.lax.map and jax.vmap. What are the specific details of these tradeoffs with respect to both GPUs and CPUs?",
        "answers": [
            "The main difference is that jax.vmap is a vectorizing transformation, while lax.map is an iterative transformation. Let's look at an example.\nExample function: vector_dot\nSuppose you have implemented a simple function that takes 1D vectors as inputs. For simplicity let's make it a simple dot product, but one that asserts the inputs are one-dimensional:\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef vector_dot(x, y):\n  assert x.ndim == y.ndim == 1, \"vector inputs required\"\n  return jnp.dot(x, y)\nWe can create some random 1D vectors to test this:\nrng = np.random.default_rng(8675309)\nx = rng.uniform(size=50)\ny = rng.uniform(size=50)\n\nprint(vector_dot(x, y))\n# 14.919376\nTo see what JAX is doing with this function under the hood, we can print the jaxpr, which is JAX's intermediate-level representation of a function:\nprint(jax.make_jaxpr(vector_dot)(x, y))\n# { lambda ; a:f32[50] b:f32[50]. let\n#     c:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] a b\n#   in (c,) }\nThis shows that JAX lowers this code to a single call to dot_general, the primitive for generalized dot products in JAX and XLA.\nIterating over vector_dot\nNow, suppose you have a 2D input, and you'd like to apply this function to each row. There are several ways you could imagine doing this: three examples are using a Python for loop, using jax.vmap, or using jax.lax.map:\ndef batched_dot_for_loop(x_batched, y):\n  return jnp.array([vector_dot(x, y) for x in x_batched])\n\ndef batched_dot_lax_map(x_batched, y):\n  return jax.lax.map(lambda x: vector_dot(x, y), x_batched)\n\nbatched_dot_vmap = jax.vmap(vector_dot, in_axes=(0, None))\nApplying these three functions to a batched input yields the same results, to within floating point precision:\nx_batched = rng.uniform(size=(4, 50))\n\nprint(batched_dot_for_loop(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_lax_map(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_vmap(x_batched, y))\n# [11.964927  12.485697  13.683528  12.9286175]\nBut if we look at the jaxpr for each, we can see that the three approaches lead to very different computational characteristics.\nThe for loop solution looks like this:\nprint(jax.make_jaxpr(batched_dot_for_loop)(x_batched, y))\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[1,50] = slice[\n      limit_indices=(1, 50)\n      start_indices=(0, 0)\n      strides=(1, 1)\n    ] a\n    d:f32[50] = squeeze[dimensions=(0,)] c\n    e:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d b\n    f:f32[1,50] = slice[\n      limit_indices=(2, 50)\n      start_indices=(1, 0)\n      strides=(1, 1)\n    ] a\n    g:f32[50] = squeeze[dimensions=(0,)] f\n    h:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] g b\n    i:f32[1,50] = slice[\n      limit_indices=(3, 50)\n      start_indices=(2, 0)\n      strides=(1, 1)\n    ] a\n    j:f32[50] = squeeze[dimensions=(0,)] i\n    k:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] j b\n    l:f32[1,50] = slice[\n      limit_indices=(4, 50)\n      start_indices=(3, 0)\n      strides=(1, 1)\n    ] a\n    m:f32[50] = squeeze[dimensions=(0,)] l\n    n:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] m b\n    o:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e\n    p:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] h\n    q:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] k\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] n\n    s:f32[4] = concatenate[dimension=0] o p q r\n  in (s,) }\nThe key feature is that the iterations in the for loop are unrolled into a single long program.\nThe lax.map version looks like this:\nprint(jax.make_jaxpr(batched_dot_lax_map)(x_batched, y))\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = scan[\n      jaxpr={ lambda ; d:f32[50] e:f32[50]. let\n          f:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] e d\n        in (f,) }\n      length=4\n      linear=(False, False)\n      num_carry=0\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] b a\n  in (c,) }\nThe key feature is that it is loaded into a scan primitive, which is XLA's native static loop operation.\nThe vmap version looks like this:\nprint(jax.make_jaxpr(batched_dot_vmap)(x_batched, y))\n{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n  in (c,) }\nThe key feature here is that the vmap transformation is able to recognize that a batched 1D dot product is equivalent to a 2D dot product, so the result is a single extremely efficient native operation.\nPerformance considerations\nThese three approaches can have very different performance characteristics. The details will depend on the specifics of the original function (here vector_dot) but in broad strokes, we can consider three aspects:\nCompilation Cost\nIf you JIT-compile your program, you'll find:\nThe for-loop based solution will have compilation times that grow super-linearly with the number of iterations. This is due to the unrolling seen in the jaxpr above.\nThe lax.map and jax.vmap solutions will have fast compilation time, which under normal circumstances will not grow with the size of the batch dimension.\nRuntime\nIn terms of runtime:\nThe for loop solution can be very fast, because XLA can often fuse operations between the unrolled iterations. This is the flip side of the long compilation times.\nThe lax.map solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.\nThe jax.vmap solution will generally be the fastest, especially on accelerators like GPU or TPU, because it can make use of native batching parallelism on the device.\nMemory Cost\nThe for loop and lax.map solutions generally have good memory performance, because they execute sequentially and don't require storage of large intermediate results.\nThe main downside of the jax.vmap solution is that it can cause memory to blow up because the entire problem must fit into memory at once. This is not an issue with the simple vector_dot function used here, but can be for more complicated functions.\nBenchmarks\nYou can see these general principles at play when benchmarking the above functions. The following timings are on a Colab T4 GPU:\ny = rng.uniform(size=1000)\nx_batched = rng.uniform(size=(200, 1000))\n%time jax.jit(batched_dot_for_loop).lower(x_batched, y).compile()\n# CPU times: user 4.96 s, sys: 55 ms, total: 5.01 s\n# Wall time: 7.24 s\n%timeit jax.jit(batched_dot_for_loop)(x_batched, y).block_until_ready()\n# 1.09 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n%time jax.jit(batched_dot_lax_map).lower(x_batched, y).compile()\n# CPU times: user 117 ms, sys: 2.71 ms, total: 120 ms\n# Wall time: 172 ms\n%timeit jax.jit(batched_dot_lax_map)(x_batched, y).block_until_ready()\n# 2.67 ms ± 56.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%time jax.jit(batched_dot_vmap).lower(x_batched, y).compile()\n# CPU times: user 51 ms, sys: 941 µs, total: 52 ms\n# Wall time: 103 ms\n%timeit jax.jit(batched_dot_vmap)(x_batched, y).block_until_ready()\n# 719 µs ± 129 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
        ],
        "link": "https://stackoverflow.com/questions/76615802/what-are-the-tradeoffs-between-jax-lax-map-and-jax-vmap"
    },
    {
        "title": "Creating a jax array using existing jax arrays of different lengths throws error",
        "question": "I am using the following code to set a particular row of a jax 2D array to a particular value using jax arrays:\nzeros_array = jnp.zeros((3, 8))\nvalue = jnp.array([1,2,3,4])\nvalue_2 = jnp.array([1])\nvalue_3 = jnp.array([1,2])\nvalues = jnp.array([value,value_2,value_3])\nzeros_array = zeros_array.at[0].set(values)\nBut, I am receiving the following error:\nValueError: All input arrays must have the same shape.\nUpon modifying the jnp to np (numpy) the error disappears. Is there any way to resolve this error? I know one walk around this would be to set each of the separate arrays in the 2D array using at[0,1].set(), at[0,2:n].set().",
        "answers": [
            "What you have in mind is a \"ragged array\", and no, there is not currently any way to do this in JAX. In older versions of NumPy, this will work by returning an array of dtype object, but in newer versions of NumPy this results in an error because object arrays are generally inconvenient and inefficient to work with (for example, there's no way to efficiently do the equivalent of the index update operation in your last line if the updates are stored in an object array).\nDepending on your use-case, there are several workarounds for this you might use in both JAX and NumPy, including storing the rows of your array as a list, or using a padded 2D array representation.\nI'll note also that the JAX team is exploring native support for ragged arrays (see e.g. https://github.com/google/jax/pull/16541) but it's still fairly far from being generally useful."
        ],
        "link": "https://stackoverflow.com/questions/76600803/creating-a-jax-array-using-existing-jax-arrays-of-different-lengths-throws-error"
    },
    {
        "title": "How to create a jax array and store dictionaries inside it later?",
        "question": "I wish to create a Jax-based array and use this array later to store dictionaries inside it. Is it possible to do so?",
        "answers": [
            "JAX arrays cannot store dictionaries as items. They can only store items of simple numerical types, including:\nintegers: int8, int16, int32, int64\nunsigned integers: uint8, uint16, uint32, uint64\nfloating point: bfloat16, float16, float32, float64\ncomplex; complex64, complex128\nAdditionally, several experimental narrow-width float and integer types from the ml_dtypes library have support on some hardware.\nDepending on your use-case, you may be able to use a struct-of-arrays pattern rather than an array-of-structs pattern, but it's hard to say whether this is applicable without more information about what you're trying to do."
        ],
        "link": "https://stackoverflow.com/questions/76594409/how-to-create-a-jax-array-and-store-dictionaries-inside-it-later"
    },
    {
        "title": "How to vectorize cho_solve?",
        "question": "This question solved my problem of using vmap on cho_solve, is it possible to vectorize cho_solve, or does the definition of cho_solve preclude it from being vectorized? vectorize seems to need the arguments to all be arrays, whereas cho_solve takes a tuple as the first argument?\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\n\nk_y = jax.random.normal(subkey, (3, 5, 10, 10))\ny = jnp.broadcast_to(jnp.eye(10), k_y.shape)\n\nmatmul = jnp.vectorize(jnp.matmul, signature='(a,b),(b,c)->(a,c)')\ncholesky = jnp.vectorize(jsp.linalg.cholesky, excluded={1}, signature='(d,d)->(d,d)')\ncho_solve = jnp.vectorize(jsp.linalg.cho_solve, signature='(d,d),(d,d)->(d,d)')  # what to put here?\n\nk_y = matmul(k_y, jnp.moveaxis(k_y, -1, -2))\nchol = cholesky(k_y, True)\nresult = cho_solve((chol, True), y)\nValueError: All input arrays must have the same shape.\nMy use case is that I have an unspecified amount of \"batch\" dimensions that I want to vmap over, and vectorize handles the auto broadcasting beautifully. I can once again write my own cho_solve using solve_triangular but this seems like a waste. Is it possible for vectorize to have a similar interface to vmap, which can take nested signatures?",
        "answers": [
            "I don't believe you can use vectorize directly with cho_solve. The vectorize API requires that the function take arrays as an argument, while cho_solve takes a tuple as the first argument. The only way you could use vectorize with this function is to wrap it in one with a different API. For example:\ncho_solve = jnp.vectorize(\n    lambda chol, flag, y: jsp.linalg.cho_solve((chol, flag), y),\n    excluded={1}, signature='(d,d),(d,d)->(d,d)')\nresult = cho_solve(chol, True, y)",
            "Here's the solve_triangular solution I managed:\nsolve_tri = jnp.vectorize(\n  jsp.linalg.solve_triangular, excluded={2, 3}, signature='(d,d),(d,d)->(d,d)')\nchol = cholesky(k_y, True)\nresult2 = solve_tri(chol, solve_tri(chol, y, 0, True), 1, True)\nresult3 = jnp.array([\n  jsp.linalg.cho_solve((chol[a, b], True), y[a, b])\n  for a in range(k_y.shape[0])\n  for b in range(k_y.shape[1])\n]).reshape(k_y.shape)\nprint(jnp.allclose(result2, result3))\nTrue"
        ],
        "link": "https://stackoverflow.com/questions/76588276/how-to-vectorize-cho-solve"
    },
    {
        "title": "Passing the returned stacked output to jax.lax.scan",
        "question": "I wish to pass on the returned stacked values from the jax.lax.scan back to one of its arguments. Is it possible to do so? For example:\nfrom jax import lax\n\n\ndef cumsum(res, el):\n    \"\"\"\n    - `res`: The result from the previous loop.\n    - `el`: The current array element.\n    \"\"\"\n    v, u = res\n    print(u)\n    v = v + el\n    return (v,u),v  # (\"carryover\", \"accumulated\")\n\n\nresult_init = 0\nresult = []\n\n(final,use), result = lax.scan(cumsum, (result_init,result), a)\nIn the above code, I want to extract the cumulated res values during the runtime and pass it back. Thus, I have passed the result as an argument in the lax function, but it always prints an empty list.",
        "answers": [
            "There's no built-in way to access the \"current state\" of the accumulated values in the course of a scan operation: in particular, the current state will be a dynamically-shaped array (it will have size 0 in the first iteration, size 1 in the second, etc.) and scan, like other JAX transformations and higher-order functions, requires static shapes.\nBut you could do something similar by passing along an array that you manually update. It might look something like this:\nfrom jax import lax\nimport jax.numpy as jnp\nfrom jax import debug\n\ndef cumsum(carry, el):\n    i, v, running_result = carry\n    v = v + el\n    running_result = running_result.at[i].set(v)\n    debug.print(\"iteration {}: running_result={}\", i, running_result)\n    return (i + 1, v, running_result), v\n\na = jnp.arange(5)\nrunning_result = jnp.zeros_like(a)\n(i, v, running_result), result = lax.scan(cumsum, (0, 0, running_result), a)\n\nprint(\"\\nfinal running result:\", running_result)\nprint(\"final result:\", result)\niteration 0: running_result=[0 0 0 0 0]\niteration 1: running_result=[0 1 0 0 0]\niteration 2: running_result=[0 1 3 0 0]\niteration 3: running_result=[0 1 3 6 0]\niteration 4: running_result=[ 0  1  3  6 10]\n\nfinal running result: [ 0  1  3  6 10]\nfinal result: [ 0  1  3  6 10]\nNotice that I used jax.debug.print to print the intermediate results, because this function is traced and compiled."
        ],
        "link": "https://stackoverflow.com/questions/76587199/passing-the-returned-stacked-output-to-jax-lax-scan"
    },
    {
        "title": "Error using JAX, Array slice indices must have static start/stop/step",
        "question": "I'll be happy to help you with your code. If I understand correctly, you want to create a 2D Gaussian patch for each value in the darkField array. The size of the patch should ideally be calculated as 2 * np.ceil(3 * sigma) + 1, where sigma is the corresponding value from the darkField array. You have fixed the size value to 10 in your example to avoid errors.\nOnce the Gaussian patch is normalized to 1, you want to multiply it by the corresponding value from the intensityRefracted2DF array to obtain the generated blur. Finally, you want to add this blur patch to the intensityRefracted3 array.\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom jax.scipy.signal import convolve2d\nfrom functools import partial\n\n@partial(jax.jit,static_argnums=(1,))\ndef gaussian_shape(sigma, size):\n    \"\"\"\n    Generate a Gaussian shape.\n\n    Args:\n        sigma (float or 2D numpy array): Standard deviation(s) of the Gaussian shape.\n        size (int): Size of the Gaussian shape.\n\n    Returns:\n        exponent (2D numpy array): Gaussian shape.\n    \"\"\"\n\n    x = jnp.arange(0, size) - jnp.floor(size / 2)\n    exponent = jnp.exp(-(x ** 2) / (2 * sigma ** 2))\n    exponent = jnp.outer(exponent, exponent)\n    exponent /= jnp.sum(exponent)\n    return exponent\n\n@partial(jax.jit)\ndef apply_dark_field(i, j, intensityRefracted2DF, intensityRefracted3, darkField):\n    currDF_ij=darkField[i,j]\n    patch = gaussian_shape(currDF_ij,10)\n    size2 = patch.shape[0] // 2\n    patch = patch * intensityRefracted2DF[i, j]\n\n\n    intensityRefracted3 = intensityRefracted3.at[i - size2:i + size2 + 1, j - size2:j + size2 + 1].add(patch * intensityRefracted2DF[i, j])\n    # intensityRefracted3 = jax.ops.index_add(intensityRefracted3, (i, j), intensityRefracted2DF[i, j] * (darkField[i, j] == 0))\n    return intensityRefracted3\n\n@jax.jit\ndef darkFieldLoop(intensityRefracted2DF, intensityRefracted3, darkField):\n    currDF = jnp.zeros_like(intensityRefracted3)\n    currDF = jnp.where(intensityRefracted2DF!=0,darkField,0)\n\n    i = jnp.nonzero(currDF,size=currDF.shape[0])\n    indices_i=i[0]\n    indices_j=i[1]\n    intensityRefracted3 = jnp.zeros_like(intensityRefracted3)\n\n    intensityRefracted3 = jax.vmap(apply_dark_field, in_axes=(0, 0, None, None, None))(indices_i, indices_j, intensityRefracted2DF, intensityRefracted3, darkField)\n\n    return intensityRefracted3\n\nintensityRefracted2DF = np.random.rand(10,10)\nintensityRefracted3 = np.zeros((10, 10))\ndarkField = np.random.rand(10, 10)\n\na=darkFieldLoop(intensityRefracted2DF,intensityRefracted3,darkField)\n\nfor i in range(a.shape[0]):\n    plt.imshow(a[i])\n    plt.show()\nAnd there is the error message :\nIndexError: Array slice indices must have static start/stop/step to   be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=3/0)>, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\nI've also try to put i,j into static_argnums using partial\n@partial(jax.jit, static_argnums=(0,1))\ndef apply_dark_field(i, j, intensityRefracted2DF, intensityRefracted3, darkField):\nand there is the error:\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 0) of type <class 'jax._src.interpreters.batching.BatchTracer'> for function apply_dark_field is non-hashable.",
        "answers": [
            "The issue comes from the fact that JAX arrays cannot have a dynamic shape, and so dynamic start & end indices cannot be used in indexing expressions.\nYour solution of marking i and j as static would work, except that you are vmapping across these values, so by definition they cannot be static.\nThe best solution here is probably to use lax.dynamic_slice and lax.dynamic_update_slice, which are operations designed exactly for the case that you have (where indices are dynamic, but slice sizes are static).\nYou can replace this line:\nintensityRefracted3 = intensityRefracted3.at[i - size2:i + size2 + 1, j - size2:j + size2 + 1].add(patch * intensityRefracted2DF[i, j])\nwith this:\nstart_indices = (i - size2, j - size2)\nupdate = jax.lax.dynamic_slice(intensityRefracted3, start_indices, patch.shape)\nupdate += patch * intensityRefracted2DF[i, j]\nintensityRefracted3 = jax.lax.dynamic_update_slice(\n    intensityRefracted3, update,  start_indices)\nand it should work correctly with dynamic i and j. Though you should be careful, because if any of the specified indices are out-of-bounds, dynamic_slice and dynamic_update_slice will clip them into the valid range."
        ],
        "link": "https://stackoverflow.com/questions/76578461/error-using-jax-array-slice-indices-must-have-static-start-stop-step"
    },
    {
        "title": "is there a more efficient equivalent of np.sum(np.cumprod(1 / (1 + y*x)))?",
        "question": "I have a 1D NumPy array, such as x:\nx = np.array([0.05, 0.06, 0.06, 0.04])\nI'm showing a small array, but in reality, x can be very large. To x, I want to perform the following calculation:\ny = 1./12.\nnp.sum(np.cumprod(1 / (1 + y * x)))\nBecause x is very large, I want to do this more efficiently. I tried to use np.exp(np.cumsum(np.log(1 / (1 + y * x)))).sum() but this makes it slower. Is there a more efficient NumPy/JAX function?",
        "answers": [
            "You pretty much vectorized as much as possible.\nWhat you could spare here, if your were writing in C, is the numerous (but implicit, that is done thanks to vectorization inside numpy's code, that is in C) for loops involved. Since what your code means is that you\nFirstly iterate all elements of x to multiply them by y. In pure python that would be compound list [y*a for a in x]\nSecondly iterate again all elements of former result, to add 1 to all of them. So, [1+z for z in [y*a for a in x]]\nThirdly, iterate again all elements of the former result to invert all of them. So [1/u for u in [1+z for z in [y*a for a in x]]]\nFourthly, iterate again to compute cumulative product. So p=1; [p:=v*p for v in [1/u for u in [1+z for z in [y*a for a in x]]]]\nFifthly, iterate again to compute sum of former\nSo, sure, all those for loops are in C, so very fast. But there are many (non-nested) of them. And each of them doesn't do much. So time spend in the iteration itself the for(int i=0; i<arr_len; i++) that occurs somewhere in numpy's C code, is not that negligible before the content of that iteration (the result[i] = y*x[i] that is repeated by this loop in numpy's C code).\nIf you were writing this in pure python\ndef cumsumprod(x,y):\n    z=[y*a for a in x]\n    u=[1+a for a in z]\n    v=[1/a for a in u]\n    p=1; w=[p:=p*a for a in v]\n    s=0\n        for a in w: s+=a\n    return s\nThat would be way less efficient than this other pure python implementation\ndef ff(x,y):\n    s=0\n    p=1\n    for a in x:\n        p/=(1+y*a)\n        s+=p\n    return s\nSame computation. But one for loop instead of 5.\nTo be quantitative in what I say, on your example, in microseconds, your code takes 9.8 μs on my machine. My 1st python code 3.6 μs. And my 2nd, 1.9 μs.\nAnd yes, with that small data, pure python codes are both faster than numpy. If array is size 1000 instead, those timings become 17.7, 464 and 288. But point is, my second code is faster than my first, unsurprisingly. And your numpy code is the equivalent of my first code, but in C.\nAnd that is even an understatement, since I just use the example of for loops. It is not the only redundant thing that numpy does. For example, it also allocates a new array for each intermediary operation.\nNot that you did anything wrong. You did exactly what you are supposed to do with numpy. Just that is what numpy does: if provides many vectorized operation that we can sequence, each of them being a for loop on all our data. And we pay the price of having several unnecessary for loops, in exchange of the reward of having them in C, when pure python would be way slower. That is pretty much the best you can have from numpy.\nIf you want to have more, a way is numba. Numba allows you to write, otherwise naive, code, and yet have it fast, in C.\nJust add @jit before my previous pure python's code\nfrom numba import jit\n\n@jit(nopython=True)\ndef ff(x,y):\n    s=0\n    p=1\n    for a in x:\n        p/=(1+y*a)\n        s+=p\n    return s\nAnd you get something that is both in C, and doesn't contain the unnecessary bu yet unavoidable operations that sequencing many numpy's vectorized operation does.\nTimings for this function is 0.25 μs for your list. So way better than 1.9 μs of the same in pure python, thanks to compilation.\nAnd for a size 1000 list, where numpy's beat pure python, timing is 3.4 μs. So not only way better than python 276 μs, but also better than numpy's 14.9 μs, thanks to the simplicity of the algorithm.\nSo long story short: numba allows to write plain, simple, naive algorithm on numpy array, that are compiled."
        ],
        "link": "https://stackoverflow.com/questions/76563019/is-there-a-more-efficient-equivalent-of-np-sumnp-cumprod1-1-yx"
    },
    {
        "title": "Oryx: How to `inverse` a JAX function with additional parameters",
        "question": "I would like to use the package Oryx to invert an affine transformation written in JAX. The transformation maps x->y and depends on a set of adjustable parameters (which I call params). Specifically, the affine transformation is defined as:\nimport jax.numpy as jnp\n\ndef affine(params, x):\n  return x * params['scale'] + params['shift']\n\nparams = dict(scale=1.5, shift=-1.)\nx_in = jnp.array(3.)\ny_out = affine(params, x_in)\nI would like to invert affine wrt to input x as a function of params. Oryx has a function oryx.core.inverse to invert JAX functions. However, inverting a function with parameters, like this:\nimport oryx\n\noryx.core.inverse(affine)(params, y_out)\ndoesn't work (AssertionError: length mismatch: [1, 3]), presumably because inverse doesn't know that I want to invert y_out but not params. What is the most elegant way to solve this problem for all possible values (i.e., as a function) of params using oryx.core.inverse? I find the inverse docs not very illuminating.\nUpdate: Jakevdp gave an excellent suggestion for a given set of params. I've clarified the question to indicate that I am wondering how to define the inverse as a function of params.",
        "answers": [
            "You can do this by closing over the static parameters, for example using partial:\nfrom functools import partial\nx = oryx.core.inverse(partial(affine, params))(y_out)\n\nprint(x)\n# 3.0\nEdit: if you want a single inverted function to work for multiple values of params, you will have to return params in the output (otherwise, there's no way from a single output value to infer all three inputs). It might look something like this:\ndef affine(params, x):\n  return params, x * params['scale'] + params['shift']\n\nparams = dict(scale=1.5, shift=-1.)\nx_in = jnp.array(3.)\n_, y_out = affine(params, x_in)\n\n_, x = oryx.core.inverse(affine)(params, y_out)\nprint(x)\n# 3.0"
        ],
        "link": "https://stackoverflow.com/questions/76538976/oryx-how-to-inverse-a-jax-function-with-additional-parameters"
    },
    {
        "title": "JAX vmap vs pmap vs Python multiprocessing",
        "question": "I am rewriting some code from pure Python to JAX. I have gotten to the point where in my old code, I was using Python's multiprocessing module to parallelize the evaluation of a function over all of the CPU cores in a single node as follows:\n# start pool process \npool = multiprocessing.Pool(processes=10) # if node has 10 CPU cores, start 10 processes\n\n# use pool.map to evaluate function(input) for each input in parallel\n# suppose len(inputs) is very large and 10 inputs are processed in parallel at a time\n# store the results in a list called out\nout = pool.map(function,inputs)\n\n# close pool processes to free memory\npool.close()\npool.join()\nI know that JAX has vmap and pmap, but I don't understand if either of those are a drop-in replacement for how I'm using multiprocessing.pool.map above.\nIs vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).",
        "answers": [
            "Is vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nNo, vmap has nothing to do with parallelization. It is a vectorizing transformation, not a parallelizing transformation. In the course of normal operation, JAX may use multiple cores via XLA, so vmapped operations may also do this. But there's no explicit parallelization in vmap.\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\npmap parallelizes over multiple XLA devices. vmap does not parallelize, but rather vectorizes on a single device. multiprocessing parallelizes over multiple Python processes.\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nYes, it could be described as SPMD across multiple python processes.\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\npmap parallelizes over multiple XLA devices, and you have configured only a single XLA device, so the requested operation is not possible.\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).\nYes, I believe that pmap can be used to compute on multiple CPU cores. Whether it's nested with vmap is irrelevant. See JAX pmap with multi-core CPU.\nNote also that jax.pmap is deprecated in favor of the newer jax.shard_map, which is a much more flexible transformation for multi-device/multi-host computation. There's some info here: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html and https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html"
        ],
        "link": "https://stackoverflow.com/questions/76536601/jax-vmap-vs-pmap-vs-python-multiprocessing"
    },
    {
        "title": "How can I test if a jitted Jax function creates new tensor or a view?",
        "question": "I have a basic code like this:\n@jit\ndef concat_permute(indices, in1, in2):\n    tensor = jnp.concatenate([jnp.atleast_1d(in1), jnp.atleast_1d(in2)])\n    return tensor[indices]\nHere is my test tensors:\nkey = jax.random.PRNGKey(758493)\nin1 = tens = jax.random.uniform(key, shape=(15,5,3))\nin2 = tens = jax.random.uniform(key, shape=(10,5,3))\nindices = jax.random.choice(key, 25, (25,), replace=False)\nAnd here is the Jaxpr of the function:\n{ lambda ; a:i32[25] b:f32[15,5,3] c:f32[10,5,3]. let\n    d:f32[25,5,3] = xla_call[\n      call_jaxpr={ lambda ; e:i32[25] f:f32[15,5,3] g:f32[10,5,3]. let\n          h:f32[15,5,3] = xla_call[\n            call_jaxpr={ lambda ; i:f32[15,5,3]. let  in (i,) }\n            name=atleast_1d\n          ] f\n          j:f32[10,5,3] = xla_call[\n            call_jaxpr={ lambda ; k:f32[10,5,3]. let  in (k,) }\n            name=atleast_1d\n          ] g\n          l:f32[25,5,3] = concatenate[dimension=0] h j\n          m:bool[25] = lt e 0\n          n:i32[25] = add e 25\n          o:i32[25] = select_n m e n\n          p:i32[25,1] = broadcast_in_dim[\n            broadcast_dimensions=(0,)\n            shape=(25, 1)\n          ] o\n          q:f32[25,5,3] = gather[\n            dimension_numbers=GatherDimensionNumbers(offset_dims=(1, 2), collapsed_slice_dims=(0,), start_index_map=(0,))\n            fill_value=None\n            indices_are_sorted=False\n            mode=GatherScatterMode.PROMISE_IN_BOUNDS\n            slice_sizes=(1, 5, 3)\n            unique_indices=False\n          ] l p\n        in (q,) }\n      name=concat_permute\n    ] a b c\n  in (d,) }\nIt seems it creates a new tensor using my permutation array but I'm not sure. Is there a more clear way to see if this opeeration is made by creating new tensor or not?\nI tried \"jax.make_jaxpr\" and see the results but not sure about the problem.",
        "answers": [
            "The short answer is, no the output of your function will not share memory with the array allocated for tensor.\nIn XLA, an array is represented by a uniformly-strided buffer, and when you select random values from an array, the result cannot in general be constructed via uniform-striding over a view of the input buffer."
        ],
        "link": "https://stackoverflow.com/questions/76479539/how-can-i-test-if-a-jitted-jax-function-creates-new-tensor-or-a-view"
    },
    {
        "title": "JAX's slow performance on simple loops",
        "question": "I'm learning JAX and trying to do a simple test on the performance of JAX. The run time of the following code using JAX is weirdly slower than numpy or even simple addition of list members using python lists. I mean the following code has no purpose other than testing the speed.\nI'm wondering what might be the reason for this? I see that there is a functionality of JAX called \"fori_loop\" that may help in this examples or I can vectorize my actual code, but I want to know why this simple loop is so slow and do I need to avoid writing code like this and completely understand things in the JAX way?\nHere is the code in JAX:\nimport jax.numpy as jnp\nfrom jax import random\nimport time\n\nkey = random.PRNGKey(0)\nx = random.uniform(key, shape=(100,3))\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\nwhich takes 11 seconds\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n11.254772663116455\nThe same code using Numpy:\nimport numpy as np\nimport time\n\nx = np.random.rand(100,3)\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\ntakes around a milli second:\n0.005955934524536133\nThank you!",
        "answers": [
            "If you're interested in comparing the speed of JAX vs. NumPy speed, the place to start is here: JAX FAQ: Is JAX Faster Than NumPy? Quoting from there:\nin summary: if you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead. If you’re running your code on GPU or TPU, or are benchmarking more complicated JIT-compiled sequences of operations on CPU, you can generally expect JAX to outperform NumPy.\nYour benchmark does many individual array operations, each of which is very inexpensive, so effectively all you're measuring is the single-operation dispatch time. This is precisely in the regime where we'd expect NumPy to be fast, and JAX to be slow.\nFor real use-cases involving repeated operations, there are several ways you might optimize the code (including wrapping the whole function in jit, using vmap for efficient batching, or using fori_loop for sequential operations) but for the example function you give it's hard to say what's best. Taking your example at face value, I'd optimize it this way:\ndef func(x):\n  return 0\nAs written, there's no need to do any operations on x at all (but I suspect that's not particularly helpful for your real use-case)."
        ],
        "link": "https://stackoverflow.com/questions/76474532/jaxs-slow-performance-on-simple-loops"
    },
    {
        "title": "Understanding how JAX's tracer vs static work",
        "question": "I'm new to JAX and trying to write a simple code using JAX where at some point I need to use a scipy method. Then I want to take its derivative.\nThe code doesn't run and gives me error. The following is the code and the error. I read a the documentation of JAX a couple of times but couldn't Figure out what to do to write the code correctly\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nimport numpy as np\nimport scipy\nkey = random.PRNGKey(1)\n\nsize = 3\nx = random.uniform(key, (size, size), dtype=jnp.float32)\n\ndef error_func(x):\n    dists = scipy.spatial.distance.cdist(x, x, metric='euclidean')\n    error = jnp.sum(jnp.array(dists))\n    return error\n\nerror_diff = grad(error_func)\n\nprint(error_func(x))\nprint(error_diff(x))\nAnd I get the followig error:\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n3.2158318\nTraceback (most recent call last):\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 26, in <module>\n    print(error_diff(x))\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/api.py\", line 646, in grad_f\n    _, g = value_and_grad_f(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/api.py\", line 722, in value_and_grad_f\n    ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/api.py\", line 2179, in _vjp\n    out_primal, out_vjp = ad.vjp(\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/interpreters/ad.py\", line 139, in vjp\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/interpreters/ad.py\", line 128, in linearize\n    jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/interpreters/partial_eval.py\", line 777, in trace_to_jaxpr_nounits\n    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 15, in error_func\n    dists = scipy.spatial.distance.cdist(x, x, metric='euclidean')\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/scipy/spatial/distance.py\", line 2909, in cdist\n    XA = np.asarray(XA)\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/jax/_src/core.py\", line 598, in __array__\n    raise TracerArrayConversionError(self)\njax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[0.7551559  0.3129729  0.12388372]\n [0.548188   0.8851279  0.30576992]\n [0.82008433 0.95633745 0.3566252 ]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[0.7551559 , 0.3129729 , 0.12388372],\n       [0.548188  , 0.8851279 , 0.30576992],\n       [0.82008433, 0.95633745, 0.3566252 ]], dtype=float32)\n  tangent = Traced<ShapedArray(float32[3,3])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[3,3]), None)\n    recipe = LambdaBinding()\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 26, in <module>\n    print(error_diff(x))\n  File \"/mnt/d/OneDrive - UW-Madison/opttest/jaxtest6.py\", line 15, in error_func\n    dists = scipy.spatial.distance.cdist(x, x, metric='euclidean')\n  File \"/home/sattarian/.local/lib/python3.9/site-packages/scipy/spatial/distance.py\", line 2909, in cdist\n    XA = np.asarray(XA)\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ConcreteArray([[0.7551559  0.3129729  0.12388372]\n [0.548188   0.8851279  0.30576992]\n [0.82008433 0.95633745 0.3566252 ]], dtype=float32)>with<JVPTrace(level=2/0)> with\n  primal = Array([[0.7551559 , 0.3129729 , 0.12388372],\n       [0.548188  , 0.8851279 , 0.30576992],\n       [0.82008433, 0.95633745, 0.3566252 ]], dtype=float32)\n  tangent = Traced<ShapedArray(float32[3,3])>with<JaxprTrace(level=1/0)> with\n    pval = (ShapedArray(float32[3,3]), None)\n    recipe = LambdaBinding()\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
        "answers": [
            "JAX transformations only work on JAX functions, not numpy or scipy functions (this is discussed briefly at the link shown in the error message above) If you want to use grad and other JAX transformations, you need to write your logic using JAX operations, not operations from numpy, scipy, or other non-JAX-compatible libraries.\nJAX does not currently include any wrappers of scipy.spatial.distance (though there are some in progress, see #16147), so the best option would be to write the code yourself. Fortunately, cdist is pretty straightforward:\ndef cdist(x, y, metric='euclidean'):\n  assert x.ndim == y.ndim == 2\n  if metric != 'euclidean':\n    raise NotImplementedError(f\"{metric=}\")\n  return jnp.sqrt(jnp.sum((x[:, None, :] - y[None, :, :]) ** 2, axis=-1))\n\ndef error_func(x):\n    dists = cdist(x, x, metric='euclidean')\n    error = jnp.sum(dists)\n    return error\n\nerror_diff = grad(error_func)\n\nprint(error_func(x))\n# 3.2158318\n\nprint(error_diff(x))\n# [[nan nan nan]\n#  [nan nan nan]\n#  [nan nan nan]]\nYou'll notice that the gradient is everywhere nan. This is the expected result due to the fact that grad(jnp.sqrt)(0.0) diverges (returns infinity), and 0.0 * inf by definition is nan."
        ],
        "link": "https://stackoverflow.com/questions/76460770/understanding-how-jaxs-tracer-vs-static-work"
    },
    {
        "title": "How to vmap over cho_solve and cho_factor?",
        "question": "The following error appears because of the last line of code below:\njax.errors.ConcretizationTypeError Abstract tracer value encountered where concrete value is expected...\nThe problem arose with the bool function.\nIt looks like it is due to the lower return value from cho_factor, which _cho_solve (note underscore) requires as static.\nI'm new to jax, so I was hoping that vmap-ing cho_factor into cho_solve would just work. What have I done wrong here?\nimport jax\n\nkey = jax.random.PRNGKey(0)\nk_y = jax.random.normal(key, (100, 10, 10))\ny = jax.random.normal(key, (100, 10, 1))\n\nmatmul = jax.vmap(jax.numpy.matmul)\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor)\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve)\n\nk_y = matmul(k_y, jax.numpy.transpose(k_y, (0, 2, 1)))\nchol, lower = cho_factor(k_y)\nresult = cho_solve((chol, lower), y)",
        "answers": [
            "The issue is that in each case, lower is a static scalar that should not be mapped over. So if you specify in_axes and out_axes so that lower is mapped over axis None, the vmap should work:\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor, out_axes=(0, None))\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve, in_axes=((0, None), 0))",
            "So I didn't manage to get cho_factor and cho_solve working, but worked around it using cholesky and solve_triangular:\n  cholesky = jax.vmap(jax.scipy.linalg.cholesky, in_axes=(0, None))\n  solve_tri = jax.vmap(jax.scipy.linalg.solve_triangular, in_axes=(0, 0, None, None))\n\n  L = cholesky(k_y, True)\n  result2 = solve_tri(L, solve_tri(L, y, 0, True), 1, True)"
        ],
        "link": "https://stackoverflow.com/questions/76458629/how-to-vmap-over-cho-solve-and-cho-factor"
    },
    {
        "title": "Use JAX to convert TIFF images to JPG?",
        "question": "I need to convert 3 million images of size 4 MB from TIFF to JPG. Could I use JAX?\nThe reason I ask is that I have an NVIDIA DGX Cloud environment with JAX set up. I'd like to do this conversion task in this environment because I'm hoping it could accelerate the process vs using a CPU-based approach. I am also new to JAX and DGX, so I wonder if this could be a learning opportunity.\nResearch Done\nI looked at the jax.image module and pix. These offer functions related to matrix manipulation (e.g. sizing, deforming, etc.).\nI also see from the documentation that\nJAX is a language for expressing and composing transformations of numerical programs. JAX is also able to compile numerical programs for CPU or accelerators (GPU/TPU). JAX works great for many numerical and scientific programs, but only if they are written with certain constraints....\nIt would therefore not surprise me that JAX does not (easily) support image type conversion. I wanted to ask the question in case there was something I'm missing.",
        "answers": [
            "No, it is not possible to perform JPEG compression in JAX."
        ],
        "link": "https://stackoverflow.com/questions/76446550/use-jax-to-convert-tiff-images-to-jpg"
    },
    {
        "title": "How to rewrite this JAX snippet to prevent TypeError: unhashable type: 'DynamicJaxprTracer'?",
        "question": "I am rewriting some Python code in JAX with the goal of speeding it up using jax.jit. There is a part of my old code where I have a dict that maps some integer to a list of functions. Every integer has a unique list of functions assigned to it. The functions are all JAX compatible, but the problem is my use case. I have a master function whose arguments are the number to be fed into these functions and the integer that tells me which list of functions to grab from the dict.\nHere is a simple example to demonstrate what I mean:\nfrom jax import jit\n\n# set up a dict that maps integers to a list of functions\ntest_dict = {1122997037:[lambda x: x**2, lambda x: 2*x],\n             1124279607:[lambda x: x**3, lambda x: 3*x]}\n             \n@jit\ndef evaluate_functions(xval,index):\n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n    \n    # given the input index, pull the relevant list of functions from test_dict\n    func1, func2 = test_dict[index]\n    \n    # evaluate both functions at the input xval and return values\n    return func1(xval), func2(xval)\n\n\nprint(evaluate_functions(2,1122997037))\nprint(evaluate_functions(2,1124279607))\nIf you remove the @jit decorator, it works as expected and prints (4,4) and (8,6). But with the @jit decorator, it raises the following error:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[60], line 16\n     11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n---> 16 print(evaluate_functions(2,1))\n     17 print(evaluate_functions(2,2))\n\n    [... skipping hidden 12 frame]\n\nCell In[60], line 11, in evaluate_functions(xval, index)\n      7 @jit\n      8 def evaluate_functions(xval,index):\n      9     \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n---> 11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n\nTypeError: unhashable type: 'DynamicJaxprTracer'\nIs there any way to rewrite this to make it work with @jit? For example using jax.lax.switch?",
        "answers": [
            "The problem with this is that you can't do conditional code execution in JAX based on input data (Edit: I was mistaken about the semantics of lax.cond). You could mark the index argument as static, which means that JAX will recompile the function each time a new value of it is used:\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef evaluate_functions(xval,index):\n    ...\nThis is the sort of compilation behavior you'll see:\nevaluate_functions(0, 0) # JIT runs, may be slow\nevaluate_functions(1, 0) # No compilation, fast \nevaluate_functions(1, 3) # JIT runs again\nevaluate_functions(7, 3) # No compilation\n# etc\nThis is a good solution if you'll be using each of the functions many times. If there's a large number of functions and you'll only be using each one once this won't help you.\nEdit: Here's a solution using jax.lax.switch, I'm not sure how the performance will be:\nkeys, branches = zip(*test_dict.items())    \nbranches1, branches2 = zip(*branches)    \n    \n@jit    \ndef evaluate_functions(xval,index):    \n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\"·    \n    ind_to_key = jnp.asarray(keys)    \n    \n    ind = jnp.where(ind_to_key == index, size=1)[0][0]    \n    out1 = jax.lax.switch(ind, branches1, xval)                                                                               \n    out2 = jax.lax.switch(ind, branches2, xval)                                                                               \n    return out1, out2 "
        ],
        "link": "https://stackoverflow.com/questions/76418151/how-to-rewrite-this-jax-snippet-to-prevent-typeerror-unhashable-type-dynamicj"
    },
    {
        "title": "data_format in JAX/FLAX",
        "question": "I did not find any settings for data_format=channels_first or data_format=channels_last in FLAX modules ( which are based on JAX ).\nOn the contrary, TensorFlow does have that designation. Does the choice of data_format is irrelevant to JAX performance ?\nUnfortunately, I did not find any kind of information on this subject.",
        "answers": [
            "JAX/Flax has no equivalent to the concept of data_format as used in Tensorflow/Keras."
        ],
        "link": "https://stackoverflow.com/questions/76413480/data-format-in-jax-flax"
    },
    {
        "title": "JAX code for minimizing Lennard-Jones potential for 2 points in Python gives unexpected results",
        "question": "I am trying to practice using JAX fo optimization problem and I am trying to do a simple problem, which is to minimize Lennard-Jones potential for just 2 points and I set both epsilon and sigma in Lennard-Jones potential equal 1, so the potential is just: F = 4(1/r^12-1/r^6) and r is the distance between the two points. And the result should be r = 2^(1/6), which is approximately 1.12.\nUsing JAX, I wrote following code, which is pretty simple and short, my initial guess values for two points are [0,1], which I think it is reasonable(because for Lennard-Jones potential it could be a problem because it approach infinite if r guess is too small). As I mentioned, I am expecting a value of r around 1.12 after the minimization, however, the result I get is [-0.71276042 1.71276042], so the distance is 2.4, which is clearly too big and I am wondering how can I fix it. I original doubt it might be the precision so I change the data type to float64, but the results are still the same. Any help will be greatly appreciated! Here is my code\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\nfrom jax import vmap\nimport matplotlib.pyplot as plt\n\nN = 2\njax.config.update(\"jax_enable_x64\", True)\nx_init = jnp.arange(N, dtype=jnp.float64)\nepsilon = 1\nsigma = 1\n\ndef potential(r):\n    r = jnp.where(r == 0, jnp.finfo(jnp.float64).eps, r)\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\ndef F(x):\n    # Compute all pairwise distances\n    r = jnp.abs(x[:, None] - x[None, :])\n    # Compute all pairwise potentials\n    pot = vmap(vmap(potential))(r)\n    # Exclude the diagonal (distance = 0) and avoid double-counting by taking upper triangular part\n    pot = jnp.triu(pot, 1)\n    # Sum up all the potentials\n    total = jnp.sum(pot)\n    return total\n\n# Minimize the function\nprint(F)\nresult = minimize(F, x_init, method='BFGS')\n\n# Extract the optimized positions of the points\nx_solutions = result.x\nprint(x_solutions)",
        "answers": [
            "This function is one that would be very difficult for any unconstrained gradient-based optimizer to correctly optimize. Holding one point at zero and varying the other point on the range (0, 10], we see the potential looks like this:\nr = jnp.linspace(0.1, 5.0, 1000)\nplt.plot(r, jax.vmap(lambda ri: F(jnp.array([0, ri])))(r))\nplt.ylim(-2, 10)\nTo the left of the minimum, the gradient quickly diverges to negative infinity, meaning for nearly any reasonable step size, the optimizer will likely overshoot the minimum. Then on the right side, if the optimizer goes even a few units too far, the gradient tends to zero, meaning for nearly any reasonable step size, the optimizer will get stuck in a regime where the potential has almost no variation.\nAdd to this the fact that you've set up the model with two degrees of freedom in a degenerate potential, and it's not surprising that gradient-based optimization methods are failing.\nYou can make some progress here by minimizing the log of the shifted potential, which has the effect of smoothing the steep gradients, and lets the BFGS minimizer find an expected minimum:\nresult = minimize(lambda x: jnp.log(2 + F(x)), x_init, method='BFGS')\nprint(result.x)\n# [-0.06123102  1.06123102]\nBut in general my suggestion would probably be to opt for a constrained optimization approach instead, perhaps one of the JAXOpt constrained optimization methods, where you can rule-out problematic regions of the parameter space."
        ],
        "link": "https://stackoverflow.com/questions/76353392/jax-code-for-minimizing-lennard-jones-potential-for-2-points-in-python-gives-une"
    },
    {
        "title": "How can I implement a vmappable sum over a dynamic range in Jax?",
        "question": "I want to implement something like the following Python function in Jax, and wrap it with a call to vmap. I want it to be fully reverse-mode differentiable (with respect to x) using grad(), even after the vmap.\ndef f(x,kmax):\n  return sum ([x**k for k in range(1,kmax+1)])\n(This is a deliberately simplified version of the function; I realize in this case I could use the closed-form expression for the geometric series; sadly the actual function I'm trying to implement does not have a closed-form sum that I'm aware of.)\nIs there any way to do this? It seems like there has to be; but fori_loop is not reverse-mode differentiable if kmax is dynamic, jax.lax.scan needs a statically-shaped array or it will throw ConcretizationTypeErrors, and similarly Python primitives like range (as used above) throw TracerIntegerConversionError if wrapped in vmap.\nI think I understand the restrictions on needing arrays to be fixed-shape, but every autodiff framework I've ever used allows you to construct arbitrarily-sized expressions dynamically somehow. A sum over a varying integer range is a pretty basic mathematical tool. How does one implement this in Jax?\nEDITED to refocus the problem definition (the issue is more vmap than grad) and provide the following examples.\nThis, specifically, is what I'd like to be able to do\nimport jax\n\ndef f(x,kmax):\n  return sum ([x**k for k in range(1,kmax+1)])\n\nfmap = jax.vmap(f,in_axes=(None,-1))\n\nx = 3.\nkmaxes = jax.numpy.array([1,2,3])\n\nprint(fmap(x,kmaxes))\n\nfmap_sum = lambda k,kmaxes:jax.numpy.sum(fmap(k,kmaxes))\n\nprint(fmap_sum(x,kmaxes))\nprint(jax.grad(fmap_sum)(x,kmaxes))\nThis throws a TracerIntegerConversionError at range(1,kmax+1). What I would like it to be doing is something like this:\nimport jax\n\ndef f(x,kmax):\n  return sum ([x**k for k in range(1,kmax+1)])\n\ndef fmap(x,kmaxes):\n  return [f(x,kmax) for kmax in kmaxes]\n\nx = 3.\nkmaxes = jax.numpy.array([1,2,3])\n\nprint(fmap(x,kmaxes))\n\ndef fmap_sum(x,kmaxes):\n  return sum(fmap(x,kmaxes))\n\nprint(fmap_sum(x,kmaxes))\nprint(jax.grad(fmap_sum)(x,kmaxes))\nwhich gives the correct result (but loses the parallelization and acceleration of vmap).",
        "answers": [
            "First, to make your function compatible with vmap, you'll need to replace the Python control flow with jax.lax control flow operations. In this case, lax.fori_loop seems applicable:\ndef f1(x, k):\n  def body_fun(i, val):\n    return val + x ** i\n  return jax.lax.fori_loop(1, k + 1, body_fun, jnp.zeros_like(x))\n\nf1map = jax.vmap(f1, (None, 0))\nprint(f1map(x, kmaxes))\n# [ 3. 12. 39.]\nBut because the size of the loop is dynamic, this is not compatible with reverse-mode autodiff:\njax.jacrev(f1map)(x, kmaxes)\n# ValueError: Reverse-mode differentiation does not work for lax.while_loop or lax.fori_loop. Try using lax.scan instead.\nTo get around this, you can modify your function such that it uses a static loop size. Here's one way you might do that:\ndef f2(x, k, kmax):  # kmax should be static\n  def body_fun(i, val):\n    return val + jnp.where(i <= k, x ** i, 0)\n  return jax.lax.fori_loop(1, kmax + 1, body_fun, jnp.zeros_like(x))\n\nf2map = jax.vmap(f2, (None, 0, None))\n\nprint(f2map(x, kmaxes, kmaxes.max()))  # compatible with vmap\n# [ 3. 12. 39.]\n\nprint(jax.jacrev(f2map)(x, kmaxes, kmaxes.max()))  # and with reverse-mode autodiff\n# [ 1.  7. 34.]"
        ],
        "link": "https://stackoverflow.com/questions/76334231/how-can-i-implement-a-vmappable-sum-over-a-dynamic-range-in-jax"
    },
    {
        "title": "How Jax use LAX-backend implementation of functions",
        "question": "I need to compute the kron procuts of two arrays and I want to test if doing it using Jax is faster than doing it using Numpy.\nNow, in numpy my code there is res = numpy.kron(x1,x2), in Jax there is jax.numpy.kron(x1,x2) but how can I use it properly? My doubs are:\nis it sufficient to replace numpy with jax.numpy as follows: res = jax.numpy.kron(x1,x2)?\nshould I first sent x1 and x2 to the device using x1_dev = jax.device_put(x1) and after that run res = jax.numpy.kron(x1_dev,x2_dev)?\nshould I add jax.block_until_ready() to the jax.numpy.kron() call?",
        "answers": [
            "This is covered in JAX's FAQ under Benchmarking JAX Code.\nIn particular, if you're interested in the speed of JAX vs NumPy I would read the second section of this, Is JAX Faster Than Numpy? which gives a broad overview of when you should expect JAX to be faster or slower than equivalent NumPy code.\nAs for benchmarking kron: following the advice there, I would benchmark them like this (using IPython's %timeit for convenience). I ran the following on a Colab T4 GPU runtime:\nimport numpy as np\nx1 = np.random.rand(1000)\nx2 = np.random.rand(1000)\n%timeit np.kron(x1, x2)\n6.52 ms ± 580 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nimport jax.numpy as jnp\nx1_jax = jnp.array(x1)\nx2_jax = jnp.array(x2)\n%timeit jnp.kron(x1_jax, x2_jax).block_until_ready()\n1.39 ms ± 2.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nIf you want to see the effect of just-in-time compilation with jax.jit, you can do something like this:\nimport jax\njit_kron = jax.jit(jnp.kron)\n_ = jit_kron(x1_jax, x2_jax)  # trigger compilation before timing\n%timeit jit_kron(x1_jax, x2_jax).block_until_ready()\n116 µs ± 33.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
        ],
        "link": "https://stackoverflow.com/questions/76322383/how-jax-use-lax-backend-implementation-of-functions"
    },
    {
        "title": "jax.numpy.delete assume_unique_indices unexpected keyword argument",
        "question": "I can not seem to get the assume_unique_indices from jax.numpy working. According to the documentation here, the jnp.delete has a keyword argument \"assume_unique_indices\" that is supposed to make this function jit compatible when we are sure that the index array is an integer array and is guaranteed to contain unique entries.\nHere is an minimum reproducible example\nimport jax\n\narr = jnp.array([1, 2, 3, 4, 5])\nidx = jnp.array([0, 2, 4])\n\nprint(jax.__version__)\n\n# Delete elements at indices idx\nout = jax.numpy.delete(arr, idx, assume_unique_indices=True)\n\nprint(out) # [2 4]\nThe error message\n0.4.8\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-bf0277118922> in <cell line: 9>()\n      7 \n      8 # Delete elements at indices idx\n----> 9 out = jax.numpy.delete(arr, idx, assume_unique_indices=False)\n     10 \n     11 print(out) # [2 4]\n\nTypeError: delete() got an unexpected keyword argument 'assume_unique_indices'\nDeleting the assume_unique_indices made it work as expected.",
        "answers": [
            "assume_unique_indices was added in https://github.com/google/jax/pull/15671, after JAX version 0.4.8 was released. If you update to version 0.4.9 or newer, your code should work.",
            "Ok, as it turns out, the 'assume_unique_indices' is only added rather recently, updating to jax version 0.4.10 did the trick"
        ],
        "link": "https://stackoverflow.com/questions/76244047/jax-numpy-delete-assume-unique-indices-unexpected-keyword-argument"
    },
    {
        "title": "Building a Neural Network using JAX",
        "question": "I tried to build a Neural network from scratch using JAX numpy moldule. In the training phase it seems that the accuracy of the model doesn't improve at all. Here is the code.\nimport jax\nimport jax.numpy as jnp\n\ndef init_params():\n    key = jax.random.PRNGKey(0)\n    W1 = jax.random.uniform(key, (10, 784), minval=-0.5, maxval=0.5)\n    b1 = jax.random.uniform(key, (10, 1), minval=-0.5, maxval=0.5)\n    W2 = jax.random.uniform(key, (10, 10), minval=-0.5, maxval=0.5)\n    b2 = jax.random.uniform(key, (10, 1), minval=-0.5, maxval=0.5)\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return jnp.maximum(Z, 0)\n\ndef softmax(Z):\n    A = jnp.exp(Z) / jnp.sum(jnp.exp(Z))\n    return A\n\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = jnp.dot(W1, X) + b1\n    A1 = ReLU(Z1)\n    Z2 = jnp.dot(W2, A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef ReLU_deriv(Z):\n    return Z > 0\n\ndef one_hot(Y):\n    one_hot_Y = jnp.zeros((Y.size, Y.max() + 1))\n    one_hot_Y = one_hot_Y.at[jnp.arange(Y.size), Y].set(1)\n    return one_hot_Y.T\n\n\ndef backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * jnp.dot(dZ2, A1.T)\n    db2 = 1 / m * jnp.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = jnp.dot(W2.T, dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 / m * jnp.dot(dZ1, X.T)\n    db1 = 1 / m * jnp.sum(dZ1, axis=1, keepdims=True)\n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2\n\n\ndef get_predictions(A2):\n    return jnp.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return jnp.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2\n\n\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 1e-3, 500)\nI'm a complete newbie to JAX and I cannot solve this. What is the issue and how to solve this ?\nI tried to change the weight updating function as it seems to have some problem with updating the model weights",
        "answers": [
            "In your softmax function, you need to specify the axis parameter in the jnp.sum function to sum along the correct axis. Here's how:\ndef softmax(Z):\n    A = jnp.exp(Z) / jnp.sum(jnp.exp(Z), axis=0)\n    return A\nBy specifying axis=0, you ensure that the exponential values are summed across the correct axis."
        ],
        "link": "https://stackoverflow.com/questions/76236539/building-a-neural-network-using-jax"
    },
    {
        "title": "Using JAX and Vectorizing a Function",
        "question": "Here is my code that returns a value if you give it a probability between 0 and 1 (it is an inverseCDF function).\nimport jax.numpy as jnp\nfrom jax import jit, vmap, lax\nfrom jaxopt import Bisection\n\ndef find_y(M, a1, a2, a3):\n    \"\"\"\n    Finds the value of y that corresponds to a given value of M(y), using the bisection method implemented with JAX.\n\n    Parameters:\n    M (float): The desired value of M(y).\n    a1 (float): The value of coefficient a1.\n    a2 (float): The value of coefficient a2.\n    a3 (float): The value of coefficient a3.\n\n    Returns:\n    float: The value of y that corresponds to the given value of M(y).\n    \"\"\"\n    # Define a function that returns the value of M(y) for a given y\n    @jit\n    def M_fn(y):\n        eps = 1e-8  # A small epsilon to avoid taking the log of a negative number\n        return a1 + a2 * jnp.log(y / (1 - y + eps)) + a3 * (y - 0.5) * jnp.log(y / (1 - y + eps))\n\n    # Define a function that returns the difference between M(y) and M\n    @jit\n    def f(y):\n        return M_fn(y) - M\n\n    # Set the bracketing interval for the root-finding function\n    interval = (1e-7, 1 - 1e-7)\n\n    # Use the bisection function to find the root\n    y = Bisection(f, *interval).run().params\n\n    # Return the value of y\n    return y\n\n## test the algorithm\na1 = 16.\na2 = 3.396\na3 = 0.0\n\ny = find_y(16, a1, a2, a3)\nprint(y)\nI would like to pass an array for argument M instead of a scalar, but no matter what I try, I get an error (usually about some Boolean trace). Any ideas? Thanks!!",
        "answers": [
            "You can do this with jax.vmap, as long as you set check_bracket=False in Bisection (see here):\ny = Bisection(f, *interval, check_bracket=False).run().params\nWith that change to your function, you can pass a vector of values for M like this:\nimport jax\nM = jnp.array([4, 8, 16, 32])\nresult = jax.vmap(find_y, in_axes=(0, None, None, None))(M, a1, a2, a3)\nprint(result)\n[0.02837202 0.08661279 0.5        0.99108815]"
        ],
        "link": "https://stackoverflow.com/questions/76231302/using-jax-and-vectorizing-a-function"
    },
    {
        "title": "CuDNN error when running JAX on GPU with apptainer",
        "question": "I have an application written in Python 3.10+ with JAX that I would like to run on GPU. I can run containers on my local computer cluster using apptainer (but not Docker) which has an NVIDIA A40 GPU. Based on the proposed Dockerfile for JAX I made an Ubuntu-based image from the following Dockerfile:\nFROM nvidia/cuda:11.8.0-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI then convert the Docker image to an apptainer image using apptainer pull docker://my-image and then run the container using apptainer run --nv docker://my-image as described in the apptainer GPU docs.\nError\nWhen I run the following code\nimport jax\njax.numpy.array(1.)\nJAX immediately crashes with the following error message:\n2023-05-11 14:41:50.580441: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1993, in array\n    out_array: Array = lax_internal._convert_element_type(out, dtype, weak_type=weak_type)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\", line 537, in _convert_element_type\n    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 360, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 363, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 817, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 117, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 253, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 246, in cached\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 208, in xla_primitive_callable\n    compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 254, in _xla_callable_uncached\n    return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 2816, in compile\n    self._executable = UnloadedMeshExecutable.from_hlo(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 3028, in from_hlo\n    xla_executable = dispatch.compile_or_get_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 526, in compile_or_get_cached\n    return backend_compile(backend, serialized_computation, compile_options,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 471, in backend_compile\n    return backend.compile(built_c, compile_options=options)\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I've tried\nBased on a github thread with a similar error message (https://github.com/google/jax/issues/4920), I have tried to add some CUDA paths:\nexport PATH=/usr/local/cuda-11/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-11/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\nHowever, this did not resolve my problem.\nLocal docker container works without gpu\nWhen I test the image built from the Dockerfile on my local machine without GPU, everything works fine:\n$ docker run -ti my-image python3 -c 'import jax; jax.numpy.array(1.)'\n$\nApptainer container detects GPU's\nI can confirm that the GPU's are detected in the apptainer container. I get the following output when I run nvidia-smi:\n$ apptainer run --nv docker://my-image nvidia-smi\nINFO:    Using cached SIF image\n\n==========\n== CUDA ==\n==========\n\nCUDA Version 11.8.0\n\nContainer image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nThu May 11 14:34:18 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A40          On   | 00000000:00:08.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A40          On   | 00000000:00:09.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A40          On   | 00000000:00:0A.0 Off |                    0 |\n|  0%   30C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A40          On   | 00000000:00:0B.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   4  NVIDIA A40          On   | 00000000:00:0C.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   5  NVIDIA A40          On   | 00000000:00:0D.0 Off |                    0 |\n|  0%   32C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   6  NVIDIA A40          On   | 00000000:00:0E.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   7  NVIDIA A40          On   | 00000000:00:0F.0 Off |                    0 |\n|  0%   30C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nEdit\nCuda 11 image with CuDNN 8.7 gives different error\nWhen I use a different base image with cudnn 8.7.0:\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI get a different error:\n$ apptainer run --nv docker://my-image python3 -c 'import jax; jax.numpy.array(1.)'\n\nCould not load library libcudnn_ops_infer.so.8. Error: libnvrtc.so: cannot open shared object file: No such file or directory\nAborted (core dumped)",
        "answers": [
            "Based on the pointers of jakevdp I managed to find a solution. What was needed was:\nThe CUDA 11 image with CuDNN 8.7.\nThe devel instead of the runtime image.\nTogether, I could succesfully run JAX on apptainer with the following Dockerfile:\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/76229252/cudnn-error-when-running-jax-on-gpu-with-apptainer"
    },
    {
        "title": "The kernel dies with jax.random.PGRNKey",
        "question": "Kernel dies\nThe kernel dies with jax.random.PGRNKey. python version is 3.10.10. jax version is 0.4.9. jaxlib version is 0.4.9. It was run on M2 MacBook using Jupyter lab. What should I do to make it work?\nI upgraded the version of packages, and it didn't work. I have no idea on why it keep crashing.",
        "answers": [
            "It looks like the jaxlib 0.4.9 release is broken on Mac ARM (see JAX Issue #15951). For now I'd recommend installing jax/jaxlib version 0.4.8 to fix the issue.\nUpdate: jax & jaxlib 0.4.10 have been released, and should fix this issue."
        ],
        "link": "https://stackoverflow.com/questions/76224371/the-kernel-dies-with-jax-random-pgrnkey"
    },
    {
        "title": "Local Jax variable not updating in `jit`ted function, but updating in standard?",
        "question": "So, I've got some code and I could really use help deciphering the behavior and how to get it to do what I want.\nSee my code as follows:\nfrom typing import Callable, List\n\nimport chex\nimport jax.numpy as jnp\nimport jax\n\nWeights = List[jnp.ndarray]\n\n\n@chex.dataclass(frozen=True)\nclass Model:\n    mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    jitted_mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    weight_updater: Callable[\n        [jnp.ndarray], None\n    ]\n\n\ndef create_weight():\n    return jnp.ones((2, 5))\n\n\ndef wrapper():\n    weights = create_weight()\n\n    def mult(input_var):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var):\n        return weights.dot(input_var)\n\n    def update_locally_created(new_weights):\n        nonlocal weights\n        weights = new_weights\n        return weights\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=update_locally_created\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult)))\n        print(jnp.sum(tester.jitted_mult(to_mult)))\n\n        if i % 2 == 0:\n            tester.weight_updater(jnp.zeros((2, 5)))\n        else:\n            tester.weight_updater(jnp.ones((2, 5)))\n\n        print(\"*\" * 10)\nTL;DR I'm defining some \"weights\" within a function closure, and I'm trying to modify the weights via a nonlocal. The problem seems to be that the jit-ted version (jitted_mult of the function doesn't recognize the \"updated\" weights, whereas the non-jit function (mult) does.\nWhat can I do to make it recognize the update? I think that I might be able to do what Build your own Haiku does, but that seems like a lot of work for an experiment",
        "answers": [
            "This is working as expected: the reason it's not respecting the update is because your function is not pure (see JAX Sharp Bits: Pure Functions). In your case, the function is not pure because the output depends on an input that is not explicitly passed to the function. This violates the assumptions made by jit and other JAX transformations, which leads to unexpected behavior.\nTo fix it I would make this implicit input explicit, so that your function is pure. It might look something like this:\ndef wrapper():\n    def mult(input_var, weights):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var, weights):\n        return weights.dot(input_var)\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=None\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    weights = create_weight()\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult, weights)))\n        print(jnp.sum(tester.jitted_mult(to_mult, weights)))\n\n        if i % 2 == 0:\n            weights = jnp.zeros((2, 5))\n        else:\n            weights = jnp.ones((2, 5))\n\n        print(\"*\" * 10)"
        ],
        "link": "https://stackoverflow.com/questions/76205477/local-jax-variable-not-updating-in-jitted-function-but-updating-in-standard"
    },
    {
        "title": "How to unroll the training loop so that Jax can train multiple steps in GPU/TPU",
        "question": "When using powerful hardware, especially TPU, it is often preferable to train multiple steps. For example, in TensorFlow, this is possible.\nwith strategy.scope():\n  model = create_model()\n  optimizer_inner = AdamW(weight_decay=1e-6)\n  optimizer_middle = SWA(optimizer_inner)\n  optimizer = Lookahead(optimizer_middle)\n  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n  training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n      'training_accuracy', dtype=tf.float32)\n\n# Calculate per replica batch size, and distribute the `tf.data.Dataset`s\n# on each TPU worker.\nactual_batch_size = 128\ngradient_accumulation_step = 1\nbatch_size = actual_batch_size * gradient_accumulation_step\nsteps_per_epoch = 60000 // batch_size\nvalidation_steps = 10000 // batch_size\n\ntrain_dataset = get_dataset(batch_size, is_training=True)\nper_replica_batch_size = batch_size // strategy.num_replicas_in_sync\n\ntrain_dataset = strategy.experimental_distribute_datasets_from_function(\n    lambda _: get_dataset(per_replica_batch_size, is_training=True))\n\n@tf.function(jit_compile=True)\ndef train_multiple_steps(iterator, steps):\n  \"\"\"The step function for one training step.\"\"\"\n\n  def step_fn(inputs):\n    \"\"\"The computation to run on each TPU device.\"\"\"\n    images, labels = inputs\n    with tf.GradientTape() as tape:\n      logits = model(images, training=True)\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          labels, logits, from_logits=True)\n      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\n    grads = tape.gradient(loss, model.trainable_variables)\n\n    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n    training_loss.update_state(loss * strategy.num_replicas_in_sync)\n    training_accuracy.update_state(labels, logits)\n\n  for _ in tf.range(steps):\n    strategy.run(step_fn, args=(next(iterator),))\n\ntrain_iterator = iter(train_dataset)\n# Convert `steps_per_epoch` to `tf.Tensor` so the `tf.function` won't get\n# retraced if the value changes.\n\nfor epoch in range(10):\n  print('Epoch: {}/10'.format(epoch))\n\n\n  train_multiple_steps(train_iterator, tf.convert_to_tensor(steps_per_epoch))\nIn Jax or Flax, however, I haven't seen a complete working example of doing so. I guess it would be something like\n@jax.jit\ndef train_for_n_steps(train_state, batches):\n    for batch in batches:\n        train_state = train_step_fn(train_state, batch)\n    return train_state\nHowever, in my case when I am trying to test the complete example, I am not sure how one can create multiple batches. Here is a working example using GPU without training multiple steps. The relevant code should probably be here:\nfor step,batch in enumerate(train_ds.as_numpy_iterator()):\n\n  # Run optimization steps over training batches and compute batch metrics\n  state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n  state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n\n  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed\n    for metric,value in state.metrics.compute().items(): # compute metrics\n      metrics_history[f'train_{metric}'].append(value) # record metrics\n    state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n    # Compute metrics on the test set after each training epoch\n    test_state = state\n    for test_batch in test_ds.as_numpy_iterator():\n      test_state = compute_metrics(state=test_state, batch=test_batch)\n\n    for metric,value in test_state.metrics.compute().items():\n      metrics_history[f'test_{metric}'].append(value)\n\n    print(f\"train epoch: {(step+1) // num_steps_per_epoch}, \"\n          f\"loss: {metrics_history['train_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n    print(f\"test epoch: {(step+1) // num_steps_per_epoch}, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")\nMy goal is to unroll 5 loops when training.\nAny suggestions are welcomed.",
        "answers": [
            "You could use more_itertools.chunked to get something like this:\nfor step, five_batches in chunked(train_ds.as_numpy_iterator()):\n    state = five_steps(state, five_batches):\nThen do the unrolling\n@jax.jit\ndef five_steps(state, batches):\n    for batch in batches:\n        state = train_step(state, batch)\n    return state\nThe reason this works is that batches has a length that isn't data dependent, so the loop will just get executed 5 times during tracing.\nThis will likely make jitting take much longer than you want, so the perferred but more difficult way is to pack the batches into [N x batch_size x ...] tensors, then use scan to loop your update function over the inputs."
        ],
        "link": "https://stackoverflow.com/questions/76191996/how-to-unroll-the-training-loop-so-that-jax-can-train-multiple-steps-in-gpu-tpu"
    },
    {
        "title": "No module named 'jax.experimental.global_device_array' when running the official Flax Example on Colab with V100",
        "question": "I have been trying to understand this official flax example, based on a Coalb pro+ account with V100. When I execute the command python main.py --workdir=./imagenet --config=configs/v100_x8.py , the returned error is\nFile \"/content/FlaxImageNet/main.py\", line 29, in <module>\nimport train\nFile \"/content/FlaxImageNet/train.py\", line 30, in <module>\nfrom flax.training import checkpoints\nFile \"/usr/local/lib/python3.10/dist-packages/flax/training/checkpoints.py\", line 34, \nin <module>\nfrom jax.experimental.global_device_array import GlobalDeviceArray\nModuleNotFoundError: No module named 'jax.experimental.global_device_array'\nI am not sure whether global_device_array has been moved from jax.experimental package or it is no longer needed or replaced by other equivalent methods.",
        "answers": [
            "GlobalDeviceArray was deprecated in JAX version 0.4.1 and removed in JAX version 0.4.7.\nWith that in mind, it seems the code in question requires JAX version 0.4.6 or older. You might consider reporting this incompatibility to the flax project: http://github.com/google/flax/."
        ],
        "link": "https://stackoverflow.com/questions/76191911/no-module-named-jax-experimental-global-device-array-when-running-the-official"
    },
    {
        "title": "Jit a JAX function that select a function from a dictionary",
        "question": "I have a JAX function that, given the order and the index, selects a polynomial from a pre-defined dictionary, as follows:\npoly_dict = {\n    (0, 0): lambda x, y, z: 1.,\n    (1, 0): lambda x, y, z: x,\n    (1, 1): lambda x, y, z: y,\n    (1, 2): lambda x, y, z: z,\n    (2, 0): lambda x, y, z: x*x,\n    (2, 1): lambda x, y, z: y*y,\n    (2, 2): lambda x, y, z: z*z,\n    (2, 3): lambda x, y, z: x*y,\n    (2, 4): lambda x, y, z: y*z,\n    (2, 5): lambda x, y, z: z*x\n}\n\ndef poly_func(order: int, index: int):\n    \n    try:\n        return poly_dict[(order, index)]\n    \n    except KeyError:\n        print(\"(order, index) must be a key in poly_dict!\")\n        return\nNow I want to jit poly_func(), but it gives an error TypeError: unhashable type: 'DynamicJaxprTracer' Moreover, if I just do\ndef poly_func(order: int, index: int):\n    \n    return poly_dict[(order, index)]\nit still gives the same error. Is there a way to resolve this issue?",
        "answers": [
            "There are two issues with your approach: first, traced values cannot be used to index into Python collections like dicts or lists. Second, JIT-compiled functions can only return array values, not functions.\nWith that in mind, it is impossible to make the function you propose work with JAX transforms like JIT. But you could modify your approach to use a list rather than a dict of functions, and then use lax.switch to dynamically select which function you call. Here's how it might look:\ndef get_index(order, index):\n  return order * 5 + index\n\npoly_list = 16 * [lambda x, y, z: 0.0]  # place-holder function\n\nfor key, val in poly_dict.items():\n  poly_list[get_index(*key)] = poly_dict[key]\n\n\ndef eval_poly_func(order: int, index: int, args):\n  ind = get_index(order, index)\n  return jax.lax.switch(ind, poly_list, *args)\n\nresult = jax.jit(eval_poly_func)(2, 0, (5.0, 6.0, 7.0))\nprint(result)\n# 25.0"
        ],
        "link": "https://stackoverflow.com/questions/76191616/jit-a-jax-function-that-select-a-function-from-a-dictionary"
    },
    {
        "title": "Fail to understand the usage of partial argument in Flax Resnet Official Example",
        "question": "I have been trying to understand this official example. However, I am very confused about the use of partial in two places.\nFor example, in line 94, we have the following:\nconv = partial(self.conv, use_bias=False, dtype=self.dtype)\nI am not sure why it is possible to apply a partial to a class, and where later in the code we fill in the missing argument (if we need to).\nComing to the final definition, I am even more confused. For example,\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n               block_cls=ResNetBlock)\nWhere do we apply the argument such as stage_size=[2,2,2,2]?\nThank you",
        "answers": [
            "functools.partial will partially evaluate a function, binding arguments to it for when it is called later. here's an example of it being used with a function:\nfrom functools import partial\n\ndef f(x, y, z):\n  print(f\"{x=} {y=} {z=}\")\n\ng = partial(f, 1, z=3)\ng(2)\n# x=1 y=2 z=3\nand here is an example of it being used on a class constructor:\nfrom typing import NamedTuple\n\nclass MyClass(NamedTuple):\n  a: int\n  b: int\n  c: int\n\nmake_class = partial(MyClass, 1, c=3)\nprint(make_class(b=2))\n# MyClass(a=1, b=2, c=3)\nThe use in the flax example is conceptually the same: partial(f) returns a function that when called, applies the bound arguments to the original callable, whether it is a function, a method, or a class constructor.\nFor example, the ResNet18 function created here:\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n                   block_cls=ResNetBlock)\nis a partially-evaluated ResNet constructor, and the function is called in a test here:\n  @parameterized.product(\n      model=(models.ResNet18, models.ResNet18Local)\n  )\n  def test_resnet_18_v1_model(self, model):\n    \"\"\"Tests ResNet18 V1 model definition and output (variables).\"\"\"\n    rng = jax.random.PRNGKey(0)\n    model_def = model(num_classes=2, dtype=jnp.float32)\n    variables = model_def.init(\n        rng, jnp.ones((1, 64, 64, 3), jnp.float32))\n\n    self.assertLen(variables, 2)\n    self.assertLen(variables['params'], 11)\nmodel here is the partially evaluated function ResNet18, and when it is called it returns the fully-instantiated ResNet object with the parameters specified in the ResNet18 partial definition."
        ],
        "link": "https://stackoverflow.com/questions/76178123/fail-to-understand-the-usage-of-partial-argument-in-flax-resnet-official-example"
    },
    {
        "title": "Failing to implement logistic regression using 'equinox' and 'optax' library",
        "question": "I am trying to implement logistic regression using equinox and optax libraries, with the support of JAX. While training the model, the loss is not decreasing over time,and model is not learning. Herewith attaching a reproducible code with toy dataset for reference:\nimport jax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jrandom\nimport equinox as eqx\nimport optax\n\ndata_key,model_key = jax.random.split(jax.random.PRNGKey(0),2)\n\n### Generating toy-data\n\nX_train = jax.random.normal(data_key, (1000,2))\ny_train = X_train[:,0]+X_train[:,1]\ny_train = jnp.where(y_train>0.5,1,0)\n\n### Using equinox and optax\nprint(\"Training using equinox and optax\")\n\nepochs = 10000             \nlearning_rate = 0.1\nn_inputs = X_train.shape[1]\n\nclass Logistic_Regression(eqx.Module):\n    weight: jax.Array\n    bias: jax.Array\n    def __init__(self, in_size, out_size, key):\n        wkey, bkey = jax.random.split(key)\n        self.weight = jax.random.normal(wkey, (out_size, in_size))\n        self.bias = jax.random.normal(bkey, (out_size,))\n        #self.weight = jnp.zeros((out_size, in_size))\n        #self.bias = jnp.zeros((out_size,))\n    def __call__(self, x):\n        return jax.nn.sigmoid(self.weight @ x + self.bias)\n\n@eqx.filter_value_and_grad\ndef loss_fn(model, x, y):\n    pred_y = jax.vmap(model)(x) \n    return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n\n@eqx.filter_jit\ndef make_step(model, x, y, opt_state):\n    loss, grads = loss_fn(model, x, y)\n    updates, opt_state = optim.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    return loss, model, opt_state\n\nin_size, out_size = n_inputs, 1\nmodel = Logistic_Regression(in_size, out_size, key=model_key)\noptim = optax.sgd(learning_rate)\nopt_state = optim.init(model)\nfor epoch in range(epochs):\n    loss, model, opt_state = make_step(model,X_train,y_train, opt_state)\n    loss = loss.item()\n    if (epoch+1)%1000 ==0:\n        print(f\"loss at epoch {epoch+1}:{loss}\")\n\n# The following code is implementation of Logistic regression using scikit-learn and pytorch, and it is working well. It is added just for reference\n\n\n### Using scikit-learn\nprint(\"Training using scikit-learn\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_train)\nprint(\"Train accuracy:\",accuracy_score(y_train,y_pred))\n\n## Using pytorch\nprint(\"Training using pytorch\")\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import SGD\nfrom torch.nn import Sequential\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\",device)\ntorch_LR= Sequential(nn.Linear(n_inputs, 1),\n                nn.Sigmoid())\ntorch_LR.to(device)\ncriterion = nn.BCELoss() # define the optimization\noptimizer = SGD(torch_LR.parameters(), lr=learning_rate)\n\ntrain_loss = []\nfor epoch in range(epochs):\n    inputs, targets = torch.tensor(X_train).to(device), torch.tensor(y_train).to(device) # move the data to GPU if available\n    optimizer.zero_grad() # clear the gradients\n    yhat = torch_LR(inputs.float()) # compute the model output\n    loss = criterion(yhat, targets.unsqueeze(1).float()) # calculate loss\n    #train_loss_batch.append(loss.cpu().detach().numpy()) # store the loss\n    loss.backward() # update model weights\n    optimizer.step()\n    if (epoch+1)%1000 ==0:\n        print(f\"loss at epoch {epoch+1}:{loss.cpu().detach().numpy()}\")\nI tried SGD and adam optmizers with different learning rates, but the result is same. Also, I tried zero weight initialisation and ranodom weight initialisation. For the same data, I tried pytorch and LogisticRegression module from scikit-learn library (I understood in sklearn SGD is not used, but just as a reference to observe performance). Scikit-learn and pytorch modeling is added in the code block for reference. I have tried this with multiple classification datasets but still facing this problem.",
        "answers": [
            "The first time you print your loss is after 1000 epochs. If you change it to print the loss of the first 10 epochs, you see that the optimizer is rapidly converging:\n    # ...\n    if epoch < 10 or (epoch + 1)%1000 ==0:\n        print(f\"loss at epoch {epoch+1}:{loss}\")\nHere is the result:\nTraining using equinox and optax\nloss at epoch 1:1.237254023551941\nloss at epoch 2:1.216030478477478\nloss at epoch 3:1.1952687501907349\nloss at epoch 4:1.174972414970398\nloss at epoch 5:1.1551438570022583\nloss at epoch 6:1.1357849836349487\nloss at epoch 7:1.1168975830078125\nloss at epoch 8:1.098482370376587\nloss at epoch 9:1.0805412530899048\nloss at epoch 10:1.0630732774734497\nloss at epoch 1000:0.6320337057113647\nloss at epoch 2000:0.6320337057113647\nloss at epoch 3000:0.6320337057113647\nBy epoch 1000, the loss has converged to a minimum value from which it does not move.\nGiven this, it looks like your optimizer is functioning correctly.\nEdit: I did some debugging and found that y_pred = jax.vmap(model)(X_train) returns an array of shape (1000, 1), so (y - y_pred) is not a length-1000 array of differences, but rather a shape (1000, 1000) array of pairwise differences between all outputs. The log-loss over these pairwise differences is not a standard logistic regression model."
        ],
        "link": "https://stackoverflow.com/questions/76146548/failing-to-implement-logistic-regression-using-equinox-and-optax-library"
    },
    {
        "title": "High Memory Consumption in JAX with Nested vmap",
        "question": "I'm working on a problem that involves computing the value of many interpolants on a three dimensional grid using jax. Following standard jax practice, I wrote everything for \"single-batch\" inputs and then vmap over all interpolants and evaluation grid points in the end. The code below is a reduced, non-sensical version of this.\nfrom collections import namedtuple\nfrom functools import partial\n\nimport jax.numpy as jnp\nfrom jax import vmap\nfrom jax.lax import dynamic_slice, stop_gradient\n\ninterpolation_params = namedtuple(\"interpolation_params\", [\"a\", \"dx\", \"f\", \"lb\", \"ub\"])\n\n\n@partial(vmap, in_axes=(None, None, 0))\ndef init_1d_interpolation_params(a, dx, f):\n    f = jnp.pad(f, 1)\n    lb, ub = a, a + (f.shape[0] - 1) * dx\n    return interpolation_params(a=a, dx=dx, f=f, lb=lb, ub=ub)\n\n\n@partial(vmap, in_axes=(None, 0))\ndef eval_interp1d(x, interpolation_params):\n    A = jnp.array([-1.0 / 16, 9.0 / 16, 9.0 / 16, -1.0 / 16])\n    B = jnp.array([1.0 / 24, -9.0 / 8, 9.0 / 8, -1.0 / 24])\n    C = jnp.array([1.0 / 4, -1.0 / 4, -1.0 / 4, 1.0 / 4])\n    D = jnp.array([-1.0 / 6, 1.0 / 2, -1.0 / 2, 1.0 / 6])\n\n    x = (\n        jnp.minimum(jnp.maximum(x, interpolation_params.lb), interpolation_params.ub)\n        - interpolation_params.a\n    )\n    ix = jnp.atleast_1d(jnp.array(x // interpolation_params.dx, int))\n    ratx = x / interpolation_params.dx - (ix + 0.5)\n    asx = A + ratx * (B + ratx * (C + ratx * D))\n    return jnp.dot(dynamic_slice(interpolation_params.f, ix, (4,)), asx)\n\n\n# Init 300 interpolants on a uniform grid with 4096 points\nx = jnp.linspace(0, 1, 4096)\nf = x**2\nff = jnp.repeat(f.reshape(1, -1), 300, axis=0)\n\nparams = init_1d_interpolation_params(x[0], x[1] - x[0], ff)\n\n\n@partial(vmap, in_axes=(0, None))\ndef foo(x, interpolation_params):\n    g_x = (eval_interp1d(x, interpolation_params)) ** 2\n    return jnp.sum(g_x)\n\n\nlarge_x_array = stop_gradient(jnp.repeat(jnp.array([0.0]), 100**3))\nfoo(large_x_array, params)\nNow, if I run the code below, I end up with very large memory foot prints (14GB) which is a little puzzling to me. Initially, I thought the issue is the computation graph tracing of jax's autodiff backend, the size of which should be naively comparable to the \"cartesian product\" of params and large_x_array. However, using stop_gradient to turn off the graph tracing didn't help, so I'm not exactly sure what's going on and how to fix this elegantly. Any thoughts on this?",
        "answers": [
            "Your code is essentially a doubly-nested vmap over axes of shape 300 and 1000000 respectively. This means that the effective memory footprint of a fully-vmapped float32 scalar in your inner function is 300 * 1000000 * 4 bytes, or just over 1GB. Your inner function constructs fully-mapped arrays of length 4, which take up about 4GB – with that in mind, its not surprising that your full function would require allocating a few times that amount of memory.\nFor what it's worth, if you want to see the array sizes implied by your outer function, one way to do that is to construct the jaxpr representing your end-to-end operation:\nimport jax\nprint(jax.make_jaxpr(foo)(large_x_array, params))\nThe output is long, so I won't paste it in full here, but in the jaxpr you see direct evidence of what I said above, for example:\n...\n    cf:f32[1000000,300,4] = add ce cc\n    cg:f32[1000000,300,4] = mul bv cf\n...\nThese are the intermediate arrays of size 1000000x300x4x4 bytes (or just over 4 GB) which are allocated in the course of executing your code.\nIf you want to reduce memory consumption, you could do so by serializing some of the computations using scan or fori_loop in place of vmap, in order to avoid allocating the full 4GB intermediate arrays."
        ],
        "link": "https://stackoverflow.com/questions/76109349/high-memory-consumption-in-jax-with-nested-vmap"
    },
    {
        "title": "Calling an initialized function from a list inside a jitted JAX function",
        "question": "Given is a jitted function, which is calling another function that maps over a batch, which again calls a function, i.e. inner_function, to compute a certain property. Also given is a list of initialized functions intialized_functions_dic, from which we want to call the proper initialized function based on some information passed as argument, e.g. info_1. Is there a way to make this work? Thanks in advance.\ninitialized_functions_dic = {1:init_function1, 2:init_function_2, 3:init_function_3}\n\n\ndef inner_function(info_1, info_2, info_3):\n    return 5 + outside_dic[info_1]\nCalling outside_dic[info_1] will throw an error due to trying to access a dictionary with a traced value.\nTrying to pass info_1 as static_argnums also fails due to info_1 being an unhashable type 'ArrayImpl'.",
        "answers": [
            "It sounds like you're looking for jax.lax.switch, which will switch between entries in a list of functions given an index:\ninitialized_functions = [init_function_1, init_function_2, init_function_3]\n\ndef inner_function(info_1, info_2, info_3):\n    idx = info_1 - 1  # lists are zero-indexed\n    args = (info_2, info_3) # tuple of arguments to pass to the function\n    return 5 + lax.switch(idx, initialized_functions, *args)"
        ],
        "link": "https://stackoverflow.com/questions/76094143/calling-an-initialized-function-from-a-list-inside-a-jitted-jax-function"
    },
    {
        "title": "How to write tensorboard events files without installing / importing TF or PyTorch?",
        "question": "Obviously events-file logging is included with TensorFlow and apparently there's an implementation included with PyTorch, but is there an officially supported standalone implementation of something like SummaryWriter for use outside of these two frameworks (e.g. if one is using JAX and doesn't want to also install/import TensorFlow or PyTorch)?\nThe flax library apparently has flax.metrics.tensorboard, but it just imports TensorFlow and implements a SummaryWriter using tf.summary.",
        "answers": [
            "TensorboardX might be helpful: https://tensorboardx.readthedocs.io/en/latest/tensorboard.html#\nIt does not require installing Tensorflow/Pytorch, but does require installation of the above, a much lighter-weight, library."
        ],
        "link": "https://stackoverflow.com/questions/76049789/how-to-write-tensorboard-events-files-without-installing-importing-tf-or-pytor"
    },
    {
        "title": "Is there a way to accept a function while taking the gradient using jax.grad?",
        "question": "I am trying to make a neural network-based differential equation solver for the differential equation y' + 2xy = 0.\nimport jax.numpy as jnp\nimport jax\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\n\ndef softplus(x):\n    return jnp.log(1 + jnp.exp(x))\n\ndef init_params():\n    params = jax.random.normal(key, shape=(241,))\n    return params\n\ndef linear_model(params, x):\n    w0 = params[:80]\n    b0 = params[80:160]\n    w1 = params[160:240]\n    b1 = params[240]\n    h = softplus(x*w0 + b0)\n    o = jnp.sum(h*w1) + b1\n    return o\n\ndef loss(derivative, initial_condition, params, model, x):\n    dfdx = jax.grad(model, 1)\n    dfdx_vect = jax.vmap(dfdx, (None, 0))\n    model_vect = jax.vmap(model, (None, 0))\n    eq_difference = dfdx_vect(params, x) - derivative(x, model(params, x))\n    condition_difference = model(params, 0) - initial_condition\n    return jnp.mean(eq_difference ** 2 - condition_difference ** 2)\n\ndef dfdx(x, y):\n    return -2. * x * y\n\nkey = jax.random.PRNGKey(0)\ninputs = np.linspace(0, 1, num=401)\nparams = init_params()\n\nepochs = 2000\nlearning_rate = 0.0005\n\n# Training Neural Network\n\nfor epoch in tqdm(range(epochs)):\n    grad_loss = jax.grad(loss)\n    gradient = grad_loss(dfdx, 1., params, linear_model, inputs)\n    params -= learning_rate*gradient\n\nmodel_vect = jax.vmap(linear_model, (None, 0))\npreds = model_vect(params, inputs)\n\nplt.plot(inputs, jnp.exp(inputs**2), label='exact')\nplt.plot(inputs, model_vect(params, inputs), label='approx')\nplt.legend()\nplt.show()\nThe issue is that Jax doesn't like taking the gradient of a function that receives another function as an argument:\nTypeError: Argument '<function dfdx at 0x7fce88340af0>' of type <class 'function'> is not a valid JAX type.\nIs there any workaround for this?",
        "answers": [
            "You just orderd arguments wrong. Jax differentiates wrt. first argument, and you don't want to differentiate wrt your function, but rather - parameters. Just make them the first argument.\ndef loss(params, derivative, initial_condition, model, x):\n    dfdx = jax.grad(model, 1)\n    dfdx_vect = jax.vmap(dfdx, (None, 0))\n    model_vect = jax.vmap(model, (None, 0))\n    eq_difference = dfdx_vect(params, x) - derivative(x, model(params, x))\n    condition_difference = model(params, 0) - initial_condition\n    return jnp.mean(eq_difference ** 2 - condition_difference ** 2)"
        ],
        "link": "https://stackoverflow.com/questions/76029743/is-there-a-way-to-accept-a-function-while-taking-the-gradient-using-jax-grad"
    },
    {
        "title": "JAX VMAP Parallelization Details",
        "question": "I was wondering how vmap's internals work. When I vectorize code using jax.lax.map, I know that each element is executed consecutively. However when I use vmap I execute the vectorized operation apparently in parallel. Can someone provide me with a more detailed explanation of how the parallelization works? How does Jax determine the number of parallel processes, and can this behaviour be influenced by the user?\nThanks in advance.",
        "answers": [
            "jax.vmap is a vectorizing/batching transform, not a parallelizing transform. Internally, it converts an unbatched function to a batched function, lowering to efficient primitive calls rather than an explicit map or loop.\nFor example, here is a simple function, where we create both a manually-looped and an automatically vectorized batched version:\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\n\ndef f(x, y):\n  return x @ y\n\nnum_batches = 3\nnum_entries = 5\n\nnp.random.seed(0)\nx = np.random.rand(num_batches, num_entries)\ny = np.random.rand(num_batches, num_entries)\n\n\nf_loop = lambda x, y: jnp.stack([f(xi, yi) for xi, yi in zip(x, y)])\nf_vmap = jax.vmap(f)\n\nprint(f_loop(x, y))\n# [1.3567398 2.1908383 1.6315514]\nprint(f_vmap(x, y))\n# [1.3567398 2.1908383 1.6315514]\nThese return the same results (by design), but they are quite different under the hood; by printing the jaxpr, we see that the loop version requires a long sequence of XLA calls:\nprint(jax.make_jaxpr(f_loop)(x, y))\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] a\n    d:f32[5] = squeeze[dimensions=(0,)] c\n    e:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] b\n    f:f32[5] = squeeze[dimensions=(0,)] e\n    g:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d f\n    h:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] a\n    i:f32[5] = squeeze[dimensions=(0,)] h\n    j:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] b\n    k:f32[5] = squeeze[dimensions=(0,)] j\n    l:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] i k\n    m:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] a\n    n:f32[5] = squeeze[dimensions=(0,)] m\n    o:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] b\n    p:f32[5] = squeeze[dimensions=(0,)] o\n    q:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] n p\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n    s:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] l\n    t:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] q\n    u:f32[3] = concatenate[dimension=0] r s t\n  in (u,) }\nOn the other hand, the vmap version lowers to just a single generalized dot product:\nprint(jax.make_jaxpr(f_vmap)(x, y))\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[3] = dot_general[dimension_numbers=(([1], [1]), ([0], [0]))] a b\n  in (c,) }\nNotice that there is nothing concerning parallelization here; rather we've automatically created an efficient batched version of our operation. The same is true of vmap applied to more complicated functions: it does not involve parallelization, rather it outputs an efficient batched version of your operation in an automated manner."
        ],
        "link": "https://stackoverflow.com/questions/75891826/jax-vmap-parallelization-details"
    },
    {
        "title": "Confused about evaluating vector-Jacobian-product with non-identity vectors (JAX)",
        "question": "I'm confused about the meaning of evaluating vector-Jacobian-products when the vector used for the VJP is a non-identity row vector. My question pertains to vector-valued functions, not scalar functions like loss. I will show a concrete example using Python and JAX but this is a very general question about reverse-mode automatic differentiation.\nConsider this simple vector-valued function for which the Jacobian is trivial to write down analytically:\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\nfrom jax import vjp, jacrev\n\n# Define a vector-valued function (3 inputs --> 2 outputs) \ndef vector_func(args):\n    x,y,z = args\n    a = 2*x**2 + 3*y**2 + 4*z**2\n    b = 4*x*y*z\n    return jnp.array([a, b])\n\n# Define the inputs\nx = 2.0\ny = 3.0\nz = 4.0\n\n# Compute the vector-Jacobian product at the fiducial input point (x,y,z)\nval, func_vjp = vjp(vector_func, (x, y, z))\n\nprint(val) \n# [99,96]\n\n# now evaluate the function returned by vjp along with basis row vectors to pull out gradient of 1st and 2nd output components \nv1 = jnp.array([1.0, 0.0])  # pulls out the gradient of the 1st component wrt the 3 inputs, i.e., first row of Jacobian\nv2 = jnp.array([0.0, 1.0])  # pulls out the gradient of the 1st component wrt the 3 inputs, i.e., second row of Jacobian \n\ngradient1 = func_vjp(v1)\nprint(gradient1)\n# [8, 18, 32]\n\ngradient2 = func_vjp(v2)\nprint(gradient2)\n# [48,32,24]\nThat much makes sense to me -- we're separately feeding [1,0] and [0,1] to vjp_func to respectively get the first and second rows of the Jacobian evaluated at our fiducial point (x,y,z)=(2,3,4).\nBut now what if we fed vjp_func a non-identity row vector like [2,0]? Is this asking how the fiducial (x,y,z) would need to be perturbed to double the first component of the output? If so, is there a way to see this by evaluating vector_func at the perturbed parameter values?\nI tried but I'm not sure:\n# suppose I want to know what perturbations in (x,y,z) cause a doubling of the first output and no change in second output component \nprint(func_vjp(jnp.array([2.0,0.0])))\n# [16,36,64] \n\n### Attempts to use the output of vjp_func to verify that val becomes [99*2, 96]\n### none of these work\n\nprint(vector_func([16,36,64]))\n# [20784, 147456]\n\nprint(vector_func([x*16,y*36,z*64])\n# [299184., 3538944.]\nWhat am I doing wrong in using the output of func_vjp to modify the fiducial parameters (x,y,z) and feed those back into vector_func to verify indeed that those parameter perturbations double the first component of the original output and leave the second component unchanged?",
        "answers": [
            "I think in your question you are confusing primal and tangent vector spaces. The function vector_func is a non-linear function that maps a vector in an input primal vector space (represented by (x, y, z)) to a vector in an output primal vector space (represented by val in your code).\nThe function func_vjp is a linear function that maps a vector in an output tangent vector space (represented by array([2, 0]) in your question) to a vector in an input tangent vector space ([16,36,64] in your question).\nBy construction, the tangent vectors in these transformations represent the gradients of the input function at the specified primal values. That is, if you infinitesimally perturb your output primal along the direction of your output tangent, it corresponds to infinitesimally perturbing the input primal along the direction of the input tangent.\nIf you want to check the values, you could do something like this:\ninput_primal = (x, y, z)\noutput_primal, func_vjp = vjp(vector_func, input_primal)\n\nepsilon = 1E-8  # note: small value so we're near the linear regime\noutput_tangent = epsilon * jnp.array([0.0, 1.0])\ninput_tangent, = func_vjp(output_tangent)\n\n# Compute the perturbed output given the perturbed input\nperturbed_input = [p + t for p, t in zip(input_primal, input_tangent)]\nperturbed_output_1 = vector_func(perturbed_input)\nprint(perturbed_output_1)\n# [99.00001728 96.00003904]\n\n# Perturb the output directly\nperturbed_output_2 = output_primal + output_tangent\nprint(perturbed_output_2)\n# [99.         96.00000001]\nNote that the results don't match exactly, because the VJP is valid in the locally linear limit, and your function is very nonlinear. But hopefully this helps clarify what these primal and tangent values mean in the context of the VJP computation. Mathematically, if we computed this in the limit where epsilon goes to zero, the results would match exactly – gradient computations are all about these kinds of infinitesimal limits."
        ],
        "link": "https://stackoverflow.com/questions/75883427/confused-about-evaluating-vector-jacobian-product-with-non-identity-vectors-jax"
    },
    {
        "title": "JAX performance problems",
        "question": "I am obviously not following best practices, but maybe that's because I don't know what they are. Anyway, my goal is to generate a tubular neighborhood about a curve in three dimensions. A curve is give by an array of length three f(t) = jnp.array([x(t), y(t), z(t)]).\nNow, first we compute the unit tangent:\ndef get_uvec2(f):\n  tanvec = jacfwd(f)\n  return lambda x: tanvec(x)/jnp.linalg.norm(tanvec(x))\nNext, we compute the derivative of the tangent:\ndef get_cvec(f):\n  return get_uvec2(get_uvec2(f))\nThird, we compute the orthogonal frame at a point:\ndef get_frame(f):\n  tt = get_uvec2(f)\n  tt2 = get_cvec(f)\n  def first2(t):\n    x = tt(t)\n    y = tt2(t)\n    tt3 = (jnp.cross(x, y))\n    return jnp.array([x, y, tt3])\n  return first2\nwhich we use to generate a point in the circle around a given point:\ndef get_point(frame, s):\n  v1 = frame[1, :]\n  v2 = frame[2, :]\n  return jnp.cos(s) * v1 + jnp.sin(s) * v2\nAnd now we generate the point on the tubular neighborhood corresponding to a pair of parameters:\ndef get_grid(f, eps):\n  ffunc = get_frame(f)\n  def grid(t, s):\n    base = f(t)\n    frame = ffunc(t)\n    return base + eps * get_point(frame, s)\n  return grid\nAnd finally, we put it all together:\ndef get_reg_grid(f, num1, num2, eps):\n  plist = []\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  for t in tarray:\n    for s in sarray:\n      plist.append(g(t, s))\n  return jnp.vstack(plist)\nFinally, use it to compute the tubular neighborhood around a circle in the xy-plane:\nf1 = lambda x: jnp.array([jnp.cos(2 * jnp.pi * x), jnp.sin(2 * jnp.pi * x), 0.0])\n\nfff = np.array(get_reg_grid(f1, 200, 200, 0.1))\nThe good news is that it all works. The bad news is that this computation takes well over an hour. Where did I go wrong?",
        "answers": [
            "JAX and numpy share one key rule-of-thumb for getting good performance: if you are writing for loops over array values, your code will probably be slow.\nTo make your code more performant, you should replace your loops with vectorized operations. One nice feature of JAX is jax.vmap, a vectorizing transform which makes this relatively easy. You can also use jax.jit to JIT-compile your function and get even faster execution.\nHere's a modified version of your get_reg_grid function that returns the same result with much faster execution:\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['f', 'num1', 'num2'])\ndef get_reg_grid(f, num1, num2, eps):\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  g = jax.vmap(g, in_axes=(None, 0))\n  g = jax.vmap(g, in_axes=(0, None))\n  return jnp.vstack(g(tarray, sarray))\nWith this approach, your code executes in about 300 microseconds:\n%timeit get_reg_grid(f1, 200, 200, 0.1).block_until_ready()\n# 296 µs ± 157 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
        ],
        "link": "https://stackoverflow.com/questions/75872342/jax-performance-problems"
    },
    {
        "title": "How to get value of jaxlib.xla_extension.ArrayImpl",
        "question": "Using type(z1[0]) I get jaxlib.xla_extension.ArrayImpl. Printing z1[0] I get Array(0.71530414, dtype=float32). How can I get the actual number 0.71530414?\nI tried z1[0][0] because z1[0] is a kind of array with a single value, but it gives me an error: IndexError: Too many indices for array: 1 non-None/Ellipsis indices for dim 0..\nI tried also a different approach: I searched on the web if it was possible to convert from jaxnumpy array to a python list, but I didn't find an answer.\nCan someone help me to get the value inside a jaxlib.xla_extension.ArrayImpl object?",
        "answers": [
            "You can use float(x[0]) to convert x[0] to a Python float:\nIn [1]: import jax.numpy as jnp\n\nIn [2]: x = jnp.array([0.71530414])\n\nIn [3]: x\nOut[3]: Array([0.71530414], dtype=float32)\n\nIn [4]: x[0]\nOut[4]: Array(0.71530414, dtype=float32)\n\nIn [5]: float(x[0])\nOut[5]: 0.7153041362762451\nIf you're interested in converting the entire JAX array to a list of Python floats, you can use the tolist() method:\nIn [6]: x.tolist()\nOut[6]: [0.7153041362762451]",
            "You can just use z1[0].item() to get the value. It's the same as, e.g., float(z1[0]), only you don't have to know the type in advance."
        ],
        "link": "https://stackoverflow.com/questions/75867636/how-to-get-value-of-jaxlib-xla-extension-arrayimpl"
    },
    {
        "title": "JAX: unable to jnp.where with known sizes inside a pmap",
        "question": "I wanted to do a pmap of a given function, with 2D arrays that might (or might not) contain nan values. That function must then apply some operations to the finite values that exist in each row (toy examples at the end of the post).\nI know how many points (per row) contain NaNs, even before I /jax.jit/ anything. Thus, I should be able to:\nimport jax.numpy as jnp \ninds = jnp.where(jnp.isfinite(line), size= Finite_points_number) \nbut I am not able to pass the size of the elements into the pmap-ed function.\nI have tried to:\ni) pmap over over the list with the number of good points per row:\ndata_array = jnp.array([\n                   [1,2,3,4], \n                   [4,5,6, jnp.nan]\n                  ]\n                )\n\nsizes = jnp.asarray((4, 3)) # Number of valid points per row\n\ndef jitt_function(line, N):\n    \"\"\"\n    Over-simplified function to showcase the problem\n    \"\"\"\n    inds = jnp.where(jnp.isfinite(line), size=N)\n    return jnp.sum(line[inds])\n\npmap_func = jax.pmap(jitt_function,\n                in_axes=(0, 0)\n         )\n\npmap_func(data_array, sizes)\nand it fails with\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations. The error occurred while tracing the function jitt_function at [...] for pmap. This concrete value was not available in Python because it depends on the value of the argument 'N'.\nii) I have also tried to turn the number of points (N) into a static argument:\njitt_function = jax.jit(jitt_function, static_argnames=(\"N\",))\n\npmap_func = jax.pmap(jitt_function,\n                in_axes=(0, 0)\n         )\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 1) of type <class 'jax.interpreters.partial_eval.DynamicJaxprTracer'> for function jitt_function is non-hashable.\nEven if I managed to transform this into a static argument, I would still need to \"know\" the line number, so that I could access the correct number of good points.\nQuestion: Is there any way for me to do this within jax?",
        "answers": [
            "If you have a different number of points per pmap batch, then the number is not static. The result of your intended operation would be a ragged array (i.e. a 2D array whose rows have differing numbers of elements) and ragged arrays are not supported in JAX.\nIf you actually have a static number of elements—meaning an equal number in every batch—then you can use the size argument of jnp.where to do this computation. It might look something like this:\nfrom functools import partial\n\ndef jitt_function(line, N):\n    \"\"\"\n    Over-simplified function to showcase the problem\n    \"\"\"\n    inds = jnp.where(jnp.isfinite(line), size=N, fill_value=0)\n    return jnp.sum(line[inds])\n\npmap_func = jax.pmap(partial(jitt_function, N=4))\n\npmap_func(data_array)\nIf you have fewer than the specified number of entries in each batch, then one option is to specify the fill_value argument to jnp.where to pad the output. In this case, since you are taking the sum along each dimension, a fill value of zero returns the expected result."
        ],
        "link": "https://stackoverflow.com/questions/75793881/jax-unable-to-jnp-where-with-known-sizes-inside-a-pmap"
    },
    {
        "title": "What is a pure functional version of this Jax function?",
        "question": "I am writing a simple classifier in an attempt to come to grips with Jax. I want to normalize the data (it's the Iris dataset from sklearn) and my little function works, but from what I've read in the Jax the Sharp Bits documentation, I should avoid using lambda functions and iterating over vectors. I am not versed in functional programming, and I am curious if there's a better, more Jax idomatic way to do this. Here is my code so far:\nimport jax.numpy as jnp \nfrom jax import jit, vmap \n# lots of imports ... \n\niris = load_it('data', 'iris.pkl')\n\ndef normalize(data): \n    return jnp.apply_along_axis(lambda x: x/jnp.linalg.norm(x), 1, data) \n\n# TODO: use a functional style, maybe use partial\n# and get rid of the lambda ...\n\ntic = time.perf_counter() \niris_data_normal = normalize(iris.data) \ntoc = time.perf_counter() \nprint(f\"It took jax {toc - tic:0.4f} seconds.\")\nWhen I run this I get: It took jax 0.0677 seconds. Any guidance is most appreciated!",
        "answers": [
            "Your current approach looks fine to me: it is pure (the function has no side-effects) and in JAX apply_along_axis is implemented in terms of vmap, so there's no problem in terms of computational efficiency.\nIf you wanted to write a similar function using direct array operations, you could equivalently do something like this:\ndef normalize(data):\n  return data / jnp.linalg.norm(data, axis=1, keepdims=True)"
        ],
        "link": "https://stackoverflow.com/questions/75776353/what-is-a-pure-functional-version-of-this-jax-function"
    },
    {
        "title": "using jax lax scan with inputs that don't change across iterations within scan but are different each time scan is called",
        "question": "Jax lax scan operates on a function that takes two arguments, a carry and a sequence of inputs. I am wondering how scan should be called if some inputs don't change across iterations of the scan. Naively, I could create a sequence of identical inputs, but this seems wasteful/redundant and more importantly, this isn't always possible, as scan can only scan over arrays. For example, one of the inputs I want to pass to my function is a train state (e.g. from flax.training import train_state) that contains my model and its parameters, which cannot be put into array. As I say in the title, these inputs may also change each time I call scan (e.g. the model parameters will change).\nAny ideas on how best to do this?\nThanks.",
        "answers": [
            "In general, you have three possible approaches for this:\nConvert the single value into a sequence of identical inputs to scan over\nPut the single value in the carry to carry it along to each step of the scan\nClose over the single value\nHere are three examples of a computation using these strategies:\nimport jax\nimport jax.numpy as jnp\n\na = jnp.arange(5)\nb = 2\n# Strategy 1: duplicate b across sequence\ndef f(carry, xs):\n  a, b = xs\n  result = a * b\n  return carry + result, result\n\nb_seq = jnp.full_like(a, b)\n\ntotal, cumulative = jax.lax.scan(f, 0, (a, b_seq))\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\n# Strategy 2: put b in the carry\ndef f(carry, xs):\n  carry, b = carry\n  a = xs\n  result = a * b\n  return (carry + result, b), result\n\n(total, _), cumulative = jax.lax.scan(f, (0, b), a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\n# Strategy 3: close over b\nfrom functools import partial\n\ndef f(carry, xs, b):\n  a = xs\n  result = a * b\n  return carry + result, result\n\ntotal, cumulative = jax.lax.scan(partial(f, b=b), 0, a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\nWhich you use probably depends on the context of where you are using it, but I personally think closure (option 3) is probably the cleanest approach."
        ],
        "link": "https://stackoverflow.com/questions/75776268/using-jax-lax-scan-with-inputs-that-dont-change-across-iterations-within-scan-b"
    },
    {
        "title": "Adding a new layer to a stax.serial object",
        "question": "I'd like to \"convert\" the following tensorflow code in jax:\ndef mlp(L, n_list, activation, Cb, Cw):\n    model = tf.keras.Sequential()\n\n    kernel_initializers_list = []\n    kernel_initializers_list.append(tf.keras.initializers.RandomNormal(0, math.sqrt(Cw/n_list[0])))\n    for l in range(1, L): \n        kernel_initializers_list.append(tf.keras.initializers.RandomNormal(0, math.sqrt(Cw/n_list[l])))\n    kernel_initializers_list.append(tf.keras.initializers.RandomNormal(0, math.sqrt(Cw/n_list[L])))\n    bias_initializer = tf.keras.initializers.RandomNormal(stddev=math.sqrt(Cb))\n\n\n    model.add(tf.keras.layers.Dense(n_list[1], input_shape=[n_list[0]], use_bias = True, kernel_initializer = kernel_initializers_list[0],\n          bias_initializer = bias_initializer))\n    for l in range(1, L): \n        model.add(tf.keras.layers.Dense(n_list[l+1], activation=activation, use_bias = True, kernel_initializer = kernel_initializers_list[l],\n              bias_initializer = bias_initializer))\n    model.add(tf.keras.layers.Dense(n_list[L+1], use_bias = True, kernel_initializer = kernel_initializers_list[L],\n              bias_initializer = bias_initializer))\n    print(model.summary())\n    return model\nIn jax can I add a stax.Dense to the thing I get calling stax.serial() with something equivalent to tensorflow's model.add()? How can I do it?",
        "answers": [
            "Yes, you can.\n#Create new model by jax\nnet_init, net_apply = stax.serial(\n    Conv(32, (3, 3), padding='SAME'),\n    Relu,\n    Conv(64, (3, 3), padding='SAME'),\n    Relu,\n    Conv(128, (3, 3), padding='SAME'),\n    Relu,\n    Conv(256, (3, 3), padding='SAME'),\n    Relu,\n    MaxPool((2, 2)),\n    Flatten,\n    Dense(128),\n    Relu,\n    Dense(10),\n    LogSoftmax,\n)\n\nnet_init(random.PRNGKey(111), input_shape=(-1, 32, 32, 3))    \n\n#Feedfoward\ninputs, targets = batch_data\nnet_apply(params, inputs)\nThis is my reference to help you."
        ],
        "link": "https://stackoverflow.com/questions/75743490/adding-a-new-layer-to-a-stax-serial-object"
    },
    {
        "title": "fitting a model perfectly using jax in machine learning",
        "question": "Link to text file Hi I am relatively new to Machine learning, I have managed to get a model like in the image attached, I would like to know what I can do more for the model to fit perfectly[model I made],i don't know much about choosing loss function efficiently,following code was made by adding the text file to an another program made to fit a function to data\n(https://i.sstatic.net/3ldRy.png)\nthe test file contains noisy voltage measurements\n#!/usr/bin/env python3\n#\n# Fit function to data\n\nimport matplotlib.pyplot as plt\nimport numpy  as np\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, random\n\n# load some noisy data\ntest = np.loadtxt('newe.txt')\n\n\n\nN = 200\nsigma = 0.05\nx = test[:, 0]\ny = test[:, 1]\n\n#plt.plot(x,y)\n#plt.show()\n\n# Match function to data\n\ndef func(params, x):\n  # Parameterised damped oscillation\n  l, omega = params\n  # Note, we \"normalise\" parameters\n  y_pred = jnp.exp(l*10 * x) * jnp.sin(2*jnp.pi* omega*10 * x)\n  return y_pred\n\ndef loss(params, x, y):\n  # Loss function\n  y_pred = func(params, x)\n  return jnp.mean((y - y_pred)**2)\n\n# Compile loss and gradient\nc_loss = jit(loss)\nd_loss = jit(grad(loss))\n\n# One iteration of gradient descent\ndef update_params(params, x, y):\n  grads = d_loss(params, x, y)\n  params = [param - 0.1 * grad for param, grad in zip (params, grads)]\n  return params\n\n# Initialise parameters\nkey = random.PRNGKey(0)\nparams = [random.normal(key, (1,)), random.normal(key, (1,))]\n\nerr = []\nfor epoch in range(100000):\n  err.append(c_loss(params, x, y))\n  params = update_params(params, x, y)\nerr.append(c_loss(params, x, y))\n\nprint(\"Damping:  \", params[0]*10)\nprint(\"Frequency:\", params[1]*10)\n\ny_pred = func(params, x)\n\n# Plot loss and predictions                                                           \nf, ax = plt.subplots(1,2)\nax[0].semilogy(err)\nax[0].set_title(\"History\")\nax[1].plot(x, y, label=\"ground truth\")\nax[1].plot(x, y_pred, label=\"predictions\")\nax[1].legend()\nplt.show()",
        "answers": [
            "Looking at the plot you provided and your code, it seems that your model is incabable of fitting the input data. The input data have y = 1 at x = 0, but your model is an attenuated sinusoid which will always have y = 0 at x = 0, regardless of what the parameters are.\nGiven this, I suspect your minimization is working correctly, and the results you're seeing are the closest fit your model is capable of providing for the data you're plugging in. If you were hoping for a better fit to the data, you should change to a model that's capable of fitting your data."
        ],
        "link": "https://stackoverflow.com/questions/75727749/fitting-a-model-perfectly-using-jax-in-machine-learning"
    },
    {
        "title": "How to use and interpret JAX Vector-Jacobian Product (VJP) for this example?",
        "question": "I am trying to learn how to find the Jacobian of a vector-valued ODE function using JAX. I am using the examples at https://implicit-layers-tutorial.org/implicit_functions/ That page implements its own ODE integrator and associated custom forward-mode and reverse-mode Jacobian functions. I am trying to reproduce that using the official jax odeint and diffrax libraries, but both of these primarily use reverse-mode Vector Jacobian Product (VJP) instead of the forward-mode Jacobian Vector Product (JVP) for which example code is available on that page.\nHere is a code snippet that I adapted from that page:\nimport matplotlib.pyplot as plt\n\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\nimport jax.numpy as jnp\nfrom jax import jit, jvp, vjp\nfrom jax.experimental.ode import odeint\n\nfrom diffrax import diffeqsolve, ODETerm, PIDController, SaveAt, Dopri5, NoAdjoint\n\n# returns time derivatives of each of our 3 state variables (vector-valued function)\ndef f(state, t, args):\n    x, y, z = state\n    rho, sigma, beta = args \n    return jnp.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z])\n\n# convenience function that calls jax-odeint given input initial conditions and parameters (this is the function that we want Jacobian/sensitivities of)\ndef evolve(y0, rho, sigma, beta): \n    return odeint(f, y0, tarr, (rho, sigma, beta))\n\n\n# set up initial conditions, timespan for integration, and fiducial parameter values\ny0 = jnp.array([5., 5., 5.])\ntarr = jnp.linspace(0, 1., 1000)\nrho = 28.\nsigma = 10.\nbeta = 8/3. \n\n\n# first just make sure evolve() works \nys = evolve(y0, rho, sigma, beta)\n\nfig, ax = plt.subplots(1,figsize=(6,4),dpi=150,subplot_kw={'projection':'3d'})   \nax.plot(ys.T[0],ys.T[1],ys.T[2],'b-',lw=0.5)\n\n# now try to take reverse-mode vector-jacobian product (VJP) since forward-mode JVP is not defined for jax-odeint\nvjp_ys, vjp_evolve = vjp(evolve,y0,rho,sigma,beta)\n\n# vjp_ys and ys are equal -- they are the solution time series of the 3 components (state variables) of y \nprint(jnp.array_equal(ys,vjp_ys))\n\n# define some perturbation in y0 and parameters \ndelta_y0 = jnp.array([0., 0., 0.])\ndelta_rho = 0.\ndelta_sigma = 0.\ndelta_beta = 1.\n\n####### THIS FAILS \n# vjp_evolve is a function but I am not sure how to use it to get perturbations delta_ys given y0/parameter variations\nvjp_evolve(delta_y0,delta_rho,delta_sigma,delta_beta)\nThat last line raises an error:\nTypeError: The function returned by `jax.vjp` applied to evolve was called with 4 arguments, but functions returned by `jax.vjp` must be called with a single argument corresponding to the single value returned by evolve (even if that returned value is a tuple or other container).\n\nFor example, if we have:\n\n  def f(x):\n    return (x, x)\n  _, f_vjp = jax.vjp(f, 1.0)\n\nthe function `f` returns a single tuple as output, and so we call `f_vjp` with a single tuple as its argument:\n\n  x_bar, = f_vjp((2.0, 2.0))\n\nIf we instead call `f_vjp(2.0, 2.0)`, with the values 'splatted out' as arguments rather than in a tuple, this error can arise.\nI suspect I am confused at the concept of reverse-mode VJP and what the input would be in the case of this vector-valued ODE. The same problem would persist if I had used diffrax solvers.\nFor what it's worth, I can reproduce the forward-mode JVP results on that website if I use a diffrax solver while specifying adjoint=NoAdjoint, so that jax.jvp can be used:\n# I am similarly confused about how to use VJP with diffrax's default reverse-mode autodiff of the ODE system\n# however I am able to use forward-mode JVP with diffrax's ODE solver if I specify adjoint=NoAdjoint\n\n# diffrax expects reverse order for inputs (time first, then state, then args) -- opposite of jax odeint \ndef f_diffrax(t, state, args):\n    x, y, z = state\n    rho, sigma, beta = args \n    return jnp.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z])\n\n# set up diffrax inputs as closely to jax-odeint as possible \nterms = ODETerm(f_diffrax)\nt0 = 0.0\nt1 = 1.0 \ndt0 = None\nmax_steps = 16**3 # not sure if this is needed\ntsave = SaveAt(ts=tarr,dense=True)\n\ndef evolve_diffrax(y0, rho, sigma, beta):\n    return diffeqsolve(terms,Dopri5(),t0,t1,dt0,y0,jnp.array([rho,sigma,beta]),saveat=tsave,\n                       stepsize_controller=PIDController(rtol=1.4e-8,atol=1.4e-8),max_steps=max_steps,adjoint=NoAdjoint())\n\n# get solution AND differentials assuming the same changes in y0 and parameters as we tried (and failed) to get above \ndiffrax_ys, diffrax_delta_ys = jvp(evolve_diffrax, (y0,rho,sigma,beta),(delta_y0,delta_rho,delta_sigma,delta_beta))\n\n# get the actual solution arrays from the diffrax Solution objects \ndiffrax_ys = diffrax_ys.ys\ndiffrax_delta_ys = diffrax_delta_ys.ys\n\n# plot \nfig, ax = plt.subplots(1,figsize=(6,4),dpi=150,subplot_kw={'projection':'3d'})   \nax.plot(diffrax_ys.T[0],diffrax_ys.T[1],diffrax_ys.T[2],color='violet',lw=0.5)\nax.quiver(diffrax_ys.T[0][::10],diffrax_ys.T[1][::10],diffrax_ys.T[2][::10],\n          diffrax_delta_ys.T[0][::10],diffrax_delta_ys.T[1][::10],diffrax_delta_ys.T[2][::10])\n    \nThat reproduces one of the main plots of that website (showing that the ODE is very sensitive to variations in the beta parameter). So I understand the concept of forward-mode JVP (given perturbations in initial conditions and/or parameters, JVP gives the corresponding perturbation in the ODE solution as a function of time). But what does reverse-mode VJP do and what would be the correct input to the vjp_evolve function above?",
        "answers": [
            "JVP is forward-mode autodiff: given tangents of the input to the function at a primal point, it returns tangents on the outputs.\nVJP is reverse-mode autodiff: given cotangents on the output of the function at a primal point, it returns cotangents on the inputs.\nSo you can call vjp_evolve with cotangents of the same shape as vjp_ys:\nprint(vjp_evolve(jnp.ones_like(vjp_ys)))\n(Array([ 1.74762118, 26.45747015, -2.03017559], dtype=float64),\n Array(871.66349663, dtype=float64),\n Array(-83.07586548, dtype=float64),\n Array(-1754.48788565, dtype=float64))\nConceptually, JVP propagates gradients forward through a computation, while VJP propagates gradients backward. The JAX docs might be useful background for understanding the JVP & VJP transformations more deeply: https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#vector-jacobian-products-vjps-aka-reverse-mode-autodiff"
        ],
        "link": "https://stackoverflow.com/questions/75711315/how-to-use-and-interpret-jax-vector-jacobian-product-vjp-for-this-example"
    },
    {
        "title": "What is the correct way to define a vectorized (jax.vmap) function in a class?",
        "question": "I want to add a function, which is vectorized by jax.vmap, as a class method. However, I am not sure where to define this function within the class. My main goal is to avoid, that the function is being redefined each time I call the class method.\nHere is a minimal example for a class that counts how often a value occurs in a jnp.array, with a non-vectorized and vectorized version:\nimport jax.numpy as jnp\nimport jax\n\nclass ValueCounter():\n\n    def __init__(self): # for completeness, not used\n        self.attribute_1 = None\n\n    @staticmethod\n    def _count_value_in_array( # non-vectorized function\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n\n    # here comes the vectorized function\n    def count_values_in_array(self, array: jnp.array, value_array: jnp.array) -> jnp.array:\n        \"\"\"Count how often each value in an array of values occurs in an array\"\"\"\n        count_value_in_array_vec = jax.vmap(\n            self._count_value_in_array, in_axes=(None, 0)\n        ) # vectorized function is defined again each time the function is called\n        return count_value_in_array_vec(array, value_array)\nExample output & input:\nvalue_counter = ValueCounter()\nvalue_counter.count_values_in_array(jnp.array([0, 1, 2, 2, 1, 1]), jnp.array([0, 1, 2]))\nResult (correct as expected)\nArray([1, 3, 2], dtype=int32)\nThe vectorized function count_value_in_array_vecis redefined each time count_values_in_array - which seems unnecessary to me. However, I am a bit stuck on how to avoid this. Does someone know how the vectorized function could be integrated into the class in a more elegant way?",
        "answers": [
            "You can decorate the static method directly; for example:\nfrom functools import partial\n\n# ...\n\n    @staticmethod\n    @partial(jax.vmap, in_axes=(None, 0))\n    def _count_value_in_array(\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n# ..."
        ],
        "link": "https://stackoverflow.com/questions/75696639/what-is-the-correct-way-to-define-a-vectorized-jax-vmap-function-in-a-class"
    },
    {
        "title": "Returning a distribution object from a jittable function",
        "question": "I want to create a jittable function that outputs a distrax distribution object. For instance:\nimport distrax\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n   dist = distrax.Categorical(logits=jnp.sin(x))\n   return dist\n\njit_f = jax.jit(f)\na = jnp.array([1,2,3])\ndist = jit_f(a)\nCurrently this code gives me the following error:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\traceback_util.py\", line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\api.py\", line 628, in cache_miss\n    out = tree_unflatten(out_pytree_def, out_flat)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\tree_util.py\", line 75, in tree_unflatten\n    return treedef.unflatten(leaves)\n  File \"F:\\jax_env\\lib\\site-packages\\distrax\\_src\\utils\\jittable.py\", line 40, in tree_unflatten\n    obj = cls(*args, **kwargs)\n  File \"F:\\jax_env\\lib\\site-packages\\distrax\\_src\\distributions\\categorical.py\", line 60, in __init__\n    self._logits = None if logits is None else math.normalize(logits=logits)\n  File \"F:\\jax_env\\lib\\site-packages\\distrax\\_src\\utils\\math.py\", line 72, in normalize\n    return jax.nn.log_softmax(logits, axis=-1)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\traceback_util.py\", line 162, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\_src\\api.py\", line 618, in cache_miss\n    keep_unused=keep_unused))\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\core.py\", line 2031, in call_bind_with_continuation\n    top_trace = find_top_trace(args)\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\core.py\", line 1122, in find_top_trace\n    top_tracer._assert_live()\n  File \"F:\\jax_env\\lib\\site-packages\\jax\\interpreters\\partial_eval.py\", line 1486, in _assert_live\n    raise core.escaped_tracer_error(self, None)\njax._src.traceback_util.UnfilteredStackTrace: jax._src.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float32[3] wrapped in a DynamicJaxprTracer to escape the scope of the transformation.\nJAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state.\nThe function being traced when the value leaked was f at <stdin>:1 traced for jit.\n------------------------------\nThe leaked intermediate value was created on line <stdin>:2 (f).\n------------------------------\nWhen the value was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n------------------------------\n<stdin>:1 (<module>)\n<stdin>:2 (f)\n------------------------------\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.UnexpectedTracerError\nI thought that using dist = jax.block_until_ready(dist) inside f could fix the problem, but it doesn't.",
        "answers": [
            "This looks like the bug in distrax v0.1.2 reported in https://github.com/deepmind/distrax/issues/162. This wass fixed by https://github.com/deepmind/distrax/pull/177, which is part of the distrax v0.1.3 release.\nTo fix the issue, you should update to distrax v0.1.3 or later."
        ],
        "link": "https://stackoverflow.com/questions/75663449/returning-a-distribution-object-from-a-jittable-function"
    },
    {
        "title": "Python function minimization not changing optimization variables",
        "question": "I need to minimize a simple function that divides two values. The optimization paramter x is a (n,m) numpy array from which I calculate a float.\n# An initial value\nnormX0 = calculate_normX(x_start)\n\ndef objective(x) -> float:\n    \"\"\"Objective function \"\"\"\n    x = x.reshape((n,m))\n    normX = calculate_normX(x)\n    return -(float(normX) / float(normX0))\ndef calculate_normX() is a wrapper function to an external (Java-)API that takes the ndarray as an input and outputs a float, in this case, the norm of a vector. For the optimization, I was using jax and jaxopt, since it supports automatic differentiation of objective.\nsolver = NonlinearCG(fun=objective, maxiter=5, verbose=True)\nres = solver.run(x.flatten())\nor the regular scipy minimize\nobjective_jac = jax.jacrev(objective)\nminimize(objective, jac=objective_jac, x0=`x.flatten(), method='L-BFGS-B', options={'maxiter': 2})\nIn both cases, however, x is not changed during the optimization step. Even initializing x with random values the optimizer does not seem to work. I also tried other solvers like Jaxopt NonlinearCG. What am I doing wrong?",
        "answers": [
            "The external, non-JAX function call is almost certainly the source of the problem. Non-JAX function calls within JAX transforms like jacrev effectively get replaced with a trace-time constant (and in most cases will error), and so it makes sense that your optimization will not change its value.\nThe best approach would be to define your calculate_normX function using JAX rather than calling out to an external API, and then everything should work automatically.\nIf you must call to an external API, one way to do this in JAX is to use pure_callback along with custom_jvp to define the autodiff rule for your external callback. There is an example of this in the JAX docs: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html#example-pure-callback-with-custom-jvp"
        ],
        "link": "https://stackoverflow.com/questions/75651326/python-function-minimization-not-changing-optimization-variables"
    },
    {
        "title": "Caching Behavior in JAX",
        "question": "I have a function f that takes in a boolean static argument flag and performs some computation based on it's value. Below is a rough outline of this function.\n@partial(jax.jit, static_argnames=['flag'])\ndef f(x, flag):\n   # Preprocessing\n   if flag:\n      ...\n   else:\n      ...\n   # Postprocessing\nEach time f is called with a different value of flag, a recompilation of this function should be triggered. However, because flag is a boolean and can take on at most two values, it would be preferable if JAX would cache the compiled version of f for each of the possible values of flag and avoid recompilations.\nIn short, I would like JAX to compile f only two times when running following piece of code:\nflag = True\nfor i in range(100):\n   f(x, flag)\n   flag = not flag\nIs there a way to tell JAX not to throw away old compiled versions of f, each time it's called with a new value of flag? And in general, are there any caching mechanisms implemented in JAX for such scenarios? (For instance if flag is an integer, but we know beforehand that it would only ever take k distinct values, and we would like to save the compiled version of f for each of these k values)\nI know that I can use jax.lax.cond or jax.lax.switch to control the flow inside f and treat flag as a regular argument rather than a static one. But this would make the code much more bloated (and difficult to read) as there are several places within the body of f where I access flag. It would be much cleaner if I declared flag to be a static argument and then controled the caching behavior of jax.jit to avoid recompilations.",
        "answers": [
            "If I understand your question correctly, then JAX by default behaves the way you would like it to behave. Each JIT-compiled function has an LRU cache of compilations based on the shape and dtype of dynamic arguments and the hash of static arguments. You can inspect the size of this cache using the _cache_size method of the compiled function. For example:\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['flag'])\ndef f(x, flag):\n   if flag:\n       return jnp.sin(x)\n   else:\n       return jnp.cos(x)\n\nprint(f._cache_size())\n# 0\n\nx = jnp.arange(10)\nf(x, True)\nprint(f._cache_size())\n# 1\n\nf(x, False)\nprint(f._cache_size())\n# 2\n\n# Subsequent calls with the same flag value hit the cache:\nflag = True\nfor i in range(100):\n    f(x, flag)\n    flag = not flag\nprint(f._cache_size())\n# 2\nSince the size of the x argument hasn't changed, we get one cache entry for each value of flag, and the cached compilations are used in subsequent calls.\nNote however if you change the shape or dtype of the dynamic argument, you get new cache entries:\nx = jnp.arange(100)\nfor i in range(100):\n    f(x, flag)\n    flag = not flag\nprint(f._cache_size())\n# 4\nThe reason this is necessary is that, in general, functions may change its behavior based on these static quantities."
        ],
        "link": "https://stackoverflow.com/questions/75641884/caching-behavior-in-jax"
    },
    {
        "title": "Nested vmap in pmap - JAX",
        "question": "I currently can run simulations in parallel on one GPU using vmap. To speed things up, I want to batch the simulations over multiple GPU devices using pmap. However, when pmapping the vmapped function I get a tracing error.\nThe code I use to get a trajectory state is:\ntraj_state = vmap(run_trajectory, in_axes=(0, None, 0))(sim_state, timings, lambda_array)\n                                                                        \nwhere lambda_array parameterises each simulation, which is run by the function run_trajectory which runs a single simulation. I then try to nest this inside a pmap:\npmap(vmap(run_trajectory, in_axes=(0, None, 0)),in_axes=(0, None, 0))(reshaped_sim_state, timings, reshaped_lambda_array)                                                                                       \nIn doing so I get the error:\nWhile tracing the function run_trajectory for pmap, this concrete value was not available in Python because it depends on the value of the argument 'timings'.\nI'm quite new to JAX and although there are documentations on errors with traced values, I'm not very sure on how to navigate this problem.",
        "answers": [
            "vmap and pmap have slightly different APIs when it comes to in_axes. In vmap, setting in_axes=None causes inputs to be unmapped and static (i.e. un-traced), while in pmap even inputs with in_axes=None will be unmapped but still traced:\nfrom jax import vmap, pmap\nimport jax.numpy as jnp\n\ndef f(x, condition):\n  # requires untraced condition:\n  return x if condition else x + 1\n\nx = jnp.arange(4)\nvmap(f, in_axes=(0, None))(x, True)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(f, in_axes=(0, None))(x, True)\n# ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: \nTo ensure that your variable is untraced in pmap, you can partially evaluate the function; for example:\nfrom functools import partial\n\nvmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\nIn your case, applying this solution might look like this:\ndef run(sim_state, lambda_array, timings=timings):\n  return run_trajectory(sim_state, timings, lambda_array)\n\nvmap(run)(sim_state, lambda_array)\n\npmap(vmap(run))(reshaped_sim_state, reshaped_lambda_array)",
            "I can seemingly avoid this problem by passing the timing values prior to vmapping using partial, that is:\nrun_trajectory = partial(run_trajectory, starting_time=timings)\n\ntraj_state = pmap(vmap(run_trajectory, in_axes=(0, 0)))(reshaped_sim_state, reshaped_lambda_array)"
        ],
        "link": "https://stackoverflow.com/questions/75625305/nested-vmap-in-pmap-jax"
    }
]