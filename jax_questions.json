[
    {
        "title": "JAX crashes with `CUDNN_STATUS_INTERNAL_ERROR` when using `joblib` or `multiprocessing`, but works in a single process",
        "question": "I am running into a FAILED_PRECONDITION: DNN library initialization failed error when trying to parallelize a JAX function using either Python's multiprocessing library or joblib.\nThe strange part is that my JAX installation is fundamentally working: if I run a simple JAX command in a single process, it correctly identifies and uses my NVIDIA GPUs. The crash only happens when I launch parallel workers.\nHere are my system and environment details:\nEnvironment:\nOS: Ubuntu 18.04 LTS\nPython: 3.10 (managed via Conda)\nGPU: 8 x NVIDIA Quadro RTX 8000\nNVIDIA Driver: 550.144.03\nCUDA Version (from driver): 12.4\nJAX Installation:\njax==0.4.26\njaxlib==0.4.26\njax-cuda12-plugin==0.4.26\nMinimal, Reproducible Example:\nThe following simplified script perfectly demonstrates the problem.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\n# Define a simple JAX function that will be executed on the GPU\ndef simple_worker(i):\n    \"\"\"A simple function that performs a JAX computation.\"\"\"\n    try:\n        # Create some data on the GPU and perform a computation\n        x = jnp.ones((100, 100))\n        y = jnp.dot(x, x)\n        # block_until_ready() ensures the computation is finished before returning\n        y.block_until_ready()\n        return i, \"Success\"\n    except Exception as e:\n        return i, f\"Failed with: {e}\"\n\nif __name__ == \"__main__\":\n\n    # --- Verification Step ---\n    print(\"--- Verifying JAX in the main process ---\")\n    try:\n        devices = jax.devices()\n        print(f\"JAX sees {len(devices)} devices: {devices}\")\n        if 'gpu' not in str(devices[0]).lower() and 'cuda' not in str(devices[0]).lower():\n            print(\"WARNING: JAX does not see the GPU in the main process!\")\n    except Exception as e:\n        print(f\"Error during JAX verification: {e}\")\n    print(\"-\" * 40)\n\n\n    # --- Test 1: Serial execution (This works perfectly) ---\n    print(\"\\n--- Running jobs serially (expected to work) ---\")\n    results_serial = []\n    for i in range(4):\n        results_serial.append(simple_worker(i))\n    print(f\"Serial results: {results_serial}\\n\")\n    print(\"-\" * 40)\n\n\n    # --- Test 2: Parallel execution with joblib (This crashes) ---\n    print(\"\\n--- Running jobs in parallel with joblib (expected to fail) ---\")\n    try:\n        # Also tried backend='threading' and backend='multiprocessing' with 'spawn' context\n        multiprocessing.set_start_method('spawn', force=True) \n        \n        results_parallel = Parallel(n_jobs=4)(\n            delayed(simple_worker)(i) for i in range(4)\n        )\n        print(f\"Parallel results: {results_parallel}\")\n    except Exception as e:\n        print(f\"Joblib Parallel failed with error: {e}\")\n    print(\"-\" * 40)\nWhat Happens:\nWhen I run this script, the verification and serial execution sections work perfectly, showing that my base JAX installation can use the GPU. However, the parallel section immediately crashes. Each worker process throws a series of errors that look like this before the main script crashes:\npython\nCopy\nE ... external/xla/xla/stream_executor/cuda/cuda_dnn.cc:536] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n...\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I Have Already Tried:\nVerifying Installation: As shown in the MRE, running JAX code in the main, single process works flawlessly. jax.devices() correctly lists all my GPUs.\nChanging Multiprocessing Start Method: I tried adding multiprocessing.set_start_method('spawn', force=True) at the beginning of my script. The crash still occurs with the same error.\nChanging joblib Backend: I tried Parallel(n_jobs=4, backend='threading'). This also results in the exact same CUDNN_STATUS_INTERNAL_ERROR crash.\nIt seems that any attempt by a child process or thread to initialize its own connection to the GPU resources fails, even though the main process can do so without any issues.\nIs there a known configuration issue with certain Linux/NVIDIA driver setups that prevents parallel workers from initializing CUDA contexts? What is the correct way to structure a parallel Python script using a library like joblib to distribute JAX workloads?",
        "answers": [
            "JAX is not compatible with multiprocessing (see https://github.com/jax-ml/jax/issues/3691#issuecomment-658270846).\nYou could try to work around this by using spawn rather than fork as suggested in https://github.com/jax-ml/jax/issues/3691#issuecomment-658270846, but even then, you appear to have only a single GPU device available, and if you try to run multiple JAX processes targeting this single device, they will run into issues because the first process will reserve most device memory.\nYou could potentially alleviate this if you have multiple devices, but in that case I suspect you'd get better performance by relying on JAX's compiler to shard your computations appropriately (see https://docs.jax.dev/en/latest/sharded-computation.html for more on this).\nOverall, the recommendation would be to not use JAX within multiprocessing or joblib."
        ],
        "link": "https://stackoverflow.com/questions/79816992/jax-crashes-with-cudnn-status-internal-error-when-using-joblib-or-multiproc"
    },
    {
        "title": "Does `jax` compilation save runtime memory by recognizing array elements that are duplicated by indexing",
        "question": "Consider the example code:\npython\nCopy\nfrom functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnums=(0,))\ndef my_function(n):\n\n    idx = jnp.tile(jnp.arange(n, dtype=int),(1,n)) # Contains duplicate indices\n    A = jnp.ones((n**2,), dtype=float)\n    B = jnp.ones((n,100,100), dtype=float)\n\n    return jnp.sum(A[...,None,None]*B[idx]) # Will data in B be duplicated in memory here?\n\nmy_function(5)\nWhen compiling through B[idx], will jax compilation recognize that there are duplicate indices and thereby avoid unnecessarily duplicating the data in B?\nI suspect probably not because it's value dependent in general, but just want to understand better.",
        "answers": [
            "No, I don't believe this is an optimization that the compiler does. I'm basing this on the fact that XLA's computational model requires all array shapes to be known at compile-time, and the values in idx are not known until runtime.\nIf you're not convinced and want to see for yourself what the compiler is doing with this code, you can use JAX's Ahead of time compilation APIs to peek at the compiled HLO produced by XLA for this code (note that the compiler will perform different optimizations on different hardware).\nFor example:\npython\nCopy\nprint(my_function.lower(5).compile().as_text())\nHloModule jit_my_function, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%region_0.1.clone (reduce_sum.0: f32[], reduce_sum.1: f32[]) -> f32[] {\n  %reduce_sum.0 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.1 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.2 = f32[] add(%reduce_sum.0, %reduce_sum.1), metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\n\n%fused_computation () -> f32[1,25,100,100] {\n  %constant.2 = f32[] constant(1)\n  %broadcast_in_dim.0 = f32[5,100,100]{2,1,0} broadcast(%constant.2), dimensions={}, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=10 source_end_line=10 source_column=8 source_end_column=42}\n  %iota.5 = s32[1,1,5,5]{3,2,1,0} iota(), iota_dimension=3, metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %bitcast.5 = s32[1,25]{1,0} bitcast(%iota.5), metadata={op_name=\"jit(my_function)/broadcast_in_dim\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=8 source_end_line=8 source_column=10 source_end_column=50}\n  %constant.1 = s32[] constant(0)\n  %broadcast.4 = s32[1,25]{1,0} broadcast(%constant.1), dimensions={}\n  %lt.0 = pred[1,25]{1,0} compare(%bitcast.5, %broadcast.4), direction=LT, metadata={op_name=\"jit(my_function)/lt\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %constant.0 = s32[] constant(5)\n  %broadcast.1 = s32[1,25]{1,0} broadcast(%constant.0), dimensions={}\n  %add.0 = s32[1,25]{1,0} add(%bitcast.5, %broadcast.1), metadata={op_name=\"jit(my_function)/add\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %select_n.0 = s32[1,25]{1,0} select(%lt.0, %add.0, %bitcast.5), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %bitcast.4 = s32[25,1]{1,0} bitcast(%select_n.0), metadata={op_name=\"jit(my_function)/select_n\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  %gather.2 = f32[25,1,100,100]{3,2,1,0} gather(%broadcast_in_dim.0, %bitcast.4), offset_dims={1,2,3}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,100,100}, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n  ROOT %bitcast.3 = f32[1,25,100,100]{3,2,1,0} bitcast(%gather.2), metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}\n}\n\nENTRY %main.2 () -> f32[] {\n  %constant.7 = f32[] constant(0)\n  %gather_bitcast_fusion = f32[1,25,100,100]{3,2,1,0} fusion(), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(my_function)/gather\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=36 source_end_column=42}, backend_config={\"outer_dimension_partitions\":[\"1\",\"2\"]}\n  %reduce-window = f32[1,1,4,4]{3,2,1,0} reduce-window(%gather_bitcast_fusion, %constant.7), window={size=1x25x32x32 stride=1x25x32x32 pad=0_0x0_0x14_14x14_14}, to_apply=%region_0.1, backend_config={\"outer_dimension_partitions\":[\"1\",\"1\",\"2\"]}\n  ROOT %reduce_sum.7 = f32[] reduce(%reduce-window, %constant.7), dimensions={0,1,2,3}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(my_function)/reduce_sum\" source_file=\"/tmp/ipython-input-3455249338.py\" source_line=12 source_end_line=12 source_column=11 source_end_column=43}\n}\nReading this output takes some practice, but the relevant piece to answer your question is the line that starts with %gather.2 = f32[25,1,100,100]{3,2,1,0}: the gather primitive is XLA's version of indexing, and you see that it's explicitly constructing the full 25x100x100 array, and not removing the duplicated indices."
        ],
        "link": "https://stackoverflow.com/questions/79798175/does-jax-compilation-save-runtime-memory-by-recognizing-array-elements-that-ar"
    },
    {
        "title": "How to Make Batching Rule for Multiple Outputs",
        "question": "I am still exploring how to make batching rule correctly. Right now, my code of batching rule doesn't work as expected for multiple outputs. Here is my code.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.extend import core\nfrom jax.interpreters import batching\n\nmy_sum_p = core.Primitive('my_sum')\nmy_sum_p.multiple_results = True\n\ndef my_sum(x):\n    return my_sum_p.bind(x)\n\n# primal evaluation rule\ndef my_sum_impl(x):\n    return jnp.sum(x), jnp.ones((3,))\nmy_sum_p.def_impl(my_sum_impl)\n\n# batching rule\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (bd_x, bd_x)\n    return output, dims\nbatching.primitive_batchers[my_sum_p] = my_sum_batching_rule\n\n# example usage\nif __name__ == \"__main__\":\n    x = jnp.array([[1.0, 2.0, 3.0],\n                   [4.0, 5.0, 6.0]])\n    \n    vms = jax.vmap(my_sum)\n    print('my_sum:', vms(x))\n\n    def original_sum(x):\n        return jnp.sum(x), jnp.ones((3,))\n    vos = jax.vmap(original_sum)\n    print('original sum:', vos(x))\nThe output is like this\npython\nCopy\nmy_sum: [Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\noriginal sum: (Array([ 6., 15.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\nThe second output of my_sum is not batched like the one of original_sum.\nOn another case, when I change the vmap setup of my_sum such as follows\npython\nCopy\nvms = jax.vmap(my_sum, out_axes=(0, None))\nit raises an error\npython\nCopy\nTraceback (most recent call last):\n  File \"/home/yahya/Projects/neuralnet/neuralnet/nn/playground.py\", line 33, in <module>\n    print('my_sum:', vms(x))\n                     ~~~^^^\nValueError: vmap out_axes specification must be a tree prefix of the corresponding value, got specification (0, None) for value tree PyTreeDef([*, *]).\nMeanwhile the same vmap setup for the original_sum makes a correct output such as follows\npython\nCopy\noriginal_sum: (Array([ 6., 15.], dtype=float32), Array([1., 1., 1.], dtype=float32))\nHow should my batching rule be? I really appreciate for any help",
        "answers": [
            "Your batching rule should look like this:\npython\nCopy\ndef my_sum_batching_rule(batched_args, batch_dims):\n    x, = batched_args\n    bd_x, = batch_dims\n    axis = [i for i in range(x.ndim) if i != bd_x]\n    output = jnp.sum(x, axis=axis), jnp.ones((3,))\n    dims = (0, None)\n    return output, dims\nThe operative change here is that dims = (bd_x, bd_x) became dims = (0, None). The output batch dimensions must reflect the batching characteristics of the output, not the input: in this case, the first output is batched along the leading axis (bdim = 0), and the second output is unbatched (bdim = None).\nYou can test the correctness of this further by comparing the results when vmapping over different input and output axes; for example:\npython\nCopy\nprint(jax.vmap(original_sum, in_axes=(1,))(x))\nprint(jax.vmap(my_sum, in_axes=(1,))(x))\npython\nCopy\n(Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)]\npython\nCopy\nprint(jax.vmap(original_sum, in_axes=(1,), out_axes=(0, None))(x))\nprint(jax.vmap(my_sum, in_axes=(1,), out_axes=[0, None])(x))\npython\nCopy\n(Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32))\n[Array([5., 7., 9.], dtype=float32), Array([1., 1., 1.], dtype=float32)]\n(notice that out_axes is a tuple in the first case, because original_fun returns a tuple, and a list in the second case, because primitives with multiple_outputs=True return a list rather than a tuple)."
        ],
        "link": "https://stackoverflow.com/questions/79790782/how-to-make-batching-rule-for-multiple-outputs"
    },
    {
        "title": "`jax.lax.fori_loop` with equal `lower` and `upper` should produce no iteration, but body still executed",
        "question": "I have a code that uses a bunch of jax.lax.fori_loop. The documentation of fori_loop says that \"setting upper <= lower will produce no iterations\". So I was naively expecting the loop to just return its init_val unchanged. But in my case, it seems like it does attempt to execute the body.\nThe code is as follows:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis returns the following error:\npython\nCopy\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 47\n     45 Hks = jnp.zeros((0,2,2), dtype=complex)\n     46 U = jnp.eye(2, dtype=complex)\n---> 47 build(U, Hks)\n\nCell In[10], line 35\n     33 Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n     34 Uks = Uks.at[0].set(U)\n---> 35 Uks = jax.lax.fori_loop(0, n, update_Uks, (Uks, Hks))[0] # n=0, so lower=upper=0. Should produce no iterations???\n     36 return Uks\n\n    [... skipping hidden 12 frame]\n\nCell In[10], line 40\n     38 def update_Uks(k, val):\n     39     Uks, Hks = val\n---> 40     Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n     41     return Uks, Hks\n\nCell In[10], line 12\n     11 def binom_conv(n, Aks, Bks):\n---> 12     return part_binom_conv(n, 0, n, Aks, Bks)\n...\n--> 930     raise IndexError(f\"index is out of bounds for axis {x_axis} with size 0\")\n    931   i = _normalize_index(i, x_shape[x_axis]) if normalize_indices else i\n    932   i_converted = lax.convert_element_type(i, index_dtype)\n\nIndexError: index is out of bounds for axis 0 with size 0\nI'm not sure I understand what is going on here. Shouldn't the fori_loop just return its init_val and not cause this error?",
        "answers": [
            "JAX code has two phases of execution: tracing and runtime (see JAX Key Concepts: tracing for a description of this). A fori_loop with upper <= lower will have no iterations at runtime, but the code is still traced. The error you're seeing is coming up during tracing, which is necessary even for an empty loop in order to determine the shape and type of the output. You could work around this by specially handling the zero-length case in your fori_loop body function.\nA similar issue with while_loop is discussed at https://github.com/jax-ml/jax/issues/3285.",
            "Following the insight by @Obaskly and @jakevdp, I went with a wrapper for the fori_loop:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nfrom jax.scipy.special import gammaln\n\n# WRAPPER FOR FORI [WORKS IN EXTERNAL LOOPS, BUT CAUSES ISSUE IN part_binom_conv()]\ndef wrapped_fori(lower, upper, body_fun, init_val, unroll=None):\n    if upper<=lower:\n        out = init_val\n    else:\n        out = jax.lax.fori_loop(lower, upper, body_fun, init_val, unroll=unroll)\n    return out\n\n\n# PRELIMINARY PART FOR MWE\n\ndef comb(n, k):\n    return jnp.round(jnp.exp(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)))\n\ndef binom_conv(n, Aks, Bks):\n    return part_binom_conv(n, 0, n, Aks, Bks)\n\ndef part_binom_conv(n, k0, k1, Aks, Bks):\n    A_shape = Aks.shape[1:]\n    A_dtype = Aks.dtype\n    init_conv = jnp.zeros(A_shape, dtype=A_dtype)\n    conv = jax.lax.fori_loop(k0, k1, update_binom_conv, (init_conv, n, Aks, Bks))[0]\n    return conv\n\ndef update_binom_conv(k, val):\n    conv, n, Aks, Bks = val\n    conv = conv + comb(n-1, k) * Aks[k] @ Bks[(n-1)-k]\n    return conv, n, Aks, Bks\n\n\n# IMPORTANT PART\n@jax.jit\ndef build(U, Hks):\n    n = Hks.shape[0] # n=0\n    H_shape = Hks.shape[1:] # H_shape=(2,2)\n    Uks_shape = (n+1,)+H_shape # Uks_shape=(1,2,2)\n    Uks = jnp.zeros(Uks_shape, dtype=Hks.dtype)\n    Uks = Uks.at[0].set(U)\n    Uks = wrapped_fori(0, n, update_Uks, (Uks, Hks))[0] # Treats the case n=0 separately\n    return Uks\n\ndef update_Uks(k, val):\n    Uks, Hks = val\n    Uks = Uks.at[k+1].set(-1j*binom_conv(k+1, Hks, Uks))\n    return Uks, Hks\n\n\n# Test\nHks = jnp.zeros((0,2,2), dtype=complex)\nU = jnp.eye(2, dtype=complex)\nbuild(U, Hks)\nThis produces the correct behavior, and it seems to trace correctly under minimal testing.\nNote however that there is still an error if I replace the inner fori_loop in part_binom_conv by this wrapper. I think it causes an issue in tracing of nested loops.\nSorry for deleting and undeleting this answer a few times, this last point had me confused for a while."
        ],
        "link": "https://stackoverflow.com/questions/79783857/jax-lax-fori-loop-with-equal-lower-and-upper-should-produce-no-iteration"
    },
    {
        "title": "Sequential compilation times of a jax-jitted recursive function",
        "question": "I have a recursively defined function my_func that is jitted using jax.jit from the jax library. It is defined below:\npython\nCopy\n# Imports\n\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\nimport time\n\n\n# Constants and subroutines used in the core recursive routing below ...\n\nsx = jnp.asarray([[0,1.],[1.,0]], dtype=complex)\nsy = jnp.asarray([[0,-1j],[1j,0]], dtype=complex)\n\ndef conj_op(A):\n    return jnp.swapaxes(A, -1,-2).conj()\n\ndef commutator_herm(A, B):\n    comm = A @ B\n    comm = comm - conj_op(comm)\n    return comm\n\ndef H(t):\n    return jnp.cos(t) * sy\n\ndef X0(t):\n    return sx\n\n# Core recursive routine ...\n\n@partial(jax.jit, static_argnames=\"k\")\ndef my_func(t, k):\n    if k==0:\n        X_k = X0(t)\n        return X_k\n    else:\n        X_km1 = lambda t: my_func(t,k-1)\n        X_k = 1j * commutator_herm(H(t), X_km1(t)) + jax.jacfwd(X_km1, holomorphic=True)(t)\n        return X_k\nwith the relevant test:\npython\nCopy\n# Tests ...\n\nt = jnp.asarray(1, dtype=complex)\n\nseq_exec_times = []\n\nfor k in range(9,10): # or toggle to range(10) to compile sequentially\n    start = time.time()\n    my_func(t, k)\n    dur = time.time() - start\n    seq_exec_times.append(dur)\n\ntotal_seq_exec_time = sum(seq_exec_times)\n\nprint(\"Sequential execution times:\")\nprint([\"{:.3e} s\".format(x) for x in seq_exec_times])\nprint(\"Total execution time:\")\nprint(\"{:.3e} s\".format(total_seq_exec_time))\nIf I execute this function the first time only for k=9, then I get a quite long compilation time, which I figure is because tracing a recursive function like this one takes an effort that scales exponentially with recursion depth. The output is:\nSequential execution times:\n['6.306e+01 s']   # First execution time when calling directly with k=9\nTotal execution time:\n6.306e+01 s\nBut then I thought that in practice, I need to evaluate my_func for increasing values of k=0,1,2,3... anyway. And if the lower step has already been traced, then you only need to trace the next level of the tree, and that should be more efficient. And indeed, executing k=1,2,3...,8 before executing k=9 yields a slightly lower execution time the first time k=9 is evaluated:\nSequential execution times:\n['3.797e-03 s',\n'2.203e-02 s',\n'3.487e-02 s',\n'7.054e-02 s',\n'1.779e-01 s',\n'4.680e-01 s',\n'1.326e+00 s',\n'4.145e+00 s',\n'1.456e+01 s',\n'5.550e+01 s']    # First execution time of k=9 after calling k=0,1,2,3...,8 first\nTotal execution time:\n7.631e+01 s\nThat said, this still scales exponentially with recursion depth, and I was naively expecting the compilation of k=9 to be more efficient. If the lower levels k=1,2,3...,8 are already compiled, then I would naively expect the compilation at the next level k=9 to be relatively simple. I would think that you can simply trace the link between k=9 and k=8, and avoid going through the whole recursion tree again at the lower levels.\nBut it looks like I was wrong, and I'm curious to know why? And if I'm not wrong, how do I make this better?\nThis was run with jax - 0.4.33 on MacOS - 15.6.1.",
        "answers": [
            "In general, you should avoid a recursive coding style when using JAX code with JIT, autodiff, or other transformations.\nThere are three different things at play here that complicate the analysis of runtimes:\ntracing: this is the general process used in transforming JAX code, whether for jit or for autodiff like jacfwd . I believe the main reason you are seeing different timings depending on the sequence of executions is because of the trace cache: for each value of k, the function will be traced only once and subsequent calls will use the cached trace.\nautodiff: the jacfwd call in your function retraces the original function and generates a sequence of operations representing the forward-jacobian. I don't believe that there is any cache for this, so each time you call jacfwd the transformation will be recomputed from the cached trace.\ncompilation: I don't believe the that compilation pass currently makes use of previously-compiled units using the trace cache. Any control flow in JAX (loops, recursive calls, etc.) are effectively flattened before being passed to the compiler: in your case the number of operations looks to scale roughly as O[3^k]. Compilation cost is superlinear—and often roughly quadratic—in the number of operations, and so you'll find compilation will become very expensive as k gets larger.\nUnfortunately, there's not really any workaround for these issues. When using JAX, you should avoid deep Python control flow like for loops and recursion. You may be able to make progress by re-expressing your recursive function as an iterative function, using one of JAX's control flow operators like fori_loop to reduce the number of lines and cut down the compilation time."
        ],
        "link": "https://stackoverflow.com/questions/79769647/sequential-compilation-times-of-a-jax-jitted-recursive-function"
    },
    {
        "title": "JAX, recompilation when using closure for a function",
        "question": "I have a jax code where I would like to scan over an array. In the body function of the scan, I have a pytree to store some parameters and functions that I want to apply during the scan. For the scan, I used lambda to bake in the object/pytree named params.\nDoes this trigger a new compilation when a new params is passed in the function example? If so, how can I avoid the recompilation?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\n\nclass Params:\n    def __init__(self, x_array, a):    \n        self.x_array = x_array\n        self.a = a\n        return\n    \n    def one_step(self,state, input):\n        x = state\n        y = input\n        next_state = (self.x_array + x + jnp.ones(self.a))*y\n        return next_state\n\n    def _tree_flatten(self):\n        children = (self.x_array,)\n        aux_data = {'a':self.a}\n        return (children, aux_data)\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n        \ntree_util.register_pytree_node(Params,\n                               Params._tree_flatten,\n                               Params._tree_unflatten)\n\ndef scan_body(params, state, input):\n    x = state\n    y = input \n    x_new = params.one_step(x, y) \n    return x_new, [x_new]\n\n@jax.jit\ndef example(params):\n    body_fun = lambda state, input: scan_body(params, state, input)\n    init_state = jnp.array([0.,1.])\n    input_array = jnp.array([1.,2.,3.])\n    last_state, result_list = jax.lax.scan(body_fun, init_state, input_array)\n    return last_state, result_list\n\nif __name__ == \"__main__\":\n\n    params1 = Params(jnp.array([1.,2.]), 2)\n    last_state, result_list = example(params1)\n    print(last_state)\n\n    params2 = Params(jnp.array([3.,4.]), 2)\n    last_state, result_list = example(params2)\n    print(last_state)",
        "answers": [
            "Passing a new params object would only trigger recompilation if the static attributes of your params were to change. Since aux_data is static, changing the value of params.a will lead to re-compilation. Since children are dynamic, then changing the shape, dtype, or sharding of params.x will lead to recompilation, but changing the array values/contents will not.\nIn your example, in both calls params.x has the same shape, dtype, and sharding, and params.a has the same value, so there should not be any recompilation (if you're unsure, you could confirm this using the approach mentioned at https://stackoverflow.com/a/70127930/2937831).\nNote in particular that the lambda functions used in the method implementations cannot affect the JIT cache key because they are not referenced in the pytree flattening output."
        ],
        "link": "https://stackoverflow.com/questions/79744486/jax-recompilation-when-using-closure-for-a-function"
    },
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\npython\nCopy\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\npython\nCopy\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\npython\nCopy\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Are JAX operations already vectorized?",
        "question": "In the documentation, JAX provides vectorization. However, aren't JAX operations already vectorized? For example, to add two vectors, I thought that the element-wise additions were vectorized internally already.\nMy guess is that vectorization is useful when: it's hard for us to add a dimension for broadcasting, so we resort to a more explicit vectorization.\nEDIT: for example, instead of vectorizing convolution2d with different kernels, I simply stack the kernels, copy and stack the channel, then perform the convolution2d with this stack of kernels.",
        "answers": [
            "I have also raised a similar question here: https://github.com/jax-ml/jax/issues/26212 By now I think there is no universal answer to this and it will remain a matter of taste to a certain degree. However in some cases there is a clearer answer:\nSome operations in JAX are not natively vectorized, such as e.g. jnp.histogram or jnp.bincount, in this case you can use vmap to get a \"batched\" version of that function (for example search for \"batched_histogram\" here http://axeldonath.com/jax-diffusion-models-pydata-boston-2025/). This is really convenient and avoids loops to improve readability as well as performance.\nvmap works over PyTrees. Some libraries (most notably equinox) use this to avoid the need for handling a batch axis in models completely and just finally vmap over the whole parameter tree by convention. This frees developers from thinking about the batch axis at all, but when working with equinox you have to stick to that convention. It also only works if operations are independent across different batches. It does not work for operations such as a \"batch norm\" (see also https://docs.kidger.site/equinox/examples/stateful/)\nIn some cases one introduces a local(!) extra dimension to an array to avoid writing a Python loop and optionally reduce after. This can often be implemented more shortly and with clearer intent using vmap (basically what you said).\nAs broadcasting and batch axes are universally accepted convention in deep learning I mostly stick with them. But I rely on vmap whenever there is no native vectorization, whenever I work with libraries that rely on vmap by convention, or whenever I need to vectorize operations along non-conventional axes (basically everything except batch axis).",
            "It's true that some JAX operations are automatically vectorized. However, not all functions are batch-aware, and even functions which are batch-aware may not have the intrinsic batching semantics you need in your code.\nAs a simple example, consider a function which multiplies two 1D vectors:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(x: jax.Array, y: jax.Array) -> jax.Array:\n  return x @ y\n\nx = jnp.ones(10)\ny = jnp.ones(10)\nresult = f(x, y)\nprint(result)  # 10.0\n10.0\nNow say you'd like to execute this over a batch of x values: due to the batching semantics of the @ operator, this just works:\npython\nCopy\nx_batch = jnp.arange(50).reshape(5, 10)\nresult = f(x_batch, y)\nprint(result)\n[ 45. 145. 245. 345. 445.]\nOn the other hand, if you want to execute this over a batch of y vectors, it won't work out so easily:\npython\nCopy\ny_batch = jnp.arange(50).reshape(5, 10)\nf(x, y_batch)\nTypeError: dot_general requires contracting dimensions to have the same shape, got (10,) and (5,).\nWithout a tool like vmap, you'd have to either re-define the function to work appropriately for batched inputs, or modify the inputs based on what you know of the function's implementation (e.g. pass f(x, y.T) in this case). However, this type of approach can be error-prone and brittle, especially as the function increases in complexity.\nWith jax.vmap, you can automatically generate an appropriately batched version of the function, which saves you the headache of re-implementing things:\npython\nCopy\nf_batched = jax.vmap(f, in_axes=(None, 0))  # x unmapped, y mapped over axis 0\nresult = f_batched(x, y_batch)\nprint(result)\n[ 45. 145. 245. 345. 445.]\nMoreover, this vmap solution will work correctly for essentially any function composed of transformable JAX operations.\nSo even though some JAX operations support implicit batching, the explicit batch transformation provided by vmap can be quite useful in practice."
        ],
        "link": "https://stackoverflow.com/questions/79718029/are-jax-operations-already-vectorized"
    },
    {
        "title": "Does vmap correctly split the RNG keys?",
        "question": "In the following code, when I remove the vmap, I have the right randomized behavior. However, with vmap, I don't anymore. Isn't this supposed to be one of the features of nnx.vmap?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\n@nnx.vmap(\n    in_axes=(None, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((4, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")",
        "answers": [
            "To make dropout work together with vmap in flax, we need to use split_rngs and StateAxes :\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\nbs = 4\n\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=bs, only='dropout')\n@nnx.vmap(\n    in_axes=(state_axes, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\nmodel.train()\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\nOutput with jax: 0.7.0.dev20250704, flax: 0.10.6\nOutput batch:\n[[0.         0.1736668  1.6533196  0.         0.        ]\n [0.         0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.",
            "I'm not sure nnx.vmap and nnx.split_rngs are necessary in vfdev's answer. Also, having a train kwarg is unnecessary in most situations since NNX models can dynamically jump between train=True, train=False with .train() and .eval()\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    x = self.linear(x)\n    x = self.dropout(x)\n    return x\n\nkey = jax.random.PRNGKey(42)\n\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n@nnx.jit\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  return model(x)\n\nbs = 4\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Enable training. This works because Dropout layers have a .deterministic property\n# that can be modified.\nmodel.train()\n\noutput_batch = batched_apply(model, batch_input)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\noutput:\nModel initialized successfully.\nDropout Rate: 0.5\n------------------------------\nInput batch shape: (4, 10)\nInput batch:\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n------------------------------\nRunning the batched model in training mode (dropout is active)...\nOutput batch shape: (4, 5)\n\nOutput batch:\n[[0.         0.1736668  0.         0.         0.        ]\n [0.         0.         1.6533196  1.0752656  0.        ]\n [0.         0.         0.         0.         0.7218913 ]\n [0.09358063 0.         0.         1.0752656  0.        ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.\nand if instead you do model.eval()\nOutput batch:\n[[0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]]\n------------------------------\n❌ Verification failed: All outputs were the same."
        ],
        "link": "https://stackoverflow.com/questions/79698307/does-vmap-correctly-split-the-rng-keys"
    },
    {
        "title": "Configuration options varying between jax installs?",
        "question": "I have a laptop I do work on for a program that includes jax, the program ends up getting run here on a small scale to test it, then it is sent off to a server for batch processing.\nIn the program I have set these flags for jax:\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\n(as well as others but these are the relevant ones)\nThis runs fine on my laptop (using sharding to CPU parallelise), but when running on the server on GPU, I get an error message:\nAttributeError: Unrecognized config option: jax_captured_constants_report_frames\n(and the same for jax_captured_constants_warn_bytes if that were to run first)\nWhy is there this discrepancy? Can I use these flags some other way that is generalised between different jax installs?\npip list | grep jax, on laptop:\njax                       0.6.2\njaxlib                    0.6.2\njaxtyping                 0.3.2\non server:\njax                       0.6.0\njax-cuda12-pjrt           0.6.0\njax-cuda12-plugin         0.6.0\njaxlib                    0.6.0\njaxtyping                 0.3.2\nEDIT: As a side note, what is the scope of jax flags? I have a jax initialisation function to set os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=\" + str(cpu_count()) before the rest of the code runs, if I set jax.config.update(..., ...) options in here, will they hold in files called after it that also import jax? Or do I have to set them again? Is there a function to check the current value of these flags?",
        "answers": [
            "The jax_captured_constants_report_frames and jax_captured_constants_warn_bytes configurations were added in JAX version 0.6.1 (Relevant PR: https://github.com/jax-ml/jax/pull/28157) If you want to use them on your server, you'll have to update JAX to v0.6.1 or later."
        ],
        "link": "https://stackoverflow.com/questions/79693916/configuration-options-varying-between-jax-installs"
    },
    {
        "title": "Jax vmapping while loop [closed]",
        "question": "Closed. This question needs debugging details. It is not currently accepting answers.\nEdit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\nClosed 5 months ago.\nImprove this question\nI have a function that has jax.lax.while_loop. Now, I want to vmap it. However, vmap makes the execution time very slow compared to the original one.\nI understand that in the case of lax.cond, it is transformed into select, which evaluates all branches and thus may decrease the computational speed.\nIs a similar thing happening here? If so, what is the best practice to do do xx while y is true with vmap?",
        "answers": [
            "A while_loop under vmap becomes a single while_loop over a batched body_fun and cond_fun, meaning effectively that every loop in the batch executes for the same number of iterations. If different batches lead to vastly different iteration times, this can result in extra computation compared to executing individual while_loops in sequence."
        ],
        "link": "https://stackoverflow.com/questions/79660448/jax-vmapping-while-loop"
    },
    {
        "title": "Looking for an efficent JAX function to reconstruct an image from patches",
        "question": "I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.\nI am trying to reconstruct the original images from the patches. Here is vanilla python code that works:\npython\nCopy\nkernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)\n\ndef fwd(x):\n    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')\n\n    # return channels last\n    return jnp.transpose(xcv, [0,2,3,1])\n\npatches = fwd(bfrc)\n\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\n\n# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT\n# H_PATCHES == IMG_WIDTH // PATCH_WIDTH\n\nreconstructed = np.zeros(EXPECTED_IMG_SHAPE)\n\nfor vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):\n    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):\n        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):\n            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):\n                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):\n                    row = vpatch * PATCH_HEIGHT + prow\n                    col = hpatch * PATCH_WIDTH + pcol\n                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]\n\n# This assert passes\nassert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0\nOf course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?",
        "answers": [
            "I'm not sure what happened here:\npython\nCopy\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\nbut I assume it's some kind of mistake.\nAssuming bfrc has shape of (batch, channels, height, width), and\npython\nCopy\nV_PATCHES = IMG_HEIGHT // PATCH_HEIGHT\nH_PATCHES = IMG_WIDTH // PATCH_WIDTH\nthen patch_reshaped_pn_c_h_w will have the shape of (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH).\nKeeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.\npython\nCopy\nV, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape\n\nH_total = V * PH\nW_total = H * PW\n\npatches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)\n\nreconstructed = patches.reshape(V, H, PH, PW, C)\nreconstructed = reconstructed.transpose(0, 2, 1, 3, 4)\nreconstructed = reconstructed.reshape(H_total, W_total, C)\nreconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)\nYou can additionally wrap it into @jax.jit, which should be slightly faster."
        ],
        "link": "https://stackoverflow.com/questions/79647350/looking-for-an-efficent-jax-function-to-reconstruct-an-image-from-patches"
    },
    {
        "title": "Would using lists rather than jax.numpy arrays lead to more accurate numerical transformations?",
        "question": "I am doing a project with RNNs using jax and flax and I have noticed some behavior that I do not really understand.\nMy code is basically an optimization loop where the user provides the initial parameters for the system they want to optimize. This system is divided onto several time steps. He feeds the initial input into the first time step of the the system, gets a certain output, feeds this output into a RNN which returns the parameters for the following time step and so on. Then it is optimized using adam (particularly using optax).\nNow the user inputs his initial parameters as a dict and then there is a function called prepare_parameters_from_dict that basically converts this dict into a list of lists (or a list of jnp arrays for that matter).\nMy question/observation is when I make this function return a list of jnp.arrays instead of a list of lists, the property I am optimizing is an order of magnitude worse!\nFor example, using a list of lists outputs 0.9997 and a list of jnp.arrays outputs 0.998 (the closer to one the better).\nNoting: the RNN output a list of jnp.arrays (it is using flax linnen) and everything in the code remains the same.\nHere are said function:\nOutputing list of lists:\npython\nCopy\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(flat_params)\n        shapes.append(len(flat_params))\n    return res, shapes\nUsing list of jnp.arrays:\npython\nCopy\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(jnp.array(flat_params))\n        shapes.append(jnp.array(flat_params).shape[0])\n    return res, shapes\nand this is an example of the users input initial params:\npython\nCopy\ninitial_params = {\n    \"param1\": {\n        \"gamma\": 0.1,\n        \"delta\": -3 * jnp.pi / 2,\n    }\n}\nThe rest of the code remains exactly the same for both.\nAfter optimization if for example there were five time steps, this is how the final optimized params for each time step would look like:\nusing list of jnp.arrays:\npython\nCopy\n[[Array([ 0.1       , -4.71238898], dtype=float64)],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nusing list of lists:\npython\nCopy\n[[ [0.1       , -4.71238898] ]],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nWould such a difference in behavior be due to how jax handles grad and jit and others with lists compared to jnp.arrays or am I missing something?",
        "answers": [
            "The main operative difference between these two cases is that Python floats are treated as weakly-typed, meaning that the list version of your code could result in operations being performed at a lower precision. For example:\npython\nCopy\nIn [1]: import jax\n\nIn [2]: import jax.numpy as jnp\n\nIn [3]: jax.config.update('jax_enable_x64', True)\n\nIn [4]: list_values = [0.1, -4.71238898]\n\nIn [5]: array_values = jax.numpy.array(list_values)\n\nIn [6]: x = jax.numpy.float32(1.0)\n\nIn [7]: x + list_values[1]\nOut[7]: Array(-3.712389, dtype=float32)\n\nIn [8]: x + array_values[1]\nOut[8]: Array(-3.71238898, dtype=float64)\nNotice that the array version leads to higher-precision computations in this case. If I had to guess what the main difference is in your two runs, I'd guess something to do with the precision implied by strict vs weak types."
        ],
        "link": "https://stackoverflow.com/questions/79634990/would-using-lists-rather-than-jax-numpy-arrays-lead-to-more-accurate-numerical-t"
    },
    {
        "title": "JAX Point Cloud Processing: Slow index_points_3d operation causing extreme XLA fusion loops in backpropagation",
        "question": "I'm trying to use JAX for implementing point cloud processing. However, I found that training becomes extremely slow due to my implementation of the following index_points_3d operation, which performs selection of features based on 3D indices.\nHere's my current implementation:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    features_expanded = features[..., None, :]\n    idx_expanded = indices[..., None]\n    return jnp.take_along_axis(features_expanded, idx_expanded, axis=1)\nWhen I traced the profiler, I found that this operation triggers extreme repetitions of loop_dynamic_update_slice_fusion, loop_add_fusion, input_reduce_fusion, and loop_select_fusion in the backpropagation stage as in following.\nThe forward pass is not a problem since the learning went fast when I stopped the gradient of the output features.\nI've tried different implementations such as using vmap on the batch dimension, but failed to achieve any performance gains.\nI'm not deeply familiar with JAX's low-level operations, so I'm unsure if this is a fundamental limitation of JAX/XLA or if there's a more efficient approach. Any help or guidance on optimizing this operation would be greatly appreciated!",
        "answers": [
            "Thanks to jakevdp's comment, I got a significant speedup using one-hot matrix multiplication. I changed to the following code:\npython\nCopy\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    B, N, C = features.shape\n    _, S, K = indices.shape\n    one_hot = jax.nn.one_hot(indices, num_classes=N, dtype=features.dtype)\n    return jnp.einsum('bskn,bnc->bskc', one_hot, features)"
        ],
        "link": "https://stackoverflow.com/questions/79631678/jax-point-cloud-processing-slow-index-points-3d-operation-causing-extreme-xla-f"
    },
    {
        "title": "Why is array manipulation in JAX much slower?",
        "question": "I'm working on converting a transformation-heavy numerical pipeline from NumPy to JAX to take advantage of JIT acceleration. However, I’ve found that some basic operations like broadcast_to and moveaxis are significantly slower in JAX—even without JIT—compared to NumPy, and even for large batch sizes like 3,000,000 where I would expect JAX to be much quicker.\npython\nCopy\n### Benchmark: moveaxis + broadcast_to ###\nNumPy: moveaxis + broadcast_to → 0.000116 s\nJAX: moveaxis + broadcast_to → 0.204249 s\nJAX JIT: moveaxis + broadcast_to → 0.054713 s\n\n### Benchmark: broadcast_to only ###\nNumPy: broadcast_to → 0.000059 s\nJAX: broadcast_to → 0.062167 s\nJAX JIT: broadcast_to → 0.057625 s\nAm I doing something wrong? Are there better ways of performing these kind of manipulations?\nHere's a minimal benchmark ChatGPT generated, comparing broadcast_to and moveaxis in NumPy, JAX, and JAX with JIT:\npython\nCopy\nimport timeit\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit\n\n# Base transformation matrix\nM_np = np.array([[1, 0, 0, 0.5],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1]])\n\nM_jax = jnp.array(M_np)\n\n# Batch size\nn = 1_000_000\n\nprint(\"### Benchmark: moveaxis + broadcast_to ###\")\n\n# NumPy\nt_numpy = timeit.timeit(\n    lambda: np.moveaxis(np.broadcast_to(M_np[:, :, None], (4, 4, n)), 2, 0),\n    number=10\n)\nprint(f\"NumPy: moveaxis + broadcast_to → {t_numpy:.6f} s\")\n\n# JAX\nt_jax = timeit.timeit(\n    lambda: jnp.moveaxis(jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)), 2, 0).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: moveaxis + broadcast_to → {t_jax:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_and_move_jax(M):\n    return jnp.moveaxis(jnp.broadcast_to(M[:, :, None], (4, 4, n)), 2, 0)\n\n# Warm-up\nbroadcast_and_move_jax(M_jax).block_until_ready()\n\nt_jit = timeit.timeit(\n    lambda: broadcast_and_move_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: moveaxis + broadcast_to → {t_jit:.6f} s\")\n\nprint(\"\\n### Benchmark: broadcast_to only ###\")\n\n# NumPy\nt_numpy_b = timeit.timeit(\n    lambda: np.broadcast_to(M_np[:, :, None], (4, 4, n)),\n    number=10\n)\nprint(f\"NumPy: broadcast_to → {t_numpy_b:.6f} s\")\n\n# JAX\nt_jax_b = timeit.timeit(\n    lambda: jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: broadcast_to → {t_jax_b:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_only_jax(M):\n    return jnp.broadcast_to(M[:, :, None], (4, 4, n))\n\nbroadcast_only_jax(M_jax).block_until_ready()\n\nt_jit_b = timeit.timeit(\n    lambda: broadcast_only_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: broadcast_to → {t_jit_b:.6f} s\")",
        "answers": [
            "There are a couple things happening here that come from the different execution models of NumPy and JAX.\nFirst, NumPy operations like broadcasting, transposing, reshaping, slicing, etc. typically return views of the original buffer. In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies. I suspect this is the largest contribution to the timing difference here.\nSecond, NumPy tends to have very fast dispatch time for individual operations. JAX has much slower dispatch time for individual operations, and this can become important when the operation itself is very cheap (like \"return a view of the array with different strides/shape\")\nYou might wonder given these points how JAX could ever be faster than NumPy. The key is JIT compilation of sequences of operations: within JIT-compiled code, sequences of operations are fused so that the output of each individual operation need not be allocated (or indeed, need not even exist at all as a buffer of intermediate values). Additionally, for JIT compiled sequences of operations the dispatch overhead is paid only once for the whole program. Compare this to NumPy where there's no way to fuse operations or to avoid paying the dispatch cost of each and every operation.\nSo in microbenchmarks like this, you can expect JAX to be slower than NumPy. But for real-world sequences of operations wrapped in JIT, you should often find that JAX is faster, even when executing on CPU.\nThis type of question comes up enough that there's a section devoted to it in JAX's FAQ: FAQ: is JAX faster than NumPy?\nAnswering the followup question:\nIs the statement \"In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies\", within a jitted environment?\nThis question is not really well-formulated, because in a jitted environment, array objects do not necessarily correspond to buffers of values. Let's make this more concrete with a simple example:\npython\nCopy\nimport jax\n\n@jax.jit\ndef f(x):\n  y = x[::2]\n  return y.sum()\nYou might ask: in this program, is y a copy or a view of x? The answer is neither, because y is never explicitly created. Instead, JIT fuses the slice and the sum into a single operation: the array x is the input, and the array y.sum() is the output, and the intermediate array y is never actually created.\nYou can see this by printing the compiled HLO for this function:\npython\nCopy\nx = jax.numpy.arange(10)\nprint(f.lower(x).compile().as_text())\npython\nCopy\nHloModule jit_f, is_scheduled=true, entry_computation_layout={(s32[10]{0})->s32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.9 (Arg_0.10: s32[], Arg_1.11: s32[]) -> s32[] {\n  %Arg_0.10 = s32[] parameter(0), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  %Arg_1.11 = s32[] parameter(1), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  ROOT %add.12 = s32[] add(s32[] %Arg_0.10, s32[] %Arg_1.11), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\n%fused_computation (param_0.2: s32[10]) -> s32[] {\n  %param_0.2 = s32[10]{0} parameter(0)\n  %iota.0 = s32[5]{0} iota(), iota_dimension=0, metadata={op_name=\"jit(f)/jit(main)/iota\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.1 = s32[] constant(2)\n  %broadcast.0 = s32[5]{0} broadcast(s32[] %constant.1), dimensions={}\n  %multiply.0 = s32[5]{0} multiply(s32[5]{0} %iota.0, s32[5]{0} %broadcast.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %bitcast.1 = s32[5,1]{1,0} bitcast(s32[5]{0} %multiply.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %gather.0 = s32[5]{0} gather(s32[10]{0} %param_0.2, s32[5,1]{1,0} %bitcast.1), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}, indices_are_sorted=true, metadata={op_name=\"jit(f)/jit(main)/gather\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.0 = s32[] constant(0)\n  ROOT %reduce.0 = s32[] reduce(s32[5]{0} %gather.0, s32[] %constant.0), dimensions={0}, to_apply=%region_0.9, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\nENTRY %main.14 (Arg_0.1: s32[10]) -> s32[] {\n  %Arg_0.1 = s32[10]{0} parameter(0), metadata={op_name=\"x\"}\n  ROOT %gather_reduce_fusion = s32[] fusion(s32[10]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\nThe output is complicated, but the main thing to look at here is the ENTRY %main section, which is the \"main\" program generated by compilation. It consists of two steps: %Arg0.1 identifies the input argument, and ROOT %gather_reduce_fusion is essentially a single compiled kernel that sums every second element of the input. No intermediate arrays are generated. The blocks above this (e.g. the %fused_computation (param_0.2: s32[10]) -> s32[] definition) give you information about what operations are done within this kernel, but represent a single fused operation.\nNotice that the sliced array represented by y in the Python code never actually appears in the main function block, so questions about its memory layout cannot be answered except by saying \"y doesn't exist in the compiled program\".",
            "According to the Jax Docs (emphasis mine):\nif you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead"
        ],
        "link": "https://stackoverflow.com/questions/79615872/why-is-array-manipulation-in-jax-much-slower"
    },
    {
        "title": "DIfference in variable values in jax non-jit runtime and jit transformed runtime",
        "question": "I have a deep learning mode which I am running in the jit transformed manner by:\npython\nCopy\nmy_function_checked = checkify.checkify(model.apply)\n    model_jitted = jax.jit(my_function_checked)\n    err, pred = model_jitted({\"params\": params}, batch, training=training, rng=rng)\n    err.throw()\nThe code is compiling fine, but now I want to debug the intermediate values after every few steps, save the arrays, and then compare them with pytorch tensors. For this, I need to repeatedly save the arrays. The easiest way to do this is to use any IDE's inbuilt debugger and evaluate the save expression after every few steps. But jax.jit transformed code doesn't allow external debuggers. But, I can do this after disabling the jit. Should I be expecting any discrepancies between the two runs? Can I assume that the values in jit and non-jit runs will remain same?",
        "answers": [
            "In general when comparing the same JAX operation with and without JIT, you should expect equivalence up to typical floating point rounding errors, but you should not expect bitwise equivalence, as the compiler may fuse operations in a way that leads to differing float error accumulation."
        ],
        "link": "https://stackoverflow.com/questions/79571227/difference-in-variable-values-in-jax-non-jit-runtime-and-jit-transformed-runtime"
    },
    {
        "title": "Why is Jax treating floating point values as tracers rather than concretizing them when nesting jitted functions?",
        "question": "I am doing some physics simulations using jax, and this involves a function called the Hamiltonian defined as follows:\npython\nCopy\n# Constructing the Hamiltonian\n@partial(jit, static_argnames=['n', 'omega'])\ndef hamiltonian(n: int, omega: float):\n    \"\"\"Construct the Hamiltonian for the system.\"\"\"\n    H = omega *  create(n) @ annhilate(n)\n    return H \nand then a bigger function def solve_diff(n, omega, kappa, alpha0): that is defined as follows:\npython\nCopy\n@partial(jit, static_argnames=['n', 'omega'])\ndef solve_diff(n, omega, kappa, alpha0):\n    # Some functionality that uses kappa and alpha0\n    \n    H = hamiltonian(n, omega)\n\n    # returns an expectation value\nWhen I try to compute the gradient of this function using jax.grad\npython\nCopy\nn = 16   \nomega = 1.0   \nkappa = 0.1  \nalpha0 = 1.0 \n\n# Compute gradients with respect to omega, kappa, and alpha0\ngrad_population = grad(solve_diff, argnums=(1, 2, 3))\ngrads = grad_population(n, omega, kappa, alpha0)\n\nprint(f\"Gradient w.r.t. omega: {grads[0]}\")\nprint(f\"Gradient w.r.t. kappa: {grads[1]}\")\nprint(f\"Gradient w.r.t. alpha0: {grads[2]}\")\nit outputs the following error:\npython\nCopy\nValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jax._src.interpreters.ad.JVPTracer'>, Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n  primal = 1.0\n  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n    pval = (ShapedArray(float32[], weak_type=True), None)\n    recipe = LambdaBinding(). The error was:\nTypeError: unhashable type: 'JVPTracer'\nThough, running solve_diff(16,1.0,0.1,1.0) on its own works as expected.\nNow if I remove omega from the list of static variables for both the hamiltonian function and the solve_diff, the grad is output as expected.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls, both n and omega are constants and indeed should not change between function calls.",
        "answers": [
            "The fundamental issue is that you cannot differentiate with respect to a static variable, and if you try to do so you will get the error you observed.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls\nIn JAX, the term \"static\" does not have to do with whether the variable is changed between function calls. Rather, a static variable is a variable that does not participate in tracing, which is the mechanism used to compute transformations like vmap, grad, jit, etc. When you differentiate with respect to a variable, it is no longer static because it is participating in the autodiff transformation, and trying to treat it as static later in the computation will lead to an error.\nFor a discussion of transformations, tracing, and related concepts, I'd start with JAX Key Concepts: transformations."
        ],
        "link": "https://stackoverflow.com/questions/79550040/why-is-jax-treating-floating-point-values-as-tracers-rather-than-concretizing-th"
    },
    {
        "title": "How to make a custom pytree node works with grad in JAX",
        "question": "I created a custom pytree node following this tutorial. My custom pytree node works for many operations and transformations, except grad.\nHere is my code.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\n\nclass MyLinear:\n    def __init__(self, w, b):\n        self.w = jnp.array(w)\n        self.b = jnp.array(b)\n\n    def __call__(self, x):\n        return jnp.dot(x, self.w) + self.b\n\n    def tree_flatten(self):\n        return (self.w, self.b), ()\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n\ntree_util.register_pytree_node(MyLinear, MyLinear.tree_flatten, MyLinear.tree_unflatten)\nThen I tested this code\npython\nCopy\ninputs = jnp.ones((2,2))\ninput = inputs[0]\nmylinear = MyLinear([1.0, 1.0], 1.)\nprint(tree_util.tree_map(lambda x: x+1, input)) # [2. 2.] works as expected\nprint(jax.vmap(mylinear)(inputs))   # [3. 3.] works as expected\nprint(jax.jit(jax.vmap(mylinear))(inputs))  # [3. 3.] works as expected\ndef loss_fn(model, x):\n    out = jax.vmap(model)(x)\n    return jnp.sum(out ** 2)\nprint(loss_fn(mylinear, inputs))    # 18.0 works as expected\nprint(jax.grad(loss_fn)(mylinear, inputs))  # <__main__.MyLinear object at 0x10e317890> doesn't work as expected\nIt seems grad doesn't recognize object of MyLinear as a flattenable tree like object of tuple, list, or dict. What should the code be so that objects of this class recognizable by all jax transformations? Thank you for the help.",
        "answers": [
            "I think there is a misunderstanding here. From what I can tell the code works exactly as intended. JAX operates on \"structs of arrays\". So your MyLinear class works as a data container for arrays, as well as their gradients. When applying jax.grad() to a PyTree, JAX will return the same PyTree, but containing the gradients of the corresponding nodes. So you can access the individual gradient like so:\npython\nCopy\ninputs = jnp.ones((2,2))\nmylinear = MyLinear([1.0, 1.0], 1.)\n\ndef loss_fn(model, x):\n    out = jax.vmap(model)(x)\n    return jnp.sum(out ** 2)\n\ngrads = jax.grad(loss_fn)(mylinear, inputs)\nprint(grads.w)\nprint(grads.b)\nI hope this clarifies the behavior!"
        ],
        "link": "https://stackoverflow.com/questions/79522105/how-to-make-a-custom-pytree-node-works-with-grad-in-jax"
    },
    {
        "title": "How can I apply member functions of a list of objects across slices of a JAX array using vmap?",
        "question": "I have a list of a objects, each of which has a function to be applied on a slice of a jax.numpy.array. There are n objects and n corresponding slices. How can I vectorise this using vmap?\nFor example, for the following code snippet:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nclass Obj:\n    def __init__(self, i):\n        self.i = i\n\n    def f1(self, x): return (x - self.i)\n\nx = jnp.arange(9).reshape(3, 3).astype(jnp.float32)\n\nfunctions_obj = [Obj(1).f1, Obj(2).f1, Obj(3).f1]\nhow would I apply the functions in functions_obj to slices of x?\nMore details, probably not relevant: My specific use-case is running the member functions of a lot of Reinforcement Learning Gym environment objects on slices of an actions array, but I believe my problem is more general and I formulated it as above. (P.S.: I know about AsyncVectorEnv by the way but that does not solve my problem as I am not trying to run the step function).",
        "answers": [
            "Use jax.lax.switch to select between the functions in the list and map over the desired axis of x at the same time:\npython\nCopy\ndef apply_func_obj(i, x_slice):\n    return jax.lax.switch(i, functions_obj, x_slice)\n\nindices = jnp.arange(len(functions_obj)) \n# Use vmap to apply the function element-wise\nresults = jax.vmap(apply_func_obj, in_axes=(0, 0))(indices, x)"
        ],
        "link": "https://stackoverflow.com/questions/79499056/how-can-i-apply-member-functions-of-a-list-of-objects-across-slices-of-a-jax-arr"
    },
    {
        "title": "Jax numpy extracting non-nan values gives NonConcreteBooleanIndexError",
        "question": "I have a jax 2d array with some nan-values\npython\nCopy\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\nand want to get an array which contains for each row only the non-nan values. The resulting array has thus the same number of rows, and either less columns or the same number but with nan values padded at the end. So in this case, the result should be\npython\nCopy\narray_2d = jnp.array([\n    [1,   2,      3],\n    [10  20,jnp.nan]\n    ])\nThe order (among non-nan values) should stay the same.\nTo make things easier, I know that each row has at most k (in this case 3) non-nan values. Getting the indices for the non-nan values is very easy, but ``moving them to the front'' is harder.\nI tried to work on a row-by-row basis; the following function works indeed:\npython\nCopy\n# we want to vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\nHowever, I can't vmap this. Even though I payed attention that the returned array always has the same size, the line ret_vals = row_vals[row_mask] makes problems, since this has a dynamic size. Does anyone know how to circumvent this? I believe that functions like `jnp.where' etc don't help either.\nHere is the full MWE:\npython\nCopy\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n# we want to get -- efficiently -- all non-nan values per row.\n# we know that each row has at most 3 non-nan values\n\n# we will vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\n\n# we now vmap\nnon_nan_vals = jax.vmap(get_non_nan_values)(array_2d) # this gives error: NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[5])\nNB: The array will be very large in practice and have many nan values, while k (the number of non-nan values) is on the order of 10 or 100.\nThank you very much!",
        "answers": [
            "By padding the array with a fill value at the end of each row first, you can rely on jnp.nonzero and its size and fill_value arguments, which define a fixed output size and fill value index, when the size requirement is not met. Here is a minimal example:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n\n@jax.vmap\ndef get_non_nan_values(row_vals, size=3):\n    padded = jnp.pad(row_vals, (0, 1), constant_values=jnp.nan)\n    non_nan = jnp.nonzero(~jnp.isnan(padded), size=size, fill_value=-1)\n    return padded[non_nan]\n\nget_non_nan_values(array_2d)\nWhich returns:\npython\nCopy\nArray([[ 1.,  2.,  3.],\n       [10., 20., nan]], dtype=float32)\nI think this solution is a bit more compact and clearer in intend, however I have not checked the performance.\nI hope this helps!",
            "I think you can do what you want with this function, which rather than sorting the array (as I commented), sorts and masks the indices of the non-nan values:\npython\nCopy\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(1,))\ndef func(array, k=3):\n    m, n = array.shape[-2:]\n    indices = jnp.broadcast_to(jnp.arange(n)[None, :], (m, n))\n    sorted_masked_indices = jnp.sort(jnp.where(jnp.isnan(array), jnp.nan, indices))\n    array_rearranged = array[jnp.arange(m)[:, None], sorted_masked_indices.astype(int)]\n    return jnp.where(jnp.isnan(sorted_masked_indices), jnp.nan, array_rearranged)[:, :k]\nTest:\npython\nCopy\nimport numpy as np\nrng = np.random.default_rng(0)\nk = 3\n\na = rng.random((12, 6))\na[np.arange(12)[:, None], rng.integers(0, 6, (12, 6))] = np.nan\n\nprint(a)\nprint(func(a, k=k))\nGives:\npython\nCopy\n[[0.63696169        nan        nan 0.01652764 0.81327024        nan]\n [       nan 0.72949656        nan        nan 0.81585355        nan]\n [       nan 0.03358558        nan        nan        nan        nan]\n [0.29971189        nan        nan        nan        nan 0.64718951]\n [       nan        nan        nan 0.98083534        nan 0.65045928]\n [       nan        nan 0.13509651 0.72148834        nan        nan]\n [       nan 0.88948783 0.93404352 0.3577952         nan        nan]\n [       nan 0.33791123 0.391619   0.89027435        nan        nan]\n [       nan 0.83264415        nan        nan 0.87648423        nan]\n [0.33611706        nan        nan 0.79632427        nan 0.0520213 ]\n [       nan        nan 0.09075305 0.58033239        nan        nan]\n [       nan 0.94211311        nan        nan 0.62910815        nan]]\n[[0.6369617  0.01652764 0.8132702 ]\n [0.72949654 0.81585354        nan]\n [0.03358557        nan        nan]\n [0.29971188 0.6471895         nan]\n [0.9808353  0.6504593         nan]\n [0.1350965  0.72148836        nan]\n [0.88948786 0.9340435  0.3577952 ]\n [0.33791122 0.391619   0.89027435]\n [0.83264416 0.8764842         nan]\n [0.33611706 0.79632425 0.0520213 ]\n [0.09075305 0.5803324         nan]\n [0.9421131  0.62910813        nan]]",
            "With the stable=True option, argsort on a boolean array is guaranteed to preserve the relative order between True and False elements. So this should do the trick:\npython\nCopy\ndef get_non_nan_values(row_vals):\n    return row_vals[jnp.argsort(jnp.isnan(rowvals), stable=True)[:3]]\nHowever, for wide rows, sorting the entire row seems unnecessary when we already know there are only at most 3 non-nan values. So another simple approach using jax.lax.top_k:\npython\nCopy\ndef get_top_3_non_nan(row_vals):\n  return row_vals[jax.lax.top_k(~jnp.isnan(row_vals), 3)[1]]",
            "I would do this using vmap of argsort of isnan:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n])\n\nresult = jax.vmap(lambda x: x[jnp.argsort(jnp.isnan(x))])(array_2d)\nprint(result)\n# [[ 1.  2.  3. nan nan]\n#  [10. 20. nan nan nan]]\nThis approach uses static shapes, and thus will be compatible with jit."
        ],
        "link": "https://stackoverflow.com/questions/79443943/jax-numpy-extracting-non-nan-values-gives-nonconcretebooleanindexerror"
    },
    {
        "title": "Problems when boolean indexing in Jax, getting NonConcreteBooleanIndexError",
        "question": "I'm currently trying to create a CustomProblem inheriting from the BaseProblem class in TensorNEAT which is a Jax based library. In trying to implement the evaluate function of this class, I'm using a boolean mask, but I have problems getting it to work. My code results in jax.errors.NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[n,n]) which I think is due to some of my arrays not having a definite shape. How do I circumvent this?\nConsider this example in np:\npython\nCopy\nimport numpy as np\n\nran_int = np.random.randint(1, 5, size=(2, 2))\nprint(ran_int)\n\nran_bool = np.random.randint(0,2, size=(2,2), dtype=bool)\nprint(ran_bool)\n\na = (ran_int[ran_bool]>0).astype(int)\nprint(a)\nIt could give an output like this:\npython\nCopy\n[[2 2]\n [3 4]]\n[[ True False]\n [ True  True]]\n[1 1 1] #Is 1D and has less elements than before boolean mask was applied!\nBut in Jax, the same way of thinking results in the NonConcreteBooleanIndexError error I got.\npython\nCopy\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #ERROR HAPPENS HERE\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #WOULD HAPPEN HERE TOO IF THIS PART WAS FIRST\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI was considering using jnp.where to solve the issue, but the resulting pairwise_labels and pairwise_predictions have a different shape than what I expect (namely (n, n)) as seen in the code below:\npython\nCopy\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) # shape (n, n)\n\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) # shape (n, n)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n ,n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI fear that the differing shapes of pairwise_predictions and pairwise_labels after using jnp.where will result in a different loss than if I had just used the boolean mask as I would in np. There is also the fact that I get another error that happens later in the pipeline with the output ValueError: max() iterable argument is empty from line 143 in the pipeline.py file of TensorNeat. This is curiously circumvented by changing pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold to pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold, which probably also results in some loss that is incorrect.\nBelow is some code that should be enough to setup a minimal running example that is similar to my setup:\npython\nCopy\nfrom tensorneat import algorithm, genome, common\nfrom tensorneat.pipeline import Pipeline\nfrom tensorneat.genome.gene.node import DefaultNode\nfrom tensorneat.genome.gene.conn import DefaultConn\nfrom tensorneat.genome.operations import mutation\nimport jax, jax.numpy as jnp\nfrom tensorneat.problem import BaseProblem\n\ndef binary_cross_entropy(prediction, target):\n    return -(target * jnp.log(prediction) + (1 - target) * jnp.log(1 - prediction))\n\n# Define the custom Problem\nclass CustomProblem(BaseProblem):\n\n    jitable = True  # necessary\n\n    def __init__(self, inputs, labels, threshold):\n        self.inputs = jnp.array(inputs) #nb! already has shape (n, 768)\n        self.labels = jnp.array(labels).reshape((-1,1)) #nb! has shape (n), must be transformed to have shape (n, 1) \n        self.threshold = threshold\n\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold #this is the thing I actually want\n        #pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold #weird fix to circumvent ValueError: max() iterable argument is empty when using jnp.where for pairwise_labels and pairwise_predictions\n        print(pairs_to_keep.shape)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape)\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\n\n    @property\n    def input_shape(self):\n        # the input shape that the act_func expects\n        return (self.inputs.shape[1],)\n\n    @property\n    def output_shape(self):\n        # the output shape that the act_func returns\n        return (1,)\n\n    def show(self, state, randkey, act_func, params, *args, **kwargs):\n        # showcase the performance of one individual\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(state, params, self.inputs)\n\n        loss = jnp.mean(jnp.square(predict - self.labels))\n\n        n_elements = 5\n        if n_elements > len(self.inputs):\n            n_elements = len(self.inputs)\n\n        msg = f\"Looking at {n_elements} first elements of input\\n\"\n        for i in range(n_elements):\n            msg += f\"for input i: {i}, target: {self.labels[i]}, predict: {predict[i]}\\n\"\n        msg += f\"total loss: {loss}\\n\"\n        print(msg)\n\nalgorithm = algorithm.NEAT(\n    pop_size=10,\n    survival_threshold=0.2,\n    min_species_size=2,\n    compatibility_threshold=3.0,  \n    species_elitism=2,  \n    genome=genome.DefaultGenome(\n        num_inputs=768,\n        num_outputs=1,\n        max_nodes=769,  # must at least be same as inputs and outputs\n        max_conns=768,  # must be 768 connections for the network to be fully connected\n        output_transform=common.ACT.sigmoid,\n        mutation=mutation.DefaultMutation(\n            # no allowing adding or deleting nodes\n            node_add=0.0,\n            node_delete=0.0,\n            # set mutation rates for edges to 0.5\n            conn_add=0.5,\n            conn_delete=0.5,\n        ),\n        node_gene=DefaultNode(),\n        conn_gene=DefaultConn(),\n    ),\n)\n\n\nINPUTS = jax.random.uniform(jax.random.PRNGKey(0), (100, 768)) #the input data x\nLABELS = jax.random.uniform(jax.random.PRNGKey(0), (100)) #the annotated labels y\n\nproblem = CustomProblem(INPUTS, LABELS, 0.25)\n\nprint(\"Setting up pipeline and running it\")\nprint(\"-----------------------------------------------------------------------\")\npipeline = Pipeline(\n    algorithm,\n    problem,\n    generation_limit=1,\n    fitness_target=1,\n    seed=42,\n)\n\nstate = pipeline.setup()\n# run until termination\nstate, best = pipeline.auto_run(state)\n# show results\npipeline.show(state, best)",
        "answers": [
            "The solution I got from the authors of TensorNEAT was to update the evaluate() function to use jnp.nan instead of -jnp.inf in the first jnp.where() calls used on pairwise_labels and pairwise_predictions. I also had to make the loss take into consideration the nan values that would be present in the loss after running the bce. The new evaluate() function that has the same behavior as boolean indexing is pasted below.\npython\nCopy\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold\n\n        #finding only the labels to keep\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, jnp.nan) #use jnp.nan here\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n\n        #finding only the predictions to keep\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, jnp.nan) #use jnp.nan here\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # loss with shape (len(labels), len(labels)), we need to reduce it to a scalar\n        loss = jnp.mean(loss, where=~jnp.isnan(loss)) #only use number values in loss\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss        \n        return -loss",
            "Yes, the mask operation makes the shape of the resulting array dependent on the content of the array. And jax only supports static shapes. The workaround you propose looks reasonable, with using the value -inf as a placeholder. The missing part is ignoring the zero entries in the mean. This you could achieve by a custom “masked” mean function along the lines of:\nfrom jax import numpy as jnp\nfrom jax import random\nimport jax\n\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (4, 4))\n\nkey, subkey = random.split(key)\nmask = random.bernoulli(key, 0.5, (4, 4))\n\n@jax.jit\ndef masked_mean(x, mask):\n    return jnp.sum(jnp.where(mask, x, 0), axis=0) / jnp.sum(mask, axis=0)\n\n\nmasked_mean(x, mask)\nI have not checked other parts of the code in detail, but e.g. the statement jnp.where(pairwise_labels > 0, True, False) has no effect. And with the masked mean you might not need the placeholder values at all.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79423352/problems-when-boolean-indexing-in-jax-getting-nonconcretebooleanindexerror"
    },
    {
        "title": "How to use jax.vmap with a tuple of flax TrainStates as input?",
        "question": "I am setting up a Deep MARL framework and I need to assess my actor policies. Ideally, this would entail using jax.vmap over a tuple of actor flax TrainStates. I have tried the following:\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.linen.initializers import constant, orthogonal\nfrom flax.training.train_state import TrainState\nimport optax\nimport distrax\n\nclass PGActor_1(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 4\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(128, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0))(x)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nclass PGActor_2(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 2\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nstate= jnp.zeros((1, 5))\n\nnetwork_1 = PGActor_1()\nnetwork_1_init_rng = jax.random.PRNGKey(42)\nparams_1 = network_1.init(network_1_init_rng, state)\n\nnetwork_2 = PGActor_2()\nnetwork_2_init_rng = jax.random.PRNGKey(42)\nparams_2 = network_2.init(network_2_init_rng, state)\n\ntx = optax.chain(\noptax.clip_by_global_norm(1),\noptax.adam(lr=1e-3)\n)\nactor_trainstates= (\n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_1),             \n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_2)\n )\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(actor_trainstates)\nbut I recieve the following error:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nDoes anybody have any idea how to make this work?\nThank you in advance.",
        "answers": [
            "This is quite similar to other questions (e.g. Jax - vmap over batch of dataclasses). The key point is that JAX transformations like vmap require data in a struct of arrays pattern, whereas you are using an array of structs pattern.\nTo work directly with an array of structs pattern in JAX, you can use Python's built-in map function – due to JAX's asynchronous dispatch, the resulting operations will be executed in parallel where possible:\npython\nCopy\npis = map(lambda x: x.apply_fn(x.params, state), actor_trainstates)\nHowever, this doesn't take advantage of the automatic vectorization done by vmap. In order to do this, you can convert your data from an array of structs to a struct of arrays, although this requires that all entries have the same structure.\nFor compatible cases, the solution would look something like this, however it errors for your data:\npython\nCopy\ntrain_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(train_states_soa)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-36-da904fa40b9c> in <cell line: 0>()\n----> 1 train_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\n\nValueError: Dict key mismatch; expected keys: ['Dense_0', 'Dense_1', 'Dense_2']\nThe problem is that your two train states do not have matching structure, and so they cannot be transformed into a single struct of arrays. You can see the difference in structure by inspecting the params:\npython\nCopy\nprint(actor_trainstates[0].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1', 'Dense_2'])\nprint(actor_trainstates[1].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1'])\nThere is no way to use vmap in a context where your inputs have different structure, so you'll either have to change the problem to ensure the same structure, or stick with the map approach."
        ],
        "link": "https://stackoverflow.com/questions/79405049/how-to-use-jax-vmap-with-a-tuple-of-flax-trainstates-as-input"
    },
    {
        "title": "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?",
        "question": "Is it possible to use vmap for auto-batching if your function isn't jittable?\nI have a function that's not jittable:\ndef testfunc(model, x1, x2, x2_mask):\n    ( ... non-jittable stuff with masks ... )\nI'm trying to wrap it in vmap so I can benefit from auto-batching as explained here.\nSo I do:\ntestfunc_batched = jax.vmap(testfunc, in_axes=(None, 0, 0, 0))\nThe intention is that in batched mode, each of x1, x2, and x2_mask will have an additional outter dimension, the batching dimension. The model shouldn't be treated differently in batched mode hence the None. Let me know if the syntax isn't right.\nI create batches of size one just to test, schematically:\nx1s = x1.reshape(1, ...)\nx2s = x2.reshape(1, ...)\nx2_masks = x2_mask.reshape(1, ...)\n\ntestfunc_batched(model, x1s, x2s, x2_masks)\nThe last line fails with ConcretizationTypeError.\nI've recently learned that stuff with masks makes functions not jittable. But does that mean that I also can't use vmap? Or am I doing something wrong?\n(There is further context in How to JIT code involving masked arrays without NonConcreteBooleanIndexError?, but you don't have to read that question to understand this one.)",
        "answers": [
            "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?\nNo. In general, functions which are incompatible with jit will also be incompatible with vmap, because both jit and vmap use the same JAX tracing mechanism to transform the program."
        ],
        "link": "https://stackoverflow.com/questions/79374152/is-it-possible-to-use-jax-vmap-for-auto-batching-if-your-function-isnt-jittable"
    },
    {
        "title": "Count onto 2D JAX coordinates of another 2D array",
        "question": "I have\nx = jnp.zeros((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\nI would like to count onto x how many times each of the individual (x,y) coordinates appear in coords. In other words, obtain the output:\nArray([[0., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI've tried x.at[coords].add(1) and this gives me:\nArray([[0., 0., 0., 0., 0.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI understand what it's doing, but not how to make it do the thing I want.\nThere's this related question[1], but I haven't been able to use it to solve my problem.\n[1] Update JAX array based on values in another array",
        "answers": [
            "For multiple indices, you should pass a tuple of index arrays:\npython\nCopy\nx = x.at[coords[:, 0], coords[:, 1]].add(1)\nprint(x)\n[[0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]",
            "The generalized operation is basically computing a histogram, especially when the coordinate arrays are float values. So depending on the context the code is used, the following alternative might communicate the intent a bit more clearly:\npython\nCopy\nfrom jax import numpy as jnp\n\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\n\nbins = jnp.arange(5 + 1) - 0.5 \nx, _ = jnp.histogramdd(coords, bins=(bins, bins))\nIt will also handle if coordinates are out of bounds. But I presume under the hood, it does the same operation as at[...].add(1). So I would not expect any relevant difference in performance."
        ],
        "link": "https://stackoverflow.com/questions/79370053/count-onto-2d-jax-coordinates-of-another-2d-array"
    },
    {
        "title": "How Can I Use GPU to Accelerate Image Augmentation?",
        "question": "When setting up image augmentation pipelines using keras.layers.Random* or other augmentation or processing methods, we often integrate these pipelines with a data loader, such as the tf.data API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.\nTo address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?",
        "answers": [
            "We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in keras. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.\nClassification\nLet’s take a classification task as an example. If we use the tf.data API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.\npython\nCopy\nimport numpy as np\nfrom keras import layers\n\na = np.ones((4, 224, 224, 3)).astype(np.float32)\nb = np.ones((4, 2)).astype(np.float32)\n\naugmentation_layers = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.2),\n    ]\n)\n\ndataset = tf.data.Dataset.from_tensor_slices((a, b))\ndataset = dataset.batch(3, drop_remainder=True)\ndataset = dataset.map(\n    lambda x, y: (augmentation_layers(x), y), \n    num_parallel_calls=tf.data.AUTOTUNE\n)\nx.shape, y.shape\n(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))\nBut for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.\npython\nCopy\ninputs = keras.Input(shape=(224, 224, 3))\nprocessed = augmentation_layers(inputs)\nbackbone = keras.applications.EfficientNetB0(\n    include_top=True, pooling='avg'\n)(processed)\noutput = keras.layers.Dense(10)(backbone)\nmodel = keras.Model(inputs, output)\nmodel.count_params() / 1e6\n5.340581\nHere, we set the augmentation pipeline right after keras.Input. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.\nSegmentation\nI'll use this dataset for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using keras-3, which might allow for multi-backend support.\npython\nCopy\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # torch, jax\n\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nkeras.__version__ # 3.4.1\npython\nCopy\n# ref https://keras.io/examples/vision/oxford_pets_image_segmentation/\n# u-net model\ndef get_model(img_size, num_classes, classifier_activation):\n    ...\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(\n        num_classes, \n        3, \n        activation=classifier_activation, \n        padding=\"same\", \n        dtype='float32'\n    )(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\nimg_size = (224, 224)\nnum_classes = 1\nclassifier_activation = 'sigmoid'\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nLet's define the augmentation pipelines.\npython\nCopy\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\")\n]\n\ndef augment_data(images, masks):\n    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = tf.cast(combined[..., 3:], tf.int32)\n    return images_augmented, masks_augmented\nLet’s define the tf.data API to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.\npython\nCopy\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    \n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = tf.cast(image, tf.int32)\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.\n        \n    return image\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask  = read_image(mask_list, mask=True)\n    return image, mask\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE) \n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n    # Augmenting on CPU\n    dataset = dataset.map(\n        augment_data, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\npython\nCopy\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nprint(\"Train Dataset:\", train_dataset)\nTrain Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))>\nNow, let's compile it and run it.\npython\nCopy\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nmodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n\n%%time\nepochs = 2\nmodel.fit(\n    train_dataset, \n    epochs=epochs, \n)\nEpoch 1/2\n318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087\nEpoch 2/2\n318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338\nCPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s\nWall time: 1min 48s\nNext, we will remove the augmentation layers from the dataloader.\npython\nCopy\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE)\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nTo offload augmentation to the GPU, we’ll create a custom model class, override the train_step, and use the augment_data method that we defined earlier. Here's how to structure it:\npython\nCopy\nclass ExtendedModel(keras.Model):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def call(self, inputs):\n        return self.model(inputs)\n\n    def save(\n        self, filepath, \n        overwrite=True, \n        include_optimizer=True, \n        save_format=None, \n        add_loss=None, \n    ):\n        # Overriding this method will allow us to use the `ModelCheckpoint`\n        self.model.save(\n            filepath=filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            include_optimizer=include_optimizer,\n        )\nNow that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.\npython\nCopy\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nemodel = ExtendedModel(model)\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nemodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\npython\nCopy\n%%time\nepochs = 2\nemodel.fit(\n    train_dataset, \n    epochs=epochs, \n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            filepath='model.{epoch:02d}-{loss:.3f}.keras',\n            monitor='loss',\n            mode='min',\n            save_best_only=True\n        )\n    ]\n)\nEpoch 1/2\n318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748\nEpoch 2/2\n318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585\nCPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s\nWall time: 1min 29s\nSo, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around 18.35% improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.\nAs shown in the ExtendedModel class above, we override the save method, allowing the callbacks.ModelCheckpoint to save the full model. Inference can then be performed as shown below.\npython\nCopy\nloaded_model = keras.saving.load_model(\n    \"/kaggle/working/model.02-0.0585.keras\"\n)\nx, y = next(iter(train_dataset))\noutput = loaded_model.predict(x)\n1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step\nUpdate\nIn order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the augment_data that is used in ExtendedModel use the following backend agnostic keras.ops functions.\npython\nCopy\ndef augment_data(images, masks):\n    combined = keras.ops.concatenate(\n        [images, keras.ops.cast(masks, 'float32')], axis=-1\n    )\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')\n    return images_augmented, masks_augmented\nAdditionally, to make the pipeline flexible for all backend, we can update the ExtendedModel as follows. Now, this code can run with tensorflow, jax, and torch backends.\npython\nCopy\nclass ExtendedModel(keras.Model):\n    ...\n\n    def train_step(self, *args, **kwargs):\n        if keras.backend.backend() == \"jax\":\n            return self._jax_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"tensorflow\":\n            return self._tensorflow_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"torch\":\n            return self._torch_train_step(*args, **kwargs)\n\n    def _jax_train_step(self, state, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step(state, (x, y))\n\n    def _tensorflow_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def _torch_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    ..."
        ],
        "link": "https://stackoverflow.com/questions/79327723/how-can-i-use-gpu-to-accelerate-image-augmentation"
    },
    {
        "title": "Efficiently custom array creation routines in JAX",
        "question": "I'm still getting a handle of best practices in jax. My broad question is the following:\nWhat are best practices for the implementation of custom array creation routines in jax?\nFor instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):\npython\nCopy\nimport numpy as np\nimport jax.numpy as jnp\n\ndef ones_at_col(shape_mat, idx):\n    idxs = jnp.arange(shape_mat[1])[None,:]\n    mat = jnp.where(idx==idxs, 1, 0)\n    mat = jnp.repeat(mat, shape_mat[0], axis=0)\n    return mat\n\nshape_mat = (5,10)\n\nprint(ones_at_col(shape_mat, 5))\n\n%timeit np.zeros(shape_mat)\n\n%timeit jnp.zeros(shape_mat)\n\n%timeit ones_at_col(shape_mat, 5)\nThe output is\npython\nCopy\n[[0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]]\n127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nMy function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.\nBut then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?\nIs there something better I can do or am I pushing the limits of what is possible in jax?",
        "answers": [
            "The best way to do this is probably something like this:\npython\nCopy\nmat = jnp.zeros(shape_mat).at[:, 5].set(1)\nRegarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.\nIf you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):\npython\nCopy\n@partial(jax.jit, static_argnames=['shape_mat', 'idx'])\ndef ones_at_col(shape_mat, idx):\n  return jnp.zeros(shape_mat).at[:, idx].set(1)\nYou could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.\nAnother side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects."
        ],
        "link": "https://stackoverflow.com/questions/79256001/efficiently-custom-array-creation-routines-in-jax"
    },
    {
        "title": "Why is JAX's jit compilation slower on the second run in my example?",
        "question": "I am new to using JAX, and I’m still getting familiar with how it works. From what I understand, when using Just-In-Time (JIT) compilation (jax.jit), the first execution of a function might be slower due to the compilation overhead, but subsequent executions should be faster. However, I am seeing the opposite behavior.\nIn the following code snippet:\npython\nCopy\nfrom icecream import ic\nimport jax\nfrom time import time\nimport numpy as np\n\n\n@jax.jit\ndef my_function(x, y):\n    return x @ y\n\n\nvectorized_function = jax.vmap(my_function, in_axes=(0, None))\n\nshape = (1_000_000, 1_000)\n\nx = np.ones(shape)\ny = np.ones(shape[1])\n\nstart = time()\nvectorized_function(x, y)\nt_1 = time() - start\n\nstart = time()\nvectorized_function(x, y)\nt_2 = time() - start\n\nprint(f'{t_1 = }\\n{t_2 = }')\nI get the following results:\npython\nCopy\nt_1 = 13.106784582138062\nt_2 = 15.664098024368286\nAs you can see, the second run (t_2) is actually slower than the first one (t_1), which seems counterintuitive to me. I expected the second run to be faster due to JAX’s JIT caching.\nHas anyone encountered a similar situation or have any insights into why this might be happening?\nPS: I know I could have done x @ y directly without invoking vmap, but this is an easy example just to test its behaviour. My actual code is more complex, and the difference in runtime is even bigger (around 8x slower). I hope this simple example works similar.",
        "answers": [
            "For general tips on running JAX microbenchmarks effectively, see FAQ: Benchmarking JAX code.\nI cannot reproduce the timings from your snippet, but in your more complicated case, I suspect you are getting fooled by JAX's Asynchronous dispatch, which means that the timing method you're using will not actually reflect the time taken by the underlying computation. To address this, you can wrap your results in jax.block_until_ready:\npython\nCopy\nstart = time()\nvectorized_function(x, y).block_until_ready()\nt_1 = time() - start"
        ],
        "link": "https://stackoverflow.com/questions/79192549/why-is-jaxs-jit-compilation-slower-on-the-second-run-in-my-example"
    },
    {
        "title": "JIT: partial or with static argnums? Non hashable input, but hashable partial",
        "question": "I am a bit lost on what exactly going on and what option to choose. Let's go trough an example:\npython\nCopy\nimport jax\nfrom functools import partial\nfrom typing import List\n\ndef dummy(a: int, b: List[str]):\n    return a + 1\nAs b argument is mutable, jitting with static argnames will be failed:\npython\nCopy\nj_dummy = jax.jit(dummy, static_argnames=['b'])\nj_dummy(2, ['kek'])\nValueError: Non-hashable static arguments are not supported\nHowever, if we do partial: jp_dummy = jax.jit(partial(dummy, b=['kek'])), we aim the goal. Somehow, partial object is indeed has __hash__ method, so we can check it with hash(partial(dummy, b=['kek'])).\nSo, I am a bit lost here: how I should proceed in a bigger picture? Should I produce partial functions with whatever arguments and then jit them or should I try to maintain my arguments hashable? What are situations when one approach is better than other? Is there any drawbacks?",
        "answers": [
            "When you use static_argnames, the static values passed to the function become part of the cache key, so if the value changes the function is re-compiled:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, s):\n  return x * len(s)\n\nf_jit = jax.jit(f, static_argnames=['s'])\n\nprint(f_jit(2, \"abc\"))  # 6\nprint(f_jit(2, \"abcd\"))  # 8\nThis is why the static arguments must be hashable: their hash is used as the JIT cache key.\nOn the other hand, when you wrap a static argument via closure, its value does not affect the cache key, and so it need not be hashable. But since it's not part of the cache key, if the global value changes, it does not trigger a recompilation and so you may get unexpected results:\npython\nCopy\nf_closure = jax.jit(lambda x: f(x, s))\n\ns = \"abc\"\nprint(f_closure(2))  # 6\ns = \"abcd\"\nprint(f_closure(2))  # 6\nFor this reason, explicit static arguments can be safer. In your case, it may be best to change your list into a tuple, as tuples are hashable and can be used as explicit static arguments."
        ],
        "link": "https://stackoverflow.com/questions/79114391/jit-partial-or-with-static-argnums-non-hashable-input-but-hashable-partial"
    },
    {
        "title": "Computing gradient using JAX of a function that outputs a list of arrays",
        "question": "I have a function which returns a list of arrays, and I need to find its derivative with respect to a single parameter. For instance, let's say we have\npython\nCopy\ndef fun(x):\n...\nreturn [a,b,c]\nwhere a,b,c and d are multi-dimensional arrays (for example, 2 by 2 by 2 real arrays). Now I want to obtain [da/dx, db/dx, dc/dx]. By db/dx I mean I want to obtain derivative of each element in the a:222 array with respect to x, so da/dx, db/dx, dc/dx are all 222 arrays.\nThis is me using JAX differentiation for the first time, and most of the examples I find online are about functions that has scalar output.\nFrom my search, I understand one way to find this is basically get the gradient of each scalar in all these arrays one at a time (probably making it faster using vmap). Is there any other way that is faster? I think JAX.jacobian might do the trick, but I am having hard time finding its documentation to see what does the function does exactly. Any help is very much appreciated.\nNow, I have tried JAX.jacobian with simple examples, and it does give me the answer that I expect. This assures me a bit, but I would like to find official documentation or assurance from others that is the right way to do it, and it is doing what I expect it.",
        "answers": [
            "You can use jax.jacobian for what you describe. Here is an example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  a = jnp.full((2, 2), 2) * x\n  b = jnp.full((2, 2), 3) * x\n  c = jnp.full((2, 2), 4) * x\n  return [a, b, c]\n\nda_dx, db_dx, dc_dx = jax.jacobian(f)(1.0)\n\nprint(da_dx)\n# [[2. 2.]\n#  [2. 2.]]\n\nprint(db_dx)\n# [[3. 3.]\n#  [3. 3.]]\n\nprint(dc_dx)\n# [[4. 4.]\n#  [4. 4.]]\njax.jacobian is an alias of jax.jacrev, and you can find the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.jacrev.html"
        ],
        "link": "https://stackoverflow.com/questions/79025241/computing-gradient-using-jax-of-a-function-that-outputs-a-list-of-arrays"
    },
    {
        "title": "Modifying multiple dimensions of Jax array simultaneously",
        "question": "When using the jax_array.at[idx] function, I wish to be able to set values at both a set of specified rows and columns within the jax_array to another jax_array containing values in the same shape. For example, given a 5x5 jax array, I might want to set the values, jax_array.at[[0,3],:][:,[1,2]] to some 2x2 array of values. However, I am coming across an issue where the _IndexUpdateRef' object is not subscriptable. I understand the idea of the error (and I get a similar one when using 2 chained .at[]s), but I want to know if there is anyway to achieve the desired functionality within 1 line.",
        "answers": [
            "JAX follows the indexing semantics of NumPy, and NumPy's indexing semantics allow you to do this via broadcasted arrays of indices (this is discussed in Integer array indexing in the NumPy docs).\nSo for example, you could do something like this:\npython\nCopy\nimport jax.numpy as jnp\n\nx = jnp.zeros((4, 6), dtype=int)\ny = jnp.array([[1, 2],\n               [3, 4]])\ni = jnp.array([0, 3])\nj = jnp.array([1, 2])\n\n# reshape indices so they broadcast \ni = i[:, jnp.newaxis]\nj = j[jnp.newaxis, :]\n\nx = x.at[i, j].set(y)\nprint(x)\npython\nCopy\n[[0 1 2 0 0 0]\n [0 0 0 0 0 0]\n [0 0 0 0 0 0]\n [0 3 4 0 0 0]]\nHere the i index has shape (2, 1), and the j index has shape (1, 2), and via broadcasting rules they index a 2x2 noncontiguous subgrid of the array x, which you can then set to the contents of y in a single statement."
        ],
        "link": "https://stackoverflow.com/questions/78985089/modifying-multiple-dimensions-of-jax-array-simultaneously"
    },
    {
        "title": "Using Jax Jit on a method as decorator versus applying jit function directly",
        "question": "I guess most people familiar with jax have seen this example in the documentation and know that it does not work:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit  # <---- How to do this correctly?\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\nc.calc(3)  \n3 workarounds are mentioned, but it appears that applying jit as a function directly, rather than a decorator works fine as well. That is, JAX does not complain about not knowing how to deal with the CustomClass type of self:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  # No decorator here !\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nbash\nCopy\n6 # works fine!\nAlthough not documented (which it maybe should be?), this appears to function identical to marking self as static via @partial(jax.jit, static_argnums=0), in that changing self does nothing for subsequent calls, i.e.:\npython\nCopy\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\nbash\nCopy\n6\n6 # no update\nSo I originally assumed that decorators in general might just deal with self as a static parameter when applying them directly. Because the method might be saved to another variable with a specific instance (copy) of self. As a sanity check, I checked if non-jit decorators indeed do this as well, but this appears not to be the case, as the below non-jit \"decorated\" function happily deals with changes to self:\npython\nCopy\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\n\ncustom = CustomClass(2, True)\ndecorated_calc = decorator(custom.calc)\nprint(decorated_calc(3))\ncustom.mul = False\nprint(decorated_calc(3))\nbash\nCopy\n6\n3\nI saw some other questions about applying decorators directly as functions versus decorator style (e.g. here and here), and there it is mentioned there is a slight difference in the two versions, but this should almost never matter. I am left wondering what it is about the jit decorator that makes these versions behave so differently, in that JAX.jit cán deal with the self type if not in decorated style. If anyone has an answer, that would be much appreciated.",
        "answers": [
            "Decorators have nothing to do with static arguments: static arguments are a concept specific to jax.jit.\nBacking up, you should keep in mind that whenever jax.jit compiles a function, it caches the compilation artifact based on several quantites, including:\nthe ID of the function or callable being compiled\nthe static attributes of any non-static arguments, such as shape and dtype\nthe hash of any arguments marked static via static_argnums or static_argnames\nthe value of any global configurations that would affect outputs\nWith this in mind, let's examine this snippet:\npython\nCopy\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\nthe reason that jitted_calc doesn't update when you update attributes of c is because nothing related to the cache key has changed: (1) the function ID is the same, (2) the shape and dtype of the argument is unchanged, (3) there are no static arguments, (4) no global configurations have changed. Thus the previous cached compilation artifact (with the previous value of mul) is executed again. This is the primary reason I didn't mention this strategy in the doc you linked to: it's rarely the behavior that users would want.\nThis approach of wrapping the bound method in JIT is incidentally similar to wrapping the method definition with @partial(jit, static_argnums=0), but the details are not the same: in the static_argnums version, self is marked as a static argument, and so its hash becomes part of the JIT cache. The default __hash__ method for a class is simply based on the ID of the instance, and so changing c.mul does not change the hash, and does not trigger re-compilation. You can see an example of how to rectify this under Strategy 2 in the doc you linked to: basically, define appropriate __hash__ and __eq__ methods for the class:\npython\nCopy\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def __hash__(self):\n    return hash((self.x, self.mul))\n\n  def __eq__(self, other):\n    return (isinstance(other, CustomClass) and\n            (self.x, self.mul) == (other.x, other.mul))\nIn your last example, you define this:\npython\nCopy\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\nThis code does not use jax.jit at all. The fact that changes to c.mul lead to changes in outputs has nothing to do with decorator syntax, but rather has to do with the fact that there is no JIT cache in play here.\nI hope that's all clear!"
        ],
        "link": "https://stackoverflow.com/questions/78918066/using-jax-jit-on-a-method-as-decorator-versus-applying-jit-function-directly"
    },
    {
        "title": "Zero length error of non-zero length array",
        "question": "I'm writing environment for rl agent training.\nMy env.step method takes as action array with length 3\n    def scan(self, f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n\n    def step_env(\n        self,\n        key: chex.PRNGKey,\n        state: EnvState,\n        action: Union[int, float, chex.Array],\n        params: EnvParams,\n    ) -> Tuple[chex.Array, EnvState, jnp.ndarray, jnp.ndarray, Dict[Any, Any]]:\n        \n        c_action = jnp.clip(action,\n                          params.min_action, \n                          params.max_action)\n        \n        _, m1 = self.scan(self.Rx, 0, action[0])\n        _, m2 = self.scan(self.Rx, 0, action[1])\n        _, m3 = self.scan(self.Rx, 0, action[2])\nI vectorize the env.step using and then call it\nobsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(rng_step,\n                                                                                          env_state,\n                                                                                          action,\n                                                                                          env_params)\nBut I got error\nCell In[9], line 65, in PCJ1_0.scan(self, f, init, xs, length)\n     63 ys = []\n     64 print(xs)\n---> 65 for x in xs:\n     66     carry, y = f(carry, x)\n     67     ys.append(y)\n\n    [... skipping hidden 1 frame]\n\nFile ~/anaconda3/envs/jax/lib/python3.10/site-packages/jax/_src/lax/lax.py:1592, in _iter(tracer)\n   1590 def _iter(tracer):\n   1591   if tracer.ndim == 0:\n-> 1592     raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n   1593   else:\n   1594     n = int(tracer.shape[0])\n\nTypeError: iteration over a 0-d array\nHow is it possible? If I plot the action array in the scan function I got array with length 5 (I vectored env.step for 5 envs), the length!=0\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0",
        "answers": [
            "When you print your value, it gives this:\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0\nHere float32[] tells you that this is a tracer with dtype float32 and shape []: that is, your array is zero-dimensional within the context of the vmapped function.\nThe purpose of vmap is to efficiently map a function over an axis of an array, so that within the function evaluation the array has one less dimension than it does outside the vmapped context. You can see that this way:\npython\nCopy\n>>> import jax\n\n>>> def f(x):\n...  print(f\"{x.shape=}\")\n...  print(f\"{x=}\")\n...\n>>> x = jax.numpy.arange(4.0)\n\n>>> f(x)\nx.shape=(4,)\nx=Array([0., 1., 2., 3.], dtype=float32)\n\n>>> jax.vmap(f)(x)\nx.shape=()\nx=Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0., 1., 2., 3.], dtype=float32)\n  batch_dim = 0\nIf you're passing a 1D input into your function and you want to manipulate the full 1D array within your function (instead of evaluating the function element-by-element), then it sounds like you should remove the vmap."
        ],
        "link": "https://stackoverflow.com/questions/78838517/zero-length-error-of-non-zero-length-array"
    },
    {
        "title": "Execution of conditional branches causing errors in Jax (kd-tree implementation)",
        "question": "I'm writing a kd-tree in Jax, and using custom written Node objects for the tree elements. Each Node is very simple, with a single data field (for holding numeric values) and left and right fields which are references to other Nodes. A leaf Node is identified as one for which the left and right fields are None.\nThe code performs conditional checks on the values of left and right as part of the tree traversal process - e.g. it will only try to traverse down the left or right branch of a node's subtree if it actually exists. Doing checks like if (current_node.left is not None) (or does it have to be jax.numpy.logical_not(current_node.left is None) in Jax - I've tried both?) was fine for this, but since converting the if statements to jax.lax.cond(...) I've been getting the error AttributeError: 'NoneType' object has no attribute 'left'.\nI think the situation might be like in the following minimum working example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef my_func(val):\n    return 2*val\n\n@jax.jit\ndef test_fn(a):\n    return jax.lax.cond(a is not None,\n                lambda: my_func(a),\n                lambda: 0)\n\nprint(test_fn(2))       # Prints 4\n# in test_fn(), a has type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\nprint(test_fn(None))    # TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n# in test_fn(), a has type <class 'NoneType'>\nIn this code, if the Jax cond statement were a regular if statement, my_func() wouldn't even be called when a is None, and no error would be raised. To the best of my understanding, Jax tries to trace the function, meaning that all branches are executed, and this leads to my_func() being called with None (when a is None), causing the error. I believe a similar situation is arising in my tree code, where conditional branches are being executed even though .left and /or .right are None, and a traditional if statement wouldn't lead to execution of the code branches.\nIs my understanding correct, and what could I do about this issue? Strangely, the minimum working example code also has the problem when the @jax.jit decorator is omitted, suggesting that both branches are still being traced.\nAs a related point, is the tree structure 'baked into' the Jax/XLA code? I have noticed that when using larger trees the code takes longer to be jit-compiled, which makes me concerned that this might not be a valid approach with the very large number of points I need to represent (about 14,000,000). I would use the regular Scipy kd-tree implementation, but this isn't compatible with Jax unfortunately, and the rest of my code requires it. I might ask this as a separate question for clarity.",
        "answers": [
            "If you are using jax.lax.cond, the input must have a valid type for both branches. When a is None, the first branch is invalid because None * 2 results in an error.\nIn this case, the condition a is not None is known statically, so rather than using lax.cond you can use a regular if statement:\npython\nCopy\n@jax.jit\ndef test_fn(a):\n  return my_func(a) if a is not None else 0"
        ],
        "link": "https://stackoverflow.com/questions/78784486/execution-of-conditional-branches-causing-errors-in-jax-kd-tree-implementation"
    },
    {
        "title": "JAX/Equinox pipeline slows down after adding an integer argument to a loss function",
        "question": "I have the following training pipeline in JAX and Equinox. I want to pass a batch index to the loss function in order to apply different logic depending on index. Without batch index training loop works for about 15 sec, but if I pass an index, then it slows down for about an hour. Could you explain, why this happens? I'm new to JAX, sorry.\ndef fit_cv(model: eqx.Module, \n           dataloader: jdl.DataLoader, \n           optimizer: optax.GradientTransformation, \n           loss: tp.Callable, \n           n_steps: int = 1000):\n    \n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    dloss = eqx.filter_jit(eqx.filter_value_and_grad(loss))\n    \n    @eqx.filter_jit\n    def step(model, data, opt_state, batch_index):\n        loss_score, grads = dloss(model, data, batch_index)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state, loss_score\n    \n    loss_history = []\n    for batch_index, batch in tqdm(zip(range(n_steps), dataloader), total=n_steps):\n        if batch_index >= n_steps:\n            break\n        batch = batch[0] # dataloader returns tuple of size (1,)\n        model, opt_state, loss_score = step(model, batch, opt_state, batch_index)\n        loss_history.append(loss_score)\n    return model, loss_history\nLoss function has the following signature\ndef loss(self, model: eqx.Module, data: jnp.ndarray, batch_index: int):\nIn particular, I want to switch between two loss functions after N steps. So, probably, I need to know the concrete value of a batch index.\nSolution:\nTo use jax.lax.cond\n        condition = (batch_index // self.switch_steps) % 2 == 1\n        ...\n        loss_value = jax.lax.cond(\n            jnp.all(condition),\n            lambda: loss1(inputs),\n            lambda: loss2(inputs),\n        )\n        return loss_value",
        "answers": [
            "I suspect the issue is excessive recompilation. You are using filter_jit, which according to the docs has the following property:\nAll JAX and NumPy arrays are traced, and all other types are held static.\nEach time a static argument to a JIT-compiled function changes, it triggers a re-compilation. This means that if batch_index is a Python int, then each time you call your function with a new value, the function will be recompiled.\nAs a fix, I would recommend using regular old jax.jit, which requires you to explicitly specify static arguments, instead of the function trying to make the choice for you (potential surprises like this are one of the reasons why JAX has made this design choice - as the Zen of Python says, explicit is better than implicit). If you use jax.jit and don't mark batch_index as static, you shouldn't see this recompilation penalty.\nAlternatively, if you want to keep using filter_jit, then you could change your step call to this:\npython\nCopy\nstep(model, batch, opt_state, jnp.asarray(batch_index))\nWith this change, filter_jit will no longer decide to make the batch index static. Of course, either of these suggestions would require that that loss is compatible with dynamic batch_index, which can't be determined from the information included in your question."
        ],
        "link": "https://stackoverflow.com/questions/78775635/jax-equinox-pipeline-slows-down-after-adding-an-integer-argument-to-a-loss-funct"
    },
    {
        "title": "Dictionary indexing with Numpy/Jax",
        "question": "I'm writing an interpolation routine and have a dictionary which stores the function values at the fitting points. Ideally, the dictionary keys would be 2D Numpy arrays of the fitting point coordinates, np.array([x, y]), but since Numpy arrays aren't hashable these are converted to tuples for the keys.\npython\nCopy\n# fit_pt_coords: (n_pts, n_dims) array\n# fn_vals: (n_pts,) array\ndef fit(fit_pt_coords, fn_vals):\n    pt_map = {tuple(k): v for k, v in zip(fit_pt_coords, fn_vals)}\n    ...\nLater in the code I need to get the function values using coordinates as keys in order to do the interpolation fitting. I'd like this to be within @jax.jited code, but the coordinate values are of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>, which can't be converted to a tuple. I've tried other things, like creating a dictionary key as (x + y, x - y), but again this requires concrete values, and calling .item() results in an ConcretizationTypeError.\nAt the moment I've @jax.jited all of the code I can, and have just left this code un-jitted. It would be great if I could jit this code as well however. Are there any better ways to do the dictionary indexing (or better Jax-compatible data structures) which would allow all of the code to be jitted? I am new to Jax and still understading how it works, so I'm sure there must be better ways of doing it...",
        "answers": [
            "There is no way to use traced JAX values as dictionary keys. The problem is that the key values will not be known until runtime within the XLA compiler, and XLA has no dictionary-like data structure that such lookups can be lowered to.\nThere are imperfect solutions, such as keeping the dictionary on the host and using something like io_callback to do the dict lookups on host, but this approach comes with performance penalties that will likely make it impractical.\nUnfortunately, your best approach for doing this efficiently under JIT would probably be to switch to a different interpolation algorithm that doesn't depend on hash table lookups.",
            "I agree with @jakevdp that this might not be the best solution. Python is not the quickest when built-ins are looped over.\nPython can do anything... Except for-loops. We use numpy for that.\nMaybe a pandas.DataFrame with columns [\"x\", \"y\", \"v\"] would be a way to go.\nCan you not use scipy.interpolate's functions?"
        ],
        "link": "https://stackoverflow.com/questions/78767142/dictionary-indexing-with-numpy-jax"
    },
    {
        "title": "weird shape when indexing a jax array",
        "question": "I am experiencing a weird issue when indexing a Jax array using a list. If I place a debugger in the middle of my code, I have the following:\nThis array are created by convering a numpy array.\nHowever, when I try this in a new instance of Python, I have the correct behavior: [\nWhat is it happening?",
        "answers": [
            "This is working as expected. JAX follows the semantics of NumPy indexing, and in the case of advanced indexing with multiple scalars and integer arrays separated by slices, the indexed dimensions are combined via broadcasting and moved to the front of the output array. You can read more about the details of this kind of indexing in the NumPy documentation: https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing. In particular:\nTwo cases of index combination need to be distinguished:\nThe advanced indices are separated by a slice, Ellipsis or newaxis. For example x[arr1, :, arr2].\nThe advanced indices are all next to each other. For example x[..., arr1, arr2, :] but not x[arr1, :, 1] since 1 is an advanced index in this regard.\nIn the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array\nThe code in your program falls under the first case, while the code in your separate interpreter falls under the second case. This is why you're seeing different results.\nHere's a concise example of this difference:\npython\nCopy\n>>> import numpy as np\n>>> x = np.zeros((3, 4, 5))\n\n>>> x[0, :, [1, 2]].shape  # size-2 dimension moved to front\n(2, 4)\n\n>>> x[:, 0, [1, 2]].shape  # size-2 dimension not moved to front\n(3, 2)"
        ],
        "link": "https://stackoverflow.com/questions/78741064/weird-shape-when-indexing-a-jax-array"
    },
    {
        "title": "Colab, Jax, and GPU: why does cell execution take 60 seconds when %%timeit says it only takes 70 ms?",
        "question": "As the basis for a project on fractals, I'm trying to use GPU computation on Google Colab using the Jax library.\nI'm using Mandelbrot on all accelerators as a model, and I'm encountering a problem.\nWhen I use the %%timeit command to measure how long it takes to calculate my GPU function (same as in the model notebook), the times are entirely reasonable, and in line with expected results -- 70 to 80 ms.\nBut actually running %%timeit takes something like a full minute. (By default, it runs the function 7 times in a row and reports the average -- but even that should take less than a second.)\nSimilarly, when I run the function in a cell and output the results (a 6 megapixel image), it takes around 60 seconds for the cell to finish -- to execute a function that supposedly only takes 70-80 ms.\nIt seems like something is producing a massive amount of overhead, that also seems to scale with the amount of computation -- e.g. when the function contains 1,000 iterative calculations %%timeit says it takes 71 ms while in reality it takes 60 seconds, but with just 20 iterations %%timeit says it takes 10 ms while in reality it takes about 10 seconds.\nI am pasting the code below, but here is a link to the Colab notebook itself -- anyone can make a copy, connect to a \"T4 GPU\" instance, and run it themselves to see.\npython\nCopy\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax\n\nassert len(jax.devices(\"gpu\")) == 1\n\ndef run_jax_kernel(c, fractal):\n    z = c\n    for i in range(1000):\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n    return fractal\n\nrun_jax_gpu_kernel = jax.jit(run_jax_kernel, backend=\"gpu\")\n\ndef run_jax_gpu(height, width):\n\n    mx = -0.69291874321833995150613818345974774914923989808007473759199\n    my = 0.36963080032727980808623018005116209090839988898368679237704\n    zw = 4 / 1e3\n\n    y, x = jax.numpy.ogrid[(my-zw/2):(my+zw/2):height*1j, (mx-zw/2):(mx+zw/2):width*1j]\n    c = x + y*1j\n    fractal = jax.numpy.full(c.shape, 1000, dtype=np.int32)\n    return np.asarray(run_jax_gpu_kernel(c, fractal).block_until_ready())\nTakes about a minute to produce an image:\npython\nCopy\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nax.imshow(run_jax_gpu(2000, 3000));\nTakes about a minute to report that the function only takes 70-80 ms to execute:\npython\nCopy\n%%timeit -o\nrun_jax_gpu(2000, 3000)",
        "answers": [
            "The first thing to realize is that %timeit will execute your code multiple times, and then return an average of the times for each run. The number of times it will execute is determined dynamically by the time of the first run.\nThe second thing to realize is that JAX code is just-in-time (JIT) compiled, meaning that on the first execution of any particular function, you will incur a one-time compilation cost. Many things affect compilation cost, but functions that use large for loops (say, 1000 or more repetitions) tend to compile very slowly, because JAX unrolls those loops before passing the operations to XLA for compilation, and XLA compilation scales approximately quadratically with the number of unrolled operations (there is some discussion of this at JAX Sharp Bits: Control Flow).\nPut these together, and you'll see why you're observing the timings that you are: under %timeit, your first run results in a very long compilation, and subsequent runs are very fast. The resulting average time is printed, and is very short compared to the first run, and to the overall time.\nWhen you run your code a single time to plot the results, you are mainly seeing the compilation time. Because it is not amortized away by multiple calls to your function, that compilation time is long.\nThe solution would be to avoid writing Python for loops in your function in order to avoid the long compilation time: one possibility would be to use lax.fori_loop, which allows you to write iterative computations without the huge compilation time penalty, though it will incur a runtime penalty on GPU compared to the for loop solution because the operations are executed sequentially rather than being parallelized by the compiler. In your case it might look like this:\npython\nCopy\ndef run_jax_kernel(c, fractal):\n    z = c\n    def body_fun(i, carry):\n        z, fractal = carry\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n        return (z, fractal)\n    z, fractal = jax.lax.fori_loop(0, 1000, body_fun, (z, fractal))\n    return fractal"
        ],
        "link": "https://stackoverflow.com/questions/78708817/colab-jax-and-gpu-why-does-cell-execution-take-60-seconds-when-timeit-says"
    },
    {
        "title": "Implementing a vectorized function over LinkedLists using Jax’s vmap function",
        "question": "Trying to implement a vectorized version of an algorithm (from computational geometry) using Jax. I have made the minimum working example using a LinkedList to particularly express my query (I am using a DCEL otherwise).\nThe idea is that this vectorized algorithm will be checking certain criteria over a DCEL. I have substituted this “criteria checking procedure” with a simple summation algorithm for the sake simplicity.\npython\nCopy\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\n\nclass Node: \n  \n    # Constructor to initialize the node object \n    def __init__(self, data): \n        self.data = data \n        self.next = None\n\nclass LinkedList: \n  \n    def __init__(self): \n        self.head = None\n  \n    def push(self, new_data): \n        new_node = Node(new_data) \n        new_node.next = self.head \n        self.head = new_node \n\n    def printList(self): \n        temp = self.head \n        while(temp): \n            print (temp.data,end=\" \") \n            temp = temp.next\n\ndef summate(list) :\n    prev = None\n    current = list.head\n    sum = 0\n    while(current is not None): \n        sum += current.data\n        next = current.next\n        current = next\n    return sum\n\nlist1 = LinkedList() \nlist1.push(20) \nlist1.push(4) \nlist1.push(15) \nlist1.push(85) \n\nlist2 = LinkedList() \nlist2.push(19)\nlist2.push(13)\nlist2.push(2)\nlist2.push(13)\n\n#list(map(summate, ([list1, list2])))\n\nvmap(summate)(jnp.array([list1, list2]))\nI get the following error.\n TypeError: Value '<__main__.LinkedList object at 0x1193799d0>' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.\nThe objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion. I have implemented what I want in basic Python, but I want to do it in Jax as there is a larger probabilistic function which I will be using this subprocedure for (it’s a Markov Chain).\nIt might be the case that I am completely unable to work over such data structures over Jax as the error suggests that only numeric types are supported. Can I use pytrees in some way to mitigate this constraint?\nIt will be tempting to suggest I use a simple list from jnp, but I am using Linkedlist just as an example of a simple(st) data structure. As mentioned earlier, am actually working over a DCEL.\nPS : the Linkedlist code was taken from GeeksForGeeks, as I wanted to come up with a minimum working example quickly.",
        "answers": [
            "The objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion.\nThis goal is not feasible using JAX. You could register your class as a custom Pytree to make it work with JAX functions (see Extending pytrees), but this won't mean you can vectorize an operation over a list of such objects.\nJAX transformations like vmap and jit work for data stored with a struct-of-arrays pattern (e.g. a single LinkedList object containing arrays that represent multiple batched linked lists) not an array-of-structs pattern (e.g. a list of multiple LinkedList objects).\nFurther, the algorithm you're using, based on a while loop, is not compatible with JAX transformations (See JAX sharp bits: control flow), and the dynamically sized tree of nodes will not fit into the static shape constraints of JAX programs.\nI'd love to point you in the right direction, but I think you either need to give up on using JAX, or give up on using dynamic linked lists. You won't be able to do both."
        ],
        "link": "https://stackoverflow.com/questions/78677115/implementing-a-vectorized-function-over-linkedlists-using-jax-s-vmap-function"
    },
    {
        "title": "Why is Flax Linear layer not identical to matrix multiplication?",
        "question": "Due to the novelty of Flax, NNX, and JAX, there’s not a lot of resources available. I’m running into the following peculiarity:\nx = jnp.random.normal((1,512), key=KEY)\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = layer.kernel@x.squeeze() + layer.bias\nprint(y1==y2) # returns all False\nMy understanding is that matrix multiplication should be identical to a linear / fully connected layer. The discrepancy demonstrated here hinders the inspection of certain behavior (and the implementation of invertible dense layers using jnp.tensorsolve).\nDoes anyone know what causes this discrepancy?",
        "answers": [
            "The matmul should be transposed; also floating point equality checks should be done via approximate rather than exact comparison, because different ways of computing the same result may lead to different floating point rounding errors:\npython\nCopy\nimport jax\nfrom flax import nnx\n\nKEY = jax.random.key(0)\nx = jax.random.normal(KEY, (1,512))\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = x @ layer.kernel + layer.bias\nprint(jax.numpy.allclose(y1, y2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78659890/why-is-flax-linear-layer-not-identical-to-matrix-multiplication"
    },
    {
        "title": "How to set a new learning rate manually in optax optimizer?",
        "question": "I have the following optimizer being create using optax:\ndef create_optimizer(learning_rate=6.25e-2, beta1=0.4, beta2=0.999,\n                     eps=2e-4, centered=False):\n\n  Returns:\n    An optax optimizer.\n  \"\"\"\n \n    return optax.adam(learning_rate, b1=beta1, b2=beta2, eps=eps)\nHow during training update this learning rate manually?\nI couldn't find any documentation about that.",
        "answers": [
            "Disclaimer. Usually, you would use a schedule to adapt the learning rate during training. This answer provides a solution to obtain direct control over the learning rate.\nIn general, you can put any optimizer's hyperparmeters (such as the learning rate) into the optimizer's state and then directly mutate the state. Moving the hyperparameters into the state is necessary as optax optimizers are pure functions. Especially, the only way to dynamically change their behaviour is by changing their input.\nSetup. I am using a stochastic gradient descent optimizer to highlight the effect of the learning rate on the update suggested by the optimizer.\npython\nCopy\nimport jax.numpy as jnp\nimport optax\n\n# Define example parameters and gradients.\nparams, grads = jnp.array([0.0, 0.0]), jnp.array([1.0, 2.0])\n\n# Ensure the learning rate is part of the optimizer's state.\nopt = optax.inject_hyperparams(optax.sgd)(learning_rate=1e-2)\nopt_state = opt.init(params)\nUpdate computation.\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.01, -0.02], dtype=float32)\nDirectly setting the learning rate.\npython\nCopy\nopt_state.hyperparams['learning_rate'] = 3e-4\nSame update computation as before (with new learning rate).\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.0003, -0.0006], dtype=float32)\nSee this discussion for more information."
        ],
        "link": "https://stackoverflow.com/questions/78527164/how-to-set-a-new-learning-rate-manually-in-optax-optimizer"
    },
    {
        "title": "jax parallel multiplication of pairs of matrix with different shapes",
        "question": "Task: I have two lists of matrices A,B with length N. For each pair of elements A[i], B[i] shapes are such that matrix product is well-defined, however for each i in $0,\\dots, N-1$ shapes can be different. Hence, I can not stack them in array. Shapes are static.\nI would like to do achieve same result as following :\npython\nCopy\nout = [None] * length(A)\nfor i, a, b in enumerate(zip(A,B)):\n   out[i] = a @ b\nHowever, I would like to do this in parallel with jax. The best option will be vmap, but it is impossible as shapes are different.\nHere I will discuss solutions that I know and why they are not satisfactory.\nWrite for loop and then jit it. This will grow compilation time super linear over length N. This is not good, as I know all shapes of input and output before running computation, so I would expect to constant compilation time (provided say list of shapes).\nUse fori_loop primitive from jax. In documentation, there is following:\nThe semantics of fori_loop are given by this Python implementation:\npython\nCopy\ndef fori_loop(lower, upper, body_fun, init_val):\n  val = init_val\n  for i in range(lower, upper):\n    val = body_fun(i, val)\n  return val\nHowever, my case is easier: I don't need to care val across iterations. This means that fori is sequential. While my case is parallel. Hence, it should be possible to do better.\nPad with zeros, use vmap, read result. I don't control distribution of shapes, so it can lead to blowing memory if only one shape is big.\nUse lax.map Here (What are the tradeoffs between jax.lax.map and jax.vmap?) I read following:\nThe lax.map solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.\nSo I don't know what to do. Thanks!\nUpd after answer:\npython\nCopy\nN = 100\nd = 1000\nkey = jrandom.key(0)\nAjnp = jrandom.normal(key, (N, d, d))\nBjnp = jrandom.normal(key, (N, d, d))\n\nAnp = list(np.random.randn(N,d,d))\nBnp = list(np.random.randn(N,d,d))\n\nvmatmul = vmap(jnp.matmul, (0,0))\n\ndef lmatmul(A,B):\n    return [a @ b for a, b in zip(A,B)]\npython\nCopy\n%timeit vmatmul(Ajnp, Bjnp).block_until_ready()  # jax vmap over arrays\n6.59 ms ± 73.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\npython\nCopy\n%timeit block_until_ready(lmatmul(list(Ajnp), list(Bjnp))) # jax loop over lists\n13 ms ± 221 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\npython\nCopy\n%timeit lmatmul(Anp, Bnp) # numpy loop over lists\n1.28 s ± 13.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)",
        "answers": [
            "I think your best approach will be something like your original formulation, though you can avoid pre-allocating the out list:\npython\nCopy\nout = [a @ b for a, b in zip(A, B)]\nBecause of JAX's Asynchronous dispatch, if you run this on an accelerator like GPU the operations will be executed in parallel to the extent possible.\nAll of your other proposed solutions either won't work due to static shape limitations, will force sequential computation, or will incur overhead that will make them worse in practice than this more straightforward approach."
        ],
        "link": "https://stackoverflow.com/questions/78502704/jax-parallel-multiplication-of-pairs-of-matrix-with-different-shapes"
    },
    {
        "title": "Jax dynamic slicing tracer array",
        "question": "To make this brief: I wrote the following codes:\npython\nCopy\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef body_func(carry,x):\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nbut got\npython\nCopy\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nHere's the full situation: I'm trying to develop a model to do phase recognition tasks, and I would like to normalize my logits phase by phase using jax. For example, suppose I have the phase labels and logits:\npython\nCopy\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\nI would like to normalize the first 4 elements in logits where in the phase labels they all belong to phase 0. Then the next 4 elements, because in the phase labels they all belong to phase 1. So the normalized logits should look like:\npython\nCopy\nnormalized_logits=[0,0.33,0.66,1.0,0,0.33,0.66,1.0,0,0.33,0.66,1.0]\nHere's what tried:\npython\nCopy\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef min_max_normalization(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndef body_func(carry,x):\n    jax.debug.print(\"carry is {}\",carry)\n    jax.debug.print(\"x is {}\",x)\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    print(min_max_normalization(jax.lax.dynamic_slice(arr, [start_idx], [jax.lax.tie_in(x, x-start_idx+1)])))\n    print(jax.lax.dynamic_slice(arr, [x+1], [jax.lax.tie_in(x, len(arr)-x-1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nBasically, this is a debug version, the actual return value should concatenate three dynamically sliced array together. But I'm getting the error below:\npython\nCopy\nTraceback (most recent call last):\n  File \"/path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 250, in scan\n    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n                                                                ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 236, in _create_jaxpr\n    jaxpr, consts, out_tree = _initial_style_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 64, in _initial_style_jaxpr\n    jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 58, in _initial_style_open_jaxpr\n    jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2155, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/slicing.py\", line 110, in dynamic_slice\n    static_sizes = core.canonicalize_shape(slice_sizes)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/core.py\", line 2086, in canonicalize_shape\n    raise _invalid_shape_error(shape, context)\njax._src.traceback_util.UnfilteredStackTrace: TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /Users/wuhaoyang/Documents/Research/Project_Surgical_Robot/Code/SSM_Med/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nThe reason why I'm not simply using a for loop is that I'm later going to wrap this function into another one that uses jit compile, so I want to do this with pure jax API. Any help is appreciated, please tell me if you need more information.",
        "answers": [
            "JAX arrays used in transformations like jit, vmap, and scan must always be statically-shaped (see Sharp bits: Dynamic Shapes for some discussion of this).\ndynamic_slice allows you to slice a static length at a dynamic position, while you're trying to use it to slice a dynamic length at a static position, and thus you're seeing this concretization error.\nTo solve your problem, I would avoid scan and instead use JAX's segment_min and segment_max functions to compute the output in a vectorized rather than iterative manner:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nlabels = jnp.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits = jnp.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\nl_min = jax.ops.segment_min(logits, labels)[labels]\nl_max = jax.ops.segment_max(logits, labels)[labels]\n\nnormalized_logits = (logits - l_min) / (l_max - l_min)\nprint(normalized_logits)\n# [0.         0.33333334 0.6666667  1.         0.         0.33333334\n#  0.6666667  1.         0.         0.33333334 0.6666667  1.        ]\nIf you want this to be compatible with jit and other transformations, you'll need to pass a static num_segments argument to your segment reductions to specify an upper-bound for the number of segments present:\npython\nCopy\nl_min = jax.ops.segment_min(logits, labels, num_segments=3)[labels]\nl_max = jax.ops.segment_max(logits, labels, num_segments=3)[labels]"
        ],
        "link": "https://stackoverflow.com/questions/78496911/jax-dynamic-slicing-tracer-array"
    },
    {
        "title": "Finite basis physics-informed neural networks (FBPINNs) JAX problem",
        "question": "I am trying to modify Ben Moseley's code available on github https://github.com/benmoseley/FBPINNs. My intention is to insert a vector of values into the loss fn that is dependent on x y coordinates, and I need the original vector Z to be interpolated as a function of x and y, and then the values at the same coordinates with which the algorithm samples x and y are extracted, so that the values match. The problem I have encountered is that within loss fn I cannot use libraries other than JAX and to my knowledge there are no functions within JAX to interpolate in 2D.\nI'm trying to get around the problem in every way but I'm not succeeding, one of my ideas was to extrapolate the x,y points sampled by the algorithm but I'm not succeeding, the code is really very articulated. Would anyone be able to give me any advice/help on this?\nThere would be the function jax.scipy.ndimage.map_coordinates but it doesn't work properly and the points it extrapolates are meaningless.",
        "answers": [
            "If linear or nearest-neighbor interpolation is sufficient, you may be able to do what you need with jax.scipy.interpolate.RegularGridInterpolator\nIf you need something more sophisticated, like spline interpolation, there is nothing included in jax itself. That said, you may be able to find downstream implementations that work for you. One I came across that might be worth trying is in the jax_cosmo project: https://jax-cosmo.readthedocs.io/en/latest/_modules/jax_cosmo/scipy/interpolate.html."
        ],
        "link": "https://stackoverflow.com/questions/78494686/finite-basis-physics-informed-neural-networks-fbpinns-jax-problem"
    },
    {
        "title": "What is an efficient method to calculate multiple \"offset-traces\" in JAX?",
        "question": "Given a matrix m with shape (n, n), I need to compute the series of \"offset traces\" [np.trace(m, offset=i) for i in range(q)] in JAX. For my application, n >> q, and q is a static parameter.\nThe obvious JAX approach using vmap does not work, possibly because although the trace has fixed output size, each offset diagonal has a different length?\nI came up with two other approaches using JAX which work but are about 100x slower than NumPy. get_traces_jax_1 is the more efficient of the two. But it does a lot of extra work when I only need a few diagonals, and I don't think that extra work gets compiled away.\nIs there a more efficient way to do this in JAX with similar performance to NumPy? I want to use JAX because:\nI need to vmap this across many matrices;\nIt is part of a larger algorithm, other parts of which are significantly sped up by JAX jit.\nBelow are the methods I explored and timings on my computer.\npython\nCopy\nimport numpy as np\nfrom numpy import random\nimport jax\njax.config.update(\"jax_enable_x64\", True) # default is float32\nfrom jax import numpy as jnp\nfrom functools import partial\n\nn, q = 1000, 5\n\n# check the methods produce the same result\ndef distance(u, v):\n    return jnp.max(jnp.abs(u - v))\n\n# numpy - this is what I want\ndef get_traces_np(mat, q):\n    return np.array([np.trace(mat, offset=i) for i in range(q)])\n\n# jax\n# !! This does not work\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_broken(mat, q):\n    return jax.vmap(lambda i: jnp.trace(mat, offset=i))(jnp.arange(q)) # !! does not work\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_0(mat, q):\n    return jnp.array([jnp.trace(mat, offset=i) for i in range(q)])\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_1(mat, q):\n    n = mat.shape[0]\n    padded = jnp.pad(mat, ((0, 0), (0, n-1)), 'constant')\n    shifts = jax.vmap(lambda v, i: jnp.roll(v, -i))(padded, jnp.arange(n))[:, :n]\n    return jnp.sum(shifts, axis=0)[:q]\n\nmat = random.uniform(size=(n, n))\n# Check they produce the same result and precompile\nd0 = distance(get_traces_np(mat, q), get_traces_jax_0(mat, q))\nd1 = distance(get_traces_np(mat, q), get_traces_jax_1(mat, q))\nprint(f'Errors: {d0}, {d1}')\n\nmat = jnp.array(mat)\nprint('Numpy:')\n%timeit get_traces_np(mat, q) # 7.43 microseconds\nprint('Jax 0:')\n%timeit get_traces_jax_0(mat, q) # 4.82ms\nprint('Jax 1:')\n%timeit get_traces_jax_1(mat, q) # 1.22ms",
        "answers": [
            "vmapping jnp.trace across offset doesn't work because as currently implemented, the offset parameter of jnp.trace must be static, and vmapped parameters are not static. You could address this by constructing your own version of the trace function that does not require a static parameter; for example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef dynamic_trace(x, offset):\n  assert x.ndim == 2\n  i = jnp.arange(x.shape[0])[:, None]\n  j = jnp.arange(x.shape[1])\n  return jnp.sum(jnp.where(i + offset == j, x, 0))\n\nx = jnp.arange(12).reshape(3, 4)\n\noffset = jnp.arange(-2, 3)\n\njax.vmap(dynamic_trace, in_axes=(None, 0))(x, offset)\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\n\njnp.array([jnp.trace(x, int(o)) for o in offset])\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\nIn terms of benchmark comparisons, you should first make sure you're measuring what you think you're measuring. For example, your benchmark is written in such a way that it doesn't just measure runtime, but also device transfer and JIT compilation costs. It also ignores potentially confounding effects of asynchronous dispatch. For a discussion of these issues, see JAX FAQ: Benchmarking JAX code.\nEven accounting for this, however, I would not expect JAX to be faster than NumPy here for the reasons discussed at JAX FAQ: Is JAX Faster than NumPy?. Namely, this benchmark is doing a short sequence of relatively small array operations on CPU: this is a regime in which NumPy will always beat JAX due to its lower per-operation dispatch overhead. But not to worry: when this operation is used as part of a larger JIT-compiled function, those dispatch costs are amortized away."
        ],
        "link": "https://stackoverflow.com/questions/78479150/what-is-an-efficient-method-to-calculate-multiple-offset-traces-in-jax"
    },
    {
        "title": "Iterators in jit JAX functions",
        "question": "I'm new to JAX and reading the docs i found that jitted functions should not contain iterators (section on pure functions)\nand they bring this example:\npython\nCopy\nimport jax.numpy as jnp\nimport jax.lax as lax\nfrom jax import jit\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\ntrying to fiddling with it a little bit in order to see if i can get directly an error instead of undefined behaviour i wrote\npython\nCopy\n@jit\ndef f(x, arr):\n    for i in range(10):\n        x += arr[i]\n    return x\n\n@jit\ndef f1(x, arr):\n    it = iter(arr)\n    for i in range(10):\n        x += next(it)\n    return x\n\nprint(f(0,array)) # 45 as expected\nprint(f1(0,array)) # still 45 \nIs it a \"chance\" that the jitted function f1() now shows the correct behaviour?",
        "answers": [
            "Your code works because of the way that JAX's tracing model works. When JAX's tracing encounters Python control flow, like for loops, the loop is fully evaluated at trace-time (There's some exploration of this in JAX Sharp Bits: Control Flow).\nBecause of this, your use of an iterator in this context is fine, because every iteration is evaluated at trace-time, and so next(it) is re-evaluated at every iteration.\nIn contrast, when using lax.fori_loop, next(iterator) is only executed a single time and its output is treated as a trace-time constant that will not change during the runtime iterations."
        ],
        "link": "https://stackoverflow.com/questions/78403517/iterators-in-jit-jax-functions"
    },
    {
        "title": "How to restore a orbax checkpoint with jax/flax?",
        "question": "I saved a orbax checkpoint with the code below:\npython\nCopy\ncheck_options = ocp.CheckpointManagerOptions(max_to_keep=5, create=True)\ncheck_path = Path(os.getcwd(), out_dir, 'checkpoint')\ncheckpoint_manager = ocp.CheckpointManager(check_path, options=check_options, item_names=('state', 'metadata'))\ncheckpoint_manager.save(\n                    step=iter_num,\n                    args=ocp.args.Composite(\n                        state=ocp.args.StandardSave(state),\n                        metadata=ocp.args.JsonSave((model_args, iter_num, best_val_loss, losses['val'].item(), config))))\nWhen I try to resume from the saved checkpoints, I used the code below to recover the state variable:\npython\nCopy\nstate, lr_schedule = init_train_state(model, params['params'], learning_rate, weight_decay, beta1, beta2, decay_lr, warmup_iters, \n                     lr_decay_iters, min_lr)  # Here state is the initialied state variable with type Train_state.\nstate = checkpoint_manager.restore(checkpoint_manager.latest_step(), items={'state': state})\nBut when I try to use the recovered state in the training loop, I got this error:\npython\nCopy\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:584, in shaped_abstractify(x)\n    583 try:\n--> 584   return _shaped_abstractify_handlers[type(x)](x)\n    585 except KeyError:\n\nKeyError: <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'>\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 37\n     34 if iter_num == 0 and eval_only:\n     35     break\n---> 37 state, loss = train_step(state, get_batch('train'))\n     39 # timing and logging\n     40 t1 = time.time()\n\n    [... skipping hidden 6 frame]\n\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:575, in _shaped_abstractify_slow(x)\n    573   dtype = dtypes.canonicalize_dtype(x.dtype, allow_extended_dtype=True)\n    574 else:\n--> 575   raise TypeError(\n    576       f\"Cannot interpret value of type {type(x)} as an abstract array; it \"\n    577       \"does not have a dtype attribute\")\n    578 return core.ShapedArray(np.shape(x), dtype, weak_type=weak_type,\n    579                         named_shape=named_shape)\n\nTypeError: Cannot interpret value of type <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'> as an abstract array; it does not have a dtype attribute\nSo, how should I correctly recover the state checkpoint and use it in the training loop?\nThanks!",
        "answers": [
            "You're mixing the old and new APIs in a way that is not allowed. Apologies that an error to that effect is not being raised, I can look into that.\nYour saving is correct, but I'd recommend that it look more like the following:\npython\nCopy\nwith ocp.CheckpointManager(path, options=options, item_names=('state', 'metadata')) as mngr:\n  mngr.save(\n      step, \n      args=ocp.args.Composite(\n          state=ocp.args.StandardSave(state),\n          metadata=ocp.args.JsonSave(...),\n      )\n  )\nWhen restoring, you're currently using items which is part of the old API, and the usage is inconsistent with the CheckpointManager's definition, which is done based on the new API.\nitem_names and args are hallmarks of the new API.\nYou should do:\npython\nCopy\nwith ocp.CheckpointManager(...) as mngr:\n  mngr.restore(\n      mngr.latest_step(), \n      args=ocp.args.Composite(\n          state=ocp.args.StandardRestore(abstract_state),\n      )\n  )\nLet me know if there's any unexpected issues with that."
        ],
        "link": "https://stackoverflow.com/questions/78376465/how-to-restore-a-orbax-checkpoint-with-jax-flax"
    },
    {
        "title": "How can we cast a `ctypes.POINTER(ctypes.c_float)` to `int`? [duplicate]",
        "question": "This question already has answers here:\nGet the memory address pointed to by a ctypes pointer (2 answers)\nClosed last year.\nI think this is a simple task, but I could not find a solution on the web to this. I have a external C++ library, which I'm using in my Python code, returning a ctypes.POINTER(ctypes.c_float) to me. I want to pass an array of these pointers to a jax.vmap function. The problem is that jax does not accept the ctypes.POINTER(ctypes.c_float) type. So, can I somehow cast this pointer to an ordinary int. Technically, this is clearly possible. But how do I do this in Python?\nHere is an example:\npython\nCopy\nlib = ctypes.cdll.LoadLibrary(lib_path)\nlib.foo.argtypes = None\nlib.foo.restype = ctypes.POINTER(ctypes.c_float)\n\nbar = jax.vmap(lambda : dummy lib.foo())(jax.numpy.empty(16))\n\nx = jax.numpy.empty(16, 256, 256, 1)\ny = jax.vmap(lib.bar, in_axes = (0, 1))(x, bar)\nSo, I want to invoke lib.foo 16-times so that I have an array bar containing all the pointers. Then I want to invoke another library function lib.bar which expects bar together with another (batched) parameter x.\nThe problem is that jax claims that ctypes.POINTER(ctypes.c_float) is not a valid jax type. This is why I think the solution is to cast the pointers to ints and store those ints in bar instead.",
        "answers": [
            "Listing:\n[SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer) - a common pitfall when working with CTypes (calling functions)\n[Python.Docs]: ctypes - A foreign function library for Python\nHere's a piece of code exemplifying how to handle pointers and their addresses. The trick is to use ctypes.addressof (documented in the 2nd URL).\ncode00.py:\npython\nCopy\n#!/usr/bin/env python\n\nimport ctypes as cts\nimport sys\n\n\nCType = cts.c_float\nCTypePtr = cts.POINTER(CType)\n\n\ndef ctype_pointer(seq):  # Helper\n    CTypeArr = (CType * len(seq))\n    ctype_arr = CTypeArr(*seq)\n    return cts.cast(ctype_arr, CTypePtr)\n\n\ndef pointer_elements(addr, count):  # Helper\n    return tuple(CType.from_address(addr + i * cts.sizeof(CType)).value for i in range(count))\n\n\ndef main(*argv):\n    seq = (2.718182, -3.141593, 1.618034, -0.618034, 0)\n    ptr = ctype_pointer(seq)\n    print(f\"Pointer: {ptr}\")\n    print(f\"\\nPointer elements: {tuple(ptr[i] for i in range(len(seq)))}\")  # Check if pointer has correct data\n    ptr_addr = cts.addressof(ptr.contents)  # @TODO - cfati: Straightforward\n    print(f\"\\nAddress: {ptr_addr} (0x{ptr_addr:016X})\\nElements from address: {pointer_elements(ptr_addr, len(seq))}\")\n    ptr_addr0 = cts.cast(ptr, cts.c_void_p).value  # @TODO - cfati: Alternative\n    print(f\"\\nAddresses match: {ptr_addr == ptr_addr0}\")\n\n\nif __name__ == \"__main__\":\n    print(\n        \"Python {:s} {:03d}bit on {:s}\\n\".format(\n            \" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n            64 if sys.maxsize > 0x100000000 else 32,\n            sys.platform,\n        )\n    )\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\nNotes:\nAlthough it adds a bit of complexity, I introduced the CType \"layer\" to show that it should work with any type, not just float (as long as the values in the sequence are of that type)\nThe only truly relevant lines are those marked with @TODO\nOutput:\npython\nCopy\n(py_pc064_03.08_test0_lancer) [cfati@cfati-5510-0:/mnt/e/Work/Dev/StackExchange/StackOverflow/q078366208]> python ./code00.py \nPython 3.8.19 (default, Apr  6 2024, 17:58:10) [GCC 11.4.0] 064bit on linux\n\nPointer: <__main__.LP_c_float object at 0x7203e97e7d40>\n\nPointer elements: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddress: 125361127594576 (0x00007203E97A9A50)\nElements from address: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddresses match: True\n\nDone."
        ],
        "link": "https://stackoverflow.com/questions/78366208/how-can-we-cast-a-ctypes-pointerctypes-c-float-to-int"
    },
    {
        "title": "Simultaneously going over different kinds of data with Keras training",
        "question": "In a regression task I'm given the following data:\nInput vectors with a known label. MSE loss should be used between the precidtion and the label.\nPairs of input vectors without a label, for which it is known that the model should give similar results. MSE loss should be used between the two predictions.\nWhat's the right way to fit a Keras model with these two kinds of data simultaneously?\nIdeally, I'd like the train loop to iterate the two kinds in an interleaved way - a superivsed (1) batch and then a self-supervised (2) batch, then supervised again etc.\nIf it matters, I'm using the Jax backend. Keras version 3.2.1.",
        "answers": [
            "I eventually found a trick that solved it for my case without too many customizations.\nBut if you do need to pass different kinds of data for training, I don't think there's an easy answer as for today.\nIt should be possible though to write your own training loop, and use any structure that you want for the data and labels. In this case you might also want to use the trainer pattern, implementing a custom version of keras.src.backend.jax.trainer.JAXTrainer."
        ],
        "link": "https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training"
    },
    {
        "title": "Flax neural network with nans in the outputs",
        "question": "I am training a neural network using Flax. My training data has a significant number of nans in the outputs. I want to ignore these and only use the non-nan values for training. To achieve this, I have tried to use jnp.nanmean to compute the losses, i.e.:\npython\nCopy\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    return jnp.nanmean((pred - targets) ** 2)\n\ndef train_step(state, inputs, targets):\n    loss, grads = jax.value_and_grad(nanloss)(state.params, inputs, targets)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\nHowever, after one training step the loss is nan.\nIs what I am trying to achieve possible? If so, how can I fix this?",
        "answers": [
            "I suspect you are hitting the issue discussed here: JAX FAQ: gradients contain NaN where using where. You've handled the NaNs in the computation itself, but they're still sneaking into the gradient due to how autodiff is implemented.\nIf this is in fact the issue, you can fix this by filtering the values before computing the loss; for example like this:\npython\nCopy\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    mask = jnp.isnan(pred) | jnp.isnan(targets)\n    pred = jnp.where(mask, 0, pred)\n    targets = jnp.where(mask, 0, targets)\n    return jnp.mean((pred - targets) ** 2, where=~mask)"
        ],
        "link": "https://stackoverflow.com/questions/78332120/flax-neural-network-with-nans-in-the-outputs"
    },
    {
        "title": "Jax ValueError: Incompatible shapes for broadcasting: shapes",
        "question": "I'm trying to write a weighted cross-entropy loss to train my model with Jax. However, I think there are some issues with my input dimension. Here are my codes:\npython\nCopy\nimport jax.numpy as np\nfrom functools import partial\nimport jax\n\n@partial(np.vectorize, signature=\"(c),(),()->()\")\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[0])\n    return -np.sum(weights* logits*one_hot_label)\n\nlogits=np.array([[1,2,3,4,5,6,7],[2,3,4,5,6,7,8]])\nlabels=np.array([1,2])\nweights=np.array([1,2,3,4,5,6,7])\nprint(weighted_cross_entropy_loss(logits,label,weights))\nHere are my error messages:\npython\nCopy\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 147, in broadcast_shapes\n    return _broadcast_shapes_cached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 284, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 277, in cached\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 153, in _broadcast_shapes_cached\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/PATH/test.py\", line 15, in <module>\n    print(weighted_cross_entropy_loss(a,label,weights))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 274, in wrapped\n    broadcast_shape, dim_sizes = _parse_input_dimensions(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 123, in _parse_input_dimensions\n    broadcast_shape = lax.broadcast_shapes(*shapes)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 149, in broadcast_shapes\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\nI'm expecting a single number that represents the cross-entropy loss between logits and labels.\nI'm fairly new to this, can somebody tell me what is going on? Any help is appreciated.",
        "answers": [
            "label is length 2, and weights is length 7, which means they cannot be broadcast together.\nIt's not clear to me from your question what your expected outcome was, but you can read more about how broadcasting works in NumPy (and in JAX, which implements NumPy's semantics) at https://numpy.org/doc/stable/user/basics.broadcasting.html.\nEdit: it looks like this is the operation you were aiming for:\npython\nCopy\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[1])\n    return -np.sum(weights * logits * one_hot_label)\nSince you want a single scalar output, I don't think vectorize is the right mechanism to use here."
        ],
        "link": "https://stackoverflow.com/questions/78323919/jax-valueerror-incompatible-shapes-for-broadcasting-shapes"
    },
    {
        "title": "Stable diffusion: AttributeError: module 'jax.random' has no attribute 'KeyArray'",
        "question": "When I run the stable diffusion on colab https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\nwith no modification, it fails on the line\nfrom diffusers import StableDiffusionPipeline \nThe error log is\nAttributeError: module 'jax.random' has no attribute 'KeyArray'\nHow can I fix this or any clue ?\nThe import should work, the ipynb should run with no error.",
        "answers": [
            "jax.random.KeyArray was deprecated in JAX v0.4.16 and removed in JAX v0.4.24. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier.\nYou can install JAX v0.4.23 with GPU support like this:\npython\nCopy\npip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nor, if you prefer targeting a local CUDA installation, like this:\npython\nCopy\npip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nFor more information on GPU installation, see JAX Installation: NVIDIA GPU.\nFrom the colab tutorial, update the second segment into:\npython\nCopy\n!pip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n!pip install diffusers==0.11.1\n!pip install transformers scipy ftfy accelerate",
            "python\nCopy\n# Change this\n# !pip install diffusers==0.11.1\n\n# To just\n!pip install diffusers \nIf you've already run pip install in your Colab runtime, you'll need to either disconnect and open a new runtime (my recommendation) or use  --upgrade.\nDiffusers v0.11.1 is now over 18 months old, and the notebook works with current v0.29.0 without any other changes. Instead of using an old version of diffusers, requiring an old version of jax, we can use the latest versions.",
            "In the end, we need to downgrade the jax, Try each from the lateset to ealier, and luckily it works for\npython\nCopy\njax==0.4.23 jaxlib==0.4.23"
        ],
        "link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray"
    },
    {
        "title": "jax: How do we solve the error: pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0?",
        "question": "I'm trying to run this simple introduction to score-based generative modeling. The code is using flax.optim, which seems to be moved to optax meanwhile (https://flax.readthedocs.io/en/latest/guides/converting_and_upgrading/optax_update_guide.html).\nI've made a copy of the colab code with the changes I think needed to be made (I'm only unsure how I need to replace optimizer = flax.jax_utils.replicate(optimizer)).\nNow, in the training section, I get the error\npmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nat the line loss, params, opt_state = train_step_fn(step_rng, x, params, opt_state). This obviously comes from the return jax.pmap(step_fn, axis_name='device') in the \"Define the loss function\" section.\nHow can I fix this error? I've googled it, but have no idea what's going wrong here.",
        "answers": [
            "This happens because you are passing a scalar argument to a pmapped function. For example:\npython\nCopy\nimport jax\nfunc = lambda x: x ** 2\npfunc = jax.pmap(func)\n\npfunc(1.0)\n# ValueError: pmap was requested to map its argument along axis 0, which implies\n# that its rank should be at least 1, but is only 0 (its shape is ())\nIf you want to operate on a scalar, you should use the function without wrapping it in pmap:\npython\nCopy\nfunc(1.0)\n# 1.0\nAlternatively, if you want to use pmap, you should operate on an array whose leading dimension matches the number of devices:\npython\nCopy\nnum_devices = len(jax.devices())\nx = jax.numpy.arange(num_devices)\npfunc(x)\n# Array([ 0,  1,  4,  9, 16, 25, 36, 49], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78244620/jax-how-do-we-solve-the-error-pmap-was-requested-to-map-its-argument-along-axi"
    },
    {
        "title": "Cannot import name 'linear_util' from 'jax'",
        "question": "I'm trying to reproduce the experiments of the S5 model, https://github.com/lindermanlab/S5, but I encountered some issues when solving the environment. When I'm running the shell script./run_lra_cifar.sh, I get the following error\npython\nCopy\nTraceback (most recent call last):\n  File \"/Path/S5/run_train.py\", line 3, in <module>\n    from s5.train import train\n  File \"/Path/S5/s5/train.py\", line 7, in <module>\n    from .train_helpers import create_train_state, reduce_lr_on_plateau,\\\n  File \"/Path/train_helpers.py\", line 6, in <module>\n    from flax.training import train_state\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py\", line 19, in <module>\n    from . import core\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py\", line 15, in <module>\n    from .axes_scan import broadcast\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py\", line 22, in <module>\n    from jax import linear_util as lu\nImportError: cannot import name 'linear_util' from 'jax' (/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py)\nI'm running this on an RTX4090 and my CUDA version is 11.8. My jax version is 0.4.25 and jaxlib version is 0.4.25+cuda11.cudnn86\nI first tried to install the dependencies using the author's\npython\nCopy\npip install -r requirements_gpu.txt\nHowever, this doesn't seem to work in my case since I can't evenimport jax. So I installed jax according to the instructions on https://jax.readthedocs.io/en/latest/installation.html by typing\npython\nCopy\npip install --upgrade pip\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo far I've tried:\nUsing a older GPU(3060 and 2070)\nDowngrading python to 3.9\nDoes anyone know what could be wrong? Any help is appreciated",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt appears that flax is the source of the linear_util import, meaning that you are using an older flax version with a newer jax version.\nTo fix your issue, you'll either need to install an older version of JAX which still has jax.linear_util, or update to a newer version of flax which is compatible with more recent JAX versions."
        ],
        "link": "https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax"
    },
    {
        "title": "Slow JAX Optimization with ScipyBoundedMinimize and Optax - Seeking Speedup Strategies",
        "question": "I'm working on optimizing a model in jax that involves fitting a large observational dataset (4800 data points) with a complex model containing interpolation. The current optimization process using jaxopt.ScipyBoundedMinimize takes around 30 seconds for 100 iterations, with most of the time spent seemingly during or before the first iteration starts. You can find the relevant code snippet below. you can find the necessary data for the relevant code at the following link.\nnecessary data (idc, sg and cpcs)\npython\nCopy\nimport jax.numpy as jnp\nimport time as ela_time\nfrom jaxopt import ScipyBoundedMinimize\nimport optax\nimport jax\nimport pickle\n\n\nfile1 = open('idc.pkl', 'rb')\nidc = pickle.load(file1)\nfile1.close()\n\nfile2 = open('sg.pkl', 'rb')\nsg = pickle.load(file2)\nfile2.close()\n\nfile3 = open('cpcs.pkl', 'rb')\ncpcs = pickle.load(file3)\nfile3.close()\n\n\ndef model(fssc, fssh, time, rv, amp):\n\n    fssp = 1.0 - (fssc + fssh)\n\n    ivis = cpcs['common'][time]['ivis']\n    areas = cpcs['common'][time]['areas']\n    mus = cpcs['common'][time]['mus']\n\n    vels = idc['vels'].copy()\n\n    ldfs_phot = cpcs['line'][time]['ldfs_phot']\n    ldfs_cool = cpcs['line'][time]['ldfs_cool']\n    ldfs_hot = cpcs['line'][time]['ldfs_hot']\n\n    lps_phot = cpcs['line'][time]['lps_phot']\n    lps_cool = cpcs['line'][time]['lps_cool']\n    lps_hot = cpcs['line'][time]['lps_hot']\n\n    lis_phot = cpcs['line'][time]['lis_phot']\n    lis_cool = cpcs['line'][time]['lis_cool']\n    lis_hot = cpcs['line'][time]['lis_hot']\n\n    coeffs_phot = lis_phot * ldfs_phot * areas * mus\n    wgt_phot = coeffs_phot * fssp[ivis]\n    wgtn_phot = jnp.sum(wgt_phot)\n\n    coeffs_cool = lis_cool * ldfs_cool * areas * mus\n    wgt_cool = coeffs_cool * fssc[ivis]\n    wgtn_cool = jnp.sum(wgt_cool)\n\n    coeffs_hot = lis_hot * ldfs_hot * areas * mus\n    wgt_hot = coeffs_hot * fssh[ivis]\n    wgtn_hot = jnp.sum(wgt_hot)\n\n    prf = jnp.sum(wgt_phot[:, None] * lps_phot + wgt_cool[:, None] * lps_cool + wgt_hot[:, None] * lps_hot, axis=0)\n    prf /= wgtn_phot + wgtn_cool + wgtn_hot\n\n    prf = jnp.interp(vels, vels + rv, prf)\n\n    prf = prf + amp\n\n    avg = jnp.mean(prf)\n\n    prf = prf / avg\n\n    return prf\n\n\ndef loss(x0s, lmbd):\n\n    noes = sg['noes']\n\n    noo = len(idc['times'])\n\n    fssc = x0s[:noes]\n    fssh = x0s[noes: 2 * noes]\n    fssp = 1.0 - (fssc + fssh)\n    rv = x0s[2 * noes: 2 * noes + noo]\n    amp = x0s[2 * noes + noo: 2 * noes + 2 * noo]\n\n    chisq = 0\n    for i, itime in enumerate(idc['times']):\n        oprf = idc['data'][itime]['prf']\n        oprf_errs = idc['data'][itime]['errs']\n\n        nop = len(oprf)\n\n        sprf = model(fssc=fssc, fssh=fssh, time=itime, rv=rv[i], amp=amp[i])\n\n        chisq += jnp.sum(((oprf - sprf) / oprf_errs) ** 2) / (noo * nop)\n\n    wp = sg['grid_areas'] / jnp.max(sg['grid_areas'])\n\n    mem = jnp.sum(wp * (fssc * jnp.log(fssc / 1e-5) + fssh * jnp.log(fssh / 1e-5) +\n                    (1.0 - fssp) * jnp.log((1.0 - fssp) / (1.0 - 1e-5)))) / sg['noes']\n\n    ftot = chisq + lmbd * mem\n\n    return ftot\n\n\nif __name__ == '__main__':\n\n    # idc: a dictionary containing observational data (150 x 32)\n    # sg and cpcs: dictionaries with related coefficients\n\n    noes = sg['noes']\n    lmbd = 1.0\n    maxiter = 1000\n    tol = 1e-5\n\n    fss = jnp.ones(2 * noes) * 1e-5\n    x0s = jnp.hstack((fss, jnp.zeros(len(idc['times']) * 2)))\n\n    minx0s = [1e-5] * (2 * noes) + [-jnp.inf] * len(idc['times']) * 2\n    maxx0s = [1.0 - 1e-5] * (2 * noes) + [jnp.inf] * len(idc['times']) * 2\n\n    bounds = (minx0s, maxx0s)\n\n    start = ela_time.time()\n\n    optimizer = ScipyBoundedMinimize(fun=loss, maxiter=maxiter, tol=tol, method='L-BFGS-B',\n                                 options={'disp': True})\n    x0s, info = optimizer.run(x0s, bounds,  lmbd)\n\n    # optimizer = optax.adam(learning_rate=0.1)\n    # optimizer_state = optimizer.init(x0s)\n    #\n    # for i in range(1, maxiter + 1):\n    #\n    #     print('ITERATION -->', i)\n    #\n    #     gradients = jax.grad(loss)(x0s, lmbd)\n    #     updates, optimizer_state = optimizer.update(gradients, optimizer_state, x0s)\n    #     x0s = optax.apply_updates(x0s, updates)\n    #     x0s = jnp.clip(x0s, jnp.array(minx0s), jnp.array(maxx0s))\n    #     print('Objective function: {:.3E}'.format(loss(x0s, lmbd)))\n\n    end = ela_time.time()\n\n    print(end - start)   # total elapsed time: ~30 seconds\nHere's a breakdown of the relevant aspects:\nNumber of free parameters (x0s): 5263\nData: Observational data stored in idc dictionary (4800 data points)\nModel: Defined in model function, also utilizes interpolation\nOptimization methods tried:\njaxopt.ScipyBoundedMinimize with L-BFGS-B method (slow ~30 seconds, with most of the time spent during or just before the first iteration)\noptax.adam (too slow ~200 seconds)\nAttempted parallelization: I attempted to parallelize optax.adam, yet due to the inherent nature of the modeling, I couldn't succeed as the x0s couldn't be divided. (assuming I understood parallelization correctly)\nQuestions:\nWhat are potential reasons for the slowness before or during the first iteration in ScipyBoundedMinimize ?\nAre there alternative optimization algorithms in jax that might be faster for my scenario (large number of free parameters and data points, complex model with interpolation)?\nDid I misunderstand parallelization with optax.adam? Are there any strategies for potential parallelization in this case?\nAre there any code optimizations within the provided snippet that could improve performance (e.g., vectorization)?\nAdditional Information:\nHardware: Intel® Core™ i7-9750H CPU @ 2.60GHz × 12, 16 GiB RAM (laptop)\nSoftware: OS Ubuntu 22.04, Python 3.10.12, JAX 0.4.25, optax 0.2.1\nI'd appreciate any insights or suggestions to improve the optimization performance.",
        "answers": [
            "JAX code is Just-in-time (JIT) compiled, meaning that the long duration of the first step is likely related to compilation costs. The longer your code is, the more time it will take to compile.\nOne common issue leading to long compile times is the use of Python control flow such as for loops. JAX's tracing machinery essentially flattens out these loops (see JAX Sharp Bits: Control Flow). In your case, you loop over 4800 entries in your data structure, and thus are creating a very long and inefficient program.\nThe typical solution in a case like this is to rewrite your program using jax.vmap. Like most JAX constructs, this works best with a struct-of-arrays pattern rather than the array-of-structs pattern used in your data. So the first step to using vmap is to restructure your data in a way that JAX can use; it might look something like this:\npython\nCopy\nitimes = jnp.arange(len(idc['times']))\nprf = jnp.array([idc['data'][i]['prf'] for i in itimes])\nerrs = jnp.array([idc['data'][i]['errs'] for i in itimes])\n\nsprf = jax.vmap(model, in_axes=[None, None, 0, 0, 0])(fssc, fssh, itimes, rv, amp)\nchi2 = jnp.sum((oprf - sprf) / oprf_errs) ** 2) / len(times) / sprf.shape[1]\nThis will not work directly: you'll also have to restructure the data used by your model function into the struct-of-arrays style, but hopefully this gives you the general idea.\nNote also that this assumes that every entry of idc['data'][i]['prf'] and idc['data'][i]['errs'] has the same shape. If that's not the case, then I'm afraid your problem is not particularly well-suited to JAX's SPMD programming model, and there's not an easy way to work around the need for long compilations."
        ],
        "link": "https://stackoverflow.com/questions/78174997/slow-jax-optimization-with-scipyboundedminimize-and-optax-seeking-speedup-stra"
    },
    {
        "title": "Equivalent of `jax.lax.cond` for multiple boolean conditions",
        "question": "Currently jax.lax.cond works for one boolean condition. Is there a way to extend it to multiple boolean conditions?\nAs an example, below is an untraceable function:\npython\nCopy\ndef func(x):\n    if x < 0: return x\n    elif (x >= 0) & (x < 1): return 2*x\n    else: return 3*x\nHow to write this function in JAX in a traceable way?",
        "answers": [
            "One compact way to write something like this is using jnp.select:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef func(x):\n  return jnp.select([x < 0, x < 1], [x, 2 * x], default=3 * x)\n\nx = jnp.array([-0.5, 0.5, 1.5])\nprint(func(x))\n# [-0.5  1.   4.5]"
        ],
        "link": "https://stackoverflow.com/questions/78122820/equivalent-of-jax-lax-cond-for-multiple-boolean-conditions"
    },
    {
        "title": "Jax scan with dynamic number of iterations",
        "question": "I wanted to perform a scan with a dynamic number of iterations. To accomplish that, I want to recompile the function each time when iters_to_do changes.\nTo avoid a huge slowdown, I'll be using a recompilation_cache but that's beside the point.\nHowever, when I mark the argument in @partial(jax.jit) I'm still obtaining a concretization error:\npython\nCopy\n@partial(jax.jit, static_argnums=(3))\ndef iterate_for_steps(self,\n                        interim_thought: Array, \n                        mask: Array,\n                        iters_to_do: int, \n                        input_arr: Array, \n                        key: PRNGKeyArray) -> Array:\n\n    # These are constants\n    input_arr = input_arr.astype(jnp.bfloat16)\n    interim_thought = interim_thought.astype(jnp.bfloat16)\n    \n    def body_fun(i: int, thought: Array) -> Array:\n        latent = jnp.concatenate([thought, input_arr], axis=-1).astype(jnp.bfloat16)\n        latent = self.main_block(latent, input_arr, mask, key).astype(jnp.bfloat16)\n        latent = jax.vmap(self.post_ln)(latent).astype(jnp.bfloat16)  # LN to keep scales tidy\n\n        return latent\n    \n    iters_to_do = iters_to_do.astype(int).item()\n    final_val = jax.lax.scan(body_fun, interim_thought, xs=None, length=iters_to_do)\n    \n    return final_val\nFull traceback is here.\nI've tried marking multiple arguments with @partial but to no avail.\nI'm not sure how to approach debugging this - with a python debugger, I'm getting no help apart from the fact that its definitely a tracer.\nMRE\npython\nCopy\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\ninit = jnp.ones((5,))\niterations = jnp.array([1, 2, 3])\n\n@partial(jax.jit, static_argnums=(0,))\ndef iterate_for_steps(iters: int):\n    def body_fun(carry):\n        return carry * 2\n    \n    iters = iters.astype(int)\n    output = jax.lax.scan(body_fun, init, xs=None, length=iters)\n    \n    return output\n\nprint(jax.vmap(iterate_for_steps)(iterations))",
        "answers": [
            "First of all, the number of iterations in a scan must be static. If you want something similar to scan that allows a dynamic number of iterations, you can take a look at while_loop.\nRegarding your code: in isolation, your fix of marking iters_to_do as static using static_argnums is probably roughly the right idea, so long as you are passing a static int in this position when you call the function.\nBut the fact that you are calling the astype array method in your function (in iters_to_do.astype(int).item()) and getting a ConcretizationError rather than an AttributeError makes me think that the error you linked to is not coming from the code as pasted in your question.\nTo help address this discrepancy, I'd suggest trying to construct a minimal reproducible example of the problem you're having. Without that, any answer to your question is going to require too much guesswork regarding what code you're actually executing.",
            "One can use equinox's (internal as of right now) while_loop implementation which would also be able to handle a dynamic amount of iterations with checkpointing to reduce memory usage.\nNote that this can be used as a drop-in replacement to jax's native while_loop. One can also use equinox's eqx.internal.scan if they wish to leverage similar checkpointing with scan."
        ],
        "link": "https://stackoverflow.com/questions/78070050/jax-scan-with-dynamic-number-of-iterations"
    },
    {
        "title": "How to train a model using gradient descent with multioutput (vector-valued) loss function in JAX?",
        "question": "I am trying to train a model that has two outputs with gradient descent. My cost function therefore returns two errors. What is the typical way to deal with this problem?\nI've seen mentions here and there of this problem, but I haven't come up with a satisfactory solution.\nThis is a toy example that reproduces my problem:\npython\nCopy\nfrom jax import jit, random, grad\nimport optax\n\n\n@jit\ndef my_model(forz, params):\n    a, b = params\n\n    a_vect = a + forz**b\n    b_vect = b + forz**a\n\n    return a_vect, b_vect*50.\n\n\n@jit\ndef rmse(predictions, targets):\n\n    rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n    return rmse\n\n\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a, loss_b\n\n\ngrad_myloss = jit(grad(my_loss, argnums=1))\n\n# synthetic true data\nkey = random.PRNGKey(758493)\nforz = random.uniform(key, shape=(1000,))\n\ntrue_params = [8.9, 6.6]\ntrue_a, true_b = my_model(forz, true_params)\n\n# Train\nmodel_params = random.uniform(key, shape=(2,))\noptimizer = optax.adabelief(1e-1)\nopt_state = optimizer.init(model_params)\n\nfor i in range(1000):\n\n    grads = grad_myloss(forz, model_params, true_a, true_b)  # this fails\n    updates, opt_state = optimizer.update(grads, opt_state)\n    model_params = optax.apply_updates(model_params, updates)\nI understand that either the two errors has to be somehow aggregated to a single one implementing some kind of normalization to the losses (my output vectors have non-comparable units),\npython\nCopy\n@jit\ndef normalized_rmse(predictions, targets):\n   std_dev_targets = jnp.std(targets)\n   rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n   return rmse/std_dev_targets\n\n\n@jit\ndef my_loss_single(forz, params, true_a, true_b):\n\n   sim_a, sim_b = my_model(forz, params)\n\n   loss_a = normalized_rmse(sim_a, true_a)\n   loss_b = normalized_rmse(sim_b, true_b)\n\n   return jnp.sqrt((loss_a ** 2) + (loss_b * 2)) \nor I should use the Jacobian matrix (jacrev) somehow?",
        "answers": [
            "optax, like most optimization frameworks, is only able to optimize a single-valued loss function. You should decide what single-valued loss makes sense for your particular problem. A good option given the RMS form of your individual losses might be the square sum:\npython\nCopy\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a ** 2 + loss_b ** 2\nWith this change, your code executes without an error."
        ],
        "link": "https://stackoverflow.com/questions/78044014/how-to-train-a-model-using-gradient-descent-with-multioutput-vector-valued-los"
    },
    {
        "title": "JAX jax.grad on simple function that takes an array: `ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected`",
        "question": "I'm trying to implement this function and use JAX to automatically build the gradient function:\npython\nCopy\n$f(x) = \\sum\\limits_{k=1}^{n-1} [100 (x_{k+1} - x_k^2)^2 + (1 - x_k)^2]$\n(sorry, I don't know how to format math on stackoverflow. Some sister sites allow TeX, but apparently this site does not?)\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n# x is an array, which does not handle type hints well.\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].item()\n        xip1 = x[i].item()\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\n\n\n# with n=2.\ndef rosenbrock2(x: any) -> float:\n    return rosenbrock(2, x)\n\n\ngrad_rosenbrock2 = jax.grad(rosenbrock2)\n\nx = jnp.array([-1.2, 1], dtype=jnp.float32).reshape(2,1)\n\n# this line fails with the error given below\ngrad_rosenbrock2(x)\nThis last line results in:\npython\nCopy\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[1].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nI'm trying to follow the docs, and I'm confused. This is my first time using JAX or Autograd, can someone help me resolve this? Thanks!",
        "answers": [
            "The problem is that the .item() method attempts to convert an array to a static Python scalar, and since you have traced arrays within your grad transformation, conversion to a static value is not possible.\nWhat you need here is to convert a size-1 array to a scalar array, which you can do using .reshape(()):\npython\nCopy\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].reshape(())\n        xip1 = x[i].reshape(())\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\nFor more background on jax transformations and traced arrays, I'd recommend How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78030853/jax-jax-grad-on-simple-function-that-takes-an-array-concretizationtypeerror-a"
    },
    {
        "title": "Can jax.vmap() do a hstack()?",
        "question": "As the title says, I currently manually hstack() the first axis of a 3D array returned by jax.vmap(). In my code, the copy operation in hstack() is a currently a speed bottleneck. Can I avoid this by instructing jax.vmap() to do this directly?\nHere is a simplified example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(a, b, c):\n  return jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n\ndef arr(m, n):\n  return jnp.arange(m*n).reshape((m, n))\n\nm = 3\n\na = arr(m, 2)\nb = arr(m, 5)\nc = arr(m, 7)\n\nfv = jax.vmap(f)\n\nvmap_output = fv(a, b, c)\ndesired_output = jnp.hstack(fv(a, b, c))\n\nprint(vmap_output)\nprint(desired_output)\nThis yields:\npython\nCopy\n# vmap() output\n[[[  1.  10.]\n  [ 21.   0.]]\n\n [[  5.  35.]\n  [ 70.   0.]]\n\n [[  9.  60.]\n  [119.   0.]]]\n# Desired output\n[[  1.  10.   5.  35.   9.  60.]\n [ 21.   0.  70.   0. 119.   0.]]\nIf this is not possible, I would resort to pre-allocating an array and simply writing to the columns manually, but I hope to avoid this. Thanks for any clue!\nUpdate from @jakevdp's answer\nAlright, it isn't possible. So I resort to writing to the columns, but this fails as well:\npython\nCopy\ndef g(output, idx, a, b, c):\n  block = jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n  jax.lax.dynamic_update_slice_in_dim(output, block, idx*2, axis=1)\n\n# Defined above: jax, jnp, m, a, b, c\n\ng_output = jnp.zeros((2, 2*m))\nidxs = jnp.arange(m)\n\ngv = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n\ngv(g_output, idxs, a, b, c)\n\nprint(g_output)\nThis yields:\npython\nCopy\n[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\nSo writing to g_output in the function g is not retained. Is there a way around this?",
        "answers": [
            "No, vmap does not have any built-in capability to stack outputs differently than the batching semantics would imply. But if you're interested in fusing the hstack operation with the vmap operation to the extent possible, you could do so by wrapping it in jit. For example:\npython\nCopy\n@jax.jit\ndef do_the_thing(a, b, c):\n  return jnp.hstack(fv(a, b, c))\n\nprint(do_the_thing(a, b, c))\nEdit: responding to your edited question: the reason the result is all zeros is because your function doesn't do anything: it returns None, so there's no way for it to affect the input array called g_output. JAX requires pure functions so side-effecting code like what you wrote above is not compatible. If you wanted to replace the hstack with an indexed update, you could do something like this:\npython\nCopy\ni = jnp.arange(2).reshape(1, 2, 1)\nj = jnp.arange(6).reshape(3, 1, 2)\ng_output = jnp.zeros((2, 2*m)).at[i, j].set(fv(a, b, c))\nbut a nontrivial scatter operation like this will not typically be faster than a simple reshape, especially if you're running on an accelerator like GPU.\nIf your arrays are large enough that reshapes are costly, you might find that a more direct implementation is better; for example:\npython\nCopy\n@jax.jit\ndef g(a, b, c):\n  output = jnp.zeros((2, 6))\n  output = output.at[0, 0::2].set(a.sum(1))\n  output = output.at[0, 1::2].set(b.sum(1))\n  output = output.at[1, 0::2].set(c.sum(1))\n  return output\n\ng_output = g(a, b, c)"
        ],
        "link": "https://stackoverflow.com/questions/78027629/can-jax-vmap-do-a-hstack"
    },
    {
        "title": "How can I use PyTorch 2.2 with Google Colab TPUs?",
        "question": "I'm having trouble getting PyTorch 2.2 running with TPUs on Google Colab. I'm getting an error about a JAX bug, but I'm confused about this because I'm not doing anything with JAX.\nMy setup process is very simple:\npython\nCopy\n!pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html\nAnd then\npython\nCopy\nimport torch\nimport torch_xla.core.xla_model as xm\nwhich gives the error\npython\nCopy\n/usr/local/lib/python3.10/dist-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')\n This a JAX bug; please report an issue at https://github.com/google/jax/issues\n  _warn(f\"cloud_tpu_init failed: {repr(exc)}\\n This a JAX bug; please report \"\n/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nThen trying\npython\nCopy\nt1 = torch.tensor(100, device=xm.xla_device())\nt2 = torch.tensor(200, device=xm.xla_device())\nprint(t1 + t2)\ngives the error\npython\nCopy\n2 frames\n/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py in xla_device(n, devkind)\n    121 \n    122   if n is None:\n--> 123     return torch.device(torch_xla._XLAC._xla_get_default_device())\n    124 \n    125   devices = xm.get_xla_supported_devices(devkind=devkind)\n\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: No ba16c7433 device found.",
        "answers": [
            "Colab currently only provides an older generation of TPUs which is not compatible with recent JAX or PyTorch releases. It’s possible that may change in the future, but I don’t know of any official timeline of when that might happen. In the meantime, you can access recent-generation TPUs via Kaggle or Google Cloud."
        ],
        "link": "https://stackoverflow.com/questions/78014487/how-can-i-use-pytorch-2-2-with-google-colab-tpus"
    },
    {
        "title": "multivariate derivatives in jax - efficiency question",
        "question": "I have the following code which computes derivatives of the function:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n\ndef f(x):\n    return jnp.prod(x)\n\n\ndf1 = jax.grad(f)\ndf2 = jax.jacobian(df1)\ndf3 = jax.jacobian(df2)\nWith this, all the partial derivatives are available, for example (with vmap additionally):\npython\nCopy\nx = jnp.array([[ 1.,  2.,  3.,  4.,  5.],\n               [ 6.,  7.,  8.,  9., 10.],\n               [11., 12., 13., 14., 15.],\n               [16., 17., 18., 19., 20.],\n               [21., 22., 23., 24., 25.],\n               [26., 27., 28., 29., 30.]])\ndf3_x0_x2_x4 = jax.vmap(df3)(x)[:, 0, 2, 4]\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nThe question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?",
        "answers": [
            "The question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?\nEssentially, you're asking for a way to compute sparse Hessians and Jacobians; JAX does not have general support for this (see previous issue threads; e.g https://github.com/google/jax/issues/1032).\nEdit\nIn this particular case, though, since you're effectively computing the gradient/jaacobian with respect to a single element per derivative pass, you can do better by just applying the JVP to a single one-hot vector in each transformation. For example:\npython\nCopy\ndef deriv(f, x, v):\n  return jax.jvp(f, [x], [v])[1]\n\ndef one_hot(i):\n  return jnp.zeros(x.shape[1]).at[i].set(1)\n\ndf_x0 = lambda x: deriv(f, x, one_hot(0))\ndf2_x0_x2 = lambda x: deriv(df_x0, x, one_hot(2))\ndf3_x0_x2_x4 = lambda x: deriv(df2_x0_x2, x, one_hot(4))\nprint(jax.vmap(df3_x0_x2_x4)(x))\n# [  8.  63. 168. 323. 528. 783.]\nPrevious answer\nIf you're willing to relax your \"leaving f with a single argument\" criterion, you could do something like this:\npython\nCopy\ndef f(*x):\n  return jnp.prod(jnp.asarray(x))\n\ndf1 = jax.grad(f, argnums=4)\ndf2 = jax.jacobian(df1, argnums=2)\ndf3 = jax.jacobian(df2, argnums=0)\n\ndf3_x0_x2_x4 = jax.vmap(df3)(*(x.T))\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nHere rather than computing all gradients and slicing out the result, you are only computing the gradients with respect to the specific three elements you are interested in."
        ],
        "link": "https://stackoverflow.com/questions/78001753/multivariate-derivatives-in-jax-efficiency-question"
    },
    {
        "title": "JAX `custom_vjp` for functions with multiple outputs",
        "question": "In the JAX documentation, custom derivatives for functions with a single output are covered. I'm wondering how to implement custom derivatives for functions with multiple outputs such as this one?\npython\nCopy\n# want to define custom derivative of out_2 with respect to *args\ndef test_func(*args, **kwargs):\n    ...\n    return out_1, out_2",
        "answers": [
            "You can define custom derivatives for functions with any number of inputs and outputs: just add the appropriate number of elements to the primals and tangents tuples in the custom_jvp rule. For example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n@jax.custom_jvp\ndef f(x, y):\n  return x * y, x / y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primals_out = f(x, y)\n  tangents_out = (x_dot * y + y_dot * x, \n                  x_dot / y - y_dot * x / y ** 2)\n  return primals_out, tangents_out\n\nx = jnp.float32(0.5)\ny = jnp.float32(2.0)\n\njax.jacobian(f, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))\nComparing this with the result computed using the standard non-custom derivative rule for the same function shows that the results are equivalent:\npython\nCopy\ndef f2(x, y):\n  return x * y, x / y\n\njax.jacobian(f2, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))"
        ],
        "link": "https://stackoverflow.com/questions/77952302/jax-custom-vjp-for-functions-with-multiple-outputs"
    },
    {
        "title": "Can't calculate matrix exponential in python",
        "question": "I want to calculate:\npython\nCopy\nfrom jax.scipy.linalg import expm\nimport jax.numpy as jnp\nfrom functools import lru_cache, reduce\n\nnum_qubits=2\ntheta = jnp.asarray(np.pi*np.random.random((15,2,2,2,2,2,2,2,2)))\n\ndef pauli_matrix(num_qubits):\n    _pauli_matrices = jnp.array(\n    [[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]]\n    )\n    return reduce(jnp.kron, (_pauli_matrices for _ in range(num_qubits)))[1:]\n\ndef SpecialUnitary(num_qubits,theta):\n    assert theta.shape[0] == 15\n    A = jnp.tensordot(theta, pauli_matrix(num_qubits), axes=[[0], [0]])\n    print(f'{A.shape= }{pauli_matrix(num_qubits).shape=}{theta.shape=}')\n    return expm(1j*A/2)\n\nSpecialUnitary(num_qubits,theta)\nShapes: A.shape= (2, 2, 2, 2, 2, 2, 2, 2, 4, 4)pauli_matrix(num_qubits).shape=(15, 4, 4)theta.shape=(15, 2, 2, 2, 2, 2, 2, 2, 2) Error: ValueError: expected A to be a square matrix\nI'm stuck because the documentation says that the expm is calculated on the last two axes, which must be square, which is done.",
        "answers": [
            "Batched expm is supported in recent JAX versions, and you should find that this works fine in JAX v0.4.7 or newer:\npython\nCopy\nimport jax.numpy as jnp\nimport jax.scipy.linalg\n\nX = jnp.arange(128.0).reshape(2, 2, 2, 4, 4)\n\nresult = jax.scipy.linalg.expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)\nIf for some reason you must use an older JAX version, you can work around this by using jax.numpy.vectorize. For example:\npython\nCopy\nexpm = jnp.vectorize(jax.scipy.linalg.expm, signature='(n,n)->(n,n)')\n\nresult = expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)"
        ],
        "link": "https://stackoverflow.com/questions/77946266/cant-calculate-matrix-exponential-in-python"
    },
    {
        "title": "JAX `vjp` fails for vmapped function with `custom_vjp`",
        "question": "Below is an example where a function with a custom-defined vector-Jacobian product (custom_vjp) is vmapped. For a simple function like this, invoking vjp fails:\npython\nCopy\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef test_func(f: Callable[..., float],\n              R: Array\n              ) -> float:\n\n    return f(jnp.dot(R, R))\n\n\ndef test_func_fwd(f, primal):\n\n    primal_out = test_func(f, primal)\n    residual = 2. * primal * primal_out\n    return primal_out, residual\n\n\ndef test_func_bwd(f, residual, cotangent):\n\n    cotangent_out = residual * cotangent\n    return (cotangent_out, )\n\n\ntest_func.defvjp(test_func_fwd, test_func_bwd)\n\ntest_func = vmap(test_func, in_axes=(None, 0))\n\n\nif __name__ == \"__main__\":\n\n    def f(x):\n        return x\n\n    # vjp\n    primal, f_vjp = vjp(partial(test_func, f),\n                        jnp.ones((10, 3))\n                        )\n\n    cotangent = jnp.ones(10)\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[0].shape)\nThe error message says:\npython\nCopy\nValueError: Shape of cotangent input to vjp pullback function (10,) must be the same as the shape of corresponding primal input (10, 3).\nHere, I think the error message is misleading, because the cotangent input should have the same shape as the primal output, which should be (10, ) in this case. Still, it's not clear to me why this error occurs.",
        "answers": [
            "The problem is that in test_func_fwd, you recursively call test_func, but you've overwritten test_func in the global namespace with its vmapped version. If you leave the original test_func unchanged in the global namespace, your code will work as expected:\npython\nCopy\n...\n\ntest_func_mapped = vmap(test_func, in_axes=(None, 0))\n\n... \n\nprimal, f_vjp = vjp(partial(test_func_mapped, f),\n                    jnp.ones((10, 3))\n                    )"
        ],
        "link": "https://stackoverflow.com/questions/77930920/jax-vjp-fails-for-vmapped-function-with-custom-vjp"
    },
    {
        "title": "How to implement the next function (the use of Dynamic Shapes) in JAX?",
        "question": "I have a simple function that takes an jax Array as input, searches for the first occurrence of 1, and replaces it with another jax Array (specified as a second input):\npython\nCopy\nrules_int = [\n    jnp.array([0,0]),\n    jnp.array([1,1,1]),\n]\n# Even with the same size of inputs, the sizes of outputs can be different\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1)[0]\n    if index.size == 0:\n        return arr\n    index = index[0]\n    new_arr = jnp.concatenate([arr[:index], rules_int[action], arr[index+1:]])\n    return new_arr\n\nreplace_first_one(jnp.array([1]), 0)\n# result is Array([0, 0], dtype=int32)\nBut when I use vmap a get an exception:\npython\nCopy\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])  # Corresponding actions for each array\n\n# Vectorize the function\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nresult = vectorized_replace_first_one(batch_arr, batch_actions)\nindex = jnp.where(arr == 1)[0] The size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations. This BatchTracer with object id 140260750414512 was created on line:\nI read on JAX docs:\nJAX code used within transforms like jax.jit, jax.vmap, jax.grad, etc. requires all output arrays and intermediate arrays to have static shape: that is, the shape cannot depend on values within other arrays.\nPlease suggest how to make it work?\nIdeally, these rules should be applied recursively until there are no rules to apply. (string rewriting system)",
        "answers": [
            "As written, it is impossible to do this with vmap because the output of your function has a shape that depends on the value of action, and so the output would have to be a ragged array, which JAX does not support (see JAX Sharp Bits: Dynamic Shapes).\nTo make the function compatible with vmap, you'll have to adjust it so that it has static shape semantics: in particular, every entry of rules_int must have the same length, and you cannot return arr alone in cases where arr doesn't have any 1 entries. Making these changes and adjusting the logic to avoid dynamically-shaped intermediates, you could write something like this:\npython\nCopy\nimport jax\n\nrules_int = jnp.array([\n    [0,0],\n    [1,1],\n])\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1, size=1)[0][0]\n    arr_to_insert = rules_int[action]\n    output_size = len(arr) - 1 + len(arr_to_insert)\n    new_arr = jnp.where(jnp.arange(output_size) < index,\n                        jnp.concatenate([arr[:-1], arr_to_insert]),\n                        jnp.concatenate([arr_to_insert, arr[1:]]))\n    return jax.lax.dynamic_update_slice(new_arr, arr_to_insert, (index,))\n\nreplace_first_one(jnp.array([1]), 0)\n# Array([0, 0], dtype=int32)\npython\nCopy\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])\n\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nvectorized_replace_first_one(batch_arr, batch_actions)\n# Array([[0, 0, 4, 5, 1],\n#        [6, 1, 1, 8, 1]], dtype=int32)\nIf adjusting the semantics of your function in this way to avoid dynamic shapes is not viable given your use-case, then your use-case is unfortunately not compatible with vmap or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77915540/how-to-implement-the-next-function-the-use-of-dynamic-shapes-in-jax"
    },
    {
        "title": "jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed",
        "question": "I am trying to run multiple sbx programs (that use JAX) concurrently using joblib. Here is my program -\npython\nCopy\n'''\nFor installation please do -\npip install gym\npip install sbx-rl\npip install mujoco\npip install shimmy\n'''\nfrom joblib import Parallel, delayed\n\nimport gym\nfrom sbx import SAC\n\n# from stable_baselines3 import SAC\ndef train():\n\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env, verbose=1)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\ndef train_model():\n\n    train()\n\n\n\nif __name__ == '__main__':\n    Parallel(n_jobs=10)(delayed(train)() for i in range(3))\nThis is the error that I am getting -\npython\nCopy\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n2024-01-30 11:19:12.354168: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:\nINTERNAL: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory\n2024-01-30 11:19:12.354264: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\njoblib.externals.loky.process_executor._RemoteTraceback: \n\"\"\"\njax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/work/LAS/usr/tbd/5_test.py\", line 23, in my_func\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 109, in __init__\n    self._setup_model()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 126, in _setup_model\n    self.key = self.policy.build(self.key, self.lr_schedule, self.qf_learning_rate)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/policies.py\", line 143, in build\n    key, actor_key, qf_key, dropout_key = jax.random.split(key, 4)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 303, in split\n    return _return_prng_keys(wrapped, _split(typed_key, num))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 289, in _split\n    return prng.random_split(key, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 769, in random_split\n    return random_split_p.bind(keys, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 444, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 447, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 935, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 781, in random_split_impl\n    base_arr = random_split_impl_base(\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 787, in random_split_impl_base\n    return split(base_arr)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 786, in <lambda>\n    split = iterated_vmap_unary(keys_ndim, lambda k: impl.split(k, shape))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 1291, in threefry_split\n    return _threefry_split(key, shape)\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/work/LAS/usr/tbd/5_test.py\", line 27, in <module>\n    Parallel(n_jobs=3)(delayed(my_func)() for i in range(3))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1952, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\n    yield from self._retrieve()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1699, in _retrieve\n    self._raise_error_fast()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1734, in _raise_error_fast\n    error_job.get_result(self.timeout)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 736, in get_result\n    return self._return_or_raise()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 754, in _return_or_raise\n    raise self._result\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\nI am using a 40 GB GPU (a100-pcie). Therefore I doubt that my GPU is running out of memory. Please let me know if any clarification is needed.\nEdit 1: This is how I call my program - export XLA_PYTHON_CLIENT_PREALLOCATE=false && python 5_test.py (The name of my program is 5_test.py)",
        "answers": [
            "It appears you are using multiple processes targeting the same GPU. In each process, JAX will attempt to reserve 75% of the available GPU memory (see GPU memory allocation), so attempting this with two or more processes will exhaust the available memory.\nYou could fix this by turning off pre-allocation as mentioned in that doc, by setting the environment variables XLA_PYTHON_CLIENT_PREALLOCATE=false or XLA_PYTHON_CLIENT_MEM_FRACTION=.XX (with .XX set to .08 or something suitable), but I suspect the end result will be less efficient than if you had just run your full program from a single JAX process: multiple host processes targeting a single GPU device concurrently will just compete with each other for resources and lead to suboptimal results."
        ],
        "link": "https://stackoverflow.com/questions/77908236/jaxlib-xla-extension-xlaruntimeerror-internal-failed-to-execute-xla-runtime-ex"
    },
    {
        "title": "Getting derivatives of NNs according to its inputs by batches in JAX",
        "question": "There is a neural network that takes as an input a two variables: net(x, t), where x is usually d-dim, and t is a scalar. The NN outputs a vector of length d. x and t might be batches, so x is of shape (b, d), and t is (b, 1), and the output is (b,d). I need to find\nderivative d out/dt of the NN output. It should be d dim vector (or (batch, d));\nderivative d out/dx of the NN\ngradient of divergence of the NN output according to x, it still should be (batch, d) vector\nSince the NN doesn’t output a scalar, I don’t think Jax grad would help here. I know how to do what I described in torch, but I’m totally new to JAX. I’d really appreciate your help with this question!\nThere is an example:\npython\nCopy\nimport jaxlib\nimport jax\nfrom jax import numpy as jnp\nimport flax.linen as nn\nfrom flax.training import train_state\n\n\n\nclass NN(nn.Module):\n    hid_dim : int # Number of hidden neurons\n    output_dim : int # Number of output neurons\n\n    @nn.compact  \n    def __call__(self, x, t):\n        out = jnp.hstack((x, t))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.Dense(features=self.output_dim)(out)\n        return out\n\nd = 3\nbatch_size = 10\nnet = NN(hid_dim=100, output_dim=d)\n\nrng_nn, rng_inp1, rng_inp2 = jax.random.split(jax.random.PRNGKey(100), 3)\ninp_x = jax.random.normal(rng_inp1, (1, d)) # batch, d\ninp_t = jax.random.normal(rng_inp2, (1, 1))\nparams_net = net.init(rng_nn, inp_x, inp_t)\n\nx = jax.random.normal(rng_inp2, (batch_size, d)) # batch, d\nt = jax.random.normal(rng_inp1, (batxh_size, 1))\n\nout_net = net.apply(params_net, x, t)\n\noptimizer = optax.adam(1e-3)\n\nmodel_state = train_state.TrainState.create(apply_fn=net.apply,\n                                            params= params_net,\n                                            tx=optimizer)\nI'd like to calculate an $L_2$ loss based on some derivatives of the NN's outputs according to its inputs. For example, I'd like to have d f/dx or d f/dt where f is the NN. ALso the gradient of the divergence by x. I assume it'd be something like\npython\nCopy\ndef find_derivatives(net, params, X, t):\n    d_dt = lambda net, params, X, t: jax.jvp(lambda time: net(params, X, t), (t, ), (jnp.ones_like(t), ))\n    d_dx = lambda net, params, X, t: jax.jvp(lambda X: net(params, X, t), (Xs_all, ), (jnp.ones_like(X), ))\n    out_f, df_dt = d_dt(net.apply, params, X, t)\n\n    d_ddx = lambda net, params, X, t: d_dx(lambda params, X, t: d_dx(net, params, X, t)[1], params, X, t)\n    df_dx, df_ddx = d_ddx(net.apply, params, X, t)\n    \n    return out_f, df_dt, df_dx, df_ddx\n\n\nout_f, df_dt, df_dx, df_ddx = find_derivatives(net, params_net, x, t)",
        "answers": [
            "I would avoid using jax.jvp here, because this is meant as a lower-level API. You can use jax.jacobian to compute the Jacobian (since your function has multiple outputs), and vmap for batching. For example:\npython\nCopy\ndf_dx = jax.vmap(\n    jax.jacobian(net.apply, argnums=1),\n    in_axes=(None, 0, 0)\n  )(params_net, x, t)\nprint(df_dx.shape)  # (10, 3, 3)\n\ndf_dt = jax.vmap(\n    jax.jacobian(net.apply, argnums=2),\n    in_axes=(None,0, 0)\n  )(params_net, x, t).reshape(10, 3)\nprint(df_dt.shape)  # (10, 3)\nHere df_dx is the batch-wise Jacobian of the 3-dimensional output vector with respect to the 3-dimensional x input vector, and df_dt is the batch-wise gradient of the 3-dimensional output vector with respect to the input t."
        ],
        "link": "https://stackoverflow.com/questions/77897419/getting-derivatives-of-nns-according-to-its-inputs-by-batches-in-jax"
    },
    {
        "title": "`jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].`",
        "question": "I am trying to perform multiprocessing to parallelize my program (that uses JAX) using pmap. I am a newbie with JAX and realize that maybe pmap isn't the right approach. The documentation here, said that pmap can express SPMD programs (which is the case here) and therefore I decided to use it.\nHere's my program. I am basically trying to run a reinforcement learning program (that uses JAX too) twice, using parallel execution -\n'''\nFor installation please do -\npip install gym\npip install sbx\npip install mujoco\npip install shimmy\n'''\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\nimport gym\nfrom sbx import SAC\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda _: my_func())(jnp.arange(2))\nI get the following error -\n(tbd) thoma@thoma-Lenovo-Legion-5-15IMH05H:~/PycharmProjects/tbd$ python new.py\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 678, in parallel_callable\n    pmap_computation = lower_parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 825, in lower_parallel_callable\n    jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 748, in stage_parallel_callable\n    jaxpr, out_sharded_avals, consts = pe.trace_to_jaxpr_final(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2233, in trace_to_jaxpr_final\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/core.py\", line 605, in __array__\n    raise TracerArrayConversionError(self)\njax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n   0% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/700,000  [ 0:00:00 < -:--:-- , ? it/s ]Exception ignored in: <function tqdm.__del__ at 0x7fa875eb8af0>\nTraceback (most recent call last):\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/std.py\", line 1149, in __del__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/rich.py\", line 120, in close\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1177, in __exit__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1163, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/live.py\", line 155, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1137, in line\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1674, in print\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1535, in _collect_renderables\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/protocol.py\", line 28, in rich_cast\nImportError: sys.meta_path is None, Python is likely shutting down\nBasically I am trying to replace the parallelization performed by joblib with JAX. Here's my original program that I am changing -\nfrom joblib import Parallel, delayed\nimport gym\nimport os\nfrom sbx import SAC\nimport multiprocessing\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\n\nParallel(n_jobs=2)(delayed(my_func)() for i in range(2))",
        "answers": [
            "The problem is here:\n File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\nThe sbx package is calling np.array on the inputs – this tells me that sbx is built on NumPy, not on JAX. JAX transformations like pmap are not compatible with NumPy functions, they require functions written with JAX operations. Unless sbx is substantially re-designed, you won't be able to use it with pmap, vmap, jit, grad, or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77892458/jax-errors-tracerarrayconversionerror-the-numpy-ndarray-conversion-method-ar"
    },
    {
        "title": "How to use JAX pmap with CPU cores",
        "question": "I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible - Here's my code -\nimport jax.numpy as jnp\nimport os\nfrom jax import pmap\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\nHere's the error -\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 682, in parallel_callable\n    pmap_executable = pmap_computation.compile()\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 923, in compile\n    executable = UnloadedPmapExecutable.from_hlo(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 993, in from_hlo\n    raise ValueError(msg.format(shards.num_global_shards,\njax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\nValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\nBased on this and this discussion, I did this os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8', but it doesn't seem to work.\nEdit 1:\nI tried this but it still doesn't work -\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\n\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)",
        "answers": [
            "XLA flags are read when JAX is imported, so you need to set them before importing JAX if you want the flags to have an effect.\nYou should also make sure you're in a clean runtime (i.e. not using a Jupyter kernel where you have previously imported jax).\nAdditionally, keep in mind that --xla_force_host_platform_device_count=8 only affects the host (CPU) device count, so the code as written above won't work if you're using GPU-enabled JAX with a single GPU device. If this is the case, you can force pmap to run on the non-default CPU devices using the devices argument:\npython\nCopy\nout = pmap(lambda x: x ** 2, devices=jax.devices('cpu')(jnp.arange(8))"
        ],
        "link": "https://stackoverflow.com/questions/77889712/how-to-use-jax-pmap-with-cpu-cores"
    },
    {
        "title": "How is it possible that jax vmap returns not iterable?",
        "question": "python\nCopy\nimport jax\nimport pgx\nfrom jax import vmap, jit\nimport jax.numpy as jnp\n\nenv = pgx.make(\"tic_tac_toe\")\nkey = jax.random.PRNGKey(42)\n\nstates = jax.jit(vmap(env.init))(jax.random.split(key, 4))\ntype(states)\nstates has a type pgx.tic_tac_toe.State. I was expecting an Iterable object with a size 4. Somehow iterable results are inside pgx.tic_tac_toe.State.\nCan you please explain how is it possible that jax vmap returns not iterable?\nHow to force vmap to return the next result:\npython\nCopy\nstates = [env.init(key) for key in jax.random.split(key, 4)]\nNote, this code works as expected:\npython\nCopy\ndef square(x):\n    return x ** 2\ninputs = jnp.array([1, 2, 3, 4])\nresult = jax.vmap(square)(inputs)\nprint(result) # list object",
        "answers": [
            "Can you please explain how is it possible that jax vmap returns not iterable?\nWhen passed a non-array object, vmap will map the leading axes of each array in its flattened pytree representation. You can see the shapes in the flattened object here:\npython\nCopy\nprint([arr.shape for arr in jax.tree_util.tree_flatten(states)[0]])\n# [(4,), (4, 3, 3, 2), (4, 2), (4,), (4,), (4, 9), (4,), (4,), (4, 9)]\nThis is an example of the struct-of-arrays pattern used by vmap, where it sounds like you were expececting an array-of-structs pattern.\nHow to force vmap to return the next result\nIf you wanted to convert this output into the list of state objects you were expecting, you could do so using utilities in jax.tree_util:\npython\nCopy\nleaves, treedef = jax.tree_util.tree_flatten(states)\nstates_list = [treedef.unflatten(leaf) for leaf in zip(*leaves)]\nprint(len(states_list))\n# 4\nThat said, it appears that pgx is built to work natively with the original struct-of-arrays pattern, so you may find that you won't actually need this unstacked version in practice."
        ],
        "link": "https://stackoverflow.com/questions/77881821/how-is-it-possible-that-jax-vmap-returns-not-iterable"
    },
    {
        "title": "finding the maximum of a function using jax",
        "question": "I have a function which I would like to find its maximum by optimizing two of its variables using Jax.\nThe current code that I have currently, which does not work, reads\npython\nCopy\nimport jax.numpy as jnp\nimport jax \nimport scipy\nimport numpy as np\n\ndef temp_func(x,y,z):\n    tmp = x + jnp.dot( jnp.power(y,3), jnp.tanh(z) )\n    return -tmp\ndef obj_func(xy, z):\n    x,y = xy[:2], xy[2:].reshape(2,2)\n    return jnp.sum(temp_func(jnp.array(x),jnp.array(y),z))\n\ngrad_tmp = jax.grad(obj_func, argnums=0) # x,y\n\nxy = jnp.concatenate([np.random.rand(2), np.random.rand(2*2) ])\nz= jnp.array( np.random.rand(2,2) )\nprint(obj_func(xy,z))\n\nresult = scipy.optimize.minimize(obj_func,\n                                 xy,\n                                 args=(z,),\n                                 method='L-BFGS-B',\n                                 jac=grad_tmp\n                                )\nWith this code, I get the error ValueError: failed in converting 7th argument g' of _lbfgsb.setulb to C/Fortran array` Do you have any suggestions to resolve the issue?",
        "answers": [
            "You might think about using the jax version of scipy.optimize.minimize, which will automatically compute and use the derivative:\npython\nCopy\nimport jax.scipy.optimize\nresult = jax.scipy.optimize.minimize(obj_func, xy, args=(z,), method='BFGS')\nThat said, the results in either case are not going to be very meaningful, because your objective function is linearly decreasing in x and y, so it will be minimized when x, y → ∞"
        ],
        "link": "https://stackoverflow.com/questions/77860052/finding-the-maximum-of-a-function-using-jax"
    },
    {
        "title": "JAX python C callbacks",
        "question": "Numba allows to create C-callbacks directly in python with the @cfunc-decorator ( https://numba.pydata.org/numba-doc/0.42.0/user/cfunc.html ):\npython\nCopy\n@cfunc(\"float64(float64)\") \ndef square(x):\n    return x**2\nTo clarify, the resulting function is a pure C-function, which can then be called directly from C-code.\nIs there an equivalent functionality available in JAX ( https://jax.readthedocs.io/en/latest/# )?\nI have been searching for a while but couldn't find anything. I would appreciate any tips.",
        "answers": [
            "No, JAX doesn't provide any API similar to Numba's cfunc."
        ],
        "link": "https://stackoverflow.com/questions/77855169/jax-python-c-callbacks"
    },
    {
        "title": "How to loop a random number of times in jax with jit compilation?",
        "question": "I am using jax in python, and I want to loop over some code for a random number of times. This is part of a function which is jit compiled later. I have a small example below which should explain what I want to do.\npython\nCopy\nnum_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\narr = []\nfor i in range(num_iters):\n  arr += [i*i]\nThis works without any error and gives arr=[0,1,4] at the end of the loop (with the fixed seed of 0 that we're using in PRNGKey).\nHowever, if this is part of a jit-compiled function:\npython\nCopy\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nI get a TracerIntegerConversionError on num_iters. The function works fine without the jit decorator. How to get this to work with jit? I basically just want to construct the list arr whose length depends on a random number. Alternatively, I can also use a list with the maximum possible size, but then I'd have to loop over it a random number of times.\nFurther context\nIt's possible to make it not throw an error using a numpy random number generator instead:\npython\nCopy\n@jax.jit\ndef do_stuff(start):\n  np_rng = np.random.default_rng()\n  num_iters = np_rng.integers(1, 10)\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nHowever, this is not what I want. There is a jax rng which is passed to my function which I wish to use to generate num_iters. Otherwise, arr always has the same length since the numpy seed is fixed to what was available at jit-compile time, and I always get the same result without any randomness. However, if I use that rng key as seed for numpy (like np.random.default_rng(seed=rng[0])) it again gives the following error:\npython\nCopy\nTypeError: SeedSequence expects int or sequence of ints for entropy not Traced<ShapedArray(uint32[])>with<DynamicJaxprTrace(level=1/0)>",
        "answers": [
            "You could use jax.lax.fori_loop for this:\npython\nCopy\nimport jax\n\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  return jax.lax.fori_loop(0, num_iters, lambda i, val: val + i * i, start)\n\nprint(do_stuff(10))\n# 15",
            "Jax complains in this case, because you try to use a traced value as a static integer. See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError for more information.\nAs one possible solution you could pass the num_iters as an argument to do_stuff, declare it as static and create the keys outside, along the lines of:\npython\nCopy\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(1,))\ndef do_stuff(start, num_iters):    \n  arr = []\n  \n  for i in range(num_iters):\n    arr += [i*i]\n  \n  for value in arr:\n    start += value\n  \n  return start\n\nkey = jax.random.PRNGKey(238)\n\nfor _ in range(4):\n  key, _ = jax.random.split(key)\n  num_iters = int(jax.random.randint(key, (1,), 1, 10))\n  print(do_stuff(0, num_iters))\nWhich prints:\npython\nCopy\n5\n0\n140\n30\nOther alternative solutions are proposed in the link I listed above.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/77844256/how-to-loop-a-random-number-of-times-in-jax-with-jit-compilation"
    },
    {
        "title": "Update JAX array based on values in another array",
        "question": "I have a Jax array X like this:\n[[[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]]\nHow do I set the values of this array to 1, whose indices are given by array Y:\n[[[1 2]\n [1 2]]\n\n [[0 2]\n [0 1]]\n\n [[1 0]\n [1 0]]]\nDesired output:\n([[[0., 1., 1.],\n        [0., 1., 1.]],\n\n       [[1., 0., 1.],\n        [1., 1., 0.]],\n\n       [[1., 1., 0.],\n        [1., 1., 0.]]]",
        "answers": [
            "There are a couple ways to approach this. First let's define the arrays:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.zeros((3, 2, 3))\nindices = jnp.array([[[1, 2],\n                      [1, 2]],\n                     [[0, 2],\n                      [0, 1]],\n                     [[1, 0],\n                      [1, 0]]])\nOne way to do this is to use typical numpy-style broadcasting of indices. It might look like this:\npython\nCopy\ni = jnp.arange(3).reshape(3, 1, 1)\nj = jnp.arange(2).reshape(2, 1)\nx = x.at[i, j, indices].set(1)\nprint(x)\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]\nAnother option is to use a double-vmap transformation to compute the batched indices:\npython\nCopy\nf = jax.vmap(jax.vmap(lambda x, i: x.at[i].set(1)))\nprint(f(x, indices))\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]"
        ],
        "link": "https://stackoverflow.com/questions/77799930/update-jax-array-based-on-values-in-another-array"
    },
    {
        "title": "How to understand and debug memory usage with JAX?",
        "question": "I am new to JAX and trying to learn use it for running some code on a GPU. In my example I want to search for regular grids in a point cloud (for indexing X-ray diffraction data).\nWith test_mats[4_000_000,3,3] the memory usage seems to be 15 MB. But with test_mats[5_000_000,3,3] I get an error about it wanting to allocate 19 GB.\nI can't tell whether this is a glitch in JAX, or because I am doing something wrong. My example code and output are below. I guess the problem is that it wants to create a temporary array of (N, 3, gvec.shape[1]) before doing the reduction, but I don't know how to see the memory profile for what happens inside the jitted/vmapped function.\npython\nCopy\nimport sys\nimport os\nimport jax\nimport jax.random\nimport jax.profiler\n\nprint('jax.version.__version__',jax.version.__version__)\n\nimport scipy.spatial.transform\nimport numpy as np\n\n# (3,N) integer grid spot positions\nhkls = np.mgrid[-3:4, -3:4, -3:4].reshape(3,-1)\n\nUmat = scipy.spatial.transform.Rotation.random( 10, random_state=42 ).as_matrix()\na0 = 10.13\ngvec = np.swapaxes( Umat.dot(hkls)/a0, 0, 1 ).reshape(3,-1)\n\ndef count_indexed_peaks_hkl( ubi, gve, tol ):\n    \"\"\" See how many gve this ubi can account for \"\"\"\n    hkl_real = ubi.dot( gve )\n    hkl_int = jax.numpy.round( hkl_real )\n    drlv2 = ((hkl_real - hkl_int)**2).sum(axis=0)\n    npks = jax.numpy.where( drlv2 < tol*tol, 1, 0 ).sum()\n    return npks\n\ndef testsize( N ):\n    print(\"Testing size\",N)\n    jfunc = jax.vmap( jax.jit(count_indexed_peaks_hkl), in_axes=(0,None,None))\n    key = jax.random.PRNGKey(0)\n    test_mats = jax.random.orthogonal(key, 3, (N,) )*a0\n    dev_gvec = jax.device_put( gvec )\n    scores = jfunc( test_mats, gvec, 0.01 )\n    jax.profiler.save_device_memory_profile(f\"memory_{N}.prof\")\n    os.system(f\"~/go/bin/pprof -top {sys.executable} memory_{N}.prof\")\n\ntestsize(400000)\ntestsize(500000)\nOutput is:\npython\nCopy\ngpu4-03:~/Notebooks/JAXFits % python mem.py \njax.version.__version__ 0.4.16\nTesting size 400000\nFile: python\nType: space\nShowing nodes accounting for 15.26MB, 99.44% of 15.35MB total\nDropped 25 nodes (cum <= 0.08MB)\n      flat  flat%   sum%        cum   cum%\n   15.26MB 99.44% 99.44%    15.26MB 99.44%  __call__\n         0     0% 99.44%    15.35MB   100%  [python]\n         0     0% 99.44%     1.53MB 10.00%  _pjit_batcher\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl_python\n         0     0% 99.44%    15.30MB 99.70%  _python_pjit_helper\n         0     0% 99.44%    15.35MB   100%  bind\n         0     0% 99.44%    15.35MB   100%  bind_with_trace\n         0     0% 99.44%    15.30MB 99.70%  cache_miss\n         0     0% 99.44%    15.30MB 99.70%  call_impl_cache_miss\n         0     0% 99.44%     1.53MB 10.00%  call_wrapped\n         0     0% 99.44%    13.74MB 89.51%  deferring_binary_op\n         0     0% 99.44%    15.35MB   100%  process_primitive\n         0     0% 99.44%    15.30MB 99.70%  reraise_with_filtered_traceback\n         0     0% 99.44%    15.35MB   100%  testsize\n         0     0% 99.44%     1.53MB 10.00%  vmap_f\n         0     0% 99.44%    15.31MB 99.74%  wrapper\nTesting size 500000\n2023-12-14 10:26:23.630474: W external/tsl/tsl/framework/bfc_allocator.cc:296] Allocator\n(GPU_0_bfc) ran out of memory trying to allocate 19.18GiB with freed_by_count=0. The caller\nindicates that this is not a failure, but this may mean that there could be performance \ngains if more memory were available.\nTraceback (most recent call last):\n  File \"~/Notebooks/JAXFits/mem.py\", line 38, in <module>\n    testsize(500000)\n  File \"~/Notebooks/JAXFits/mem.py\", line 33, in testsize\n    scores = jfunc( test_mats, gvec, 0.01 )\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to \nallocate 20596777216 bytes.\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following \nexception. Set JAX_TRACEBACK_FILTERING=off to include these.```",
        "answers": [
            "The vmapped function is attempting to create an intermediate array of shape [N, 3, 3430]. For N=400_000, with float32 this amounts to 15GB, and for N=500_000 this amounts to 19GB.\nYour best option in this situation is probably to split your computation into sequentially-executed batches using lax.map or similar. Unfortunately there's not currently any automatic way to do that kind of chunked vmao, but there is a relevant feature request at https://github.com/google/jax/issues/11319, and there are some useful suggestions in that thread."
        ],
        "link": "https://stackoverflow.com/questions/77659069/how-to-understand-and-debug-memory-usage-with-jax"
    },
    {
        "title": "JAX dynamic slice inside of control flow function",
        "question": "I would like to do dynamic slicing inside of lax.while_loop() using a variable carried over, getting an error as below. I know in the case of a simple function, I can pass the variable as a static value, using partial , but how can I handle the case in which the variable (in my case length) is carried over?\nnew_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got (0, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThis is how I coded. This code is just to illustrate the problem. What I would like to do is to extract a part of u and do some operations. Thank you.\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import lax\nfrom functools import partial\nimport jax\n\nu = jnp.array([[1,2,3,4,5],[0,0,0,0,0]])\n\ndef body_fun(carry):\n    length, sum_u = carry\n    new_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n    #new_u = lax.dynamic_slice(u,(0,0),(2,4))\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n\ndef cond_fun(carry):\n    length, sum_u = carry\n    keep_condition = sum_u < 5\n    return keep_condition\n\ninit_carry = (5,10)\nout = lax.while_loop(cond_fun, body_fun, init_carry)\nprint(out)",
        "answers": [
            "The problem is that you are attempting to construct a dynamically-shaped array, and JAX does not support dynamically-shaped arrays (length is a dynamic variable in your loop). See JAX Sharp Bits: Dynamic Shapes for more.\nA typical strategy in these cases is to use a statically-sized array while masking out a dynamic range of values; in your case, you could use a value of 0 for the masked values so that they don't contribute to the sum. It might look like this:\npython\nCopy\ndef body_fun(carry):\n    length, sum_u = carry\n    idx = jnp.arange(u.shape[1])\n    new_u = jnp.where(idx < length, u, 0)\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n(Side-note: it seems like you were using dynamic_slice in hopes that you could generate dynamic array shapes, but the dynamic in dynamic_slice refers to the dynamic offset, not a dynamic size)."
        ],
        "link": "https://stackoverflow.com/questions/77603954/jax-dynamic-slice-inside-of-control-flow-function"
    },
    {
        "title": "Efficient copying of an ensemble in JAX",
        "question": "I have an ensemble of models and want to assign the same parameters to each of the models. Both the models' parameters as well as the new parameters have the same underlying structure. Currently I use the following approach that uses a for-loop.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nmodel1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nmodel2 = [\n    [jnp.asarray([2]), jnp.asarray([3, 4])],\n    [jnp.asarray([5]), jnp.asarray([6, 7])],\n]\n\nmodels = [model1, model2]\n\nparams = [\n    [jnp.asarray([3]), jnp.asarray([4, 5])],\n    [jnp.asarray([6]), jnp.asarray([7, 8])],\n]\n\nmodels = [jax.tree_map(jnp.copy, params) for _ in range(len(models))]\nIs there a more efficient way in JAX to assign the parameters from params to each model in models?",
        "answers": [
            "Since JAX arrays are immutable, there's no need to copy the parameter arrays, and you could achieve the same result like this:\npython\nCopy\nmodels = len(models) * [params]"
        ],
        "link": "https://stackoverflow.com/questions/77595758/efficient-copying-of-an-ensemble-in-jax"
    },
    {
        "title": "Weighted sum of pytrees in JAX",
        "question": "I have a pytree represented by a list of lists holding parameter tuples. The sub-lists all have the same structure (see example).\nNow I would like to create a weighted sum so that the resulting pytree has the same structure as one of the sub-lists. The weights for each sub-list are stored in a separate array / list.\nSo far I have the following code that seems to works but requires several steps and for-loop that I would like avoid for performance reasons.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n]\n\nlist_2 = [\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n]\n\nlist_3 = [\n    [jnp.asarray([[7, 1], [4, 4]]), jnp.asarray([6, 2])],\n    [jnp.asarray([[6, 4], [3, 7]]), jnp.asarray([7, 3])],\n]\n\nweights = [1, 2, 3] \npytree = [list_1, list_2, list_3]\n\nweighted_pytree = [jax.tree_map(lambda tree: weight * tree, tree) for weight, tree in zip(weights, pytree)]\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *weighted_pytree)",
        "answers": [
            "I think this will do what you have in mind:\npython\nCopy\ndef wsum(*args, weights=weights):\n  return jnp.asarray(weights) @ jnp.asarray(args)\n\nreduced = jax.tree_util.tree_map(wsum, *pytree)\nFor the edited question, where tree elements have more general shapes, you can define wsum like this instead:\npython\nCopy\ndef wsum(*args, weights=weights):\n  return sum(weight * arg for weight, arg in zip(weights, args))"
        ],
        "link": "https://stackoverflow.com/questions/77550969/weighted-sum-of-pytrees-in-jax"
    },
    {
        "title": "Add noise to parameters of ensemble in JAX",
        "question": "I use the following code to create parameters for an ensemble of models stored as list of lists holding tuples of weights and biases. How do I efficiently add random noise to all parameters of the ensemble with JAX? I tried to use tree_map() but run into many errors probably caused by the nested structure.\nCould you please provide guidance on how to use tree_map() in this case or point to other methods that JAX provides for such a case?\npython\nCopy\nfrom jax import Array\nfrom jax import random\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = 0 * random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = 0 * random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\nif __name__ == \"__main__\":\n    num_models = 2\n    layer_dims = [2, 3, 4]\n    \n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    # Add noise to ensemble.",
        "answers": [
            "Here's one way you could do this:\npython\nCopy\nfrom jax import tree_util\nleaves, tree = tree_util.tree_flatten(ensemble)\nkey, *subkeys = random.split(key, len(leaves) + 1)\nsubkeys = tree_util.tree_unflatten(tree, subkeys)\n\ndef add_noise(val, key, eps=0.1):\n  return val + eps * random.normal(key, val.shape)\n\nensemble_with_noise = tree_util.tree_map(add_noise, ensemble, subkeys)\nEssentially, you create a tree of subkeys with the same structure as the tree of parameters, then use tree_map to apply the noise function to the tree.",
            "Another possiblity is to use ravel_pytree to first flatten the parameters into a single vector, then add noise, then unravel the noised vector back to the original tree structure:\npython\nCopy\nimport jax\nimport jax.flatten_util\n\nvector, unravel = jax.flatten_util.ravel_pytree(ensemble)\nnoisy_vector = vector + jax.random.normal(key, vector.shape)\nnoisy_ensemble = unravel(vector)"
        ],
        "link": "https://stackoverflow.com/questions/77539206/add-noise-to-parameters-of-ensemble-in-jax"
    },
    {
        "title": "JAX grad: derivate with respect an specific variable in a matrix",
        "question": "I am using Jax to do the grad of a matrix. For example I have a function f(A) where A is a matrix like A = \\[\\[a,b\\], \\[c,d\\]\\]. I want to just do the grad of f(A) for a,c and d (more specific for the lower-triangular part). How can I do that? also for a general NxN matrix not just the 2x2.\nI tried to convert the regular grad in a lower-triangular, but I am not sure if that is the same of if the output is correct.",
        "answers": [
            "JAX does not offer any way to take the gradient with respect to individual matrix elements. There are two ways you could proceed; first, you could take the gradient with respect to the entire array and extract the elements you're interested in; for example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(A):\n  return (A ** 2).sum()\n\nA = jnp.array([[1.0, 2.0], [3.0, 4.0]])\ndf_dA = jax.grad(f)(A)\nprint(df_dA[0, 0], df_dA[0, 1], df_dA[1, 2])\npython\nCopy\n2.0 4.0 8.0\nAlternatively, you could split the entries of the array into individual function arguments, and then use argnums to take the gradient with respect to just the ones you're interested in:\npython\nCopy\ndef f(a, b, c, d):\n  A = jnp.array([[a, b], [c, d]])\n  return (A ** 2).sum()\n\ndf_da, df_db, df_dc = jax.grad(f, argnums=(0, 1, 2))(1.0, 2.0, 3.0, 4.0)\nprint(df_da, df_db, df_dc)\npython\nCopy\n2.0 4.0 8.0\nIn general you'll probably find the first approach to be both easier to use in practice, and also more efficient. It does have some wasted computation, but sticking with vectorized computations will generally be a net win, especially if you're running on accelerators like GPU or TPU."
        ],
        "link": "https://stackoverflow.com/questions/77517357/jax-grad-derivate-with-respect-an-specific-variable-in-a-matrix"
    },
    {
        "title": "JAX vmap JIT behind the scenes? [closed]",
        "question": "Closed. This question needs to be more focused. It is not currently accepting answers.\nWant to improve this question? Guide the asker to update the question so it focuses on a single, specific problem. Narrowing the question will help others answer the question concisely. You may edit the question if you feel you can improve it yourself. If edited, the question will be reviewed and might be reopened.\nClosed 4 months ago.\nImprove this question\nI'm trying to vmap a function. My understanding of vmap is essentially anywhere I would write a ~for loop/list comprehension I should instead consider vmapping. I have a few points of confusion:\nDoes vmap need fixed sizes for everything through the function(s) being vmapped?\nDoes vmap try to JIT my function behind the scenes? (Wondering bc. 1 is a behavior I expect from JIT, I didn't expect it from vmap but I don't really know vmap).\nIf vmap is jit-ing something, how would one use something like a static-arguments with vmap?\nWhat is the best practice for dealing with ~extraneous information (eg if some outputs are sized a and some sized b, do you just make an array sized max(a,b) then ~ignore the extra values?)\nThe reason I'm asking is that it seems like vmap, like JIT, runs into all sorts of ConcretizationTypeError and seems (not 100% clear yet) to need constant sized items for everything. I associate this behavior with any function I'm trying to Jit, but not necessarily any function I write in Jax.",
        "answers": [
            "Does vmap need fixed sizes for everything through the function(s) being vmapped?\nyes – vmap, like all JAX transformations, requires any arrays defined in the function to have static shapes.\nDoes vmap try to JIT my function behind the scenes? (Wondering bc. 1 is a behavior I expect from JIT, I didn't expect it from vmap but I don't really know vmap).\nNo, vmap does not jit-compile a function by default, although you can always compose both if you wish (e.g. jit(vmap(f)))\nIf vmap is jit-ing something, how would one use something like a static-arguments with vmap?\nAs mentioned, vmap is unrelated to jit, but an analogy of jit static_argnums is passing None to in_axes, which will keep the argument unmapped and therefore static within the transformation.\nWhat is the best practice for dealing with ~extraneous information (eg if some outputs are sized a and some sized b, do you just make an array sized max(a,b) then ~ignore the extra values?)",
            "A section of my code now looks like:\nvmaped_f = jax.vmap(my_func, parallel_axes, 0)\nn_batches = int(num_items / batch_size)\nif num_items % batch_size != 0:\n    n_batches += 1 #Round up\n    \nall_vals = []\nfor i in range(n_batches):\n    top = min([num_items, (i+1)*batch_size])\n    batch_inds = jnp.arange(i*batch_size, top)\n    batch_inds_1, batch_inds_2 = jnp.array(inds_1)[batch_inds], \\\n                                 jnp.array(inds_2)[batch_inds]\n    f_vals = vmaped_f(batch_inds_1, batch_inds2, other_relevant_inputs)\n    all_vals.extend(f_vals.tolist())\nThe vmap'd function basically takes in all of my data, and the indices of that data to use (which will be constant sized except for potentially the last batch, so only need to jit compile 2x if you'd want to jit it)."
        ],
        "link": "https://stackoverflow.com/questions/77427904/jax-vmap-jit-behind-the-scenes"
    },
    {
        "title": "Idiomatic ways to handle errors in JAX jitted functions",
        "question": "As the title states, I'd like to know what idiomatic methods are available to raise exceptions or handle errors in JAX jitted functions. The functional nature of JAX makes it unclear how to accomplish this.\nThe closest official documentation I could find is the jax.experimental.checkify module, but this wasn't very clear and seemed incomplete.\nThis Github comment claims that Python exceptions can be raised by using jax.debug.callback() and jax.lax.cond() functions. I attempted to do this, but an error is thrown during compilation. A minimum working example is below:\npython\nCopy\nimport jax\nfrom jax import jit\n\ndef _raise(ex):\n    raise ex\n\n\n@jit\ndef error_if_positive(x):\n    jax.lax.cond(\n        x > 0,\n        lambda : jax.debug.callback(_raise, ValueError(\"x is positive\")),\n        lambda : None,\n    )\n\nif __name__ == \"__main__\":\n\n    error_if_positive(-1)\nThe abbreviated error statement:\npython\nCopy\nTypeError: Value ValueError('x is positive') with type <class 'ValueError'> is not a valid JAX type",
        "answers": [
            "You can use callbacks to raise errors, for example:\npython\nCopy\nimport jax\nfrom jax import jit\n\ndef _raise_if_positive(x):\n  if x > 0:\n    raise ValueError(\"x is positive\")\n\n@jit\ndef error_if_positive(x):\n  jax.debug.callback(_raise_if_positive, x)\n\nif __name__ == \"__main__\":\n  error_if_positive(-1)  # no error\n  error_if_positive(1)\n  # XlaRuntimeError: INTERNAL: Generated function failed: CpuCallback error: ValueError: x is positive\nThe reason your approach didn't work is becuase your error is raised at trace-time rather than at runtime, and both branches of the cond will always be traced."
        ],
        "link": "https://stackoverflow.com/questions/77381356/idiomatic-ways-to-handle-errors-in-jax-jitted-functions"
    },
    {
        "title": "Why matrix multiplication results with JAX are different if the data is sharded differently on the GPU",
        "question": "I am running a tutorial on muatrix multiplication with JAX with data sharded in different ways across multiple GPUs. I found not only the computation time is different for different way of sharding, the results are also slightly different.\nHere are my observations:\nResults of method 1 is exactly the same as method 0; but different from method\nComputation speed is the fastest in method 1, then method 2, then method 0.\nCan anyone help me understand these two observations? One additional question is: if the way of sharding is so important, will mainstream machine learning algorithms have ways to deal with it automatically so that different way of sharding won't give different models?\nMethod 0: Perform the matrix multiplication on the same GPU device (just use 1 device).\nMethod 1:\nx = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\ny = jax.device_put(x, sharding.reshape(4, 2).replicate(1))\nz = jax.device_put(x, sharding.reshape(4, 2).replicate(0))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)\nMethod 2:\nx = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\ny = jax.device_put(x, sharding.reshape(4, 2))\nz = jax.device_put(x, sharding.reshape(4, 2))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\n\nw = jnp.dot(y, z)",
        "answers": [
            "Regarding your observation of differing results: this is to be expected with floating point operations. Every time you do a floating point operation, it accumulates a small amount of error, and when you express the \"same\" floating point computation in different ways, the errors accumulate differently.\nHere's an example of this using NumPy:\npython\nCopy\nimport numpy as np\nx = np.random.rand(10000).astype('float32')\nx_reversed = x[::-1]\nnp.dot(x, x) == np.dot(x_reversed, x_reversed)\n# False\nIf we were dealing with real numbers, we'd expect these two to be identical. But because we're representing our computation with floating point values, the two approaches return slightly different results. This is similar to the situation in your question: different sharding layouts lead to different ordering of the dot product accumulations, which leads to slightly different results.\nRegarding your observation about computation speed: the results seem reasonable. Method 0 is the slowest because it only uses a single device, and method 1 is faster than method 2 because pre-replicating the data means that less data movement is required during the actual computation."
        ],
        "link": "https://stackoverflow.com/questions/77362635/why-matrix-multiplication-results-with-jax-are-different-if-the-data-is-sharded"
    },
    {
        "title": "vectorized minimization and root finding in jax",
        "question": "I have a family of functions parameterized by args\npython\nCopy\nf(x, args)\nand want to determine the minimum of f over x for N = 1000 values of args. I have access to both the function and its derivative. My first attempt was to loop through the different values of args and use a scipy.optimizer at each iteration, but it takes too long. I believe the operations can be sped up with vectorization. My next attempt was to use jax.vmap inside a jax.scipy.optimize.minimize or jaxopt.ScipyMinimize, but I can't seem to pass more than one value for args.\nAlternatively, I can code my own vectorized optimization method, e.g. bisection, where by vectorized I mean doing operations on arrays for a fixed number of iterations and not stopping early if one of the optimization problems has reached a certain error tolerance level early. I was hoping to use some optimized off-shelf algorithm.\nI was hoping to use some already optimized, off-the-shelf algorithm if an implementation is available in jax.this thread is related, but the args are not changing.",
        "answers": [
            "You can define a function to find the minimum given particular args, and then wrap it in jax.vmap to automatically vectorize it. For example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy import optimize\n\ndef f(x, args):\n  a, b = args\n  return jnp.sum(a + (x - b) ** 2)\n\ndef find_min(a, b):\n  x0 = jnp.array([1.0])\n  args = (a, b)\n  return optimize.minimize(f, x0, (args,), method=\"BFGS\")\n\na_grid, b_grid = jnp.meshgrid(jnp.arange(5.0), jnp.arange(5.0))\n\nresults = jax.vmap(find_min)(a_grid.ravel(), b_grid.ravel())\n\nprint(results.success)\n# [ True  True  True  True  True  True  True  True  True  True  True  True\n#   True  True  True  True  True  True  True  True  True  True  True  True\n#   True]\n\nprint(results.x.T)\n# [[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n#   3. 3. 3. 3. 3. 4. 4. 4. 4. 4.]]"
        ],
        "link": "https://stackoverflow.com/questions/77359908/vectorized-minimization-and-root-finding-in-jax"
    },
    {
        "title": "Cross dimensional segmented operation",
        "question": "Say you have the following a array\npython\nCopy\n>>> a = np.arange(27).reshape((3,3,3))\n>>> a\narray([[[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8]],\n\n       [[ 9, 10, 11],\n        [12, 13, 14],\n        [15, 16, 17]],\n\n       [[18, 19, 20],\n        [21, 22, 23],\n        [24, 25, 26]]], dtype=int64)\nAnd m, an array that specifies segment ids\npython\nCopy\n>>> m = np.linspace(start=0, stop=6, num=27).astype(int).reshape(a.shape)\n>>> m\narray([[[0, 0, 0],\n        [0, 0, 1],\n        [1, 1, 1]],\n\n       [[2, 2, 2],\n        [2, 3, 3],\n        [3, 3, 3]],\n\n       [[4, 4, 4],\n        [4, 5, 5],\n        [5, 5, 6]]])\nWhen using JAX and wishing to perform, say, a sum over the scalars in a that share the same id in m, we can rely on jax.ops.segment_sum.\npython\nCopy\n>>> jax.ops.segment_sum(data=a.ravel(), segment_ids=m.ravel())\nArray([10, 26, 42, 75, 78, 94, 26], dtype=int64)\nNote that I had to resort to numpy.ndarray.ravel since ~.segment_sum assumes m to indicate the segments of data along its leading axis.\nQ1 : Can you confirm there is no better approach, either with or without JAX ?\nQ2 : How would one then build n, an array that results from the replacement of the ids with the just-performed sums ? Note that I am not interested in non-vectorized approaches such as numpy.where.\npython\nCopy\n>>> n\narray([[[10, 10, 10],\n        [10, 10, 26],\n        [26, 26, 26]],\n\n       [[42, 42, 42],\n        [42, 75, 75],\n        [75, 75, 75]],\n\n       [[78, 78, 78],\n        [78, 94, 94],\n        [94, 94, 26]]], dtype=int64)",
        "answers": [
            "Use np.bincount with a as the weights parameter:\npython\nCopy\ns = np.bincount(m.ravel(), weights = a.ravel())\ns\nOut[]: array([10., 26., 42., 75., 78., 94., 26.])\nAnd to put the values back in the array:\npython\nCopy\nn = s[m]\nn\nOut[]: \narray([[[10., 10., 10.],\n        [10., 10., 26.],\n        [26., 26., 26.]],\n\n       [[42., 42., 42.],\n        [42., 75., 75.],\n        [75., 75., 75.]],\n\n       [[78., 78., 78.],\n        [78., 94., 94.],\n        [94., 94., 26.]]])",
            "The segment_sum operation is somewhat more specialized than what you're asking about. In the case you describe, I would use ndarray.at directly:\npython\nCopy\nsums = jnp.zeros(m.max() + 1).at[m].add(a)\nprint(sums[m])\npython\nCopy\n[[[10. 10. 10.]\n  [10. 10. 26.]\n  [26. 26. 26.]]\n\n [[42. 42. 42.]\n  [42. 75. 75.]\n  [75. 75. 75.]]\n\n [[78. 78. 78.]\n  [78. 94. 94.]\n  [94. 94. 26.]]]\nThis will also work when the segments are non-adjacent."
        ],
        "link": "https://stackoverflow.com/questions/77344277/cross-dimensional-segmented-operation"
    },
    {
        "title": "Why does jnp.einsum produce a different result from manual looping?",
        "question": "Let's say I want to compute an inner product along the last dimension of two matrices\npython\nCopy\na = jax.random.normal(jax.random.PRNGKey(0), shape=(64,16), dtype=jnp.float32)\nb = jax.random.normal(jax.random.PRNGKey(1), shape=(64,16), dtype=jnp.float32)\nI can do it with jnp.einsum:\npython\nCopy\ninner_prod1 = jnp.einsum('i d, j d -> i j', a, b)\nor manually call jnp.dot in a loop:\npython\nCopy\ninner_prod2 = jnp.zeros((64,64))\nfor i1 in range(64):\n  for i2 in range(64):\n    inner_prod2 = inner_prod2.at[i1, i2].set(jnp.dot(a[i1], b[i2]))\npython\nCopy\nprint(jnp.amax(inner_prod1 - inner_prod2)) # 0.03830552\nThis is quite a large difference between the two, even if they are mathematically equivalent. What gives?",
        "answers": [
            "All operations in floating point accumulate rounding errors, so in general when you express the same operation in two different ways, you should expect the results to not be bitwise-equivalent.\nThe magnitude of the difference you're seeing is larger than is typical for float32 precision; it makes me think you're probably running your code on TPU, where matrix multiplication is done at lower-precision by default. You can adjust this using the default_matmul_precision configuration; for example like this:\npython\nCopy\nwith jax.default_matmul_precision('float32'):\n  inner_prod1 = jnp.einsum('i d, j d -> i j', a, b)\n  inner_prod2 = jnp.zeros((64,64))\n  for i1 in range(64):\n    for i2 in range(64):\n      inner_prod2 = inner_prod2.at[i1, i2].set(jnp.dot(a[i1], b[i2]))\nIf you do the computation this way, I suspect you'll probably see a smaller difference more typical of float32 computations, on order 1E-6 or so."
        ],
        "link": "https://stackoverflow.com/questions/77334401/why-does-jnp-einsum-produce-a-different-result-from-manual-looping"
    },
    {
        "title": "How to get get the index position of a value with jit?",
        "question": "What would be a workaround of this code in jitted function?\nj = indices.index(list(neighbor)) where neighbor is, for example, (2,3), indices = [[1,2], [4,5], ...]\nOther alternatives like partial didn't work. One issue when using partial is that indices is not hashable so can't use partial function.",
        "answers": [
            "list.index is a Python function that will not work within JIT if the contents of the list are traced values. I would recommend converting your lists to arrays, and do something like this:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nindices = jnp.array([[1, 2], [4, 5], [3, 6], [2, 3], [5, 7]])\nneighbor = jnp.array([2, 3])\n\n@jax.jit\ndef get_index(indices, neighbor):\n  return jnp.where((indices == neighbor).all(-1), size=1)[0]\n\nidx = get_index(indices, neighbor)\nprint(idx)\n# [3]"
        ],
        "link": "https://stackoverflow.com/questions/77328302/how-to-get-get-the-index-position-of-a-value-with-jit"
    },
    {
        "title": "What is the difference between xmap vs pmap as the parallel processing model?",
        "question": "Can anyone help me understand the similarities and differences between the xmap and pmap in JAX? Read through the documentation multiple times, but still cannot understand the new concepts in the tutorials.\nI am specifically interested in if there are any examples for how to convert pmap train setup to xmap train setup.\nIt seems to me that: mesh+xmap can do similar things as pmap. Am I understanding correctly?",
        "answers": [
            "pmap is a simple parallelizing transform that only supports distribution of data over a single leading axis. It is deprecated, and will likely be removed in a future version of JAX.\nxmap is a generalization of pmap that allows for more flexible parallelization over multiple named axes. It has always been experimental, and will likely be removed in a future version of JAX.\nThe best way to do parallel computation in JAX going forward is either implicitly via sharded inputs into jit, or explicitly via shard_map. Unfortunately, neither of these approaches is very well documented at the moment; there is some information at Distributed Arrays and Automatic Parallelization and shard_map for simple per-device code but both are written more for developers than for end-users. That said, more comprehensive docs for the newer parallelism models in JAX are currently in progress, and should be on the website soon."
        ],
        "link": "https://stackoverflow.com/questions/77275923/what-is-the-difference-between-xmap-vs-pmap-as-the-parallel-processing-model"
    },
    {
        "title": "aggregate calculation vmap jax",
        "question": "I'm trying to implement fast routine to calculate array of energies and find the smallest calculated value and its index. Here is my code that is working fine:\npython\nCopy\n@jit\ndef findMinEnergy(x):\n  def calcEnergy(a):\n    return a*a  # very simplified body, it is actually 15 lines of code\n  energies = vmap(calcEnergy, in_axes=(0))(x)\n  idx = energies.argmin(axis=0)\n  minenrgy = energies[idx]\n  return idx, minenrgy\nI wonder if it is possible to not use the (separate) argmin call, but return the min calculated energy value and it's index from the vmap (similar like other aggregate functions work, e.g. jax.sum)? I hope that it could be more efficient.",
        "answers": [
            "If you JIT-compile your current approach, you should find that it's as efficient as doing something more sophisticated.\nLooking at the implementation of argmin, you'll see that it computes both the value and the index before returning only the index: https://github.com/google/jax/blob/jax-v0.4.18/jax/_src/lax/lax.py#L3892-L3914\nIf you want, you could follow this implementation and define a function using lax.reduce that returns both these values in a single pass:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef min_and_argmin_onepass(x):\n  # This only works for 1D float arrays, but you could generalize it.\n  assert x.ndim == 1\n  assert jnp.issubdtype(x.dtype, jnp.floating)\n  def reducer(op_val_index, acc_val_index):\n    op_val, op_index = op_val_index\n    acc_val, acc_index = acc_val_index\n    pick_op_val = (op_val < acc_val) | jnp.isnan(op_val)\n    pick_op_index = pick_op_val | ((op_val == acc_val) & (op_index < acc_index))\n    return (jnp.where(pick_op_val, op_val, acc_val),\n            jnp.where(pick_op_index, op_index, acc_index))\n  indices = jnp.arange(len(x))\n  return jax.lax.reduce((x, indices), (jnp.inf, 0), reducer, (0,))\nTesting this, we see it matches the output of the less sophisticated approach:\npython\nCopy\n@jax.jit\ndef min_and_argmin(x):\n  i = jnp.argmin(x)\n  return x[i], i\n\nx = jax.random.uniform(jax.random.key(0), (1000000,))\nprint(min_and_argmin_onepass(x))\n# (Array(9.536743e-07, dtype=float32), Array(24430, dtype=int32))\nprint(min_and_argmin(x))\n# (Array(9.536743e-07, dtype=float32), Array(24430, dtype=int32))\nIf you compare the runtime of the two, you'll see comparable runtimes:\npython\nCopy\n%timeit jax.block_until_ready(min_and_argmin_onepass(x))\n# 2.17 ms ± 68.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%timeit jax.block_until_ready(min_and_argmin(x))\n# 2.07 ms ± 66.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nThe jax.jit decorator here means that the compiler optimizes the sequence of operations in the less sophisticated approach, and the result is that you don't gain much advantage from trying to express things more cleverly. Given this, I think your best option is to stick with your original code rather than trying to out-optimize the XLA compiler.",
            "Assuming that by efficient you mean not having to keep a large array (energies) in memory, you can just stack the individual values of idx and minenergy into a single array within calcEnergy and return a (2,) array to vmap instead of an (N,) array. It's not pretty as you'll (presumably) have to cast both values to the same dtype, but it should work fine."
        ],
        "link": "https://stackoverflow.com/questions/77254706/aggregate-calculation-vmap-jax"
    },
    {
        "title": "How to use FLAX LSTM in 2023",
        "question": "I am wondering if anyone here knows how to get FLAX LSTM layers to work in 2023. I have tried some of the code snippets on the actual Flax documentation, such as:\nhttps://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.scan.html\nand, the first example provided there,\npython\nCopy\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\n\nclass LSTM(nn.Module):\n  features: int\n\n  @nn.compact\n  def __call__(self, x):\n    ScanLSTM = nn.scan(\n      nn.LSTMCell, variable_broadcast=\"params\",\n      split_rngs={\"params\": False}, in_axes=1, out_axes=1)\n\n    lstm = ScanLSTM(self.features)\n    input_shape =  x[:, 0].shape\n    carry = lstm.initialize_carry(jax.random.key(0), input_shape)\n    carry, x = lstm(carry, x)\n    return x\n\nx = jnp.ones((4, 12, 7))\nmodule = LSTM(features=32)\ny, variables = module.init_with_output(jax.random.key(0), x)\nthrows an error. I have looked for other examples but it seems they have changed their API at some point in 2023, so what I could find online wasn't working anymore.\nIn short, what I am looking for is a simple example on how to pass a time series into an LSTM in FLAX.\nThank you for your help.",
        "answers": [
            "The snippet you provided runs correctly with the most recent version of flax (version 0.7.4). If you're using an older version of flax, you should change jax.random.key to jax.random.PRNGKey. For some information about this JAX PRNG key change, see JEP 9263: Typed Keys and Pluggable PRNGs."
        ],
        "link": "https://stackoverflow.com/questions/77222395/how-to-use-flax-lstm-in-2023"
    },
    {
        "title": "Should models be trained using fori_loop?",
        "question": "When optimizing weights and biases of a model, does it make sense to replace:\npython\nCopy\nfor _ in range(epochs):\n    w, b = step(w, b)\nWith:\npython\nCopy\nw, b = lax.fori_loop(0, epochs, lambda wb: step(wb[0], wb[1]), (w, b))\nIf I understand correctly, this means that the entire training process can then be a single compiled JAX function (takes in training data, outputs optimized weights and biases).\nIs this a standard approach? What are the tradeoffs to consider?",
        "answers": [
            "It's fine to train your model with fori_loop, particularly for simple models. It may be slightly faster, but in general XLA won't fuse operations across different loop steps. It's also not possible to return early within a fori_loop when you reach a certain loss threshold (though you could do that with while_loop if you wish).\nFor more complicated models, you often will want to do some sort of I/O at every step (e.g. loading new training data, logging fit parameters, etc.) While this is possible to do within fori_loop via jax.experimental.io_callback, it is somewhat less convenient than doing it directly from the host within a Python for loop, so in general users tend to use for loops for their training iterations."
        ],
        "link": "https://stackoverflow.com/questions/77217387/should-models-be-trained-using-fori-loop"
    },
    {
        "title": "JAX produces memory error for simple program on GPU",
        "question": "I installed JAX (pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html) and even for a simple code like\npython\nCopy\na = jnp.array([1,2,3])\na.dot(a)\nI get the following error:\n2023-09-08 10:12:55.791658: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:445] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n2023-09-08 10:12:55.791696: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:449] Memory usage: 8058437632 bytes free, 8513978368 bytes total.\nIt looks like a memory issue. I tried the tips mentioned here\nhttps://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\nbut to no success.\nThis is the result of nvidia-smi on my system:\nNVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   64C    P0    37W /  N/A |    364MiB /  8119MiB |      2%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\nIt seems strange to me that there seems to be a out of memory issue when it is just a single array.\nAny ideas?",
        "answers": [
            "I don't believe this is a memory issue; rather it looks like you have a mismatch between your CUDA and CUDNN versions.\nOne way to ensure your CUDA versions are compatible is to use the pip-based installation (see JAX pip installation: GPU (CUDA, installed via pip, easier)). This should ensure that you install mutually-compatible CUDA, CUDNN, and jaxlib versions on your system. Installing JAX using pip-installed CUDA looks something like this:\n$ pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nIt looks like this may be the approach you used; if so, you should check that your system path (e.g. LD_LIBRARY_PATH) is not pre-empting the pip-installed CUDA with a local version. There is some relevant discussion at https://github.com/google/jax/issues/17497.\nIf you want to use a local CUDA installation, you can follow JAX pip installation: GPU (CUDA, installed locally, harder), but then it is up to you to ensure that your CUDA, CUDNN, and jaxlib versions are mutually compatible. Installing JAX using local CUDA looks something like this:\n$ pip install --upgrade \"jax[cuda12_local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nbut if you use this approach, be sure to read the details at the above link."
        ],
        "link": "https://stackoverflow.com/questions/77065313/jax-produces-memory-error-for-simple-program-on-gpu"
    },
    {
        "title": "Parallelize with JAX over all GPU cores",
        "question": "In order to minimize the function x^2+y^2, I tried to implement the Adam optimizer from scratch with JAX:\npython\nCopy\n@jax.jit\ndef fit(X, batches, params=[0.001, 0.9, 0.99, 1e-8]):\n    global fun\n\n    # batches is array containing n batches\n\n    @jax.jit\n    def adam_update(X, t, grads, m, v, alpha, b1, b2, epsilon):\n        m = b1*m + (1-b1)*grads\n        v = b2*v + (1-b2)*grads**2\n        m_hat = m / (1-b1**t)\n        v_hat = v / (1-b2**t)\n        X = X - alpha*(1-b2**t)**(1/2)*m_hat/(1-b1**t)/((v_hat)**(1/2)+epsilon)\n        return [X, m, v]\n\n    dim=jnp.shape(X)[0]\n\n    params = jnp.array(params)\n    alpha = params[0]\n    b1=params[1]\n    b2=params[2]\n    epsilon=params[3]\n    \n    adam_update = jax.jit(partial(adam_update, alpha=alpha, b1=b1, b2=b2, epsilon=epsilon))\n\n    m=jnp.zeros(dim)\n    v=jnp.zeros(dim)\n\n    for t, batch in enumerate(batches):\n        fun_ = jax.jit(partial(fun, batch=batch))\n        grads = jax.grad(fun_)(X)\n        X, m, v = adam_update(X, t+1, grads, m, v)\n    return X\nWith JAX I could parallelize this simply with jax.pmap, however it would only be parallelized over the 8 GPUs, instead over all GPU cores. Is there a way too parallelize this code over all cores?\nCan it be that all cores of one GPU are miraculously used upon using @jax.jit. Also, why does it need 200 seconds for compiling for 1000 iterations, while the optax-Adam optimizer does not take so long too compile?",
        "answers": [
            "Can it be that all cores of one GPU are miraculously used upon using @jax.jit\nIn general, yes. For computations on a single device, the XLA GPU compiler will use all available cores of the GPU to complete a computation.\nAlso, why does it need 200 seconds for compiling for 1000 iterations, while the optax-Adam optimizer does not take so long too compile?\nThis is because you are JIT-compiling a Python for-loop. Python loops within JIT are unrolled by JAX into a linear program (see JAX Sharp Bits: Control Flow), and compilation time grows with the size of the program.\nBy contrast, the optax quick-start recommends JIT-compiling the step function, but does not JIT-compile the fitting loop. This would lead to much faster compilation times than the pattern used in your code, where the full for-loop is within a JIT-compiled function."
        ],
        "link": "https://stackoverflow.com/questions/77036442/parallelize-with-jax-over-all-gpu-cores"
    },
    {
        "title": "how does the bind() function work in JAX when making a new primitive?",
        "question": "While I am checking and studying the jax code, I see lots of bind() usage. I wanted to know how it works and what's its functionality. Here is one of the example.\npython\nCopy\ndef test_unimplemented_interpreter_rules(self):\n    foo_p = core.Primitive('foo')\n    def foo(x):\n      return foo_p.bind(x)",
        "answers": [
            "You can think of bind() as essentially equivalent to calling the function represented by the primitive. But because JAX primitives are designed to work with its abstract evaluation and transformation framework, binding a primitive is a bit more complicated than just calling a function, because the meaning of binding a primitive changes depending on the context. For example:\nin normal, non-transformed code, it will result in a call to the primitive's impl rule\nduring general abstract evaluation like jax.make_jaxpr or jax.eval_shape, it will call the primitive's abstract evaluation rule\ninside jax.vmap, it will result in a call to the primitive's batching rule\ninside jax.grad, it will result in a call to the primitive's autodiff-related rules (jvp and/or transpose)\nIf you want to understand more about the details of JAX primitives and how they interact with JAX tracing, abstract evaluation, and transformations, a good resource is Autodidax: JAX core from scratch, which walks through creating a simplified version of JAX from the ground up."
        ],
        "link": "https://stackoverflow.com/questions/76908334/how-does-the-bind-function-work-in-jax-when-making-a-new-primitive"
    },
    {
        "title": "Installing jaxlib for cuda 11.8",
        "question": "I'm trying to install jax and jaxlib on my Ubuntu 18 with python 3.8 for snerg (https://github.com/google-research/google-research/tree/master/snerg). Unfortunately when I try to install jax and jaxlib for Cuda 11.8 with the following command :\npip install --upgrade jax jaxlib==0.1.69+cuda118 -f https://storage.googleapis.com/jax-releases/jax_releases.html \nI get the following error:\nERROR: Ignored the following versions that require a different python version: 0.4.14 Requires-Python >=3.9\nERROR: Could not find a version that satisfies the requirement jaxlib==0.1.69+cuda118 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15, 0.3.18, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13)\nERROR: No matching distribution found for jaxlib==0.1.69+cuda118\nWould appreciate any help. Thanks",
        "answers": [
            "Follow the following instructions which are primarily obtained from the source:\nUninstall previous versions (if any):\n$ pip uninstall jax jaxlib jaxtyping -y\nUpgrade your pip:\n$ pip install --upgrade pip\nFind out which CUDA is already installed on your machine:\n$ nvidia-smi\n\nThu Jan  4 11:24:58 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A1000 6GB Lap...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   58C    P0              12W /  35W |      8MiB /  6144MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      3219      G   /usr/lib/xorg/Xorg                            4MiB |\n+---------------------------------------------------------------------------------------+\nDepending on the CUDA version of your machine( wheels only available on linux ), run EITHER of the following:\n# CUDA 12.X installation\n$ pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n#### OR ####\n\n# CUDA 11.X installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nTo double check if you have have successfully configured the gpu:\n$ python -c \"import jax; print(f'Jax backend: {jax.default_backend()}')\"\nJax backend: gpu ",
            "jaxlib version 0.1.69 is quite old (it was released in July 2021) CUDA 11.8 was released over a year later, in September 2022. Thus I would not expect there to be pre-built binaries for jaxlib version 0.1.69 targeting CUDA 11.8.\nIf possible, your best bet would be to install a newer version of jaxlib, one which has builds targeting CUDA 11.8. The current jaxlib+CUDA GPU installation instructions can be found here.\nIf for some reason you absolutely need this very old jaxlib version, you'll probably first have to install an older CUDA version on your system. The CUDA jaxlib installation instructions from jaxlib 0.1.69 can be found here: it looks like it was built to target CUDA 10.1-10.2, 11.0, or 11.1-11.3."
        ],
        "link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8"
    },
    {
        "title": "Does Haiku cache parameters when combined with jax.vmap?",
        "question": "I have a haiku Module with a call function as follows\npython\nCopy\nclass MyModule(hk.Module):\n    __call__(self, x):\n        A = hk.get_parameter(\"A\", shape=[self.Ashape], init=A_init)\n        B = hk.get_parameter(\"B\", shape=[self.Bshape], init=B_init)\n        C = self.demanding_computation(A, B)\n\n        res = easy_computation(C, x)\n        return res\nI use this module via\npython\nCopy\ndef _forward(x):\n    module = MyModule()\n    return module(x)\n\n\nforward = hk.without_apply_rng(hk.transform(_forward))\nx_test = jnp.ones(1)\nparams = forward.init(jax.random.PRNGKey(42), x_test)\nf = jax.vmap(forward.apply, in_axes=(None, 0))\nThen I apply f with the same params to many different x. Is the demanding_computation (that is not depending on x) then cached within the jax.vmap call? If not, what is the correct pattern to separate these computations and get demanding_computation cached?\nI have tried to test this by adding a print statement from jax.experimental.host_callback:\npython\nCopy\n    def demanding_computation(self, A, B):\n        C = compute(A, B)\n        id_print(C)\n        return C\nand it indeed only printed once. Is that sufficient evidence that this computation is actually cached or is only the printing omitted in subsequent iterations?",
        "answers": [
            "demanding_computation will only be called once, but not because of caching.\nvmap doesn't loop over the batched axes, it replaces the operations with vectorized versions (e.g. scalar additions become vector additions). Since demanding_computation doesn't involve inputs with batch axes it won't be modified by this use of vmap. (Even if it did, it would still only be run once, it would just be a vectorized version)."
        ],
        "link": "https://stackoverflow.com/questions/76788819/does-haiku-cache-parameters-when-combined-with-jax-vmap"
    },
    {
        "title": "How to compute the number of gradient evaluations in Jax.Scipy.minimize.optimize?",
        "question": "I wish to obtain the total number of gradient evaluations during optimization using jax.scipy.optimize.minimize. How can I do so?",
        "answers": [
            "You can find this in the njev (number of Jacobian evaluations) attribute of the output of minimize. For example:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\n\ndef f(x):\n  return jnp.sum(x ** 2)\n\nout = minimize(f, jnp.array([1., 2.]), method=\"BFGS\")\nprint(out.njev)\n3\nYou can find a list of the information available in the optimization output in the documentation for OptimizeResults."
        ],
        "link": "https://stackoverflow.com/questions/76721987/how-to-compute-the-number-of-gradient-evaluations-in-jax-scipy-minimize-optimize"
    },
    {
        "title": "pytorch and jax networks give different accuracy with same settings",
        "question": "I have pytorch code which performs with more than 95% accuracy. The code essentially implements a feedforward neural network using PyTorch to classify the digits dataset. It trains the model using the Adam optimizer and computes the cross-entropy loss, and then evaluates the model's performance on the test set by calculating the accuracy.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the digits dataset\ndigits = load_digits()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, test_size=0.2, random_state=42\n)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert the data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Define the FFN model\nclass FFN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, output_size):\n        super(FFN, self).__init__()\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_sizes)):\n            if i == 0:\n                self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[i]))\n            else:\n                self.hidden_layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n            self.hidden_layers.append(nn.ReLU())\n        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n\n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.output_layer(x)\n        return x\n\n# Define the training parameters\ninput_size = X_train.shape[1]\nhidden_sizes = [64, 32]  # Modify the hidden layer sizes as per your requirement\noutput_size = len(torch.unique(y_train_tensor))\nlearning_rate = 0.001\nnum_epochs = 200\nbatch_size = len(X_train)  # Set batch size to the size of the training dataset\n\n# Create the FFN model\nmodel = FFN(input_size, hidden_sizes, output_size)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n\n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\n# Evaluate the model on the test set\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs.data, 1)\n    for j in range(len(predicted)):\n        print(predicted[j], y_test_tensor[j])\n    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0) * 100\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\nAlso I have the equivalent jax code, with performs with less than 10% of accuracy\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, random, value_and_grad\nfrom jax.scipy.special import logsumexp\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom jax.example_libraries.optimizers import adam, momentum, sgd, nesterov, adagrad, rmsprop\nfrom jax import nn as jnn\n\n\n# Load the digits dataset\ndigits = load_digits()\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n\n# Reshape the target variables\ny_train_reshaped = jnp.reshape(y_train, (-1, 1))\ny_test_reshaped = jnp.reshape(y_test, (-1, 1))\n\nX_train_reshaped = jnp.reshape(X_train, (-1, 1))\nX_test_reshaped = jnp.reshape(X_test, (-1, 1))\n#print(np.shape(X_train),np.shape(y_train_reshaped),np.shape(y_train))\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_reshaped)\ny_test_scaled = scaler.transform(y_test_reshaped)\n\n# Convert the data to JAX arrays\nX_train_array = jnp.array(X_train, dtype=jnp.float32)\ny_train_array = jnp.array(y_train_reshaped, dtype=jnp.int32)\nX_test_array = jnp.array(X_test, dtype=jnp.float32)\ny_test_array = jnp.array(y_test_reshaped, dtype=jnp.int32)\n\n# Define the FFN model\ndef init_params(rng_key):\n    sizes = [X_train_array.shape[1]] + hidden_sizes + [output_size]\n    keys = random.split(rng_key, len(sizes))\n    params = []\n    for i in range(1, len(sizes)):\n        params.append((random.normal(keys[i], (sizes[i-1], sizes[i])), \n                       random.normal(keys[i], (sizes[i],))))\n    return params\n\ndef forward(params, x):\n    for w, b in params[:-1]:\n        x = jnp.dot(x, w) + b\n        x = jax.nn.relu(x)\n    w, b = params[-1]\n    x = jnp.dot(x, w) + b\n    return x\n\ndef softmax(logits):\n    logsumexp_logits = logsumexp(logits, axis=1, keepdims=True)\n    return jnp.exp(logits - logsumexp_logits)\n\ndef cross_entropy_loss(logits, labels):\n    log_probs = logits - logsumexp(logits, axis=1, keepdims=True)\n    return -jnp.mean(jnp.sum(log_probs * labels, axis=1))\n\n# Define the training parameters\ninput_size = X_train_array.shape[1]\nhidden_sizes = [64, 32]  # Modify the hidden layer sizes as per your requirement\noutput_size = len(jnp.unique(y_train_array))\nlearning_rate = 0.001\nnum_epochs = 200\nbatch_size = len(X_train_array)  # Set batch size to the size of the training dataset\n# Create the FFN model\nrng_key = random.PRNGKey(0)\nparams = init_params(rng_key)\n\n# Define the loss function\ndef loss_fn(params, x, y):\n    logits = forward(params, x)\n    probs = softmax(logits)\n    labels = jax.nn.one_hot(y, output_size)\n    return cross_entropy_loss(logits, labels)\n\n# Create the optimizer\nopt_init, opt_update, get_params = adam(learning_rate)\nopt_state = opt_init(params)\n\n# Define the update step\n@jit\ndef update(params, x, y, opt_state):\n    grads = grad(loss_fn)(params, x, y)\n    return opt_update(0, grads, opt_state)\n\n# Train the model\nfor epoch in range(num_epochs):\n    perm = random.permutation(rng_key, len(X_train_array))\n    for i in range(0, len(X_train_array), batch_size):\n        batch_idx = perm[i:i+batch_size]\n        X_batch = X_train_array[batch_idx]\n        y_batch = y_train_array[batch_idx]\n        params = get_params(opt_state)\n        opt_state = update(params, X_batch, y_batch, opt_state)\n\n    if (epoch + 1) % 10 == 0:\n        params = get_params(opt_state)\n        loss = loss_fn(params, X_train_array, y_train_array)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n\n# Evaluate the model on the test set\nparams = get_params(opt_state)\nlogits = forward(params, X_test_array)\npredicted = jnp.argmax(logits, axis=1)\n\nfor j in range(len(predicted)):\n    print(predicted[j], y_test_array[j])\n\naccuracy = jnp.mean(predicted == y_test_array) * 100\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\nI dont understand why the jax code performs poorly. Could you please help me in underding the bug in the jax code.",
        "answers": [
            "There are 2 probles in your jax code that are, actually, in data processing:\nYour data are not scaled. If you look at your X_train_array definition, it is the jax version of X_train, that is the raw data. Please consider using:\npython\nCopy\n# Scale the features\nscaler = StandardScaler().fit(X_train)  # No need to flat it!\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert the data to JAX arrays\nX_train_array = jnp.array(X_train, dtype=jnp.float32)\ny_train_array = jnp.array(y_train_reshaped, dtype=jnp.int32)\nX_test_array = jnp.array(X_test, dtype=jnp.float32)\ny_test_array = jnp.array(y_test_reshaped, dtype=jnp.int32)\nYour labels are of shape (N, 1) before one-hot encoding. After one-hot encoding it is (N, 1, n_out) while your predictions are of shape (N, n_out) so when you make your loss computation the two arrays are cast in (N, n_out, n_out) with repetitions and your loss is wrong. You can solve it very simply by remove the 1 in the reshape:\npython\nCopy\n# Reshape the target variables\ny_train_reshaped = jnp.reshape(y_train, (-1,))\ny_test_reshaped = jnp.reshape(y_test, (-1,))\nI tested your code with 300 epochs and lr=0.01 and I got an accuracy of 90% in test (and the loss decreased to 0.0001)"
        ],
        "link": "https://stackoverflow.com/questions/76703681/pytorch-and-jax-networks-give-different-accuracy-with-same-settings"
    },
    {
        "title": "Jaxlib 0.4.7 can't be installed or built in my OSX 10.13, Python 3.9.13",
        "question": "I've been trying to upgrade my jaxlib but it is impossible. Jax is fine. None of the wheels in the google repository work for me. I suppose it is because I'm using OSX 10.13?\npython\nCopy\nRuntimeError: jaxlib is version 0.3.10, but this version of jax requires version >= 0.4.7.\nWhen I attempt to install Jaxlib through pip, it throws this:\npython\nCopy\n$ pip install --upgrade jaxlib==0.4.7 -f https://storage.googleapis.com/jax-releases/jax_releases.html\nLooking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\nERROR: Could not find a version that satisfies the requirement jaxlib==0.4.7 (from versions: 0.1.60, 0.1.63, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10)\nERROR: No matching distribution found for jaxlib==0.4.7\nI am using the latest version of pip, but also tried downgrading pip. The problem still occurs. Please help me solve this issue.\nI tried installing through pip, building my own jaxlib as the site's instructions (https://jax.readthedocs.io/en/latest/developer.html) and installing through the google repositories. I also downgraded pip, with no luck.\nMy objective is to have Jaxlib 0.4.7 running in my computer.",
        "answers": [
            "Recent jaxlib releases require OSX version 10.14 or newer (see https://github.com/google/jax/blob/main/CHANGELOG.md#jaxlib-0314-june-27-2022).\nYour options are either to update your OSX to a more recent version, to build jaxlib yourself (this can be tricky; see building from source), or to use an older jaxlib release where your OS version is supported."
        ],
        "link": "https://stackoverflow.com/questions/76665537/jaxlib-0-4-7-cant-be-installed-or-built-in-my-osx-10-13-python-3-9-13"
    },
    {
        "title": "How to slice jax arrays using jax tracer?",
        "question": "I am trying to modify a code base to create a subarray using an existing array and indices in the form of Jax tracer. When I try to pass these Jax tracers directly for indices. I get the following error:\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\nWhat is a possible workaround/ solution for this?",
        "answers": [
            "There are two main workarounds here that may be applicable depending on your problem: using static indices, or using dynamic_slice.\nQuick background: one constraint of arrays used in JAX transformations like jit, vmap, etc. is that they must be statically shaped (see JAX Sharp Bits: Dynamic Shapes for some discussion of this).\nWith that in mind, a function like f below will always fail, because i and j are non-static variables and so the shape of the returned array cannot be known at compile time:\npython\nCopy\n@jit\ndef f(x, i, j):\n  return x[i:j]\nOne workaround for this is to make i and j static arguments in jit, so that the shape of the returned array will be static:\npython\nCopy\n@partial(jit, static_argnames=['i', 'j'])\ndef f(x, i, j):\n  return x[i:j]\nThat's the only possible workaround to use jit in such a situation, because of the static shape constraint.\nAnother flavor of slicing problem that can lead to the same error might look like this:\npython\nCopy\n@jit\ndef f(x, i):\n  return x[i:i + 5]\nThis will also result in a non-static index error. It could be fixed as above by marking i as static, but there is more information here: assuming that 0 <= i < len(x) - 5 holds, we know that the shape of the output array is (5,). This is a case where jax.lax.dynamic_slice is applicable (when you have a fixed slice size at a dynamic location):\npython\nCopy\n@jit\ndef f(x, i):\n  return jax.lax.dynamic_slice(x, (i,), (5,))\nNote that this will have different semantics than x[i:i + 5] in cases where the slice overruns the bounds of the array, but in most cases of interest it is equivalent.\nThere are other examples where neither of these two workarounds are applicable, for example when your program logic is predicated on creating dynamic-length arrays. In these cases, there is no easy work-around, and your best bet is to either (1) re-write your algorithm in terms of static array shapes, perhaps using padded array representations, or (2) not use JAX."
        ],
        "link": "https://stackoverflow.com/questions/76626143/how-to-slice-jax-arrays-using-jax-tracer"
    },
    {
        "title": "Selecting all elements of subsets if at least one element is selected (JAX)",
        "question": "I have the following problem, for which I cannot manage to write a solution in JAX that is jittable and efficient.\nI have a set of elements. Some of these elements are included (on the basis of a condition, which is not important now). The included elements are denoted by 1, the not-included elements by 0. For example, the array arr = jnp.array([1, 0, 0, 0, 0, 0]) indicates that I have 6 elements, the first of them included based on my condition.\nThese elements are grouped into subsets. I have a second array that indicates where each subset starts in the first array arr. For example, the array subsets = jnp.array([0, 2]) indicates that the first subset starts at position 0 and the second subset starts at position 2.\nNow, if one element is included based on arr, I would like to include all the elements in the same subset. In this example, the output should then be [1, 1, 0, 0, 0, 0].\nI have tried with a jax.lax.fori_loop, but it is slow.\npython\nCopy\n@jax.jit                                                                        \ndef select_subsets(arr, subsets):                 \n    new_arr = arr.copy()            \n    n_resid = subsets.shape[0]    \n    indices = jnp.arange(arr.shape[0])       \n                                                                                    \n    def func(i, new_arr):    \n        start = subsets[i]    \n        stop = subsets[i+1]    \n        arr_sliced = jnp.where((indices >= start) & (indices < stop), arr, 0.0)    \n        sum_ = jnp.sum(arr_sliced)    \n        new_arr = jnp.where(sum_ > 0.5, jnp.where((indices >= start) & (indices < stop), 1, new_arr), new_arr)    \n        return new_arr                          \n                                                 \n    new_arr = jax.lax.fori_loop(0, n_resid-1, func, new_arr)    \n    return new_arr\nthis function works if I use a subsets with the last element equal to the number of elements in arr, subsets = jnp.array([0, 2, 6]).\nI then thought about writing a vectorized version (using jax.numpy operations), but I cannot manage to do it.\nIs there a JAX guru that can help me with this?\nThanks a lot!",
        "answers": [
            "Here's a vectorized version. It instantiates a mask with shape len(subsets) x len(arr), which might be undesirable depending on how big those values are.\npython\nCopy\n           \n@jax.jit                                                                             \ndef vectorized_select_subsets(arr, subsets):                         \n    l, = arr.shape               \n                               \n    indices = jnp.arange(l)[None, :]\n                \n    # Broadcast to mask of shape (n_subsets, input_length)\n    subset_masks = (\n        (indices >= subsets[:-1, None])\n        & (indices < subsets[1:, None])        \n    )                                                           \n                                                    \n    # Shape (n_subsets,) array indicating whether each subset is included\n    include_subset = jnp.any(subset_masks & arr[None, :], axis=1)          \n                                                          \n    # Reduce down columns \n    result = jnp.any(subset_masks & include_subset[:, None], axis=0).astype(jnp.int32)   \n    return result\nI timed this against the loop-based version on an array with length 512 and 32 subsets:\npython\nCopy\nLoop: 6254.647 it/s\nVectorized: 37940.335 it/s"
        ],
        "link": "https://stackoverflow.com/questions/76617821/selecting-all-elements-of-subsets-if-at-least-one-element-is-selected-jax"
    },
    {
        "title": "Creating a jax array using existing jax arrays of different lengths throws error",
        "question": "I am using the following code to set a particular row of a jax 2D array to a particular value using jax arrays:\npython\nCopy\nzeros_array = jnp.zeros((3, 8))\nvalue = jnp.array([1,2,3,4])\nvalue_2 = jnp.array([1])\nvalue_3 = jnp.array([1,2])\nvalues = jnp.array([value,value_2,value_3])\nzeros_array = zeros_array.at[0].set(values)\nBut, I am receiving the following error:\npython\nCopy\nValueError: All input arrays must have the same shape.\nUpon modifying the jnp to np (numpy) the error disappears. Is there any way to resolve this error? I know one walk around this would be to set each of the separate arrays in the 2D array using at[0,1].set(), at[0,2:n].set().",
        "answers": [
            "What you have in mind is a \"ragged array\", and no, there is not currently any way to do this in JAX. In older versions of NumPy, this will work by returning an array of dtype object, but in newer versions of NumPy this results in an error because object arrays are generally inconvenient and inefficient to work with (for example, there's no way to efficiently do the equivalent of the index update operation in your last line if the updates are stored in an object array).\nDepending on your use-case, there are several workarounds for this you might use in both JAX and NumPy, including storing the rows of your array as a list, or using a padded 2D array representation.\nI'll note also that the JAX team is exploring native support for ragged arrays (see e.g. https://github.com/google/jax/pull/16541) but it's still fairly far from being generally useful."
        ],
        "link": "https://stackoverflow.com/questions/76600803/creating-a-jax-array-using-existing-jax-arrays-of-different-lengths-throws-error"
    },
    {
        "title": "How to vectorize cho_solve?",
        "question": "This question solved my problem of using vmap on cho_solve, is it possible to vectorize cho_solve, or does the definition of cho_solve preclude it from being vectorized? vectorize seems to need the arguments to all be arrays, whereas cho_solve takes a tuple as the first argument?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\nkey = jax.random.PRNGKey(0)\nkey, subkey = jax.random.split(key)\n\nk_y = jax.random.normal(subkey, (3, 5, 10, 10))\ny = jnp.broadcast_to(jnp.eye(10), k_y.shape)\n\nmatmul = jnp.vectorize(jnp.matmul, signature='(a,b),(b,c)->(a,c)')\ncholesky = jnp.vectorize(jsp.linalg.cholesky, excluded={1}, signature='(d,d)->(d,d)')\ncho_solve = jnp.vectorize(jsp.linalg.cho_solve, signature='(d,d),(d,d)->(d,d)')  # what to put here?\n\nk_y = matmul(k_y, jnp.moveaxis(k_y, -1, -2))\nchol = cholesky(k_y, True)\nresult = cho_solve((chol, True), y)\nValueError: All input arrays must have the same shape.\nMy use case is that I have an unspecified amount of \"batch\" dimensions that I want to vmap over, and vectorize handles the auto broadcasting beautifully. I can once again write my own cho_solve using solve_triangular but this seems like a waste. Is it possible for vectorize to have a similar interface to vmap, which can take nested signatures?",
        "answers": [
            "I don't believe you can use vectorize directly with cho_solve. The vectorize API requires that the function take arrays as an argument, while cho_solve takes a tuple as the first argument. The only way you could use vectorize with this function is to wrap it in one with a different API. For example:\npython\nCopy\ncho_solve = jnp.vectorize(\n    lambda chol, flag, y: jsp.linalg.cho_solve((chol, flag), y),\n    excluded={1}, signature='(d,d),(d,d)->(d,d)')\nresult = cho_solve(chol, True, y)",
            "Here's the solve_triangular solution I managed:\npython\nCopy\nsolve_tri = jnp.vectorize(\n  jsp.linalg.solve_triangular, excluded={2, 3}, signature='(d,d),(d,d)->(d,d)')\nchol = cholesky(k_y, True)\nresult2 = solve_tri(chol, solve_tri(chol, y, 0, True), 1, True)\nresult3 = jnp.array([\n  jsp.linalg.cho_solve((chol[a, b], True), y[a, b])\n  for a in range(k_y.shape[0])\n  for b in range(k_y.shape[1])\n]).reshape(k_y.shape)\nprint(jnp.allclose(result2, result3))\nTrue"
        ],
        "link": "https://stackoverflow.com/questions/76588276/how-to-vectorize-cho-solve"
    },
    {
        "title": "Error using JAX, Array slice indices must have static start/stop/step",
        "question": "I'll be happy to help you with your code. If I understand correctly, you want to create a 2D Gaussian patch for each value in the darkField array. The size of the patch should ideally be calculated as 2 * np.ceil(3 * sigma) + 1, where sigma is the corresponding value from the darkField array. You have fixed the size value to 10 in your example to avoid errors.\nOnce the Gaussian patch is normalized to 1, you want to multiply it by the corresponding value from the intensityRefracted2DF array to obtain the generated blur. Finally, you want to add this blur patch to the intensityRefracted3 array.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom jax.scipy.signal import convolve2d\nfrom functools import partial\n\n@partial(jax.jit,static_argnums=(1,))\ndef gaussian_shape(sigma, size):\n    \"\"\"\n    Generate a Gaussian shape.\n\n    Args:\n        sigma (float or 2D numpy array): Standard deviation(s) of the Gaussian shape.\n        size (int): Size of the Gaussian shape.\n\n    Returns:\n        exponent (2D numpy array): Gaussian shape.\n    \"\"\"\n\n    x = jnp.arange(0, size) - jnp.floor(size / 2)\n    exponent = jnp.exp(-(x ** 2) / (2 * sigma ** 2))\n    exponent = jnp.outer(exponent, exponent)\n    exponent /= jnp.sum(exponent)\n    return exponent\n\n@partial(jax.jit)\ndef apply_dark_field(i, j, intensityRefracted2DF, intensityRefracted3, darkField):\n    currDF_ij=darkField[i,j]\n    patch = gaussian_shape(currDF_ij,10)\n    size2 = patch.shape[0] // 2\n    patch = patch * intensityRefracted2DF[i, j]\n\n\n    intensityRefracted3 = intensityRefracted3.at[i - size2:i + size2 + 1, j - size2:j + size2 + 1].add(patch * intensityRefracted2DF[i, j])\n    # intensityRefracted3 = jax.ops.index_add(intensityRefracted3, (i, j), intensityRefracted2DF[i, j] * (darkField[i, j] == 0))\n    return intensityRefracted3\n\n@jax.jit\ndef darkFieldLoop(intensityRefracted2DF, intensityRefracted3, darkField):\n    currDF = jnp.zeros_like(intensityRefracted3)\n    currDF = jnp.where(intensityRefracted2DF!=0,darkField,0)\n\n    i = jnp.nonzero(currDF,size=currDF.shape[0])\n    indices_i=i[0]\n    indices_j=i[1]\n    intensityRefracted3 = jnp.zeros_like(intensityRefracted3)\n\n    intensityRefracted3 = jax.vmap(apply_dark_field, in_axes=(0, 0, None, None, None))(indices_i, indices_j, intensityRefracted2DF, intensityRefracted3, darkField)\n\n    return intensityRefracted3\n\nintensityRefracted2DF = np.random.rand(10,10)\nintensityRefracted3 = np.zeros((10, 10))\ndarkField = np.random.rand(10, 10)\n\na=darkFieldLoop(intensityRefracted2DF,intensityRefracted3,darkField)\n\nfor i in range(a.shape[0]):\n    plt.imshow(a[i])\n    plt.show()\nAnd there is the error message :\npython\nCopy\nIndexError: Array slice indices must have static start/stop/step to   be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=3/0)>, Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=3/0)>, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\nI've also try to put i,j into static_argnums using partial\npython\nCopy\n@partial(jax.jit, static_argnums=(0,1))\ndef apply_dark_field(i, j, intensityRefracted2DF, intensityRefracted3, darkField):\nand there is the error:\npython\nCopy\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 0) of type <class 'jax._src.interpreters.batching.BatchTracer'> for function apply_dark_field is non-hashable.",
        "answers": [
            "The issue comes from the fact that JAX arrays cannot have a dynamic shape, and so dynamic start & end indices cannot be used in indexing expressions.\nYour solution of marking i and j as static would work, except that you are vmapping across these values, so by definition they cannot be static.\nThe best solution here is probably to use lax.dynamic_slice and lax.dynamic_update_slice, which are operations designed exactly for the case that you have (where indices are dynamic, but slice sizes are static).\nYou can replace this line:\npython\nCopy\nintensityRefracted3 = intensityRefracted3.at[i - size2:i + size2 + 1, j - size2:j + size2 + 1].add(patch * intensityRefracted2DF[i, j])\nwith this:\npython\nCopy\nstart_indices = (i - size2, j - size2)\nupdate = jax.lax.dynamic_slice(intensityRefracted3, start_indices, patch.shape)\nupdate += patch * intensityRefracted2DF[i, j]\nintensityRefracted3 = jax.lax.dynamic_update_slice(\n    intensityRefracted3, update,  start_indices)\nand it should work correctly with dynamic i and j. Though you should be careful, because if any of the specified indices are out-of-bounds, dynamic_slice and dynamic_update_slice will clip them into the valid range."
        ],
        "link": "https://stackoverflow.com/questions/76578461/error-using-jax-array-slice-indices-must-have-static-start-stop-step"
    },
    {
        "title": "JAX vmap vs pmap vs Python multiprocessing",
        "question": "I am rewriting some code from pure Python to JAX. I have gotten to the point where in my old code, I was using Python's multiprocessing module to parallelize the evaluation of a function over all of the CPU cores in a single node as follows:\npython\nCopy\n# start pool process \npool = multiprocessing.Pool(processes=10) # if node has 10 CPU cores, start 10 processes\n\n# use pool.map to evaluate function(input) for each input in parallel\n# suppose len(inputs) is very large and 10 inputs are processed in parallel at a time\n# store the results in a list called out\nout = pool.map(function,inputs)\n\n# close pool processes to free memory\npool.close()\npool.join()\nI know that JAX has vmap and pmap, but I don't understand if either of those are a drop-in replacement for how I'm using multiprocessing.pool.map above.\nIs vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).",
        "answers": [
            "Is vmap(function,in_axes=0)(inputs) distributing to all available CPU cores or what?\nNo, vmap has nothing to do with parallelization. It is a vectorizing transformation, not a parallelizing transformation. In the course of normal operation, JAX may use multiple cores via XLA, so vmapped operations may also do this. But there's no explicit parallelization in vmap.\nHow is pmap(function,in_axes=0)(inputs) different from vmap and multiprocessing.pool.map?\npmap parallelizes over multiple XLA devices. vmap does not parallelize, but rather vectorizes on a single device. multiprocessing parallelizes over multiple Python processes.\nIs my usage of multiprocessing.pool.map above an example of a \"single-program, multiple-data (SPMD)\" code that pmap is meant for?\nYes, it could be described as SPMD across multiple python processes.\nWhen I actually do pmap(function,in_axes=0)(inputs) I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?\npmap parallelizes over multiple XLA devices, and you have configured only a single XLA device, so the requested operation is not possible.\nFinally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).\nYes, I believe that pmap can be used to compute on multiple CPU cores. Whether it's nested with vmap is irrelevant. See JAX pmap with multi-core CPU.\nNote also that jax.pmap is deprecated in favor of the newer jax.shard_map, which is a much more flexible transformation for multi-device/multi-host computation. There's some info here: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html and https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html"
        ],
        "link": "https://stackoverflow.com/questions/76536601/jax-vmap-vs-pmap-vs-python-multiprocessing"
    },
    {
        "title": "JAX's slow performance on simple loops",
        "question": "I'm learning JAX and trying to do a simple test on the performance of JAX. The run time of the following code using JAX is weirdly slower than numpy or even simple addition of list members using python lists. I mean the following code has no purpose other than testing the speed.\nI'm wondering what might be the reason for this? I see that there is a functionality of JAX called \"fori_loop\" that may help in this examples or I can vectorize my actual code, but I want to know why this simple loop is so slow and do I need to avoid writing code like this and completely understand things in the JAX way?\nHere is the code in JAX:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import random\nimport time\n\nkey = random.PRNGKey(0)\nx = random.uniform(key, shape=(100,3))\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\nwhich takes 11 seconds\npython\nCopy\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n11.254772663116455\nThe same code using Numpy:\npython\nCopy\nimport numpy as np\nimport time\n\nx = np.random.rand(100,3)\n\ndef func(x):\n    for i in range(len(x)):\n        for j in range(i+1,len(x)):\n            x[i]+x[j]\n    return 0\n\na = time.time()\nres = func(x)\nb = time.time()\nprint(b-a)\ntakes around a milli second:\npython\nCopy\n0.005955934524536133\nThank you!",
        "answers": [
            "If you're interested in comparing the speed of JAX vs. NumPy speed, the place to start is here: JAX FAQ: Is JAX Faster Than NumPy? Quoting from there:\nin summary: if you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead. If you’re running your code on GPU or TPU, or are benchmarking more complicated JIT-compiled sequences of operations on CPU, you can generally expect JAX to outperform NumPy.\nYour benchmark does many individual array operations, each of which is very inexpensive, so effectively all you're measuring is the single-operation dispatch time. This is precisely in the regime where we'd expect NumPy to be fast, and JAX to be slow.\nFor real use-cases involving repeated operations, there are several ways you might optimize the code (including wrapping the whole function in jit, using vmap for efficient batching, or using fori_loop for sequential operations) but for the example function you give it's hard to say what's best. Taking your example at face value, I'd optimize it this way:\npython\nCopy\ndef func(x):\n  return 0\nAs written, there's no need to do any operations on x at all (but I suspect that's not particularly helpful for your real use-case)."
        ],
        "link": "https://stackoverflow.com/questions/76474532/jaxs-slow-performance-on-simple-loops"
    },
    {
        "title": "How to vmap over cho_solve and cho_factor?",
        "question": "The following error appears because of the last line of code below:\njax.errors.ConcretizationTypeError Abstract tracer value encountered where concrete value is expected...\nThe problem arose with the bool function.\nIt looks like it is due to the lower return value from cho_factor, which _cho_solve (note underscore) requires as static.\nI'm new to jax, so I was hoping that vmap-ing cho_factor into cho_solve would just work. What have I done wrong here?\npython\nCopy\nimport jax\n\nkey = jax.random.PRNGKey(0)\nk_y = jax.random.normal(key, (100, 10, 10))\ny = jax.random.normal(key, (100, 10, 1))\n\nmatmul = jax.vmap(jax.numpy.matmul)\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor)\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve)\n\nk_y = matmul(k_y, jax.numpy.transpose(k_y, (0, 2, 1)))\nchol, lower = cho_factor(k_y)\nresult = cho_solve((chol, lower), y)",
        "answers": [
            "The issue is that in each case, lower is a static scalar that should not be mapped over. So if you specify in_axes and out_axes so that lower is mapped over axis None, the vmap should work:\npython\nCopy\ncho_factor = jax.vmap(jax.scipy.linalg.cho_factor, out_axes=(0, None))\ncho_solve = jax.vmap(jax.scipy.linalg.cho_solve, in_axes=((0, None), 0))",
            "So I didn't manage to get cho_factor and cho_solve working, but worked around it using cholesky and solve_triangular:\npython\nCopy\n  cholesky = jax.vmap(jax.scipy.linalg.cholesky, in_axes=(0, None))\n  solve_tri = jax.vmap(jax.scipy.linalg.solve_triangular, in_axes=(0, 0, None, None))\n\n  L = cholesky(k_y, True)\n  result2 = solve_tri(L, solve_tri(L, y, 0, True), 1, True)"
        ],
        "link": "https://stackoverflow.com/questions/76458629/how-to-vmap-over-cho-solve-and-cho-factor"
    },
    {
        "title": "How to rewrite this JAX snippet to prevent TypeError: unhashable type: 'DynamicJaxprTracer'?",
        "question": "I am rewriting some Python code in JAX with the goal of speeding it up using jax.jit. There is a part of my old code where I have a dict that maps some integer to a list of functions. Every integer has a unique list of functions assigned to it. The functions are all JAX compatible, but the problem is my use case. I have a master function whose arguments are the number to be fed into these functions and the integer that tells me which list of functions to grab from the dict.\nHere is a simple example to demonstrate what I mean:\npython\nCopy\nfrom jax import jit\n\n# set up a dict that maps integers to a list of functions\ntest_dict = {1122997037:[lambda x: x**2, lambda x: 2*x],\n             1124279607:[lambda x: x**3, lambda x: 3*x]}\n             \n@jit\ndef evaluate_functions(xval,index):\n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n    \n    # given the input index, pull the relevant list of functions from test_dict\n    func1, func2 = test_dict[index]\n    \n    # evaluate both functions at the input xval and return values\n    return func1(xval), func2(xval)\n\n\nprint(evaluate_functions(2,1122997037))\nprint(evaluate_functions(2,1124279607))\nIf you remove the @jit decorator, it works as expected and prints (4,4) and (8,6). But with the @jit decorator, it raises the following error:\npython\nCopy\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[60], line 16\n     11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n---> 16 print(evaluate_functions(2,1))\n     17 print(evaluate_functions(2,2))\n\n    [... skipping hidden 12 frame]\n\nCell In[60], line 11, in evaluate_functions(xval, index)\n      7 @jit\n      8 def evaluate_functions(xval,index):\n      9     \"\"\" index can be assumed to be one of the keys of test_dict \"\"\" \n---> 11     f1, f2 = test_dict[index]\n     13     return f1(xval), f2(xval)\n\nTypeError: unhashable type: 'DynamicJaxprTracer'\nIs there any way to rewrite this to make it work with @jit? For example using jax.lax.switch?",
        "answers": [
            "The problem with this is that you can't do conditional code execution in JAX based on input data (Edit: I was mistaken about the semantics of lax.cond). You could mark the index argument as static, which means that JAX will recompile the function each time a new value of it is used:\npython\nCopy\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef evaluate_functions(xval,index):\n    ...\nThis is the sort of compilation behavior you'll see:\npython\nCopy\nevaluate_functions(0, 0) # JIT runs, may be slow\nevaluate_functions(1, 0) # No compilation, fast \nevaluate_functions(1, 3) # JIT runs again\nevaluate_functions(7, 3) # No compilation\n# etc\nThis is a good solution if you'll be using each of the functions many times. If there's a large number of functions and you'll only be using each one once this won't help you.\nEdit: Here's a solution using jax.lax.switch, I'm not sure how the performance will be:\npython\nCopy\nkeys, branches = zip(*test_dict.items())    \nbranches1, branches2 = zip(*branches)    \n    \n@jit    \ndef evaluate_functions(xval,index):    \n    \"\"\" index can be assumed to be one of the keys of test_dict \"\"\"·    \n    ind_to_key = jnp.asarray(keys)    \n    \n    ind = jnp.where(ind_to_key == index, size=1)[0][0]    \n    out1 = jax.lax.switch(ind, branches1, xval)                                                                               \n    out2 = jax.lax.switch(ind, branches2, xval)                                                                               \n    return out1, out2 "
        ],
        "link": "https://stackoverflow.com/questions/76418151/how-to-rewrite-this-jax-snippet-to-prevent-typeerror-unhashable-type-dynamicj"
    },
    {
        "title": "JAX code for minimizing Lennard-Jones potential for 2 points in Python gives unexpected results",
        "question": "I am trying to practice using JAX fo optimization problem and I am trying to do a simple problem, which is to minimize Lennard-Jones potential for just 2 points and I set both epsilon and sigma in Lennard-Jones potential equal 1, so the potential is just: F = 4(1/r^12-1/r^6) and r is the distance between the two points. And the result should be r = 2^(1/6), which is approximately 1.12.\nUsing JAX, I wrote following code, which is pretty simple and short, my initial guess values for two points are [0,1], which I think it is reasonable(because for Lennard-Jones potential it could be a problem because it approach infinite if r guess is too small). As I mentioned, I am expecting a value of r around 1.12 after the minimization, however, the result I get is [-0.71276042 1.71276042], so the distance is 2.4, which is clearly too big and I am wondering how can I fix it. I original doubt it might be the precision so I change the data type to float64, but the results are still the same. Any help will be greatly appreciated! Here is my code\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy.optimize import minimize\nfrom jax import vmap\nimport matplotlib.pyplot as plt\n\nN = 2\njax.config.update(\"jax_enable_x64\", True)\nx_init = jnp.arange(N, dtype=jnp.float64)\nepsilon = 1\nsigma = 1\n\ndef potential(r):\n    r = jnp.where(r == 0, jnp.finfo(jnp.float64).eps, r)\n    return 4 * epsilon * ((sigma/r)**12 - (sigma/r)**6)\n\ndef F(x):\n    # Compute all pairwise distances\n    r = jnp.abs(x[:, None] - x[None, :])\n    # Compute all pairwise potentials\n    pot = vmap(vmap(potential))(r)\n    # Exclude the diagonal (distance = 0) and avoid double-counting by taking upper triangular part\n    pot = jnp.triu(pot, 1)\n    # Sum up all the potentials\n    total = jnp.sum(pot)\n    return total\n\n# Minimize the function\nprint(F)\nresult = minimize(F, x_init, method='BFGS')\n\n# Extract the optimized positions of the points\nx_solutions = result.x\nprint(x_solutions)",
        "answers": [
            "This function is one that would be very difficult for any unconstrained gradient-based optimizer to correctly optimize. Holding one point at zero and varying the other point on the range (0, 10], we see the potential looks like this:\npython\nCopy\nr = jnp.linspace(0.1, 5.0, 1000)\nplt.plot(r, jax.vmap(lambda ri: F(jnp.array([0, ri])))(r))\nplt.ylim(-2, 10)\nTo the left of the minimum, the gradient quickly diverges to negative infinity, meaning for nearly any reasonable step size, the optimizer will likely overshoot the minimum. Then on the right side, if the optimizer goes even a few units too far, the gradient tends to zero, meaning for nearly any reasonable step size, the optimizer will get stuck in a regime where the potential has almost no variation.\nAdd to this the fact that you've set up the model with two degrees of freedom in a degenerate potential, and it's not surprising that gradient-based optimization methods are failing.\nYou can make some progress here by minimizing the log of the shifted potential, which has the effect of smoothing the steep gradients, and lets the BFGS minimizer find an expected minimum:\npython\nCopy\nresult = minimize(lambda x: jnp.log(2 + F(x)), x_init, method='BFGS')\nprint(result.x)\n# [-0.06123102  1.06123102]\nBut in general my suggestion would probably be to opt for a constrained optimization approach instead, perhaps one of the JAXOpt constrained optimization methods, where you can rule-out problematic regions of the parameter space."
        ],
        "link": "https://stackoverflow.com/questions/76353392/jax-code-for-minimizing-lennard-jones-potential-for-2-points-in-python-gives-une"
    },
    {
        "title": "jax.numpy.delete assume_unique_indices unexpected keyword argument",
        "question": "I can not seem to get the assume_unique_indices from jax.numpy working. According to the documentation here, the jnp.delete has a keyword argument \"assume_unique_indices\" that is supposed to make this function jit compatible when we are sure that the index array is an integer array and is guaranteed to contain unique entries.\nHere is an minimum reproducible example\npython\nCopy\nimport jax\n\narr = jnp.array([1, 2, 3, 4, 5])\nidx = jnp.array([0, 2, 4])\n\nprint(jax.__version__)\n\n# Delete elements at indices idx\nout = jax.numpy.delete(arr, idx, assume_unique_indices=True)\n\nprint(out) # [2 4]\nThe error message\npython\nCopy\n0.4.8\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-bf0277118922> in <cell line: 9>()\n      7 \n      8 # Delete elements at indices idx\n----> 9 out = jax.numpy.delete(arr, idx, assume_unique_indices=False)\n     10 \n     11 print(out) # [2 4]\n\nTypeError: delete() got an unexpected keyword argument 'assume_unique_indices'\nDeleting the assume_unique_indices made it work as expected.",
        "answers": [
            "assume_unique_indices was added in https://github.com/google/jax/pull/15671, after JAX version 0.4.8 was released. If you update to version 0.4.9 or newer, your code should work.",
            "Ok, as it turns out, the 'assume_unique_indices' is only added rather recently, updating to jax version 0.4.10 did the trick"
        ],
        "link": "https://stackoverflow.com/questions/76244047/jax-numpy-delete-assume-unique-indices-unexpected-keyword-argument"
    },
    {
        "title": "CuDNN error when running JAX on GPU with apptainer",
        "question": "I have an application written in Python 3.10+ with JAX that I would like to run on GPU. I can run containers on my local computer cluster using apptainer (but not Docker) which has an NVIDIA A40 GPU. Based on the proposed Dockerfile for JAX I made an Ubuntu-based image from the following Dockerfile:\nFROM nvidia/cuda:11.8.0-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI then convert the Docker image to an apptainer image using apptainer pull docker://my-image and then run the container using apptainer run --nv docker://my-image as described in the apptainer GPU docs.\nError\nWhen I run the following code\npython\nCopy\nimport jax\njax.numpy.array(1.)\nJAX immediately crashes with the following error message:\n2023-05-11 14:41:50.580441: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\", line 1993, in array\n    out_array: Array = lax_internal._convert_element_type(out, dtype, weak_type=weak_type)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\", line 537, in _convert_element_type\n    return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 360, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 363, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 817, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 117, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 253, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\", line 246, in cached\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 208, in xla_primitive_callable\n    compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 254, in _xla_callable_uncached\n    return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 2816, in compile\n    self._executable = UnloadedMeshExecutable.from_hlo(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\", line 3028, in from_hlo\n    xla_executable = dispatch.compile_or_get_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 526, in compile_or_get_cached\n    return backend_compile(backend, serialized_computation, compile_options,\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 471, in backend_compile\n    return backend.compile(built_c, compile_options=options)\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\nWhat I've tried\nBased on a github thread with a similar error message (https://github.com/google/jax/issues/4920), I have tried to add some CUDA paths:\nbash\nCopy\nexport PATH=/usr/local/cuda-11/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-11/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\nHowever, this did not resolve my problem.\nLocal docker container works without gpu\nWhen I test the image built from the Dockerfile on my local machine without GPU, everything works fine:\nbash\nCopy\n$ docker run -ti my-image python3 -c 'import jax; jax.numpy.array(1.)'\n$\nApptainer container detects GPU's\nI can confirm that the GPU's are detected in the apptainer container. I get the following output when I run nvidia-smi:\n$ apptainer run --nv docker://my-image nvidia-smi\nINFO:    Using cached SIF image\n\n==========\n== CUDA ==\n==========\n\nCUDA Version 11.8.0\n\nContainer image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nThu May 11 14:34:18 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A40          On   | 00000000:00:08.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A40          On   | 00000000:00:09.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A40          On   | 00000000:00:0A.0 Off |                    0 |\n|  0%   30C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A40          On   | 00000000:00:0B.0 Off |                    0 |\n|  0%   31C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   4  NVIDIA A40          On   | 00000000:00:0C.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   5  NVIDIA A40          On   | 00000000:00:0D.0 Off |                    0 |\n|  0%   32C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   6  NVIDIA A40          On   | 00000000:00:0E.0 Off |                    0 |\n|  0%   31C    P8    30W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   7  NVIDIA A40          On   | 00000000:00:0F.0 Off |                    0 |\n|  0%   30C    P8    31W / 300W |      0MiB / 46068MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nEdit\nCuda 11 image with CuDNN 8.7 gives different error\nWhen I use a different base image with cudnn 8.7.0:\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nI get a different error:\n$ apptainer run --nv docker://my-image python3 -c 'import jax; jax.numpy.array(1.)'\n\nCould not load library libcudnn_ops_infer.so.8. Error: libnvrtc.so: cannot open shared object file: No such file or directory\nAborted (core dumped)",
        "answers": [
            "Based on the pointers of jakevdp I managed to find a solution. What was needed was:\nThe CUDA 11 image with CuDNN 8.7.\nThe devel instead of the runtime image.\nTogether, I could succesfully run JAX on apptainer with the following Dockerfile:\nFROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nRUN apt update && apt install python3-pip -y\nRUN pip install \"jax[cuda11_cudnn86]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/76229252/cudnn-error-when-running-jax-on-gpu-with-apptainer"
    },
    {
        "title": "Local Jax variable not updating in `jit`ted function, but updating in standard?",
        "question": "So, I've got some code and I could really use help deciphering the behavior and how to get it to do what I want.\nSee my code as follows:\npython\nCopy\nfrom typing import Callable, List\n\nimport chex\nimport jax.numpy as jnp\nimport jax\n\nWeights = List[jnp.ndarray]\n\n\n@chex.dataclass(frozen=True)\nclass Model:\n    mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    jitted_mult: Callable[\n        [jnp.ndarray],\n        jnp.ndarray\n    ]\n\n    weight_updater: Callable[\n        [jnp.ndarray], None\n    ]\n\n\ndef create_weight():\n    return jnp.ones((2, 5))\n\n\ndef wrapper():\n    weights = create_weight()\n\n    def mult(input_var):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var):\n        return weights.dot(input_var)\n\n    def update_locally_created(new_weights):\n        nonlocal weights\n        weights = new_weights\n        return weights\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=update_locally_created\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult)))\n        print(jnp.sum(tester.jitted_mult(to_mult)))\n\n        if i % 2 == 0:\n            tester.weight_updater(jnp.zeros((2, 5)))\n        else:\n            tester.weight_updater(jnp.ones((2, 5)))\n\n        print(\"*\" * 10)\nTL;DR I'm defining some \"weights\" within a function closure, and I'm trying to modify the weights via a nonlocal. The problem seems to be that the jit-ted version (jitted_mult of the function doesn't recognize the \"updated\" weights, whereas the non-jit function (mult) does.\nWhat can I do to make it recognize the update? I think that I might be able to do what Build your own Haiku does, but that seems like a lot of work for an experiment",
        "answers": [
            "This is working as expected: the reason it's not respecting the update is because your function is not pure (see JAX Sharp Bits: Pure Functions). In your case, the function is not pure because the output depends on an input that is not explicitly passed to the function. This violates the assumptions made by jit and other JAX transformations, which leads to unexpected behavior.\nTo fix it I would make this implicit input explicit, so that your function is pure. It might look something like this:\npython\nCopy\ndef wrapper():\n    def mult(input_var, weights):\n        return weights.dot(input_var)\n\n    @jax.jit\n    def jitted_mult(input_var, weights):\n        return weights.dot(input_var)\n\n    return Model(\n        mult=mult,\n        jitted_mult=jitted_mult,\n        weight_updater=None\n    )\n\n\nif __name__ == '__main__':\n    tester = wrapper()\n    to_mult = jnp.ones((5, 2))\n    weights = create_weight()\n    for i in range(5):\n        print(jnp.sum(tester.mult(to_mult, weights)))\n        print(jnp.sum(tester.jitted_mult(to_mult, weights)))\n\n        if i % 2 == 0:\n            weights = jnp.zeros((2, 5))\n        else:\n            weights = jnp.ones((2, 5))\n\n        print(\"*\" * 10)"
        ],
        "link": "https://stackoverflow.com/questions/76205477/local-jax-variable-not-updating-in-jitted-function-but-updating-in-standard"
    },
    {
        "title": "No module named 'jax.experimental.global_device_array' when running the official Flax Example on Colab with V100",
        "question": "I have been trying to understand this official flax example, based on a Coalb pro+ account with V100. When I execute the command python main.py --workdir=./imagenet --config=configs/v100_x8.py , the returned error is\nFile \"/content/FlaxImageNet/main.py\", line 29, in <module>\nimport train\nFile \"/content/FlaxImageNet/train.py\", line 30, in <module>\nfrom flax.training import checkpoints\nFile \"/usr/local/lib/python3.10/dist-packages/flax/training/checkpoints.py\", line 34, \nin <module>\nfrom jax.experimental.global_device_array import GlobalDeviceArray\nModuleNotFoundError: No module named 'jax.experimental.global_device_array'\nI am not sure whether global_device_array has been moved from jax.experimental package or it is no longer needed or replaced by other equivalent methods.",
        "answers": [
            "GlobalDeviceArray was deprecated in JAX version 0.4.1 and removed in JAX version 0.4.7.\nWith that in mind, it seems the code in question requires JAX version 0.4.6 or older. You might consider reporting this incompatibility to the flax project: http://github.com/google/flax/."
        ],
        "link": "https://stackoverflow.com/questions/76191911/no-module-named-jax-experimental-global-device-array-when-running-the-official"
    },
    {
        "title": "Fail to understand the usage of partial argument in Flax Resnet Official Example",
        "question": "I have been trying to understand this official example. However, I am very confused about the use of partial in two places.\nFor example, in line 94, we have the following:\nconv = partial(self.conv, use_bias=False, dtype=self.dtype)\nI am not sure why it is possible to apply a partial to a class, and where later in the code we fill in the missing argument (if we need to).\nComing to the final definition, I am even more confused. For example,\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n               block_cls=ResNetBlock)\nWhere do we apply the argument such as stage_size=[2,2,2,2]?\nThank you",
        "answers": [
            "functools.partial will partially evaluate a function, binding arguments to it for when it is called later. here's an example of it being used with a function:\npython\nCopy\nfrom functools import partial\n\ndef f(x, y, z):\n  print(f\"{x=} {y=} {z=}\")\n\ng = partial(f, 1, z=3)\ng(2)\n# x=1 y=2 z=3\nand here is an example of it being used on a class constructor:\npython\nCopy\nfrom typing import NamedTuple\n\nclass MyClass(NamedTuple):\n  a: int\n  b: int\n  c: int\n\nmake_class = partial(MyClass, 1, c=3)\nprint(make_class(b=2))\n# MyClass(a=1, b=2, c=3)\nThe use in the flax example is conceptually the same: partial(f) returns a function that when called, applies the bound arguments to the original callable, whether it is a function, a method, or a class constructor.\nFor example, the ResNet18 function created here:\npython\nCopy\nResNet18 = partial(ResNet, stage_sizes=[2, 2, 2, 2],\n                   block_cls=ResNetBlock)\nis a partially-evaluated ResNet constructor, and the function is called in a test here:\npython\nCopy\n  @parameterized.product(\n      model=(models.ResNet18, models.ResNet18Local)\n  )\n  def test_resnet_18_v1_model(self, model):\n    \"\"\"Tests ResNet18 V1 model definition and output (variables).\"\"\"\n    rng = jax.random.PRNGKey(0)\n    model_def = model(num_classes=2, dtype=jnp.float32)\n    variables = model_def.init(\n        rng, jnp.ones((1, 64, 64, 3), jnp.float32))\n\n    self.assertLen(variables, 2)\n    self.assertLen(variables['params'], 11)\nmodel here is the partially evaluated function ResNet18, and when it is called it returns the fully-instantiated ResNet object with the parameters specified in the ResNet18 partial definition."
        ],
        "link": "https://stackoverflow.com/questions/76178123/fail-to-understand-the-usage-of-partial-argument-in-flax-resnet-official-example"
    },
    {
        "title": "Calling an initialized function from a list inside a jitted JAX function",
        "question": "Given is a jitted function, which is calling another function that maps over a batch, which again calls a function, i.e. inner_function, to compute a certain property. Also given is a list of initialized functions intialized_functions_dic, from which we want to call the proper initialized function based on some information passed as argument, e.g. info_1. Is there a way to make this work? Thanks in advance.\npython\nCopy\ninitialized_functions_dic = {1:init_function1, 2:init_function_2, 3:init_function_3}\n\n\ndef inner_function(info_1, info_2, info_3):\n    return 5 + outside_dic[info_1]\nCalling outside_dic[info_1] will throw an error due to trying to access a dictionary with a traced value.\nTrying to pass info_1 as static_argnums also fails due to info_1 being an unhashable type 'ArrayImpl'.",
        "answers": [
            "It sounds like you're looking for jax.lax.switch, which will switch between entries in a list of functions given an index:\npython\nCopy\ninitialized_functions = [init_function_1, init_function_2, init_function_3]\n\ndef inner_function(info_1, info_2, info_3):\n    idx = info_1 - 1  # lists are zero-indexed\n    args = (info_2, info_3) # tuple of arguments to pass to the function\n    return 5 + lax.switch(idx, initialized_functions, *args)"
        ],
        "link": "https://stackoverflow.com/questions/76094143/calling-an-initialized-function-from-a-list-inside-a-jitted-jax-function"
    },
    {
        "title": "JAX VMAP Parallelization Details",
        "question": "I was wondering how vmap's internals work. When I vectorize code using jax.lax.map, I know that each element is executed consecutively. However when I use vmap I execute the vectorized operation apparently in parallel. Can someone provide me with a more detailed explanation of how the parallelization works? How does Jax determine the number of parallel processes, and can this behaviour be influenced by the user?\nThanks in advance.",
        "answers": [
            "jax.vmap is a vectorizing/batching transform, not a parallelizing transform. Internally, it converts an unbatched function to a batched function, lowering to efficient primitive calls rather than an explicit map or loop.\nFor example, here is a simple function, where we create both a manually-looped and an automatically vectorized batched version:\npython\nCopy\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\n\ndef f(x, y):\n  return x @ y\n\nnum_batches = 3\nnum_entries = 5\n\nnp.random.seed(0)\nx = np.random.rand(num_batches, num_entries)\ny = np.random.rand(num_batches, num_entries)\n\n\nf_loop = lambda x, y: jnp.stack([f(xi, yi) for xi, yi in zip(x, y)])\nf_vmap = jax.vmap(f)\n\nprint(f_loop(x, y))\n# [1.3567398 2.1908383 1.6315514]\nprint(f_vmap(x, y))\n# [1.3567398 2.1908383 1.6315514]\nThese return the same results (by design), but they are quite different under the hood; by printing the jaxpr, we see that the loop version requires a long sequence of XLA calls:\npython\nCopy\nprint(jax.make_jaxpr(f_loop)(x, y))\npython\nCopy\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] a\n    d:f32[5] = squeeze[dimensions=(0,)] c\n    e:f32[1,5] = slice[limit_indices=(1, 5) start_indices=(0, 0) strides=(1, 1)] b\n    f:f32[5] = squeeze[dimensions=(0,)] e\n    g:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d f\n    h:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] a\n    i:f32[5] = squeeze[dimensions=(0,)] h\n    j:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=(1, 1)] b\n    k:f32[5] = squeeze[dimensions=(0,)] j\n    l:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] i k\n    m:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] a\n    n:f32[5] = squeeze[dimensions=(0,)] m\n    o:f32[1,5] = slice[limit_indices=(3, 5) start_indices=(2, 0) strides=(1, 1)] b\n    p:f32[5] = squeeze[dimensions=(0,)] o\n    q:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] n p\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] g\n    s:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] l\n    t:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] q\n    u:f32[3] = concatenate[dimension=0] r s t\n  in (u,) }\nOn the other hand, the vmap version lowers to just a single generalized dot product:\npython\nCopy\nprint(jax.make_jaxpr(f_vmap)(x, y))\npython\nCopy\n{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[3] = dot_general[dimension_numbers=(([1], [1]), ([0], [0]))] a b\n  in (c,) }\nNotice that there is nothing concerning parallelization here; rather we've automatically created an efficient batched version of our operation. The same is true of vmap applied to more complicated functions: it does not involve parallelization, rather it outputs an efficient batched version of your operation in an automated manner."
        ],
        "link": "https://stackoverflow.com/questions/75891826/jax-vmap-parallelization-details"
    },
    {
        "title": "JAX performance problems",
        "question": "I am obviously not following best practices, but maybe that's because I don't know what they are. Anyway, my goal is to generate a tubular neighborhood about a curve in three dimensions. A curve is give by an array of length three f(t) = jnp.array([x(t), y(t), z(t)]).\nNow, first we compute the unit tangent:\npython\nCopy\ndef get_uvec2(f):\n  tanvec = jacfwd(f)\n  return lambda x: tanvec(x)/jnp.linalg.norm(tanvec(x))\nNext, we compute the derivative of the tangent:\npython\nCopy\ndef get_cvec(f):\n  return get_uvec2(get_uvec2(f))\nThird, we compute the orthogonal frame at a point:\npython\nCopy\ndef get_frame(f):\n  tt = get_uvec2(f)\n  tt2 = get_cvec(f)\n  def first2(t):\n    x = tt(t)\n    y = tt2(t)\n    tt3 = (jnp.cross(x, y))\n    return jnp.array([x, y, tt3])\n  return first2\nwhich we use to generate a point in the circle around a given point:\npython\nCopy\ndef get_point(frame, s):\n  v1 = frame[1, :]\n  v2 = frame[2, :]\n  return jnp.cos(s) * v1 + jnp.sin(s) * v2\nAnd now we generate the point on the tubular neighborhood corresponding to a pair of parameters:\npython\nCopy\ndef get_grid(f, eps):\n  ffunc = get_frame(f)\n  def grid(t, s):\n    base = f(t)\n    frame = ffunc(t)\n    return base + eps * get_point(frame, s)\n  return grid\nAnd finally, we put it all together:\npython\nCopy\ndef get_reg_grid(f, num1, num2, eps):\n  plist = []\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  for t in tarray:\n    for s in sarray:\n      plist.append(g(t, s))\n  return jnp.vstack(plist)\nFinally, use it to compute the tubular neighborhood around a circle in the xy-plane:\npython\nCopy\nf1 = lambda x: jnp.array([jnp.cos(2 * jnp.pi * x), jnp.sin(2 * jnp.pi * x), 0.0])\n\nfff = np.array(get_reg_grid(f1, 200, 200, 0.1))\nThe good news is that it all works. The bad news is that this computation takes well over an hour. Where did I go wrong?",
        "answers": [
            "JAX and numpy share one key rule-of-thumb for getting good performance: if you are writing for loops over array values, your code will probably be slow.\nTo make your code more performant, you should replace your loops with vectorized operations. One nice feature of JAX is jax.vmap, a vectorizing transform which makes this relatively easy. You can also use jax.jit to JIT-compile your function and get even faster execution.\nHere's a modified version of your get_reg_grid function that returns the same result with much faster execution:\npython\nCopy\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['f', 'num1', 'num2'])\ndef get_reg_grid(f, num1, num2, eps):\n  tarray = jnp.linspace(start = 0.0, stop = 1.0, num = num1)\n  sarray = jnp.linspace(start = 0.0, stop = 2 * jnp.pi, num = num2)\n  g = get_grid(f, eps)\n  g = jax.vmap(g, in_axes=(None, 0))\n  g = jax.vmap(g, in_axes=(0, None))\n  return jnp.vstack(g(tarray, sarray))\nWith this approach, your code executes in about 300 microseconds:\npython\nCopy\n%timeit get_reg_grid(f1, 200, 200, 0.1).block_until_ready()\n# 296 µs ± 157 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
        ],
        "link": "https://stackoverflow.com/questions/75872342/jax-performance-problems"
    },
    {
        "title": "using jax lax scan with inputs that don't change across iterations within scan but are different each time scan is called",
        "question": "Jax lax scan operates on a function that takes two arguments, a carry and a sequence of inputs. I am wondering how scan should be called if some inputs don't change across iterations of the scan. Naively, I could create a sequence of identical inputs, but this seems wasteful/redundant and more importantly, this isn't always possible, as scan can only scan over arrays. For example, one of the inputs I want to pass to my function is a train state (e.g. from flax.training import train_state) that contains my model and its parameters, which cannot be put into array. As I say in the title, these inputs may also change each time I call scan (e.g. the model parameters will change).\nAny ideas on how best to do this?\nThanks.",
        "answers": [
            "In general, you have three possible approaches for this:\nConvert the single value into a sequence of identical inputs to scan over\nPut the single value in the carry to carry it along to each step of the scan\nClose over the single value\nHere are three examples of a computation using these strategies:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\na = jnp.arange(5)\nb = 2\npython\nCopy\n# Strategy 1: duplicate b across sequence\ndef f(carry, xs):\n  a, b = xs\n  result = a * b\n  return carry + result, result\n\nb_seq = jnp.full_like(a, b)\n\ntotal, cumulative = jax.lax.scan(f, 0, (a, b_seq))\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\npython\nCopy\n# Strategy 2: put b in the carry\ndef f(carry, xs):\n  carry, b = carry\n  a = xs\n  result = a * b\n  return (carry + result, b), result\n\n(total, _), cumulative = jax.lax.scan(f, (0, b), a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\npython\nCopy\n# Strategy 3: close over b\nfrom functools import partial\n\ndef f(carry, xs, b):\n  a = xs\n  result = a * b\n  return carry + result, result\n\ntotal, cumulative = jax.lax.scan(partial(f, b=b), 0, a)\nprint(total) # 20\nprint(cumulative) # [0 2 4 6 8]\nWhich you use probably depends on the context of where you are using it, but I personally think closure (option 3) is probably the cleanest approach."
        ],
        "link": "https://stackoverflow.com/questions/75776268/using-jax-lax-scan-with-inputs-that-dont-change-across-iterations-within-scan-b"
    },
    {
        "title": "What is the correct way to define a vectorized (jax.vmap) function in a class?",
        "question": "I want to add a function, which is vectorized by jax.vmap, as a class method. However, I am not sure where to define this function within the class. My main goal is to avoid, that the function is being redefined each time I call the class method.\nHere is a minimal example for a class that counts how often a value occurs in a jnp.array, with a non-vectorized and vectorized version:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\nclass ValueCounter():\n\n    def __init__(self): # for completeness, not used\n        self.attribute_1 = None\n\n    @staticmethod\n    def _count_value_in_array( # non-vectorized function\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n\n    # here comes the vectorized function\n    def count_values_in_array(self, array: jnp.array, value_array: jnp.array) -> jnp.array:\n        \"\"\"Count how often each value in an array of values occurs in an array\"\"\"\n        count_value_in_array_vec = jax.vmap(\n            self._count_value_in_array, in_axes=(None, 0)\n        ) # vectorized function is defined again each time the function is called\n        return count_value_in_array_vec(array, value_array)\nExample output & input:\npython\nCopy\nvalue_counter = ValueCounter()\nvalue_counter.count_values_in_array(jnp.array([0, 1, 2, 2, 1, 1]), jnp.array([0, 1, 2]))\nResult (correct as expected)\npython\nCopy\nArray([1, 3, 2], dtype=int32)\nThe vectorized function count_value_in_array_vecis redefined each time count_values_in_array - which seems unnecessary to me. However, I am a bit stuck on how to avoid this. Does someone know how the vectorized function could be integrated into the class in a more elegant way?",
        "answers": [
            "You can decorate the static method directly; for example:\npython\nCopy\nfrom functools import partial\n\n# ...\n\n    @staticmethod\n    @partial(jax.vmap, in_axes=(None, 0))\n    def _count_value_in_array(\n        array: jnp.array, value: float\n    ) -> jnp.array:\n        \"\"\"Count how often a value occurs in an array\"\"\"\n        return jnp.count_nonzero(array == value)\n# ..."
        ],
        "link": "https://stackoverflow.com/questions/75696639/what-is-the-correct-way-to-define-a-vectorized-jax-vmap-function-in-a-class"
    },
    {
        "title": "Python function minimization not changing optimization variables",
        "question": "I need to minimize a simple function that divides two values. The optimization paramter x is a (n,m) numpy array from which I calculate a float.\npython\nCopy\n# An initial value\nnormX0 = calculate_normX(x_start)\n\ndef objective(x) -> float:\n    \"\"\"Objective function \"\"\"\n    x = x.reshape((n,m))\n    normX = calculate_normX(x)\n    return -(float(normX) / float(normX0))\ndef calculate_normX() is a wrapper function to an external (Java-)API that takes the ndarray as an input and outputs a float, in this case, the norm of a vector. For the optimization, I was using jax and jaxopt, since it supports automatic differentiation of objective.\npython\nCopy\nsolver = NonlinearCG(fun=objective, maxiter=5, verbose=True)\nres = solver.run(x.flatten())\nor the regular scipy minimize\npython\nCopy\nobjective_jac = jax.jacrev(objective)\nminimize(objective, jac=objective_jac, x0=`x.flatten(), method='L-BFGS-B', options={'maxiter': 2})\nIn both cases, however, x is not changed during the optimization step. Even initializing x with random values the optimizer does not seem to work. I also tried other solvers like Jaxopt NonlinearCG. What am I doing wrong?",
        "answers": [
            "The external, non-JAX function call is almost certainly the source of the problem. Non-JAX function calls within JAX transforms like jacrev effectively get replaced with a trace-time constant (and in most cases will error), and so it makes sense that your optimization will not change its value.\nThe best approach would be to define your calculate_normX function using JAX rather than calling out to an external API, and then everything should work automatically.\nIf you must call to an external API, one way to do this in JAX is to use pure_callback along with custom_jvp to define the autodiff rule for your external callback. There is an example of this in the JAX docs: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html#example-pure-callback-with-custom-jvp"
        ],
        "link": "https://stackoverflow.com/questions/75651326/python-function-minimization-not-changing-optimization-variables"
    },
    {
        "title": "Nested vmap in pmap - JAX",
        "question": "I currently can run simulations in parallel on one GPU using vmap. To speed things up, I want to batch the simulations over multiple GPU devices using pmap. However, when pmapping the vmapped function I get a tracing error.\nThe code I use to get a trajectory state is:\npython\nCopy\ntraj_state = vmap(run_trajectory, in_axes=(0, None, 0))(sim_state, timings, lambda_array)\n                                                                        \nwhere lambda_array parameterises each simulation, which is run by the function run_trajectory which runs a single simulation. I then try to nest this inside a pmap:\npython\nCopy\npmap(vmap(run_trajectory, in_axes=(0, None, 0)),in_axes=(0, None, 0))(reshaped_sim_state, timings, reshaped_lambda_array)                                                                                       \nIn doing so I get the error:\npython\nCopy\nWhile tracing the function run_trajectory for pmap, this concrete value was not available in Python because it depends on the value of the argument 'timings'.\nI'm quite new to JAX and although there are documentations on errors with traced values, I'm not very sure on how to navigate this problem.",
        "answers": [
            "vmap and pmap have slightly different APIs when it comes to in_axes. In vmap, setting in_axes=None causes inputs to be unmapped and static (i.e. un-traced), while in pmap even inputs with in_axes=None will be unmapped but still traced:\npython\nCopy\nfrom jax import vmap, pmap\nimport jax.numpy as jnp\n\ndef f(x, condition):\n  # requires untraced condition:\n  return x if condition else x + 1\n\nx = jnp.arange(4)\nvmap(f, in_axes=(0, None))(x, True)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(f, in_axes=(0, None))(x, True)\n# ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: \nTo ensure that your variable is untraced in pmap, you can partially evaluate the function; for example:\npython\nCopy\nfrom functools import partial\n\nvmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\n\npmap(partial(f, condition=True), in_axes=0)(x)\n# Array([0, 1, 2, 3], dtype=int32)\nIn your case, applying this solution might look like this:\npython\nCopy\ndef run(sim_state, lambda_array, timings=timings):\n  return run_trajectory(sim_state, timings, lambda_array)\n\nvmap(run)(sim_state, lambda_array)\n\npmap(vmap(run))(reshaped_sim_state, reshaped_lambda_array)",
            "I can seemingly avoid this problem by passing the timing values prior to vmapping using partial, that is:\npython\nCopy\nrun_trajectory = partial(run_trajectory, starting_time=timings)\n\ntraj_state = pmap(vmap(run_trajectory, in_axes=(0, 0)))(reshaped_sim_state, reshaped_lambda_array)"
        ],
        "link": "https://stackoverflow.com/questions/75625305/nested-vmap-in-pmap-jax"
    },
    {
        "title": "Defining the correct vectorization axes for JAX vmap with arrays of different shapes and sizes",
        "question": "Following the answer to this post, the following function that 'f_switch' that dynamically switches between multiple functions based on an index array is defined (based on 'jax.lax.switch'):\npython\nCopy\nimport jax\nfrom jax import vmap;\nimport jax.random as random\n\ndef g_0(x, y, z, u): return x + y + z + u\ndef g_1(x, y, z, u): return x * y * z * u\ndef g_2(x, y, z, u): return x - y + z - u\ndef g_3(x, y, z, u): return x / y / z / u\ng_i = [g_0, g_1, g_2, g_3]\n\n\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g)(i)\nWith input arrays: i_ar of shape (len_i,), x_ar y_ar and z_ar of shapes (len_xyz,) and u_ar of shape (len_u, len_xyz), out = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar), yields out of shape (len_i, len_xyz, len_u):\npython\nCopy\nlen_i = 50\ni_ar = random.randint(random.PRNGKey(5), shape=(len_i,), minval=0, maxval= len(g_i)) #related to \n\nlen_xyz = 3000\nx_ar = random.uniform(random.PRNGKey(0), shape=(len_xyz,))\ny_ar = random.uniform(random.PRNGKey(1), shape=(len_xyz,))\nz_ar = random.uniform(random.PRNGKey(2), shape=(len_xyz,))\n\nlen_u = 1000\nu_0 = random.uniform(random.PRNGKey(3), shape=(len_u,))\nu_1 = jnp.repeat(u_0, len_xyz)\nu_ar = u_1.reshape(len_u, len_xyz)\n\nout = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nprint('The shape of out is', out.shape)\nThis worked. **But, How can the f_switch function be defined such that the result out of out = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar) has a shape of (j_len, k_len, l_len) when the function is applied along the following axes: i_ar[j], x_ar[j], y_ar[j, k], z_ar[j, k], u_ar[l]? I am not sure about how ** Examples of these input arrays are here:\npython\nCopy\nj_len = 82;\nk_len = 20;\nl_len = 100;\ni_ar = random.randint(random.PRNGKey(0), shape=(j_len,), minval=0, maxval=len(g_i))\nx_ar = random.uniform(random.PRNGKey(1), shape=(j_len,))\ny_ar = random.uniform(random.PRNGKey(2), shape=(j_len,k_len))\nz_ar = random.uniform(random.PRNGKey(3), shape=(j_len,k_len))\nu_ar = random.uniform(random.PRNGKey(4), shape=(l_len,))\nI tried to resolve this (i.e. with given input array to get output of shape: (j_len, k_len, l_len), with a nested vmap:\npython\nCopy\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  g_map = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n  wrapper = lambda x, y, z, u: g_map(i, x, y, z, u)\n  return jax.vmap(wrapper, in_axes=(0, None, None, None, 0))(x, y, z, u)\nand to broadcast u_ar: u_ar_broadcast = jnp.broadcast_to(u_ar, (j_len, k_len, l_len)), and then apply it inside of the original f_switch. But, both of these attempts failed.",
        "answers": [
            "It looks like maybe you want something like this?\npython\nCopy\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  g = jax.vmap(g, (None, None, None, None, 0))\n  g = jax.vmap(g, (None, None, 0, 0, None))\n  g = jax.vmap(g, (0, 0, 0, 0, None))\n  return g(i, x, y, z, u)\n\nout = f_switch(i_ar, x_ar, y_ar, z_ar, u_ar)\nprint(out.shape)\n# (82, 20, 100)\nYou should read the in_axes from bottom to top (because the bottom vmap is the outer one, and is therefore applied to the inputs first). Schematically, you can think of the effect of the maps on the shapes as something like this:\npython\nCopy\n                               (i[82], x[82], y[82,20], z[82,20], u[100])\n(0, 0, 0, 0, None)          -> (i,     x,     y[20],    z[20],    u[100])\n(None, None, 0, 0, None)    -> (i,     x,     y,        z,        u[100])\n(None, None, None, None, 0) -> (i,     x,     y,        z,        u)\nThat said, often it is easier to rely on numpy-style broadcasting rather than on multiple nested vmaps. For example, you could also do something like this:\npython\nCopy\n@jax.jit\ndef f_switch(i, x, y, z, u):\n  g = lambda i, x, y, z, u: jax.lax.switch(i, g_i, x, y, z, u)\n  return jax.vmap(g, in_axes=(0, 0, 0, 0, None))(i, x, y, z, u)\n\nout = f_switch(i_ar, x_ar[:, None, None], y_ar[:, :, None], z_ar[:, :, None], u_ar)\nprint(out.shape)\n# (82, 20, 100)"
        ],
        "link": "https://stackoverflow.com/questions/75465486/defining-the-correct-vectorization-axes-for-jax-vmap-with-arrays-of-different-sh"
    },
    {
        "title": "using jax.vmap to vectorize along with broadcasting",
        "question": "Consider the following toy example:\npython\nCopy\nx = np.arange(3)\n# np.sum(np.sin(x - x[:, np.newaxis]), axis=1)\n\ncfun = lambda x: np.sum(np.sin(x - x[:, np.newaxis]), axis=1)\ncfuns = jax.vmap(cfun)\n\n# for a 2d x:\nx = np.arange(6).reshape(3,2)\ncfuns(x)\nwhere x-x[:,None] is the broadcasting part and give a 3x3 array. I want cfuns to be vectorized over each row of x.\nbash\nCopy\nThe numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int64[2,2])>with<BatchTrace(level=1/0)> with\n  val = Array([[[ 0,  1],\n        [-1,  0]],\n\n       [[ 0,  1],",
        "answers": [
            "JAX transformations like vmap, jit, grad, etc. are not compatible with standard numpy operations. Instead you should use jax.numpy, which provides a similar API built on JAX-compatible operations:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(3)\n\ncfun = lambda x: jnp.sum(jnp.sin(x - x[:, jnp.newaxis]), axis=1)\ncfuns = jax.vmap(cfun)\n\n# for a 2d x:\nx = jnp.arange(6).reshape(3,2)\n\nprint(cfuns(x))\n# [[ 0.84147096 -0.84147096]\n#  [ 0.84147096 -0.84147096]\n#  [ 0.84147096 -0.84147096]]"
        ],
        "link": "https://stackoverflow.com/questions/75386255/using-jax-vmap-to-vectorize-along-with-broadcasting"
    },
    {
        "title": "JAX best way to iterate RNGKeys?",
        "question": "In JAX I find myself needing a PRNGKey that changes on each iteration of a loop. I'm not sure of the best pattern. I've considered\na) split\npython\nCopy\nfor i in range(N):\n  rng, _ = jax.random.split(rng)\n\n  # Alternatively.\n  rng = jax.random.split(rng, 1)[0]\nb) fold_in\npython\nCopy\nfor i in range(N):\n  rng = jax.random.fold_in(rng, i)\nc) use the iterator index? seems bad since the rng doesn't depend on a prior rng.\npython\nCopy\nfor i in range(N):\n rng = jax.random.PRNGKey(i)\nWhich of these is the best pattern and why? I am leaning towards (b) as it maintains dependency on the previous rng key (e.g. passed in as an argument) but im not sure if this is really the intended use-case for jax.random.fold_in",
        "answers": [
            "JAX docs (including the PRNG design doc) recommend something similar to (a):\npython\nCopy\nfor i in range(N):\n  key, subkey = jax.random.split(key)\n  values = random.uniform(subkey, shape)\n  # key carries over to the next iteration\nThe reason this is better than splitting and throwing away the subkey is that it ensures that the streams in each iteration are independent.\nYour option (b) is also safe, and in fact is the pattern that developers had in mind when creating fold_in (see e.g. https://github.com/google/jax/discussions/12395).\nIf you have a fixed number of iterations, it may be better to do all the splits once; for example:\npython\nCopy\nfor i, key in enumerate(random.split(key, N)):\n  values = random.uniform(key, shape)\nOr if your iterations do not have sequential dependence, it's better to use vmap to vectorize the operation:\npython\nCopy\ndef f(key):\n  return random.uniform(key, shape)\n\njax.vmap(f)(random.split(key, N))"
        ],
        "link": "https://stackoverflow.com/questions/75338838/jax-best-way-to-iterate-rngkeys"
    },
    {
        "title": "computational complexity of higher order derivatives with AD in jax",
        "question": "Let f: R -> R be an infinitely differentiable function. What is the computational complexity of calculating the first n derivatives of f in Jax? Naive chain rule would suggest that each multiplication gives a factor of 2 increase, hence the nth derivative would require at least 2^n more operations. I imagine though that clever manipulation of formal series would reduce the number of required calculations and eliminate duplications, esspecially if the derivaives are Jax jitted? Is there a different between the Jax, Tensorflow and Torch implementations?\nhttps://openreview.net/forum?id=SkxEF3FNPH discusses this topic, but doesn t provide a computational complexity.",
        "answers": [
            "What is the computational complexity of calculating the first n derivatives of f in Jax?\nThere's not much you can say in general about computational complexity of Nth derivatives. For example, with a function like jnp.sin, the Nth derivative is O[1], oscillating between negative and positive sin and cos calls as N grows. For an order-k polynomial, the Nth derivative is O[0] for N > k. Other functions may have complexity that is linear or polynomial or even exponential with N depending on the operations they contain.\nI imagine though that clever manipulation of formal series would reduce the number of required calculations and eliminate duplications, esspecially if the derivaives are Jax jitted\nYou imagine correctly! One implementation of this idea is the jax.experimental.jet module, which is an experimental transform designed for computing higher-order derivatives efficiently and accurately. It doesn't cover all JAX functions, but it may be complete enough to do what you have in mind.",
            "If L is the complexity of evaluating the scalar function f, then L*(n+1)^2 is an upper bound for the complexity of finding the first n derivatives of f as coefficients of a truncated Taylor series.\nThe general idea is that each elementary function can be implemented for truncated Taylor series in the equivalent of one or two truncated series multiplications."
        ],
        "link": "https://stackoverflow.com/questions/75324482/computational-complexity-of-higher-order-derivatives-with-ad-in-jax"
    },
    {
        "title": "Why is this JAX jitted function so much slower than the non-jitted JAX version?",
        "question": "I'm in the process of rewriting some code from pure Python to JAX. I have a function that I need to call a lot. Why is the jitted version of the following function so much slower than the non-jitted version?\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef regular(M,R,a):\n    return (3+a)*M*R**a / (4*jnp.pi * R**(3+a))\n\n@jit\ndef jitted(M,R,a):\n    return (3+a)*M*R**a / (4*jnp.pi * R**(3+a))\n\n%timeit regular(1e10,100.,-2.)\n# 346 ns ± 2.07 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n%timeit jitted(1e10,100.,-2.)\n# 4.2 µs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)",
        "answers": [
            "There is some helpful information on benchmarking in JAX's FAQ: Benchmarking JAX Code: in particular note the discussion of JAX dispatch overhead for individual operations.\nRegarding your particular example, the first thing to point out is that you're not comparing jit-compiled JAX code against non-jit-compiled JAX code; you're comparing jit-compiled JAX code against pure Python code. Because you're passing python scalars to the function, none of the operations in regular have anything to do with JAX (even jnp.pi is just a Python float), so you're just executing built-in Python arithmetic operators on Python scalars.\nIf you want to compare JAX jit to non-jit code, you can use JAX values rather than scalar values as inputs; for example:\npython\nCopy\na = jnp.array(1e10)\nb = jnp.array(100.)\nc = jnp.array(-2.)\n\n%timeit regular(a, b, c).block_until_ready()\n# 86.8 µs ± 1.56 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n%timeit jitted(a, b, c).block_until_ready()\n# 3.71 µs ± 59.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\nHere you see that JIT gives you about a 20x speedup over un-jitted JAX code.\nBut JIT or not, why is JAX so much slower than the native Python version? The reason is because each JAX function call incurs a few microseconds of dispatch overhead, while each native Python operation has much less dispatch overhead. You've written a function where the actual computations are so small they are virtually free; to first order all you are measuring is dispatch overhead.\nIn situations JAX was designed for (executing JIT-compiled sequences of operations over large arrays on accelerators), this one-time, few-microsecond dispatch cost is generally not significant in comparison to the full computation."
        ],
        "link": "https://stackoverflow.com/questions/75318282/why-is-this-jax-jitted-function-so-much-slower-than-the-non-jitted-jax-version"
    },
    {
        "title": "jax automatic differentiation",
        "question": "I have the following three functions implements in JAX.\npython\nCopy\ndef helper_1(params1):\n   ...calculations...\n   return z\n\ndef helper_2(z, params2):\n   ...calculations...\n   return y\n\ndef main(params1, params2):\n   z = helper_1(params1)\n   y = helper_2(z, params2)\n   return z,y\nI am interested in the partial derivatives of the output from main, i.e. z and y, with respect to both params1 and params2. As params1 and params2 are low dimensional and z and y are high dimensional, I am using the jax.jacfwd function.\nWhen calling\npython\nCopy\njax.jacfwd(main,argnums=(0,1))(params1,params2)\nJax computes the derivatives of z with respect to params1 (and params2, which in this case is just a bunch of zeros). My question is: does Jax recompute dz/d_param1 for the derivatives of y with respect to params1 and params2, or does it somehow figure out this has already been computed?\nI don't know if this is relevant, but the 'helper_1' function contains functions from the TensorFlow library for Jax. Thanks!",
        "answers": [
            "In general, in the situation you describe JAX's forward-mode autodiff approach will re-use the derivative of z when computing the derivative of y. If you wish, you can confirm this by looking at the jaxpr of your differentiated function:\npython\nCopy\nprint(jax.make_jaxpr(jax.jacfwd(main, (0, 1)))(params1, params2))\nThough if your function is more than moderately complicated, the output might be hard to understand.\nAs a general note, though, JAX's autodiff implementation does tend to produce a small number of unnecessary or duplicated computations. As a simple example consider this:\npython\nCopy\nimport jax\nprint(jax.make_jaxpr(jax.grad(jax.lax.sin))(1.0))\n# { lambda ; a:f32[]. let\n#     _:f32[] = sin a\n#     b:f32[] = cos a\n#     c:f32[] = mul 1.0 b\n#   in (c,) }\nHere the primal value sin(a) is computed even though it is never used in computing the final output.\nIn practice this can be addressed by wrapping your computation in jit, in which case the XLA compiler takes care of optimization, including dead code elimination and de-duplication when applicable:\npython\nCopy\nresult = jit(jax.jacfwd(main, (0, 1)))(params1, params2)"
        ],
        "link": "https://stackoverflow.com/questions/75144091/jax-automatic-differentiation"
    },
    {
        "title": "Given batch of samples find x,y positions - python, jax",
        "question": "I would like to find the positions of where the is a '1' in the following batch of sampled array with dimension [batch, 4,4] = [2,4,4].\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\na = jnp.array([[[0., 0., 0., 1.],\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.],\n                [0., 0., 1., 1.]],\n             \n               [[1., 0., 1., 0.],\n                [1., 0., 0., 0.],\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.]]])\nI tried going through the dimension of batches (with vmap) and use the jax function to find the coordinates with\npython\nCopy\nb = jax.vmap(jnp.where)(a)\nprint('b', b)\nbut I get an error that I don't know how to fix:\npython\nCopy\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThis Tracer was created on line /home/imi/Desktop/Backflow/backflow/src/debug.py:17 (<module>)\nI expect the following output:\npython\nCopy\nb = [[[0,3], [2,1],[2,3],[3,2],[3,3]],\n\n     [[0,0],[0,2],[1,0],[3,1],[3,3]]\nThe first line of [x,y] coordinates correspond to the positions of where there is a '1' in the first batch, and for the second line in the second batch.",
        "answers": [
            "JAX transformations like vmap require arrays to be statically-sized, so there is no way to do exactly the computation you have in mind (because the number of 1 entries, and thus the size of the output array, is data-dependent).\nBut if you know a priori that there are five entries per batch, you can do something like this:\npython\nCopy\nfrom functools import partial\nindices = jax.vmap(partial(jnp.where, size=5))(a)\nprint(jnp.stack(indices, axis=2))\npython\nCopy\n[[[0 3]\n  [2 1]\n  [2 3]\n  [3 2]\n  [3 3]]\n\n [[0 0]\n  [0 2]\n  [1 0]\n  [3 1]\n  [3 3]]]\nIf you don't know a priori how many 1 entries there are, then you have a few options: one is to avoid JAX transformations and call an un-transformed jnp.where on each batch:\npython\nCopy\nresult = [jnp.column_stack(jnp.where(b)) for b in a]\nprint(result)\npython\nCopy\n[DeviceArray([[0, 3],\n             [2, 1],\n             [2, 3],\n             [3, 2],\n             [3, 3]], dtype=int32), DeviceArray([[0, 0],\n             [0, 2],\n             [1, 0],\n             [3, 1],\n             [3, 3]], dtype=int32)]\nNote that for this case, it's not possible in general to store the results in a single array, because there may be different numbers of 1 entries in each batch, and JAX does not support ragged arrays.\nThe other option is to set the size to some maximum value, and output padded results:\npython\nCopy\nmax_size = a[0].size  # size of slice is the upper bound\nfill_value = a[0].shape  # fill with out-of-bound indices\nindices = jax.vmap(partial(jnp.where, size=max_size, fill_value=fill_value))(a)\nprint(jnp.stack(indices, axis=2))\npython\nCopy\n[[[0 3]\n  [2 1]\n  [2 3]\n  [3 2]\n  [3 3]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]]\n\n [[0 0]\n  [0 2]\n  [1 0]\n  [3 1]\n  [3 3]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]\n  [4 4]]]\nWith padded results, you could then write the remainder of your code to anticipate these padded values."
        ],
        "link": "https://stackoverflow.com/questions/75134947/given-batch-of-samples-find-x-y-positions-python-jax"
    },
    {
        "title": "Negative Sampling in JAX",
        "question": "I'm implementing a negative sampling algorithm in JAX. The idea is to sample negatives from a range excluding from this range a number of non-acceptable outputs. My current solution is close to the following:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nmax_range = 5\nn_samples = 2\ntrue_cases = jnp.array(\n    [\n        [1,2],\n        [1,4],\n        [0,5]\n    ]\n)\n# i combine the true cases in a dictionary of the following form:\nnon_acceptable_as_negatives = {\n    0: jnp.array([5]),\n    1: jnp.array([2,4]),\n    2: jnp.array([]),\n    3: jnp.array([]),\n    4: jnp.array([]),\n    5: jnp.array([])\n}\nnegatives = []\nkey = jax.random.PRNGKey(42)\nfor i in true_cases[:,0]:\n    key,use_key  = jax.random.split(key,2)\n    p = jnp.ones((max_range+1,))\n    p = p.at[non_acceptable_as_negatives[int(i)]].set(0)\n    p = p / p.sum()\n    negatives.append(\n        jax.random.choice(use_key,\n            jnp.arange(max_range+1),\n            (1, n_samples),\n            replace=False,\n            p=p,\n            )\n    )\nHowever this seems\nrather complicated and\nis not very performant as the true cases in the original contain ~200_000 entries and max range is ~ 50_000.\nHow can I improve this solution? And is there a more JAX way to store arrays of varying size which I currently store in the non_acceptable_as_negatives dict?",
        "answers": [
            "You'll generally achieve better performance in JAX (as in NumPy) if you can avoid loops and use vectorized operations instead. If I'm understanding your function correctly, I think the following does roughly the same thing, but using vmap.\nSince JAX does not support dictionary lookups based on traced values, I replaced your dict with a padded array\npython\nCopy\nimport jax.numpy as jnp\nimport jax\nmax_range = 5\nn_samples = 2\nfill_value = max_range + 1\n\ntrue_cases = jnp.array([\n  [1,2],\n  [1,4],\n  [0,5]\n])\n\nnon_acceptable_as_negatives = jnp.array([\n    [5, fill_value],\n    [2, 4],\n])\n\n@jax.vmap\ndef func(key, true_case):\n  p = jnp.ones(max_range + 1)\n  idx = true_cases[0]\n  replace = non_acceptable_as_negatives.at[idx].get(fill_value=fill_value)\n  p = p.at[replace].set(0, mode='drop')\n  return jax.random.choice(key, max_range + 1, (n_samples,), replace=False, p=p)\n\n\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, len(true_cases))\nresult = func(keys, true_cases)\nprint(result)\npython\nCopy\n[[3 1]\n [5 1]\n [1 5]]",
            "Jax array are immutable. It means that you can't edit it without copying the entire array. Here the main problem is that you create the vector p two times at each iteration. I advice you to compute the probabilities only once via numpy:\npython\nCopy\nimport numpy as np\n\nnon_acceptable_as_negatives = {\n    0: np.array([5]),\n    1: np.array([2,4]),\n    2: np.array([]),\n    3: np.array([]),\n    4: np.array([]),\n    5: np.array([])\n}\n\nprobas = np.ones((max_range+1, max_range+1))\nfor k, idx in non_acceptable_as_negatives.items():\n    for i in idx:\n        probas[k, i] = 0\nprobas = probas / probas.sum(axis=1, keepdims=True)\nprobas = jnp.array(probas)\nThen, to further speed-up the algorithm, you can compile the choice function. You can try:\npython\nCopy\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=1)\ndef sample(key, max_range, probas):\n    key, use_key  = jax.random.split(key, 2)\n    return jax.random.choice(use_key,\n            jnp.arange(max_range+1),\n            (1, n_samples),\n            replace=False,\n            p=probas[i],\n            ), key\nAnd finally:\npython\nCopy\nfor i in true_cases[:,0]:\n    neg, key = aux(key, max_range, probas)\n    negatives.append(neg)"
        ],
        "link": "https://stackoverflow.com/questions/75081519/negative-sampling-in-jax"
    },
    {
        "title": "Is there a way to return tuple of mixed variables in Jax helper function?",
        "question": "On my path learning Jax, I tried to achieve something like\ndef f(x):\n    return [x + 1, [1,2,3], \"Hello\"]\n\nx = 1\nnew_x, a_list, str = jnp.where(\n    x > 0,\n    test(x),\n    test(x + 1)\n)\nWell, Jax clearly does not support this. I tried searching online and went through quite a few docs, but I couldn't find a good answer.\nAny help on how can I achieve this in Jax?",
        "answers": [
            "In general, JAX functions like jnp.where only accept array arguments, not list or string arguments. Since you're using a function that is not compatible with JAX in the first place, it might be better to just avoid JAX conditionals and just use standard Python conditionals instead:\npython\nCopy\nimport jax.numpy as jnp\n\ndef f(x):\n    return [x + 1, [1,2,3], \"Hello\"]\n\nx = 1\n\nnew_x, a_list, str_ = f(x) if x > 0 else f(x + 1)"
        ],
        "link": "https://stackoverflow.com/questions/75065495/is-there-a-way-to-return-tuple-of-mixed-variables-in-jax-helper-function"
    },
    {
        "title": "jax.lax.select vs jax.numpy.where",
        "question": "Was taking a look at the dropout implementation in flax:\npython\nCopy\ndef __call__(self, inputs, deterministic: Optional[bool] = None):\n    \"\"\"Applies a random dropout mask to the input.\n\n    Args:\n      inputs: the inputs that should be randomly masked.\n      deterministic: if false the inputs are scaled by `1 / (1 - rate)` and\n        masked, whereas if true, no mask is applied and the inputs are returned\n        as is.\n\n    Returns:\n      The masked inputs reweighted to preserve mean.\n    \"\"\"\n    deterministic = merge_param(\n        'deterministic', self.deterministic, deterministic)\n\n    if (self.rate == 0.) or deterministic:\n      return inputs\n\n    # Prevent gradient NaNs in 1.0 edge-case.\n    if self.rate == 1.0:\n      return jnp.zeros_like(inputs)\n\n    keep_prob = 1. - self.rate\n    rng = self.make_rng(self.rng_collection)\n    broadcast_shape = list(inputs.shape)\n    for dim in self.broadcast_dims:\n      broadcast_shape[dim] = 1\n    mask = random.bernoulli(rng, p=keep_prob, shape=broadcast_shape)\n    mask = jnp.broadcast_to(mask, inputs.shape)\n    return lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs))\nParticularly, I'm interested in last line lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs)). Wondering why lax.select is used here instead of:\npython\nCopy\nreturn jnp.where(mask, inputs / keep_prob, 0)\nor even more simply:\npython\nCopy\nreturn mask * inputs / keep_prob",
        "answers": [
            "jnp.where is basically the same as lax.select, except more flexible in its inputs: for example, it will broadcast inputs to the same shape or cast to the same dtype, whereas lax.select requires more strict matching of inputs:\npython\nCopy\n>>> import jax.numpy as jnp\n>>> from jax import lax\n>>> x = jnp.arange(3)\npython\nCopy\n# Implicit broadcasting\n>>> jnp.where(x < 2, x[:, None], 0)\nDeviceArray([[0, 0, 0],\n             [1, 1, 0],\n             [2, 2, 0]], dtype=int32)\n\n>>> lax.select(x < 2, x[:, None], 0)\nTypeError: select cases must have the same shapes, got [(), (3, 1)].\npython\nCopy\n# Implicit type promotion\n>>> jnp.where(x < 2, jnp.zeros(3), jnp.arange(3))\nDeviceArray([0., 0., 2.], dtype=float32)\n\n>>> lax.select(x < 2, jnp.zeros(3), jnp.arange(3))\nTypeError: lax.select requires arguments to have the same dtypes, got int32, float32. (Tip: jnp.where is a similar function that does automatic type promotion on inputs).\nLibrary code is one place where the stricter semantics can be useful, because rather than smoothing-over potential implementation bugs and returning an unexpected output, it will complain loudly. But performance-wise (especially once JIT-compiled) the two are essentially equivalent.\nAs for why the flax developers chose lax.select vs. multiplying by a mask, I can think of two reasons:\nMultiplying by a mask is subject to implicit type promotion semantics, and it takes a lot more thought to anticipate problematic outputs than a simple select, which is specifically-designed for the intended operation.\nUsing multiplication causes the compiler to treat this operation as a multiplication, which it is not. A select is a much more narrow and precise operation than a multiplication, and by specifying operations precisely it often allows the compiler to optimize the results to a greater extent."
        ],
        "link": "https://stackoverflow.com/questions/74972850/jax-lax-select-vs-jax-numpy-where"
    },
    {
        "title": "JAX: JIT compatible sparse matrix slicing",
        "question": "I have a boolean sparse matrix that I represent with row indices and column indices of True values.\npython\nCopy\nimport numpy as np\nimport jax\nfrom jax import numpy as jnp\nN = 10000\nM = 1000\nX = np.random.randint(0, 100, size=(N, M)) == 0  # data setup\nrows, cols = np.where(X == True)\nrows = jax.device_put(rows)\ncols = jax.device_put(cols)\nI want to get a column slice of the matrix like X[:, 3], but just from rows indices and column indices.\nI managed to do that by using jnp.isin like below, but the problem is that this is not JIT compatible because of the data-dependent shaped array rows[cols == m].\npython\nCopy\ndef not_jit_compatible_slice(rows, cols, m):\n  return jnp.isin(jnp.arange(N), rows[cols == m])\nI could make it JIT compatible by using jnp.where in the three-argument form, but this operation is much slower than the previous one.\npython\nCopy\ndef jit_compatible_but_slow_slice(rows, cols, m):\n  return jnp.isin(jnp.arange(N), jnp.where(cols == m, rows, -1))\nIs there any fast and JIT compatible solution to acheive the same output?",
        "answers": [
            "You can do a bit better than the first answer by using the mode argument of set() to drop out-of-bound indices, eliminating the final slice:\npython\nCopy\nout = jnp.zeros(N, bool).at[jnp.where(cols==3, rows, N)].set(True, mode='drop')",
            "I figured out that the implementation below returns the same output much faster, and it’s JIT compatible.\npython\nCopy\ndef slice(rows, cols, m):\n  res = jnp.zeros(N + 1, dtype=bool)\n  res = res.at[jnp.where(cols == m, rows, -1)].set(True)\n  return res[:-1]"
        ],
        "link": "https://stackoverflow.com/questions/74946632/jax-jit-compatible-sparse-matrix-slicing"
    },
    {
        "title": "Why does jax.grad(lambda v: jnp.linalg.norm(v-v))(jnp.ones(2)) produce nans?",
        "question": "Can someone explain the following behaviour? Is it a bug?\npython\nCopy\nfrom jax import grad\nimport jax.numpy as jnp\n\nx = jnp.ones(2)\ngrad(lambda v: jnp.linalg.norm(v-v))(x) # returns DeviceArray([nan, nan], dtype=float32)\n\ngrad(lambda v: jnp.linalg.norm(0))(x) # returns DeviceArray([0., 0.], dtype=float32)\nI've tried looking up the error online but didn't find anything relevant.\nI also skimmed through https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html",
        "answers": [
            "When you compute grad(lambda v: jnp.linalg.norm(v-v))(x), your function looks roughly like this:\npython\nCopy\nf(x) = sqrt[(x - x)^2]\nso, evaluating with the chain rule, the derivative is\npython\nCopy\ndf/dx = (x - x) / sqrt[(x - x)^2]\nwhich, when you plug-in any finite x evaluates to\npython\nCopy\n0 / sqrt(0)\nwhich is undefined, and represented by NaN in floating point arithmetic.\nWhen you compute grad(lambda v: jnp.linalg.norm(0))(x), your function looks roughly like this:\npython\nCopy\ng(x) = sqrt[0.0^2]\nand because it has no dependence on x the derivative is simply\npython\nCopy\ndg/dx = 0.0\nDoes that answer your question?"
        ],
        "link": "https://stackoverflow.com/questions/74864427/why-does-jax-gradlambda-v-jnp-linalg-normv-vjnp-ones2-produce-nans"
    },
    {
        "title": "Is there a way to update multiple indexes of Jax array at once?",
        "question": "Since array is immutable in Jax, so when one updates N indexes, it creates N arrays with\nx = x.at[idx].set(y)\nWith hundreds of updates per training cycle, it will ultimately create hundreds of arrays if not millions. This seems a little wasteful, is there a way to update multiple index at one go? Does anyone know if there is overhead or if it's significant? Am I overlook at this?",
        "answers": [
            "You can perform multiple updates in a single operation using the syntax you mention. For example:\npython\nCopy\nimport jax.numpy as jnp\n\nx = jnp.zeros(10)\nidx = jnp.array([3, 5, 7, 9])\ny = jnp.array([1, 2, 3, 4])\n\nx = x.at[idx].set(y)\nprint(x)\n# [0. 0. 0. 1. 0. 2. 0. 3. 0. 4.]\nYou're correct that outside JIT, each update operation will create an array copy. But within JIT-compiled functions, the compiler is able to perform such updates in-place when it is possible (for example, when the original array is not referenced again). You can read more at JAX Sharp Bits: Array Updates.",
            "This sounds very like a job for scatter update. I'm not really familiar with Jax itself, but major frameworks have it:\nhttps://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html\nWhat it does in a nutshell:\nsetup your output tensor (x)\naccumulate required updates in the other tensor (y in your case)\naccumulate in list/tensor indices where to apply you updates (create tensor/list full of index)\nfeed 1)-3) to scatter_updated"
        ],
        "link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once"
    },
    {
        "title": "How to improve Julia's performance using just in time compilation (JIT)",
        "question": "I have been playing with JAX (automatic differentiation library in Python) and Zygote (the automatic differentiation library in Julia) to implement Gauss-Newton minimisation method. I came upon the @jit macro in Jax that runs my Python code in around 0.6 seconds compared to ~60 seconds for the version that does not use @jit. Julia ran the code in around 40 seconds. Is there an equivalent of @jit in Julia or Zygote that results is a better performance?\nHere are the codes I used:\nPython\ncss\nCopy\nfrom jax import grad, jit, jacfwd\nimport jax.numpy as jnp\nimport numpy as np\nimport time\n\ndef gaussian(x, params):\n    amp = params[0]\n    mu  = params[1]\n    sigma = params[2]\n    amplitude = amp/(jnp.abs(sigma)*jnp.sqrt(2*np.pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*jnp.exp(-0.5*(arg**2))\n\ndef myjacobian(x, params):\n    return jacfwd(gaussian, argnums = 1)(x, params)\n\ndef op(jac):\n    return jnp.matmul(\n        jnp.linalg.inv(jnp.matmul(jnp.transpose(jac),jac)),\n        jnp.transpose(jac))\n                         \ndef res(x, data, params):\n    return data - gaussian(x, params)\n@jit\ndef step(x, data, params):\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    temp = jnp.matmul(jacobian_operation, residuals)\n    return params + temp\n\nN = 2000\nx = np.linspace(start = -100, stop = 100, num= N)\ndata = gaussian(x, [5.65, 25.5, 37.23])\n\nini = jnp.array([0.9, 5., 5.0])\nt1 = time.time()\nfor i in range(5000):\n    ini = step(x, data, ini)\nt2 = time.time()\nprint('t2-t1: ', t2-t1)\nini\nJulia\njulia\nCopy\nusing Zygote\n\nfunction gaussian(x::Union{Vector{Float64}, Float64}, params::Vector{Float64})\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]\n    \n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x.-mu)./sigma)\n    return amplitude.*exp.(-0.5.*(arg.^2))\n    \nend\n\nfunction myjacobian(x::Vector{Float64}, params::Vector{Float64})\n    output = zeros(length(x), length(params))\n    for (index, ele) in enumerate(x)\n        output[index,:] = collect(gradient((params)->gaussian(ele, params), params))[1]\n    end\n    return output\nend\n\nfunction op(jac::Matrix{Float64})\n    return inv(jac'*jac)*jac'\nend\n\nfunction res(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    return data - gaussian(x, params)\nend\n\nfunction step(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    \n    temp = jacobian_operation*residuals\n    return params + temp\nend\n\nN = 2000\nx = collect(range(start = -100, stop = 100, length= N))\nparams = vec([5.65, 25.5, 37.23])\ndata = gaussian(x, params)\n\nini = vec([0.9, 5., 5.0])\n@time for i in range(start = 1, step = 1, length = 5000)\n    ini = step(x, data, ini)\nend\nini",
        "answers": [
            "Your Julia code doing a number of things that aren't idiomatic and are worsening your performance. This won't be a full overview, but it should give you a good idea to start.\nThe first thing is passing params as a Vector is a bad idea. This means it will have to be heap allocated, and the compiler doesn't know how long it is. Instead, use a Tuple which will allow for a lot more optimization. Secondly, don't make gaussian act on a Vector of xs. Instead, write the scalar version and broadcast it. Specifically, with these changes, you will have\nlua\nCopy\nfunction gaussian(x::Number, params::NTuple{3, Float64})\n    amp, mu, sigma = params\n    \n    # The next 2 lines should probably be done outside this function, but I'll leave them here for now.\n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*exp(-0.5*(arg^2))\nend",
            "One straightforward way to speed this up is to use ForwardDiff not Zygote, since you are taking a gradient of a vector of length 3, many times. Here this gets me from 16 to 3.5 seconds, with the last factor of 2 involving Chunk(3) to improve type-stability. Perhaps this can be improved further.\ncss\nCopy\nfunction myjacobian(x::Vector, params)\n    # return rand(eltype(x), length(x), length(params))  # with no gradient, takes 0.5s\n    output = zeros(eltype(x), length(x), length(params))\n    config = ForwardDiff.GradientConfig(nothing, params, ForwardDiff.Chunk(3))\n    for (i, xi) in enumerate(x)\n        # grad = gradient(p->gaussian(xi, p), params)[1]       # original, takes 16s\n        # grad = ForwardDiff.gradient(p-> gaussian(xi, p))     # ForwardDiff, takes 7s\n        grad = ForwardDiff.gradient(p-> gaussian(xi, p), params, config)  # takes 3.5s\n        copyto!(view(output,i,:), grad)  # this allows params::Tuple, OK for Zygote, no help\n    end\n    return output\nend\n# This needs gaussian.(x, Ref(params)) elsewhere to use on many x, same params\nfunction gaussian(x::Real, params)\n    # amp, mu, sigma = params  # with params::Vector this is slower, 19 sec\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]  # like this, 16 sec\n    T = typeof(x)  # avoids having (2*pi)::Float64 promote everything\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nHowever, this is still computing many small gradient arrays in a loop. It could easily compute one big gradient array instead.\nWhile in general Julia is happy to compile loops to something fast, loops that make individual arrays tend to be a bad idea. And this is especially true for Zygote, which is fastest on matlab-ish whole-array code.\nHere's how this looks, it gets me under 1s for the whole program:\nphp\nCopy\nfunction gaussian(x::Real, amp::Real, mu::Real, sigma::Real)\n    T = typeof(x)\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nfunction myjacobian2(x::Vector, params)  # with this, 0.9s\n    amp = fill(params[1], length(x))\n    mu  = fill(params[2], length(x))\n    sigma = fill(params[3], length(x))  # use same sigma & different x value at each row:\n    grads = gradient((amp, mu, sigma) -> sum(gaussian.(x, amp, mu, sigma)), amp, mu, sigma)\n    hcat(grads...)\nend\n# Check that it agrees:\nmyjacobian2(x, params) ≈ myjacobian(x, params)\nWhile this has little effect on the speed, I think you probably also want op(jac::Matrix) = Hermitian(jac'*jac) \\ jac' rather than inv."
        ],
        "link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit"
    },
    {
        "title": "jax.lax.fori_loop Abstract tracer value encountered where concrete value is expected",
        "question": "I've a JAX loop that looks like this where inside the step function I use min between the two arguments\npython\nCopy\nimport jax\n\ndef step(timestep: int, order: int = 4) -> int:\n    order = min(timestep + 1, order)\n    return order\n\nnum_steps = 10\norder = 100\norder = jax.lax.fori_loop(0, num_steps, step, order)\nThe above code fails with a jax._src.errors.ConcretizationTypeError. This is is the full stacktrace:\npython\nCopy\nWARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n---------------------------------------------------------------------------\nUnfilteredStackTrace                      Traceback (most recent call last)\n<ipython-input-4-9ec280f437cb> in <module>\n      2 order = 100\n----> 3 order = jax.lax.fori_loop(0, num_steps, step, order)\n\n16 frames\n/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)\n    161     try:\n--> 162       return fun(*args, **kwargs)\n    163     except Exception as e:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in fori_loop(lower, upper, body_fun, init_val)\n   1691 \n-> 1692     (_, result), _ = scan(_fori_scan_body_fun(body_fun), (lower_, init_val),\n   1693                           None, length=upper_ - lower_)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)\n    161     try:\n--> 162       return fun(*args, **kwargs)\n    163     except Exception as e:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in scan(f, init, xs, length, reverse, unroll)\n    258   # necessary, a second time with modified init values.\n--> 259   init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n    260   new_init_flat, changed = _promote_weak_typed_inputs(init_flat, carry_avals, carry_avals_out)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in _create_jaxpr(init)\n    244     carry_avals = tuple(_map(_abstractify, init_flat))\n--> 245     jaxpr, consts, out_tree = _initial_style_jaxpr(\n    246         f, in_tree, (*carry_avals, *x_avals), \"scan\")\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/common.py in _initial_style_jaxpr(fun, in_tree, in_avals, primitive_name)\n     59                          primitive_name: Optional[str] = None):\n---> 60   jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n     61       fun, in_tree, in_avals, primitive_name)\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/common.py in _initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name)\n     53   debug = pe.debug_info(fun, in_tree, False, primitive_name or \"<unknown>\")\n---> 54   jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n     55   return jaxpr, consts, out_tree()\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/profiler.py in wrapper(*args, **kwargs)\n    313     with TraceAnnotation(name, **decorator_kwargs):\n--> 314       return func(*args, **kwargs)\n    315     return wrapper\n\n/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)\n   1980     main.jaxpr_stack = ()  # type: ignore\n-> 1981     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n   1982       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n\n/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)\n   1997     in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n-> 1998     ans = fun.call_wrapped(*in_tracers_)\n   1999     out_tracers = map(trace.full_raise, ans)\n\n/usr/local/lib/python3.8/dist-packages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)\n    166     try:\n--> 167       ans = self.f(*args, **dict(self.params, **kwargs))\n    168     except:\n\n/usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py in scanned_fun(loop_carry, _)\n   1607     i, x = loop_carry\n-> 1608     return (i + 1, body_fun()(i, x)), None\n   1609   return scanned_fun\n\n<ipython-input-2-2e3345899235> in step(timestep, order)\n      1 def step(timestep: int, order: int = 100) -> int:\n----> 2     order = min(timestep + 1, order)\n      3     return order\n\n/usr/local/lib/python3.8/dist-packages/jax/core.py in __bool__(self)\n    633   def __nonzero__(self): return self.aval._nonzero(self)\n--> 634   def __bool__(self): return self.aval._bool(self)\n    635   def __int__(self): return self.aval._int(self)\n\n/usr/local/lib/python3.8/dist-packages/jax/core.py in error(self, arg)\n   1266   def error(self, arg):\n-> 1267     raise ConcretizationTypeError(arg, fname_context)\n   1268   return error\n\nUnfilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 for scan. This concrete value was not available in Python because it depends on the values of the argument 'loop_carry'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nConcretizationTypeError                   Traceback (most recent call last)\n<ipython-input-4-9ec280f437cb> in <module>\n      1 num_steps = 10\n      2 order = 100\n----> 3 order = jax.lax.fori_loop(0, num_steps, step, order)\n\n<ipython-input-2-2e3345899235> in step(timestep, order)\n      1 def step(timestep: int, order: int = 100) -> int:\n----> 2     order = min(timestep + 1, order)\n      3     return order\n\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function scanned_fun at /usr/local/lib/python3.8/dist-packages/jax/_src/lax/control_flow/loops.py:1606 for scan. This concrete value was not available in Python because it depends on the values of the argument 'loop_carry'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nEverything works fine if instead of using jax.lax.fori_loop i use a simple python loop, but my original code will end up very slow. How can I fix this issue?",
        "answers": [
            "Use jax.numpy.minimum in place of min:\npython\nCopy\ndef step(timestep: int, order: int = 4) -> int:\n    order = jax.numpy.minimum(timestep + 1, order)\n    return order\nThe reason min does not work is that in the course of executing code within jit, grad, vmap, fori_loop, etc., JAX replaces concrete values with abstract tracers, and Python functions like min don't know how to handle these abstract values. See How to Think in JAX for more background on this."
        ],
        "link": "https://stackoverflow.com/questions/74640431/jax-lax-fori-loop-abstract-tracer-value-encountered-where-concrete-value-is-expe"
    },
    {
        "title": "`jax.jit` not improving in place update performance for large arrays?",
        "question": "I am trying to apply a number of in place updates to a 2D matrix.\nIt appears that using jit to the in place update does not have any effect in computation time (which is many orders of magnitude longer than the equivalent numpy implementation).\nHere is code that demonstrates my problem and research.\npython\nCopy\nnode_count = 10000\n\n# NUMPY IMPLEMENTATION\nb = onp.zeros([node_count,node_count])\nprint(\"`numpy` in place update.\")\n%timeit b[1,1] = 1.\n# 86.9 ns ± 1.42 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\n# JAX IN PLACE IMPLEMENTATION\na = np.zeros([node_count,node_count])\nprint(\"`jax.np` in place update.\")\n%timeit a.at[1,1].set(1.)\n# 112 ms ± 14.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n## TEST JIT IMPLEMENTATION\ndef update(mat, index, val):\n    return mat.at[tuple(index)].set(val)\nupdate_jit = jit(update)\n\n# Run once for trace.\nupdate_jit(a, [1,1], 1.).block_until_ready()\n\nprint(\"`jax.np` jit in place update.\")\n%timeit update_jit(a, [1,1],1.).block_until_ready()\n# 99.6 ms ± 358 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)",
        "answers": [
            "This has nothing to do with inlining of inplace updates. This has to do with the fact that, unless otherwise requested, a JIT-compiled function will always return its result in a new, distinct buffer. The only exception to this is if you use buffer donation to explicitly mark that the input buffer can be re-used in the output:\npython\nCopy\nupdate_jit = jit(update, donate_argnums=[0])\nNote, however, that buffer donation is currently only available on GPU and TPU runtimes.\nYou'll not be able to use %timeit in this case, because the donated input buffer is no longer available for use after the first iteration, but you can confirm via %time that this improves the computation speed:\npython\nCopy\n# Following is run on a Colab T4 GPU runtime\n\nupdate_jit = jit(update)\n_ = update_jit(b, [1,1], 1.)\n%time _ = update_jit(b, [1,1], 1.).block_until_ready()\n# CPU times: user 607 µs, sys: 112 µs, total: 719 µs\n# Wall time: 5.89 ms\n\nupdate_jit_donate = jit(update, donate_argnums=[0])\nb = update_jit_donate(b, [1,1], 1.)\n%time _ = update_jit_donate(b, [1,1], 1.).block_until_ready()\n# CPU times: user 467 µs, sys: 86 µs, total: 553 µs\n# Wall time: 332 µs\nThe buffer donation version is still quite a bit slower than the NumPy version, but this is expected for the reasons discussed at FAQ: Is JAX Faster Than Numpy?.\nI suspect you're performing these micro-benchmarks to assure yourself that the compiler performs updates in-place within a JIT-compiled sequence of operations rather than making internal copies, as is mentioned in Sharp Bits: Array Updates. If so, you can confirm this by other means; for example:\npython\nCopy\n@jit\ndef sum(x):\n  return x.sum()\n\n@jit\ndef update_and_sum(x):\n  return x.at[0, 0].set(1).sum()\n\n_ = sum(b)\n%timeit sum(b).block_until_ready()\n# 1.66 ms ± 7.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n_ = update_and_sum(b)\n%timeit update_and_sum(b).block_until_ready()\n# 1.66 ms ± 20.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\nThe identical timings here show that the update operation is being performed in-place rather than causing the input buffer to be copied.",
            "The JIT version is slower because it does not operate in-place as opposed to the Numpy version: it creates a copy of the array and then modify the items. You can see that by growing the array: the execution time is proportional to the size of the array. You can also check the array a is left unmodified after the call to set. You can also see that the time of b.fill(42.0) is very close to the speed of the JAX function (but strangely not b.copy()). The out-of-place version is significantly slower because the RAM is slow and it takes a lot of time to operate on the whole array than just setting 1 value."
        ],
        "link": "https://stackoverflow.com/questions/74587875/jax-jit-not-improving-in-place-update-performance-for-large-arrays"
    },
    {
        "title": "Execute function specifically on CPU in Jax",
        "question": "I have a function that will instantiate a huge array and do other things. I am running my code on TPUs so my memory is limited.\nHow can I execute my function specifically on the CPU?\nIf I do:\npython\nCopy\ny = jax.device_put(my_function(), device=jax.devices(\"cpu\")[0])\nI guess that my_function() is first executed on TPU and the result is put on CPU, which gives me memory error.\nand using jax.config.update('jax_platform_name', 'cpu') at the beginning of my code seems to have no effect.\nAlso please note that I can't modify my_function()\nThanks!",
        "answers": [
            "To directly specify the device on which a function should be executed, use the device argument of jax.jit. For example (using a GPU runtime because it's the accelerator I have access to at the moment):\npython\nCopy\nimport jax\n\ngpu_device = jax.devices('gpu')[0]\ncpu_device = jax.devices('cpu')[0]\n\ndef my_function(x):\n  return x.sum()\n\nx = jax.numpy.arange(10)\n\nx_gpu = jax.jit(my_function, device=gpu_device)(x)\nprint(x_gpu.device())\n# gpu:0\n\nx_cpu = jax.jit(my_function, device=cpu_device)(x)\nprint(x_cpu.device())\n# TFRT_CPU_0\nThis can also be controlled with the jax.default_device decorator around the call-site:\npython\nCopy\nwith jax.default_device(cpu_device):\n  print(jax.jit(my_function)(x).device())\n  # TFRT_CPU_0\n\nwith jax.default_device(gpu_device):\n  print(jax.jit(my_function)(x).device())\n  # gpu:0",
            "I'm going to make a guess here. I can't run it either so you may have to fiddle with it\npython\nCopy\nwith jax.default_device(jax.devices(\"cpu\")[0]):\n    y = my_function()\nSee the docs here and here."
        ],
        "link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax"
    },
    {
        "title": "Test jax.pmap before deploying on multi-device hardware",
        "question": "I am coding on a single-device laptop and I am using jax.pmap because my code will run on multiple TPUs. I would like to \"fake\" having multiple devices to test my code and try different things.\nIs there any way to achieve this? Thanks!",
        "answers": [
            "You can spoof multiple XLA devices backed by a single device by setting the following environment variable:\n$ set XLA_FLAGS=\"--xla_force_host_platform_device_count=8\"\nIn Python, you could do it like this\npython\nCopy\n# Note: must set this env variable before jax is imported\nimport os\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\n\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3),\n#  CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n\nimport jax.numpy as jnp\nout = jax.pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\n# [ 0  1  4  9 16 25 36 49]\nNote that when a only a single physical device is present, all the \"devices\" here will be backed by the same threadpool. This will not improve performance of the code, but it can be useful for testing the semantics of parallel implementations on a single-device machine."
        ],
        "link": "https://stackoverflow.com/questions/74466352/test-jax-pmap-before-deploying-on-multi-device-hardware"
    },
    {
        "title": "Getting a type error while using fori_loop with JAX",
        "question": "I'm developing a code using JAX, and I wanted to JIT some parts of that had big loops. I didn't want the code to be unrolled so I used fori_loop, but I'm getting an error and can't figure out what I am doing wrong.\nThe error is:\npython\nCopy\n  self.arr = self.arr.reshape(new_shape+new_shape)\nTypeError: 'aval_method' object is not callable\nI was able to reduce the code to the following:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\nclass UB():\n    def __init__(self, arr, new_shape):\n\n        self.arr = arr\n        self.shape = new_shape\n        if type(arr) is not object:\n            self.arr = self.arr.reshape(new_shape+new_shape)\n\n        \n    def _tree_flatten(self):\n        children = (self.arr,)  # arrays / dynamic values\n        aux_data = {\n            'new_shape': self.shape            \n        }  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\n\nclass UM():\n    def __init__(self, arr, r=None):\n\n        self.arr = arr\n        self.r = tuple(r)\n    \n    def _tree_flatten(self):\n        children = (self.arr,)  # arrays / dynamic values\n        aux_data = {\n            'r': self.r\n        }  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\n\n\nfor C in [UB, UM]:\n    jax.tree_util.register_pytree_node(\n        C,\n        C._tree_flatten,\n        C._tree_unflatten,\n    )\n\n\ndef s_w(ub, ums):\n    e  = jnp.identity(2)\n    u = UM(e, [2])\n    ums[0] = u\n    return ub, ums\n\ndef s_c(t, uns):\n    n = 20\n    ums = []\n    for un in uns:\n        ums.append(UM(un, [2]))\n\n    tub = UB(t.arr, t.r)\n    \n    s_loop_body = lambda i,x: s_w( ub=x[0], ums=x[1])\n    \n    tub, ums = jax.lax.fori_loop(0, n, s_loop_body, (tub, ums))\n    # for i in range(n):\n    #     tub, ums = s_loop_body(i, (tub, ums))\n\n    return jnp.array([u.arr.flatten() for u in ums])\n\n\nuns = jnp.array([jnp.array([1, 2, 3, 4]) for _ in range(6)])\nt = UM(jnp.array([1, 0, 0, 1]), r=[2])\nuns = s_c(t, uns)\nHas anyone encountered this issue or can explain how to fix it?",
        "answers": [
            "The issue is discussed here: https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization\nNamely, in JAX pytrees are used as general containers, and are sometimes initialized with abstract values or other place-holders, and so you cannot assume that arguments to a custom PyTree will be of array type. You might account for this by doing something like the following:\npython\nCopy\nclass UB():\n    def __init__(self, arr, new_shape):\n        self.arr = arr\n        self.shape = new_shape\n        if isinstance(arr, jnp.ndarray):\n            self.arr = self.arr.reshape(new_shape+new_shape)\nWhen I run your code with this modification, it gets past the error you asked about, but unfortunately does trigger another error due to the body function of the fori_loop not having a valid signature (namely, the arr attributes of the ums have different shapes on input and output, which is not supported by fori_loop).\nHopefully this gets you on the path toward working code!"
        ],
        "link": "https://stackoverflow.com/questions/74296697/getting-a-type-error-while-using-fori-loop-with-jax"
    },
    {
        "title": "Indexing a BCOO in Jax",
        "question": "I came across another problem in my attempts to learn jax: I have a sparse BCOO array, and an array holding indices. I need to obtain all values at these indices in the BCOO array. It would be ideal if the returned array would be a sparse BCOO as well. Using the usual slicing syntax seems to not work. Is there a standard way to achieve this? e.g.\npython\nCopy\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\n\nindices = jnp.array([\n    1,1,0\n])\n\nfull_array = jnp.array(\n        [\n            [\n                [0,0,0],\n                [2,2,2],\n                [0,0,0],\n                [0,0,0]\n            ],\n            [\n                [1,1,1],\n                [0,0,0],\n                [0,0,0],\n                [0,0,0]\n            ],\n            [\n                [1,1,1],\n                [0,0,0],\n                [0,0,0],\n                [0,0,0]\n            ]\n        ]\n)\nfull_array[jnp.arange(3),indices]\n# results in:\n#    [2,2,2],\n#    [0,0,0],\n#    [1,1,1] \n\nsparse_array = sparse.bcoo_fromdense(full_array)\n\n# Trying the same thing on a sparse array:\nsparse_array[jnp.arange(3),indices]\n# produces an NotImplementedError",
        "answers": [
            "[Edit: 2022-11-15] As of jax version 0.3.25, this kind of sparse indexing is directly supported in JAX:\npython\nCopy\nimport jax\nprint(jax.__version__)\n# 0.3.25\n\nsparse_array = sparse.bcoo_fromdense(full_array)\nresult = sparse_array[jnp.arange(3),indices]\n\nprint(result.todense())\n# [[2 2 2]\n#  [0 0 0]\n#  [1 1 1]]\nOriginal answer:\nThanks for the question. Unfortunately, general indexing support has not been added yet to jax.experimental.sparse. The types of indexing operations currently supported are limited to static scalars and slices; for example:\npython\nCopy\nprint(sparse_array[0].todense())\n# [[0 0 0]\n#  [2 2 2]\n#  [0 0 0]\n#  [0 0 0]]\nWith this in mind, you may be able to build the operation you have in mind using concatenation. For example:\npython\nCopy\nresult = sparse.sparsify(jnp.vstack)([\n    sparse_array[0][1],  # only single indices supported currently\n    sparse_array[1][1],\n    sparse_array[2][0],\n])\nprint(result.todense())\n# [[2 2 2]\n#  [0 0 0]\n#  [1 1 1]]\nAdmittedly only supporting static indices is not very convenient, but we hope to add more indexing support in the future."
        ],
        "link": "https://stackoverflow.com/questions/74281298/indexing-a-bcoo-in-jax"
    },
    {
        "title": "How can I redefine a subfunction of a JAX-jitted function?",
        "question": "I have a function foo which is jitted with JAX. foo calls bar.\npython\nCopy\nfrom jax import jit\n\ndef bar(x):\n  return x ** 2\n\n@jit\ndef foo(x):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4)}')\nPrints foo(4) = 17 as expected.\nIf I redefine bar at runtime, what is the best way to re-jit foo?\nBonus: Is it possible to just tell jax bar changed, so that it can re-jit everything dependent upon bar?\nDetails\nIf I redefine bar and print again,\npython\nCopy\ndef bar(x):\n  return 2 * x\n\nprint(f'foo(4) = {foo(4)}')\nthe output is still foo(4) = 17. Clearly, the old bar is still in the jitted foo.\nIn order to get the desired foo with updated bar, I need to re-jit. I can do this by rewriting foo, so that it is re-jitted\npython\nCopy\n@jit\ndef foo(x):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4)}')\nwhich now prints foo(4) = 9 as desired. But rewriting all of foo is silly. I can also get the desired output by just rewrapping foo as\npython\nCopy\nfoo = jit(foo)\nBut this feels dangerous, since we're passing an already-jitted foo into the jit compiler again. I was surprised to see that it worked, and I suspect weird corner cases with this approach.\nAlso, both of these approaches require knowing that foo called bar. It's trivial in this toy example, but in more complex software, there might be many functions which need to be re-jitted. It's cumbersome to do it individually, and easy to forget some as well. Hence the 'bonus' part of my question.\nThe jit docs and jit caching docs did not answer my question, AFAICT. Nor did a reasonable search of stackoverflow or the github discussion pages.",
        "answers": [
            "The reason this is not working as expected is because the function is not pure: that is, its output depends not just on the inputs, but also on some global state. JAX transforms like JIT only work correctly for pure functions; see JAX Sharp Bits: Pure Functions for more discussion of this.\nThe best way to approach this is probably to change the function's call signature so that all relevant state is explicitly passed to the function. For example, you could pass bar explicitly:\npython\nCopy\nfrom jax import jit\nfrom functools import partial\n\ndef bar(x):\n  return x ** 2\n\n# Mark `bar` as a static argument: when it changes it will trigger re-compilation\n@partial(jit, static_argnames=['bar'])\ndef foo(x, bar):\n  return 1 + bar(x)\n\nprint(f'foo(4) = {foo(4, bar)}')\n# foo(4) = 17\n\ndef bar(x):\n  return 2 * x\n\nprint(f'foo(4) = {foo(4, bar)}')\n# foo(4) = 9\nNow when you change bar the output of the function changes."
        ],
        "link": "https://stackoverflow.com/questions/74185207/how-can-i-redefine-a-subfunction-of-a-jax-jitted-function"
    },
    {
        "title": "multidimensional jax.isin()",
        "question": "i am trying to filter an array of triples. The criterion by which I want to filter is whether another array of triples contains at least one element with the same first and third element. E.g\npython\nCopy\nimport jax.numpy as jnp\narray1 = jnp.array(\n  [\n    [0,1,2],\n    [1,0,2],\n    [0,3,3],\n    [3,0,1],\n    [0,1,1],\n    [1,0,3],\n  ]\n)\narray2 = jnp.array([[0,1,3],[0,3,2]])\n# the mask to filter the first array1 should look like this:\njnp.array([True,False,True,False,False,False])\nWhat would be a computationally efficient way to achieve this mask using jax? I am looking forward to your input.",
        "answers": [
            "You can do this by reducing over a broadcasted equality check:\npython\nCopy\nimport jax.numpy as jnp\narray1 = jnp.array(\n  [\n    [0,1,2],\n    [1,0,2],\n    [0,3,3],\n    [3,0,1],\n    [0,1,1],\n    [1,0,3],\n  ]\n)\narray2 = jnp.array([[0,1,2],[0,3,2]])  # note adjustment to match first entry of array1\n\nmask = (array1[:, None] == array2[None, :]).all(-1).any(-1)\nprint(mask)\n# [ True False False False False False]\nXLA doesn't have any binary search-like primitive, so the best approach in general is to generate the full equality matrix and reduce. If you're running the code on an accelerator like a GPU/TPU, this sort of vectorized operation is efficiently parallelized and so it will be computed quite efficiently in practice."
        ],
        "link": "https://stackoverflow.com/questions/74154196/multidimensional-jax-isin"
    },
    {
        "title": "Issues with non-hashable static arguments when forming",
        "question": "I have a vector-jacobian product that I want to compute.\nThe function func takes four arguments, the final two of which are static:\npython\nCopy\ndef func(variational_params, e, A, B):\n    ...\n    return model_params, dlogp, ...\nThe function jits perfectly fine via\npython\nCopy\nfunc_jitted = jit(func, static_argnums=(2, 3))\nThe primals are the variational_params, and the cotangents are dlogp (the second output of the function).\nCalculating the vector-jacobian product naively (by forming the jacobian) works fine:\npython\nCopy\njacobian_func = jacobian(func_jitted, argnums=0, has_aux=True)\njacobian_jitted = jit(jacobian_func, static_argnums=(2, 3))\njac, func_output = jacobian_jitted(variational_params, e, A, B)\nnaive_vjp = func_output.T @ jac \nWhen trying to form the vjp in an efficient manner via\npython\nCopy\nf_eval, vjp_function, aux_output = vjp(func_jitted, variational_params, e, A, B, has_aux=True)\nI get the following error:\npython\nCopy\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.ad.JVPTracer'> for function func is non-hashable.\nI am a little confused as the function func jitted perfectly fine... there is no option for adding static_argnums to the vjp function, so I am not too sure what this means.",
        "answers": [
            "For higher-level transformation APIs like jit, JAX generally provides a mechanism like static_argnums or argnums to allow specification of static vs. dynamic variables.\nFor lower-level transformation routines like jvp and vjp, these mechanisms are not provided, but you can still accomplish the same thing by passing partially-evaluated functions. For example:\npython\nCopy\nfrom functools import partial\n\nf_eval, vjp_function, aux_output = vjp(partial(func_jitted, A=A, B=B), variational_params, e, has_aux=True)\nThis is effectively how transformation parameters like argnums and static_argnums are implemented under the hood."
        ],
        "link": "https://stackoverflow.com/questions/74065210/issues-with-non-hashable-static-arguments-when-forming"
    },
    {
        "title": "Execute Markov chains with tree-structured state in parallel with JAX",
        "question": "I have a Markov chain function implemented in JAX that advances the chain from state s -> s' based on some training data (X_train).\npython\nCopy\ndef step(state: dict, key, X_train) -> dict:\n    new_state = advance(state, key, X_train)\n    return new_state\nHere, state is a fairly complicated tree-structured dict of array's that was generated by Haiku. For example,\npython\nCopy\nstate = {\n    'layer1': {\n        'weights': array(...),\n        'bias': array(...),\n    },\n    'layer2': {\n        'weights': array(...),\n        'bias': array(...),\n    },\n}\nI would like to run multiple Markov chains, with different states, in parallel. At first glance, jax.vmap function looks like a good candidate. However, state is not an array but a (tree-structured) dict.\nWhat is the best way to approach this?\nThanks!",
        "answers": [
            "Yes, you could use vmap for any pytree. But this is how you should construct it:\npython\nCopy\nstates = {'layer1':{'weights':jnp.array([[1, -2, 3],\n                                         [4, 5, 6]])},\n          'layer2':{'weights':jnp.array([[1, .2, 3],\n                                         [.4, 5, 6]])}}\nSo in your first run, your weights will be [1, -2, 3] and [1, .2, 3] for layer1 and layer2 respectively (second run will be [4, 5, 6] and [.4, 5, 6]). But markov chain should be handled by jax.lax.scan. And you could use jit compilation to speed things up. Here is a trivial example. In each step chain calculates the following:\npython\nCopy\nimport jax\nimport jax.numpy as jnp \nfrom functools import partial\n\n@jax.jit\ndef step(carry, k):\n    # this function runs a single step in the chain\n    # X_train dim:(3,3)\n    # w1 dim: (1,3)\n    # w2 dim: (3,1)\n    # X_new = log(Relu(w1@X_old)@w2) + e\n    # e~Normal(0, 1)\n    \n    state, X_train, rng = carry\n    rng, rng_input = jax.random.split(rng)\n    e = jax.random.normal(rng) # generate pseudorandom\n    w1 = state['layer1']['weights'] # it is a column vector\n    w2 = state['layer2']['weights'][None, :] # make it a row vector\n    \n    X_train = jax.nn.relu(w1@X_train)[:, None]+1\n    X_train = jnp.log(X_train@w2)\n    X_train = X_train + e\n    \n    return [state, X_train, rng], e\n\n@partial(jax.jit, static_argnums = 3)\ndef fi(state, X_train, key, number_of_steps):\n    rng = jax.random.PRNGKey(key)\n    carry = [state, X_train, rng]\n    carry, random_normals = jax.lax.scan(step, carry, xs = jnp.arange(number_of_steps))\n    state, X, rng = carry\n    return X\n\nX_train = jnp.array([[1., -1., 0.5],\n                     [1., 1, 2.],\n                     [4, 2, 0.1]])\n\nstates = {'layer1':{'weights':jnp.array([[1, -2, 3],\n                                         [4, 5, 6]])},\n          'layer2':{'weights':jnp.array([[1, .2, 3],\n                                         [.4, 5, 6]])}}\n\nvmap_fi = jax.vmap(fi, (0, None, None, None)) # only map on first argument axis 0\n\nkey = 42 # random seed\nnumber_of_steps = 100 # chain runs 100 steps\n\nlast_states = vmap_fi(states, X_train, key, number_of_steps)\nprint(last_states)\nOutput:\npython\nCopy\n[[[ 1.8478627   0.23842478  2.946475  ]\n  [ 1.3278859  -0.28155205  2.4264982 ]\n  [ 2.0921988   0.48276085  3.1908112 ]]\n\n [[ 2.9374144   5.4631433   5.645465  ]\n  [ 3.4333894   5.959118    6.1414394 ]\n  [ 3.4612248   5.9869533   6.169275  ]]]\nIn this example, you could make states dictionaries more complicated. You just need to parallelize on their 0th axis."
        ],
        "link": "https://stackoverflow.com/questions/73995540/execute-markov-chains-with-tree-structured-state-in-parallel-with-jax"
    },
    {
        "title": "Error when trying to jit the computation of the Jacobian in JAX: \"ValueError: Non-hashable static arguments are not supported\"",
        "question": "This question is similar to the question here, but I cannot link with what I should alter.\nI have a function\npython\nCopy\ndef elbo(variational_parameters, eps, a, b):\n    ...\n    return theta, _\n\nelbo = jit(elbo, static_argnames=[\"a\", \"b\"])\nwhere variational_parameters is a vector (one-dimensional array) of length P, eps is a two-dimensional array of dimensions K by N, and a, b are fixed values.\nThe elbo has been successfully vmapped over the rows of eps, and has been jitted by setting by passing a and b to static_argnames, to return theta, which is a two-dimensional array of dimensions K by P.\nI want to take the Jacobian of the output theta with respect to variational_parameters through the elbo function. The first value returned by\npython\nCopy\njacobian(elbo, argnums=0, has_aus=True)(variational_parameters, eps, a, b)\ngives me a three-dimensional array of dimensions K by P by N. This is what I want. As soon as I try to jit this function\npython\nCopy\njit(jacobian(elbo, argnums=0, has_aus=True))(variational_parameters, eps, a, b)\nI get the error\npython\nCopy\nValueError: Non-hashable static arguments are not supported, which can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.partial_eval.DynamicJaxprTracer'> for function elbo is non-hashable.\nAny help would be greatly appreciated; thanks!",
        "answers": [
            "Any parameters you pass to a JIT-compiled function will no longer be static, unless you explicitly mark them as such. So this line:\npython\nCopy\njit(jacobian(elbo, argnums=0, has_aus=True))(variational_parameters, eps, a, b)\nMakes variational_parameters, eps, a, and b non-static. Then within the transformed function these non-static parameters are passed to this function:\npython\nCopy\nelbo = jit(elbo, static_argnames=[\"a\", \"b\"])\nwhich means that you are attempting to pass non-static values as static arguments, which causes an error.\nTo fix this, you should mark the static parameters as static any time they enter a jit-compiled function. In your case it might look something like this:\npython\nCopy\njit(jacobian(elbo, argnums=0, has_aus=True),\n    static_argnums=(2, 3))(variational_parameters, eps, a, b)"
        ],
        "link": "https://stackoverflow.com/questions/73941657/error-when-trying-to-jit-the-computation-of-the-jacobian-in-jax-valueerror-no"
    },
    {
        "title": "How to use JAX vmap to efficiently calculate importance sampling estimate",
        "question": "I have code to calculate the off-policy importance sampling estimate commonly used in reinforcement learning. It is not important to know what that is, but for someone who does it might help them understand this question a little better. Basically, I have a 1D array of instances of a custom Episode class. An Episode has four attributes, all of which are arrays of floats. I have a function which loops over all episodes and for each one, it does a computation based only on the arrays in that episode. The result of that computation is a float, which I then store in a result array. Don't worry about what model.get_prob_this_action() does, you can consider it a black box that takes two floats as input and returns a float. The code for this function before optimizing with JAX is:\npython\nCopy\ndef IS_estimate(model, theta, episodes):\n    \"\"\" Calculate the unweighted importance sampling estimate\n    for each episode in episodes.\n    Return as an array, one element per episode\n    \"\"\"\n    # episodes is an array of custom Python class instances\n    \n    gamma = 1.0\n    result = np.zeros(len(episodes))\n    for ii, ep in enumerate(episodes):\n        obs = ep.observations # 1D array of floats\n        actions = ep.actions # 1D array of floats\n        rewards = ep.rewards # 1D array of floats\n        action_probs = ep.action_probs # 1D array of floats\n\n        pi_news = np.zeros(len(obs))\n        for jj in range(len(obs)):\n            pi_news[jj] = model.get_prob_this_action(obs[jj],actions[jj])\n\n        pi_ratio_prod = np.prod(pi_news / action_probs)\n\n        weighted_return = weighted_sum_gamma(rewards, gamma)\n        result[ii] = pi_ratio_prod * weighted_return\n\n    return np.array(result)\nUnfortunately, I cannot just rewrite the function to work on a single episode and then use jax.vmap to vectorize over that function. The reason is that the argument I want to vectorize is a custom Episode object, which JAX won't support.\nI can get rid of the inner loop to get pi_news using vmap, like:\npython\nCopy\ndef IS_estimate(model, theta, episodes):\n    \"\"\" Calculate the unweighted importance sampling estimate\n    for each episode in episodes.\n    Return as an array, one element per episode\n    \"\"\"\n    # episodes is an array of custom Python class instances\n    \n    gamma = 1.0\n    result = np.zeros(len(episodes))\n    for ii, ep in enumerate(episodes):\n        obs = ep.observations # 1D array of floats\n        actions = ep.actions # 1D array of floats\n        rewards = ep.rewards # 1D array of floats\n        action_probs = ep.action_probs # 1D array of floats\n\n        vmapped_get_prob_this_action = vmap(model.get_prob_this_action,in_axes=(0,0))\n        pi_news = vmapped_get_prob_this_action(obs,actions)\n\n        pi_ratio_prod = np.prod(pi_news / action_probs)\n\n        weighted_return = weighted_sum_gamma(rewards, gamma)\n        result[ii] = pi_ratio_prod * weighted_return\n\n    return np.array(result)\nand this does help some. But ideally, I'd like to vmap my outer loop as well. Does anyone know how I would do this?",
        "answers": [
            "The computation you're describing is an \"array-of-structs\" style computation; JAX's vmap does not support this. What it does support is a \"struct-of-arrays` style computation.\nAs a quick demonstration of this, here's how you might do a simple per-episode computation using first the array-of-structs pattern (with Python for-loops) and then the struct-of-arrays pattern (with jax.vmap):\npython\nCopy\nfrom typing import NamedTuple\nimport jax.numpy as jnp\nimport numpy as np\nimport jax\n\nclass Episode(NamedTuple):\n  observations: jnp.ndarray\n  actions: jnp.ndarray\n\n  def compute_result(self):\n    # stand-in for computing some value from attributes\n    return jnp.dot(self.observations, self.actions)\n\n# Computing result per episode on array of structs:\nrng = np.random.RandomState(42)\nepisodes = [\n    Episode(\n        observations=jnp.array(rng.rand(4)),\n        actions=jnp.array(rng.rand(4)))\n    for i in range(5)\n]\nresult1 = jnp.array([ep.compute_result() for ep in episodes])\nprint(result1)\n# [0.767802   0.83237386 0.49223748 0.5156544  1.1290307 ]\n\n# Computing results on struct of arrays via vmap:\nepisodes_struct_of_arrays = Episode(\n    observations = jnp.vstack([ep.observations for ep in episodes]),\n    actions = jnp.vstack([ep.actions for ep in episodes])\n)\nresult2 = jax.vmap(lambda self: self.compute_result())(episodes_struct_of_arrays)\nprint(result2)\n# [0.767802   0.83237386 0.49223748 0.5156544  1.1290307 ]\nIf you want to use JAX's vmap for this computation, you'll have to use a struct-of-arrays approach like the second one. Note that this also assumes that your Episode class is registered as a pytree (see extending pytrees) which is true by default for NamedTuple types."
        ],
        "link": "https://stackoverflow.com/questions/73921013/how-to-use-jax-vmap-to-efficiently-calculate-importance-sampling-estimate"
    },
    {
        "title": "Exception occurred while installing jaxlib on Ubuntu x86_64",
        "question": "I am unable to figure out how to troubleshoot these errors in jaxlib installation.\nIf somebody could please guide me on how to go about it, it will be much appreciated, Thanks.\nBelow are the commands I am using with the corresponding outputs.\n:~$ uname -a\nLinux pc-name-15-3567 5.15.0-47-generic #51-Ubuntu SMP Thu Aug 11 07:51:15 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n\n:~$ pip install --upgrade pip\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pip in ./.local/lib/python3.10/site-packages (22.2.2)\n\n:~$ pip install --upgrade \"jax\"\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: jax in ./.local/lib/python3.10/site-packages (0.3.17)\nCollecting jax\n  Downloading jax-0.3.19.tar.gz (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 221.8 kB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: absl-py in ./.local/lib/python3.10/site-packages (from jax[cpu]) (1.2.0)\nRequirement already satisfied: etils[epath] in ./.local/lib/python3.10/site-packages (from jax[cpu]) (0.8.0)\nRequirement already satisfied: numpy>=1.20 in /usr/lib/python3/dist-packages (from jax) (1.21.5)\nRequirement already satisfied: opt_einsum in ./.local/lib/python3.10/site-packages (from jax[cpu]) (3.3.0)\nRequirement already satisfied: scipy>=1.5 in /usr/lib/python3/dist-packages (from jax) (1.8.0)\nRequirement already satisfied: typing_extensions in ./.local/lib/python3.10/site-packages (from jax[cpu]) (4.3.0)\nCollecting jaxlib==0.3.15\n  Downloading jaxlib-0.3.15-cp310-none-manylinux2014_x86_64.whl (72.0 MB)\n     ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/72.0 MB 248.9 kB/s eta 0:04:33\nERROR: Exception:\nTraceback (most recent call last):\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 435, in _error_catcher\n    yield\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 516, in read\n    data = self._fp.read(amt) if not fp_closed else b\"\"\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n    data = self.__fp.read(amt)\n  File \"/usr/lib/python3.10/http/client.py\", line 465, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/python3.10/ssl.py\", line 1273, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/usr/lib/python3.10/ssl.py\", line 1129, in read\n    return self._sslobj.read(len, buffer)\nTimeoutError: The read operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n    status = run_func(*args)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\n    return func(self, options, args)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/commands/install.py\", line 369, in run\n    requirement_set = resolver.resolve(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n    result = self._result = resolver.resolve(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n    state = resolution.resolve(requirements, max_rounds=max_rounds)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\n    failure_causes = self._attempt_to_pin_criterion(name)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n    criteria = self._get_updated_criteria(candidate)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\n    self._add_to_criteria(criteria, requirement, parent=candidate)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n    if not criterion.candidates:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n    return bool(self._sequence)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n    return any(self)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n    return (c for c in iterator if id(c) not in self._incompatible_ids)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n    candidate = func()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n    self._link_candidate_cache[link] = LinkCandidate(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\n    super().__init__(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\n    self.dist = self._prepare()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\n    dist = self._prepare_distribution()\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 308, in _prepare_distribution\n    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 438, in prepare_linked_requirement\n    return self._prepare_linked_requirement(req, parallel_builds)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 483, in _prepare_linked_requirement\n    local_file = unpack_url(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 165, in unpack_url\n    file = get_http_url(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/operations/prepare.py\", line 106, in get_http_url\n    from_path, content_type = download(link, temp_dir.path)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/network/download.py\", line 147, in __call__\n    for chunk in chunks:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n    for chunk in iterable:\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n    for chunk in response.raw.stream(\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 573, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 509, in read\n    with self._error_catcher():\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/abbas/.local/lib/python3.10/site-packages/pip/_vendor/urllib3/response.py\", line 440, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\npip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.",
        "answers": [
            "The issue got solved for me by running these commands to install jaxlib:\npip install --upgrade \"jax\"\npip install --upgrade \"jaxlib\""
        ],
        "link": "https://stackoverflow.com/questions/73874153/exception-occurred-while-installing-jaxlib-on-ubuntu-x86-64"
    },
    {
        "title": "How to write this function jax.jit-able>",
        "question": "def factory(points):\n  points.sort()\n  @jax.jit\n  def fwd(x):\n    for i in range(1, len(points)):\n      if x < points[i][0]:\n        return (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * x + (points[i][1] - (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * points[i][0])\n    i = len(points) - 1\n    return (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * x + (points[i][1] - (points[i][1] - points[i - 1][1]) / (points[i][0] - points[i - 1][0]) * points[i][0])\n  return fwd\nI want to write a function that creates jitted function, given argument: points, a list contain pairs of numbers. I aware that if/else statement can't be jitted and jax.lax.cond() allow conditions but I want something like a break as you can see in the above code. Is there any way to work with conditions?",
        "answers": [
            "The challenge in converting this to JAX-compatible is that your function relies on control flow triggered by values in the array; to make this compatible with JAX you should convert it to a vector-based operation. Here's how you might express your operation in terms of np.where rather than for loops:\npython\nCopy\ndef factory_v2(points):\n  points.sort()\n  def fwd(x):\n    matches = np.where(x < points[1:, 0])[0]\n    i = matches[0] + 1 if len(matches) else len(points) - 1\n    return (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * x + (points[i, 1] - (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * points[i, 0])\n  return fwd\n\nx = 2\npoints = np.array([[4, 0], [2, 1], [6, 5], [4, 6], [5, 7]])\nprint(factory(points)(x))\n# 3.0\n\nprint(factory_v2(points)(x))\n# 3.0\nThis is closer to a JAX-compatible operation, but unfortunately it relies on creating dynamically-shaped arrays. You can get around this by using the size argument to jnp.where. Here's a JAX-compatible version that uses the JAX-only size and fill_value arguments to jnp.where to work around this dynamic size issue:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef factory_jax(points):\n  points = jnp.sort(points)\n  @jax.jit\n  def fwd(x):\n    i = 1 + jnp.where(x < points[1:, 0], size=1, fill_value=len(points) - 2)[0][0]\n    return (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * x + (points[i, 1] - (points[i, 1] - points[i - 1, 1]) / (points[i, 0] - points[i - 1, 0]) * points[i, 0])\n  return fwd\n\nprint(factory_jax(points)(x))\n# 3.0\nIf I've understood the intended input shapes for your code, I believe this should compute the same results as your orginal function."
        ],
        "link": "https://stackoverflow.com/questions/73693195/how-to-write-this-function-jax-jit-able"
    },
    {
        "title": "How to map the kronecker product along array dimensions?",
        "question": "Given two tensors A and B with the same dimension (d>=2) and shapes [A_{1},...,A_{d-2},A_{d-1},A_{d}] and [A_{1},...,A_{d-2},B_{d-1},B_{d}] (shapes of the first d-2 dimensions are identical).\nIs there a way to calculate the kronecker product over the last two dimensions? The shape of my_kron(A,B)should be [A_{1},...,A_{d-2},A_{d-1}*B_{d-1},A_{d}*B_{d}]. For example with d=3,\npython\nCopy\nA.shape=[2,3,3]\nB.shape=[2,4,4]\nC=my_kron(A,B)\nC[0,...] should be the kronecker product of A[0,...] and B[0,...] and C[1,...] the kronecker product of A[1,...] and B[1,...].\nFor d=2 this is simply what the jnp.kron(or np.kron) function does.\nFor d=3 this can be achived with jax.vmap. jax.vmap(lambda x, y: jnp.kron(x[0, :], y[0, :]))(A, B)\nBut I was not able to find a solution for general (unknown) dimensions. Any suggestions?",
        "answers": [
            "In numpy terms I think this is what you are doing:\npython\nCopy\nIn [104]: A = np.arange(2*3*3).reshape(2,3,3)\nIn [105]: B = np.arange(2*4*4).reshape(2,4,4)\n\nIn [106]: C = np.array([np.kron(a,b) for a,b in zip(A,B)])\nIn [107]: C.shape\nOut[107]: (2, 12, 12)\nThat treats the initial dimension, the 2, as a batch. One obvious generalization is to reshape the arrays, reducing the higher dimensions to 1, e.g. reshape(-1,3,3), etc. And then afterwards, reshape C back to the desired n-dimensions.\nnp.kron does accept 3d (and higher), but it's doing some sort of outer on the shared 2 dimension:\npython\nCopy\nIn [108]: np.kron(A,B).shape\nOut[108]: (4, 12, 12)\nAnd visualizing that 4 dimension as (2,2), I can take the diagonal and get your C:\npython\nCopy\nIn [109]: np.allclose(np.kron(A,B)[[0,3]], C)\nOut[109]: True\nThe full kron does more calculations than needed, but is still faster:\npython\nCopy\nIn [110]: timeit C = np.array([np.kron(a,b) for a,b in zip(A,B)])\n108 µs ± 2.23 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nIn [111]: timeit np.kron(A,B)[[0,3]]\n76.4 µs ± 1.36 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nI'm sure it's possible to do your calculation in a more direct way, but doing that requires a better understanding of how the kron works. A quick glance as the np.kron code suggest that is does an outer(A,B)\npython\nCopy\nIn [114]: np.outer(A,B).shape\nOut[114]: (18, 32)\nwhich has the same number of elements, but it then reshapes and concatenates to produce the kron layout.\nBut following a hunch, I found that this is equivalent to what you want:\npython\nCopy\nIn [123]: D = A[:,:,None,:,None]*B[:,None,:,None,:]\nIn [124]: np.allclose(D.reshape(2,12,12),C)\nOut[124]: True\nIn [125]: timeit np.reshape(A[:,:,None,:,None]*B[:,None,:,None,:],(2,12,12))\n14.3 µs ± 184 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\nThat is easily generalized to more leading dimensions.\npython\nCopy\ndef my_kron(A,B):\n   D = A[...,:,None,:,None]*B[...,None,:,None,:]\n   ds = D.shape\n   newshape = (*ds[:-4],ds[-4]*ds[-3],ds[-2]*ds[-1])\n   return D.reshape(newshape)\n\nIn [137]: my_kron(A.reshape(1,2,1,3,3),B.reshape(1,2,1,4,4)).shape\nOut[137]: (1, 2, 1, 12, 12)"
        ],
        "link": "https://stackoverflow.com/questions/73673599/how-to-map-the-kronecker-product-along-array-dimensions"
    },
    {
        "title": "How to vmap over specific funciton in jax?",
        "question": "I have this function which works for single vector:\npython\nCopy\ndef vec_to_board(vector, player, dim, reverse=False):\n    player_board = np.zeros(dim * dim)\n    player_pos = np.argwhere(vector == player)\n    if not reverse:\n        player_board[mapping[player_pos.T]] = 1\n    else:\n        player_board[reverse_mapping[player_pos.T]] = 1\n    return np.reshape(player_board, [dim, dim])\nHowever, I want it to work for a batch of vectors.\nWhat I have tried so far:\npython\nCopy\nstates = jnp.array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]])\n batch_size = 1\nb_states = vmap(vec_to_board)((states, 1, 4), batch_size)\nThis doesn't work. However, if I understand correctly vmap should be able to handle this transformation for batches?",
        "answers": [
            "There are a couple issues you'll run into when trying to vmap this function:\nThis function is defined in terms of numpy arrays, not jax arrays. How do I know? JAX arrays are immutable, so things like arr[idx] = 1 will raise errors. You need to replace these with equivalent JAX operations (see JAX Sharp Bits: in-place updates) and ensure your function works with JAX array operations rather than numpy array operations.\nYour function makes used of dynamically-shaped arrays; e.g. player_pos, has a shape dependent on the number of nonzero entries in vector == player. You'll have to rewrite your function in terms of statically-shaped arrays. There is some discussion of this in the jnp.argwhere docstring; for example, if you know a priori how many True entries you expect in the array, you can specify the size to make this work.\nGood luck!"
        ],
        "link": "https://stackoverflow.com/questions/73588309/how-to-vmap-over-specific-funciton-in-jax"
    },
    {
        "title": "Rewriting for loop with jax.lax.scan",
        "question": "I'm having troubles understanding the JAX documentation. Can somebody give me a hint on how to rewrite simple code like this with jax.lax.scan?\npython\nCopy\nnumbers = numpy.array( [ [3.0, 14.0], [15.0, -7.0], [16.0, -11.0] ])\nevenNumbers = 0\nfor row in numbers:\n      for n in row:\n         if n % 2 == 0:\n            evenNumbers += 1",
        "answers": [
            "Assuming a solution should demonstrate the concepts rather than optimize the example shown, the function to be jax.lax.scanned must match the expected signature and any dynamic condition has to be replaced with jax.lax.cond. The code below is the closest to the original I could think of, but please be aware that I'm anything but an jaxpert.\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef f(carry, row):\n\n    even = 0\n    for n in row:\n        even += jax.lax.cond(n % 2 == 0, lambda: 1, lambda: 0)\n\n    return carry + even, even\n\nnumbers = jnp.array([[3.0, 14.0], [15.0, -7.0], [16.0, -11.0]])\njax.lax.scan(f, 0, numbers)\nOutput\npython\nCopy\n(DeviceArray(2, dtype=int32, weak_type=True),\n DeviceArray([1, 0, 1], dtype=int32, weak_type=True))"
        ],
        "link": "https://stackoverflow.com/questions/73564732/rewriting-for-loop-with-jax-lax-scan"
    },
    {
        "title": "JAX: Getting rid of zero-gradient",
        "question": "Is there a way how to modify this function (MyFunc) so that it gives the same result, but its derivative is not zero gradient?\npython\nCopy\nfrom jax import grad\nimport jax.nn as nn\nimport numpy as np\n\ndef MyFunc(coefs):\n   a = coefs[0]\n   b = coefs[1]\n   c = coefs[2]\n   \n   if a > b:\n      return 30.0\n   elif b > c:\n      return 20.0\n   else:\n      return 10.0   \n   \nmyFuncDeriv = grad (MyFunc)   \n\n# prints [0. 0. 0.]\nprint (myFuncDeriv(np.random.sample(3)))\n# prints [0. 0. 0.]\nprint (myFuncDeriv(np.array([1.0, 2.0, 3.0])))\nEDIT: Similar function which doesn't give zero gradient - but it doesn't return 30/20/10\npython\nCopy\ndef MyFunc2(coefs):\n    a = coefs[0]\n    b = coefs[1]\n    c = coefs[2]\n    if a > b:\n        return nn.sigmoid(a)*30.0\n    if b > c:\n        return nn.sigmoid(b)*20.0\n    else:\n        return nn.sigmoid(c)*10.0\n\n\nmyFunc2Deriv = grad (MyFunc2)   \n\n# prints [0.         0.         0.45176652]\nprint (myFuncDeriv(np.array([1.0, 2.0, 3.0])))\n# prints for example [6.1160526 0.        0.       ]\nprint (myFunc2Deriv(np.random.sample(3)))",
        "answers": [
            "The gradient of your function is zero because this is the correct result for the gradient as your function is defined. For more information on this phenomenon, see FAQ: Why are gradients zero for functions based on sort order?\nIf you want a sort-based function with non-zero gradients, you can achieve this by replacing your step-wise function with a smooth approximation. The sigmoid version you included in your question seems like a reasonable approach for this approximation.\nBut note that the answer to your exact question – how to make a function that produces the same output but has nonzero gradients – is impossible, because a function returning the same outputs as yours for all inputs has a zero gradient by definition."
        ],
        "link": "https://stackoverflow.com/questions/73507935/jax-getting-rid-of-zero-gradient"
    },
    {
        "title": "How to install objax with GPU support?",
        "question": "I have followed the objax documentation to install the library with GPU support: https://objax.readthedocs.io/en/stable/installation_setup.html\ni.e.\npip install --upgrade objax\nCUDA_VERSION=11.6\npip install -f https://storage.googleapis.com/jax-releases/jax_releases.html jaxlib==`python3 -c 'import jaxlib; print(jaxlib.__version__)'`+cuda`echo $CUDA_VERSION | sed s:\\\\\\.::g` \nHowever the last step doesn't work. I get the following error message:\nERROR: Could not find a version that satisfies the requirement jaxlib==0.3.15+cuda116 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15) ERROR: No matching distribution found for jaxlib==0.3.15+cuda116\nI have tried with multiple versions of python/CUDA, but I always get this error.\nExecuting pip install --upgrade pip at the begining does not help.\nSystem description:\nOperating system: Ubuntu 20.04.4 LTS\nCUDA Version: 11.6\nPython version: 3.8.13",
        "answers": [
            "JAX recently updated its GPU installation instructions, which you can find here: https://github.com/google/jax#pip-installation-gpu-cuda\nIn particular, the CUDA wheels are now located at https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo, for example, you can install JAX with\npython\nCopy\n$ pip install \"jax[cuda11_cudnn805]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nand replace cuda11 and cudnn805 respectively with the appropriate CUDA and CUDNN version for your system, ensuring that they match the versions listed in the index at the above URL.\nI've sent a pull request to the objax repository to update the instructions you were following: https://github.com/google/objax/pull/246",
            "The doc page was last updated 2 year ago. It's outdated and due to this it currently lies. Take a close look at https://storage.googleapis.com/jax-releases/jax_releases.html — there're currently no releases with CUDA.\nAs for no-CUDA versions you can install them directly from PyPI:\npython\nCopy\npip install jaxlib\nFor CUDA version — try to compile from sources. Or use conda: https://anaconda.org/search?q=jaxlib"
        ],
        "link": "https://stackoverflow.com/questions/73472417/how-to-install-objax-with-gpu-support"
    },
    {
        "title": "Local devices VS non local devices in multi GPU processing",
        "question": "I'm reading JAX documentation on jax.local_devices and in it, it is written:\nLike jax.devices(), but only returns devices local to a given process.\nAnd in jax.devices() it is written:\nReturns a list of all devices for a given backend.\nI don't know what exactly are these local and non-local devices. Could you please elaborate on the difference between these?",
        "answers": [
            "This is discussed in JAX's documentation in Using JAX in multi-host and multi-process environments:\nA process’s local devices are those that it can directly address and launch computations on. For example, on a GPU cluster, each host can only launch computations on the directly attached GPUs. On a Cloud TPU pod, each host can only launch computations on the 8 TPU cores attached directly to that host (see the Cloud TPU System Architecture documentation for more details). You can see a process’s local devices via jax.local_devices().\nThe global devices are the devices across all processes. A computation can span devices across processes and perform collective operations via the direct communication links between devices, as long as each process launches the computation on its local devices. You can see all available global devices via jax.devices(). A process’s local devices are always a subset of the global devices."
        ],
        "link": "https://stackoverflow.com/questions/73458553/local-devices-vs-non-local-devices-in-multi-gpu-processing"
    },
    {
        "title": "JAX: average per-example gradients different from aggregated gradients",
        "question": "I want to compute per-example gradients to perform per-example clipping before the final gradient descent step.\nI wanted to ensure that the per-example gradients are correct. Therefore I implemented this minimal example below to test standard gradient computation and per-example gradient computation with JAX.\nThe problem I have is, that the average of the per-example gradients differs from the gradients of the standard computation.\nDoes someone see where I went wrong?\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\n\ndef loss(params, x, t):\n    w, b = params\n    y = jnp.dot(w, x.T) + b\n    return ((t.T - y)**2).sum()\n\n\ndef main():\n\n    n_samples = 3\n    dims_in = 7\n    dims_out = 5\n\n    key = random.PRNGKey(0)\n\n    # Random data\n    x = random.normal(key, (n_samples, dims_in), dtype=jnp.float32)\n    t = random.normal(key, (n_samples, dims_out), dtype=jnp.float32)\n    \n    # Random weights\n    w = random.normal(key, (dims_out, dims_in), dtype=jnp.float32)\n    b = random.normal(key, (dims_out, 1), dtype=jnp.float32)\n    params = (w, b)\n\n    # Standard gradient\n    reduced_grads = jax.grad(loss)\n    dw0, db0 = reduced_grads(params, x, t)\n    print(f\"{dw0.shape = }\") \n    print(f\"{db0.shape = }\") \n\n    # Per-example gradients\n    perex_grads = jax.vmap(jax.grad(loss), in_axes=((None, None), 0, 0))\n    dw1, db1 = perex_grads(params, x, t)\n    print(f\"{dw1.shape = }\") \n    print(f\"{db1.shape = }\")\n\n    # Gradients are different!\n    print(jnp.allclose(dw0, jnp.mean(dw1, axis=0)))  # should be True\n    print(jnp.allclose(db0, jnp.mean(db1, axis=0)))  # should be True\n    \n\nif __name__ == \"__main__\":\n    main()",
        "answers": [
            "THe issue is that vmap effectively passes 1D inputs to your function, and your loss function operates differently on 1D vs. 2D inputs. You can see this by comparing the following:\npython\nCopy\nprint(loss(params, x[0], t[0]))\n# 87.5574\n\nprint(loss(params, x[:1], t[:1]))\n# 18.089672\nIf you inspect the shapes of the intermediate results within the loss function, it should become clear why these differ (e.g. you're summing over a shape (5, 5) array of differences instead of a shape (5, 1) array of differences).\nTo use per-example gradients, you'll need to change the implementation of your loss function so that it returns the correct result whether the input is 1D or 2D. The easiest way to do this is probably to use jnp.atleast_2d to ensure that the inputs are two-dimensional:\npython\nCopy\ndef loss(params, x, t):\n    x = jnp.atleast_2d(x)\n    t = jnp.atleast_2d(t)\n    w, b = params\n    y = jnp.dot(w, x.T) + b\n    return ((t.T - y)**2).sum()\nAt this point the sum (not the mean) of the per-example gradients will match the full computation:\npython\nCopy\nprint(jnp.allclose(dw0, jnp.sum(dw1, axis=0)))\n# True\nprint(jnp.allclose(db0, jnp.sum(db1, axis=0)))\n# True"
        ],
        "link": "https://stackoverflow.com/questions/73435394/jax-average-per-example-gradients-different-from-aggregated-gradients"
    },
    {
        "title": "Struggling to understand nested vmaps in JAX",
        "question": "I just about understand unnested vmaps, but try as I may, and I have tried my darnedest, nested vmaps continue to elude me. Take the snippet from this text for example\nI don't understand what the axis are in this case. Is the nested vmap(kernel, (0, None)) some sort of partial function application? Why is the function mapped twice? Can someone please explain what is going on behind the scene in other words. What does a nested vmap desugar to?? All the answers that I have found are variants of the same curt explanation: mapping over both axis, which I am struggling with.",
        "answers": [
            "Each time vmap is applied, it maps over a single axis. So say for simplicity that you have a function that takes two scalars and outputs a scalar:\npython\nCopy\ndef f(x, y):\n  assert jnp.ndim(x) == jnp.ndim(y) == 0  # x and y are scalars\n  return x + y\n\nprint(f(1, 2))\n# 0\nIf you want to apply this function to an array of x values and a single y value, you can do this with vmap:\npython\nCopy\nf_mapped_over_x = jax.vmap(f, in_axes=(0, None))\n\nx = jnp.arange(5)\nprint(f_mapped_over_x(x, 1))\n# [1 2 3 4 5]\nin_axes=(0, None) means that it is mapped along the leading axis of the first argument, x, and there is no mapping of the second argument, y.\nLikewise, if you want to apply this function to a single x value and an array of y values, you can specify this via in_axes:\npython\nCopy\nf_mapped_over_y = jax.vmap(f, in_axes=(None, 0))\n\ny = jnp.arange(5, 10)\nprint(f_mapped_over_y(1, y))\n# [ 6  7  8  9 10]\nIf you wish to map the function over both arrays at once, you can do this by specifying in_axes=(0, 0), or equivalently in_axes=0:\npython\nCopy\nf_mapped_over_x_and_y = jax.vmap(f, in_axes=(0, 0))\n\nprint(f_mapped_over_x_and_y(x, y))\n# [ 5  7  9 11 13]\nBut suppose you want to map first over x, then over y, to get a sort of \"outer-product\" version of the function. You can do this via a nested vmap, first mapping over just x, then mapping over just y:\npython\nCopy\nf_mapped_over_x_then_y = jax.vmap(jax.vmap(f, in_axes=(None, 0)), in_axes=(0, None))\n\nprint(f_mapped_over_x_then_y(x, y))\n# [[ 5  6  7  8  9]\n#  [ 6  7  8  9 10]\n#  [ 7  8  9 10 11]\n#  [ 8  9 10 11 12]\n#  [ 9 10 11 12 13]]\nThe nesting of vmaps is what lets you map over two axes separately."
        ],
        "link": "https://stackoverflow.com/questions/73212780/struggling-to-understand-nested-vmaps-in-jax"
    },
    {
        "title": "Is it safe to read the value of numpy.empty or jax.numpy.empty?",
        "question": "In Flax, we typically initialize a model by passing in a random vector and let the library figure the correct shape for the parameters via shape inference. For example, this is what the tutorial did\npython\nCopy\ndef create_train_state(rng, learning_rate, momentum):\n  \"\"\"Creates initial `TrainState`.\"\"\"\n  cnn = CNN()\n  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n  tx = optax.sgd(learning_rate, momentum)\n  return train_state.TrainState.create(\n      apply_fn=cnn.apply, params=params, tx=tx)\nIt is worth noting that the concrete value of jnp.ones([1, 28, 28, 1]) does not matter, as shape inference only relies on its shape. I can replace it with jnp.zeros([1, 28, 28, 1]) or jnp.random(jax.random.PRNGKey(42), [1, 28, 28, 1]), and it will give me the exactly same result.\nMy question is, can I use jnp.empty([1, 28, 28, 1]) instead? I want to use jnp.empty to clarify that we don't care about the value (and it could also be faster but the speedup is negligible). However, there is something called trap representation in C, and it looks like reading from jnp.empty without overwriting it first could trigger undefined behavior. Since Numpy is a light wrapper around C, should I worry about that?\nBonus question: let's forget about Jax and focus on vanilla Numpy. It is safe to read from np.empty([...])? Again, I don't care about the value, but I do care about not getting a segfault.",
        "answers": [
            "Because XLA does not provide a mechanism to create uninitialized memory, in JAX jnp.empty is currently (v0.3.14) equivalent to jnp.zeros (see https://github.com/google/jax/blob/jax-v0.3.14/jax/_src/numpy/lax_numpy.py#L2007-L2009)\nSo at least in the current release, it is safe to refer to the contents of jnp.empty arrays. But if you're going to rely on that property, I'd suggest using jnp.zeros instead, so that if the jnp.empty implementation changes in the future your assumptions will still be valid.\nnp.empty is different: it will include uninitialized values, and so your program's behavior may change unpredictably from run to run if you rely on those values. There's no danger of memory corruption/segfaults when accessing these uninitialized values: the memory is allocated, it's just that the contents are uninitialized and so the values will reflect whatever bits happened to be stored there at the time the block was allocated."
        ],
        "link": "https://stackoverflow.com/questions/72905359/is-it-safe-to-read-the-value-of-numpy-empty-or-jax-numpy-empty"
    },
    {
        "title": "How to reorder different sets of parameters in dm-haiku",
        "question": "In dm-haiku, parameters of neural networks are defined in dictionaries where keys are module (and submodule) names. If you would like to traverse through the values, there are multiple ways of doing so as shown in this dm-haiku issue. However, the dictionary doesn't respect the ordering of the modules and makes it hard to parse submodules. For example, if I have 2 linear layers, each followed by a mlp layer, then using hk.data_structures.traverse(params) will (roughly) return:\npython\nCopy\n['linear', 'linear_2', 'mlp/~/1', 'mlp/~/2'].\nwhereas I would like it to return:\npython\nCopy\n['linear', 'mlp/~/1', 'linear_2', 'mlp/~/2'].\nMy reason for wanting this form is if creating an invertible neural network and wanting to reverse the order the params are called, isolating substituent parts for other purposes (e.g. transfer learning), or, in general, wanting more control of how and where to (re)use trained parameters.\nTo deal with this, I've resorted to regex the names and put them in the order that I want, then using hk.data_structures.filter(predicate, params) to filter by the sorted module names. Although, this is quite tedious if I have to remake a regex every time I want to do this.\nI'm wondering if there is a way to convert a dm-haiku dictionary of params to something like a pytree with a hierarchy and ordering that makes this easier? I believe equinox handles parameters in this manner (and I'm going to look more into how that is done soon), but wanted to check to see if I'm overlooking a simple method to allow grouping, reversing, and other permutations of the params's dictionary?",
        "answers": [
            "According to source code https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/filtering.py#L42#L46 haiku use the sorted function of dict (haiku parameters are vanilla dict since 0.0.6) for hk.data_structures.traverse. Therefore you can't get the result you want without modifying the function itself. By the way, I don't get precisely what do you mean by \"to reverse the order the params are called\". All parameters are passed together in input and then the only thing that determines the order of use is the architecture of the function itself so you should manually invert the forward pass but you don't need to change something in params."
        ],
        "link": "https://stackoverflow.com/questions/72860276/how-to-reorder-different-sets-of-parameters-in-dm-haiku"
    },
    {
        "title": "Using JAX with NetworkX",
        "question": "Can I use JIT from JAX with NetworkX algorithms? For instance, if were to compute the average clustering coefficient for a NetworkX graph object, is it possible to use the @jit decorator to speed up my analysis pipeline?",
        "answers": [
            "No, JAX's JIT and other transforms only work with functions implemented via JAX primitives (generally operations defined in jax.lax, jax.numpy, and related submodules). They cannot be used to compile/transform arbitrary Python code."
        ],
        "link": "https://stackoverflow.com/questions/72807939/using-jax-with-networkx"
    },
    {
        "title": "Modify an array from indexes contained in another array",
        "question": "I have an array of the shape (2,10) such as:\npython\nCopy\narr = jnp.ones(shape=(2,10)) * 2\nor\npython\nCopy\n[[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\nand another array, for example [2,4].\nI want the second array to tell from which index the elements of arr should be masked. Here the result would be:\npython\nCopy\n[[2. 2. -1. -1. -1. -1. -1. -1. -1. -1.]\n [2. 2. 2. 2.  -1. -1. -1. -1. -1. -1.]]\nI need to use jax.numpy and the answer to be vectorized and fast if possible, i.e. not using loops.",
        "answers": [
            "You can do this with a vmapped three-term jnp.where statement. For example:\npython\nCopy\nimport jax.numpy as jnp\nimport jax\n\narr = jnp.ones(shape=(2,10)) * 2\nidx = jnp.array([2, 4])\n\n@jax.vmap\ndef f(row, ind):\n  return jnp.where(jnp.arange(len(row)) < ind, row, -1)\n\nf(arr, idx)\n# DeviceArray([[ 2.,  2., -1., -1., -1., -1., -1., -1., -1., -1.],\n#              [ 2.,  2.,  2.,  2., -1., -1., -1., -1., -1., -1.]], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/72553056/modify-an-array-from-indexes-contained-in-another-array"
    },
    {
        "title": "Get dictionary keys for given batched values - Python",
        "question": "I defined a dictionary A and would like to find the keys given a batch of values a:\npython\nCopy\ndef dictionary(r):\n return dict(enumerate(r))\n\ndef get_key(val, my_dict):\n   for key, value in my_dict.items():\n      if np.array_equal(val,value):\n          return key\n    \n\n # dictionary\n A = jnp.array([[0, 0],[1,1],[2,2],[3,3]])\n A = dictionary(A)\n\n a = jnp.array([[[1, 1],[2, 2], [3,3]],[[0, 0],[3, 3], [2,2]]])\n keys = jax.vmap(jax.vmap(get_key, in_axes=(0,None)), in_axes=(0,None))(a, A)\nThe expected output should be: keys = [[1,2,3],[0,3,2]]\nWhy am I getting Noneas an output?",
        "answers": [
            "JAX transforms like vmap work by tracing the function, meaning they replace the value with an abstract representation of the value to extract the sequence of operations encoded in the function (See How to think in JAX for a good intro to this concept).\nWhat this means is that to work correctly with vmap, a function can only use JAX methods, not numpy methods, so your use of np.array_equal breaks the abstraction.\nUnfortunately, there's not really any replacement for it, because there's no mechanism to look up an abstract JAX value in a concrete Python dictionary. If you want to do dict lookups of JAX values, you should avoid transforms and just use Python loops:\npython\nCopy\nkeys = jnp.array([[get_key(x, A) for x in row] for row in a])\nOn the other hand, I suspect this is more of an XY problem; your goal is not to look up dictionary values within a jax transform, but rather to solve some problem. Perhaps you should ask a question about how to solve the problem, rather than how to get around an issue with the solution you have tried.\nBut if you're willing to not directly use the dict, an alternative get_key implementation that is compatible with JAX might look something like this:\npython\nCopy\ndef get_key(val, my_dict):\n  keys = jnp.array(list(my_dict.keys()))\n  values = jnp.array(list(my_dict.values()))\n  return keys[jnp.where((values == val).all(-1), size=1)]"
        ],
        "link": "https://stackoverflow.com/questions/72390628/get-dictionary-keys-for-given-batched-values-python"
    },
    {
        "title": "Get batched indices from stacked matrices - Python Jax",
        "question": "I would like to extract the indices of stacked matrices.\nLet us say we have an array a of dimension (3, 2, 4), meaning that we have three arrays of dimension (2,4) and a list of indices (3, 2).\npython\nCopy\ndef get_cols(x,idx):  \n  x = x[:,idx]\n  return x\n\n\nidx = jnp.array([[0,1],[2,3],[1,2]])\n\na = jnp.array([[[1,2,3,4],\n            [3,2,2,4]],\n           \n           [[100,20,3,50],\n            [5,5,2,4]],\n                         \n           [[1,2,3,4],\n            [3,2,2,4]]\n           ])\n\n\n\ne = jax.vmap(get_cols, in_axes=(None,0))(a,idx)\nI want to extract the columns of the different matrices given a batch of indices. I expect the following result:\npython\nCopy\ne = [[[[1,2],\n  [3,2]],\n\n  [[100,20],\n  [5,5]],\n\n  [[1,2],\n  [3,2]]],\n \n \n \n [[[3,4],\n  [2,4]],\n  \n  [[3,50],\n  [2,4]],\n  \n  [[3,4],\n  [2,4]]],\n \n \n \n \n[[[2,3],\n[2,2]],\n           \n[[20,3],\n [5,2]],\n                         \n[[2,3],\n[2,2]]]]\nWhat am I missing?",
        "answers": [
            "It looks like you're interested in a double vmap over the inputs; e.g. something like this:\npython\nCopy\ne = jax.vmap(jax.vmap(get_cols, in_axes=(0, None)), in_axes=(None, 0))(a, idx)\nprint(e)\npython\nCopy\n[[[[  1   2]\n   [  3   2]]\n\n  [[100  20]\n   [  5   5]]\n\n  [[  1   2]\n   [  3   2]]]\n\n\n [[[  3   4]\n   [  2   4]]\n\n  [[  3  50]\n   [  2   4]]\n\n  [[  3   4]\n   [  2   4]]]\n\n\n [[[  2   3]\n   [  2   2]]\n\n  [[ 20   3]\n   [  5   2]]\n\n  [[  2   3]\n   [  2   2]]]]"
        ],
        "link": "https://stackoverflow.com/questions/72351994/get-batched-indices-from-stacked-matrices-python-jax"
    },
    {
        "title": "JAX pmap with multi-core CPU",
        "question": "What is the correct method for using multiple CPU cores with jax.pmap?\nThe following example creates an environment variable for SPMD on CPU core backends, tests that JAX recognises the devices, and attempts a device lock.\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'\n\nimport jax as jx\nimport jax.numpy as jnp\n\njx.local_device_count()\n# WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n# 2\n\njx.devices(\"cpu\")\n# [CpuDevice(id=0), CpuDevice(id=1)]\n\ndef sfunc(x): while True: pass\n\njx.pmap(sfunc)(jnp.arange(2))\nExecuting from a jupyter kernel and observing htop shows that only one core is locked\nI receive the same output from htop when omitting the first two lines and running:\n$ env XLA_FLAGS=--xla_force_host_platform_device_count=2 python test.py\nReplacing sfunc with\ndef sfunc(x): return 2.0*x\nand calling\njx.pmap(sfunc)(jnp.arange(2))\n# ShardedDeviceArray([0., 2.], dtype=float32, weak_type=True)\ndoes return a SharedDeviecArray.\nClearly I am not correctly configuring JAX/XLA to use two cores. What am I missing and what can I do to diagnose the problem?",
        "answers": [
            "As far as I can tell, you are configuring the cores correctly (see e.g. Issue #2714). The problem lies in your test function:\npython\nCopy\ndef sfunc(x): while True: pass\nThis function gets stuck in an infinite loop at trace-time, not at run-time. Tracing happens in your host Python process on a single CPU (see How to think in JAX for an introduction to the idea of tracing within JAX transformations).\nIf you want to observe CPU usage at runtime, you'll have to use a function that finishes tracing and begins running. For that you could use any long-running function that actually produces results. Here is a simple example:\npython\nCopy\ndef sfunc(x):\n  for i in range(100):\n    x = (x @ x)\n  return x\n\njx.pmap(sfunc)(jnp.zeros((2, 1000, 1000)))"
        ],
        "link": "https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu"
    },
    {
        "title": "JAX return types after transformations",
        "question": "Why is the return type of jax.grad different to other jax transformations in the following scenario?\nConsider a function to be transformed by JAX which takes a custom container as an argument\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom collections import namedtuple\n\n# Define NTContainer using namedtuple factory\nNTContainer = namedtuple(\n    'NTContainer',['name','flag','array']\n)\n\n# Instantiate container\ncontainer = NTContainer(\n   name='composite-container',\n   flag=0,\n   array=jnp.reshape(jnp.arange(6,dtype='f4'),(3,2)),\n)\n\n# Test function\ndef test_func(container : NTContainer):\n    if container.flag == 0:\n        return container.array.sum()\n    return container.array.prod()\nJAX needs to be informed how to handle NTContainer correctly (default namedtuple pytree cannot be used)\npython\nCopy\n# Register NTContainer pytree to handle static and traceable members\n\ndef unpack_NTC(c):\n    active, passive = (c.array,), (c.name, c.flag)\n    return active, passive\n\ndef repack_NTC(passive, active):\n    (name, flag), (array,) = passive, active\n    return NTContainer(name,flag,value)\n\njax.tree_util.register_pytree_node(\n    NTContainer, unpack_NTC, repack_NTC,\n)\nNow performing several jax transforms and calling with container results in\npython\nCopy\njax.jit(test_func)(container)\n# DeviceArray(15., dtype=float32)\n\njax.vmap(test_func)(container)\n# DeviceArray([1., 5., 9.], dtype=float32)\n\njax.grad(test_func)(container)\n# NTContainer(name='composite-container',\n#     flag=0,\n#     array=DeviceArray([[1., 1.],\n#         [1., 1.],\n#         [1., 1.]], dtype=float32))\nWhy does the jax.grad transform call return a NTContainer rather than a DeviceArray?",
        "answers": [
            "The short answer is that this is the return value because this is how grad is defined, and to be honest I'm not sure how else it could be defined.\nThinking about these transforms;\njit(f)(x) computes the output of f applied to x: the result is a scalar.\nvmap(f)(x) computes the output of f applied to each element of the arrays in x: the result is a vector.\ngrad(f)(x) computes the gradient of f with respect to each component of x, and the result is one value per component of x.\nSo it makes sense that grad(f)(x) in this case has to return a collection of multiple values, but how should this collection of gradients be packaged?\nJAX could have defined grad so that it always returns these as a list or tuple, but this might make it hard to understand how the output gradients relate to the inputs, particularly for more complicated structures like nested lists, tuples, and dicts. So instead JAX takes these per-component gradients and re-bundles them in the same structure as the input argument: therefore, in this case the result is a NTContainer containing those per-component gradients."
        ],
        "link": "https://stackoverflow.com/questions/72066135/jax-return-types-after-transformations"
    },
    {
        "title": "Repeating rows from array",
        "question": "I have a problem becase I would like to repeat n time all rows from array(X, Y) without using loops and get array(n*X, Y)\npython\nCopy\nimport jax.numpy as jnp\n\narr = jnp.array([[12, 14, 12, 0, 1],\n                [0, 14, 12, 0, 1],\n                [0, 0, 12, 0, 1]])\nn = 3\n\nresult = jnp.array([[12 14 12 0 1],\n                    [12 14 12 0 1],\n                    [12 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 14 12 0 1],\n                    [0 0 12 0 1],\n                    [0 0 12 0 1],\n                    [0 0 12 0 1]])\nI haven't found any built-in method to perform this operation, tried with jnp.tile, jnp.repeat.\njnp.repeat\npython\nCopy\narr_r = jnp.repeat(arr, n, axis=1)\n\nOutput:\n[[12 12 12 14 14 14 12 12 12  0  0  0  1  1  1]\n [ 0  0  0 14 14 14 12 12 12  0  0  0  1  1  1]\n [ 0  0  0  0  0  0 12 12 12  0  0  0  1  1  1]]\n\narr_t = jnp.tile(arr, n)\n\nOutput:\n[[12 14 12  0  1 12 14 12  0  1 12 14 12  0  1]\n [ 0 14 12  0  1  0 14 12  0  1  0 14 12  0  1]\n [ 0  0 12  0  1  0  0 12  0  1  0  0 12  0  1]]\nMaybe I may construct result array from array_t...",
        "answers": [
            "You say you tried jnp.repeat but don't explain why it doesn't do what you want. I'm guessing you're neglecting the axis parameter:\npython\nCopy\njnp.repeat(arr, n, axis=0)"
        ],
        "link": "https://stackoverflow.com/questions/71974684/repeating-rows-from-array"
    },
    {
        "title": "Parameters do not converge at a lower tolerance in nonlinear least square implementation in python",
        "question": "I am translating some of my R codes to Python as a learning process, especially trying JAX for autodiff.\nIn functions to implement non-linear least square, when I set tolerance at 1e-8, the estimated parameters are nearly identical after several iterations, but the algorithm never appear to converge.\nHowever, the R codes converge at the 12th inter at tol=1e-8 and 14th inter at tol=1e-9. The estimated parameters are almost the same as the ones resulted from Python implementation.\nI think this has something to do with floating point, but not sure which step I could improve to make the converge as quickly as seen in R.\nHere are my codes, and most steps are the same as in R\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport scipy.linalg as ola\n\n\ndef update_parm(X, y, fun, dfun, parm, theta, wt):\n    len_y = len(y)\n    mean_fun = fun(X, parm)\n\n    if (type(wt) == bool):\n        if (wt):\n            var_fun = np.exp(theta * np.log(mean_fun))\n            sqrtW = 1 / np.sqrt(var_fun ** 2)\n        else:\n            sqrtW = 1\n    else:\n        sqrtW = wt\n        \n    gradX = dfun(x, parm)\n    weighted_X = sqrtW.reshape(len_y, 1) * gradX\n    z = gradX @ parm + (y - mean_fun)\n    weighted_z = sqrtW * z\n    qr_gradX = ola.qr(weighted_X, mode=\"economic\")\n    Q = qr_gradX[0]\n    R = qr_gradX[1]\n    new_parm = ola.solve(R, np.dot(Q.T, weighted_z))\n    \n    return new_parm\n\n\ndef nls_irwls(X, y, fun, dfun, init, theta = 1, tol = 1e-8, maxiter = 500):\n\n    old_parm = init\n    iter = 0\n\n    while (iter < maxiter):\n        new_parm = update_parm(X, y, fun, dfun, parm=old_parm, theta=theta, wt=True)\n        parm_diff = np.max(np.abs(new_parm - old_parm) / np.abs(old_parm))\n        print(parm_diff)\n\n        if (parm_diff < tol) :\n            break\n        else:\n            old_parm = new_parm\n            iter += 1\n            print(new_parm)\n\n    if (iter == maxiter):\n        print(\"The algorithm failed to converge\")\n    else:\n        return {\"Estimated coefficient\": new_parm}\n\n\nx = np.array([0.25, 0.5, 0.75, 1, 1.25, 2, 3, 4, 5, 6, 8])\ny = np.array([2.05, 1.04, 0.81, 0.39, 0.30, 0.23, 0.13, 0.11, 0.08, 0.10, 0.06])\n\ndef model(x, W):\n    comp1 = jnp.exp(W[0])\n    comp2 = jnp.exp(-jnp.exp(W[1]) * x)\n    comp3 = jnp.exp(W[2])\n    comp4 = jnp.exp(-jnp.exp(W[3]) * x)\n    return comp1 * comp2 + comp3 * comp4\n\n\ninit = np.array([0.69, 0.69, -1.6, -1.6])\n\n#autodiff\nmodel_grad = jax.jit(jax.jacfwd(model, argnums=1))\n\n#manual derivative\ndef dModel(x, W):\n    e1 = np.exp(W[1])\n    e2 = np.exp(W[3])\n    e5 = np.exp(-(x * e1))\n    e6 = np.exp(-(x * e2))\n    e7 = np.exp(W[0])\n    e8 = np.exp(W[2])\n    b1 = e5 * e7\n    b2 = -(x * e5 * e7 * e1) \n    b3 = e6 * e8 \n    b4 = -(x * e6 * e8 * e2)\n\n    return np.array([b1, b2, b3, b4]).T\n\nnls_irwls(x, y, model, model_grad, init=init, theta=1, tol=1e-8, maxiter=50)\nnls_irwls(x, y, model, dModel, init=init, theta=1, tol=1e-8, maxiter=50)",
        "answers": [
            "One thing to be aware of is that by default, JAX performs computations in 32-bit, while tools like R and numpy perform computations in 64-bit. Since 1E-8 is at the edge of 32-bit floating point precision, I suspect this is why your program is failing to converge.\nYou can enable 64-bit computation by putting this at the beginning of your script:\npython\nCopy\nfrom jax import config\nconfig.update('jax_enable_x64', True)\nAfter doing this, your program converges as expected. For more information, see JAX Sharp Bits: Double Precision."
        ],
        "link": "https://stackoverflow.com/questions/71902257/parameters-do-not-converge-at-a-lower-tolerance-in-nonlinear-least-square-implem"
    },
    {
        "title": "What is the best way to index 3d matrix with vectors?",
        "question": "python\nCopy\nimport jax.numpy as jnp\nvectors and array are jnp.array(dtype=jnp.int32)\nI have an array with shape [x, d, y] (3x3x3)\npython\nCopy\n[[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]]]\nand vectors x = [2 0 3], y = [ 2 0 1], d = [0 0 1]\nI want to have something like this by indexing but I tried and don't really know how, with jax.numpy.\npython\nCopy\n[[[0 0 2],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 0 0],\n [0 0 0]],\n\n[[0 0 0],\n [0 3 0],\n [0 0 0]]]\nEdit: I would like to specify that I wanted to put number from x with its index to the array but only when x > 0. I tried with boolean mask. Something like this\npython\nCopy\nmask = x > 0\narray = array.at[mask, d, y].set(array[mask, d, y] + x)",
        "answers": [
            "You have a three-dimensional array, so you can index it with three arrays of indices. Since you want d and y to be associated with the second and third dimensions, you'll need to create another array of indices for the first dimension:\npython\nCopy\nimport jax.numpy as jnp\n\narr = jnp.zeros((3, 3, 3), dtype='int32')\nx = jnp.array([2, 0, 3])\ny = jnp.array([2, 0, 1])\nd = jnp.array([0, 0, 1])\n\ni = jnp.arange(len(x))\nmask = x > 0\n\nout = arr.at[i[mask], d[mask], y[mask]].set(x[mask])\nprint(out)\n# [[[0 0 2]\n#   [0 0 0]\n#   [0 0 0]]\n\n#  [[0 0 0]\n#   [0 0 0]\n#   [0 0 0]]\n\n#  [[0 0 0]\n#   [0 3 0]\n#   [0 0 0]]]\nIn this case the result will be the same whether or not you use the mask (i.e. arr.at[i, d, y].set(x) will give the same result) but because your question explicitly specified that you only want to use values x > 0 I included it."
        ],
        "link": "https://stackoverflow.com/questions/71880876/what-is-the-best-way-to-index-3d-matrix-with-vectors"
    },
    {
        "title": "DenseElementsAttr could not be constructed from the given buffer",
        "question": "I try to run a code written with JAX. At one part of the code, key for training set is defined as\nkey_train = random.PRNGKey(0).\nHere the type of the key jaxlib.xla_extension.DeviceArray. Then in the following part, keys are defined as keys = random.split(key_train, N). Here N is an integer which is equal to 10000. At that part of the code it gives an error like:\nDenseElementsAttr could not be constructed from the given buffer. This may mean that the Python buffer layout does not match that MLIR expected layout and is a bug.\nCould you please help me about the error?\nEdit: I try to run the code on Win10. Here (https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets/blob/main/Antiderivative/DeepONet_antideriv.ipynb) you can find the code that I try to run. For simplicity you can try to run the code below as well. You will get the exact same error.\nfrom jax import random\nN=10000\nkey_train=random.PRNGKey(0)\nkeys=random.split(key_train, N)\nJax and Jaxlib versions are 0.3.5 with cuda 11",
        "answers": [
            "I had the same error. Deleting c:\\python37\\lib\\site-packages\\jaxlib\\cuda_prng.py fixed the issue (replace the prefix by your python path). It could be cuda_prng.py was an old file.",
            "By following Erwin Coumans's advice and indeed the mistake disappeared. The version installed on Windows is ( jaxlib 0.3.7 with cuda 11.3 and cudnn 8.2 , on anaconda python 3.9.0) the latest found on : https://whls.blob.core.windows.net/unstable/index.html"
        ],
        "link": "https://stackoverflow.com/questions/71827814/denseelementsattr-could-not-be-constructed-from-the-given-buffer"
    },
    {
        "title": "JAX(XLA) vs Numba(LLVM) Reduction",
        "question": "Is it possible to make CPU only reductions with JAX comparable to Numba in terms of computation time?\nThe compilers come straight from conda:\npython\nCopy\n$ conda install -c conda-forge numba jax\nHere is a 1-d NumPy array example\npython\nCopy\nimport numpy as np\nimport numba as nb\nimport jax as jx\n\n@nb.njit\ndef reduce_1d_njit_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\n@jx.jit\ndef reduce_1d_jax_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\nN = 2**10\na = np.random.randn(N)\nUsing timeit on the following\nnp.add.reduce(a) gives 1.99 µs ...\nreduce_1d_njit_serial(a) gives 1.43 µs ...\nreduce_1d_jax_serial(a).item() gives 23.5 µs ...\nNote that jx.numpy.sum(a) and using jx.lax.fori_loop gives comparable (marginally slower) comp. times to reduce_1d_jax_serial.\nIt seems there is a better way to craft the reduction for XLA.\nEDIT: compile times were not included as a print statement proceeded to check results.",
        "answers": [
            "When performing these kinds of microbenchmarks with JAX, you have to be careful to ensure you're measuring what you think you're measuring. There are some tips in the JAX Benchmarking FAQ. Implementing some of these best practices, I find the following for your benchmarks:\npython\nCopy\nimport jax.numpy as jnp\n\n# Native jit-compiled XLA sum\njit_sum = jx.jit(jnp.sum)\n\n# Avoid including device transfer cost in the benchmarks\na_jax = jnp.array(a)\n\n# Prevent measuring compilation time\n_ = reduce_1d_njit_serial(a)\n_ = reduce_1d_jax_serial(a_jax)\n_ = jit_sum(a_jax)\n\n%timeit np.add.reduce(a)\n# 100000 loops, best of 5: 2.33 µs per loop\n\n%timeit reduce_1d_njit_serial(a)\n# 1000000 loops, best of 5: 1.43 µs per loop\n\n%timeit reduce_1d_jax_serial(a_jax).block_until_ready()\n# 100000 loops, best of 5: 6.24 µs per loop\n\n%timeit jit_sum(a_jax).block_until_ready()\n# 100000 loops, best of 5: 4.37 µs per loop\nYou'll see that for these microbenchmarks, JAX is a few milliseconds slower than both numpy and numba. So does this mean JAX is slow? Yes and no; you'll find a more complete answer to that question in JAX FAQ: is JAX faster than numpy?. The short summary is that this computation is so small that the differences are dominated by Python dispatch time rather than time spent operating on the array. The JAX project has not put much effort into optimizing for Python dispatch of microbenchmarks: it's not all that important in practice because the cost is incurred once per program in JAX, as opposed to once per operation in numpy."
        ],
        "link": "https://stackoverflow.com/questions/71701041/jaxxla-vs-numballvm-reduction"
    },
    {
        "title": "jax: sample many observations from random.choice with replacement between them",
        "question": "I'd like to pick two indices out of an array. These indices must not be the same. One such sample can be obtained with:\nrandom.choice(next(key), num_items, (2,), replace=False)\nFor performance reasons, I'd like to batch the sampling:\nnum_samples = 100\nsamples = random.choice(next(key), num_items, (num_samples, 2), replace=False)\nThis doesn't work because of replace=False. It raises the error:\nValueError: Cannot take a larger sample than population when 'replace=False'\nFor each new sample, I'd like to have replace=True. Within one sample, I'd like to have replace=False. Is there a way to do this?\nThe next(key) in my random sampling is syntactic sugar. I'm using this snippet for convenience:\ndef reset_key(seed=42):\n    key = random.PRNGKey(seed)\n    while True:\n        key, subkey = random.split(key)\n        yield subkey\n        \nkey = reset_key()",
        "answers": [
            "The best way to do this is using jax.vmap to map across individual samples. For example:\npython\nCopy\nfrom jax import random, vmap\n\ndef sample_two(key, num_items):\n  return random.choice(key , num_items, (2,), replace=False)\n\nkey = random.PRNGKey(0)\nnum_samples = 10\nnum_items = 5\n\nkey_array = random.split(key, num_samples)\nprint(vmap(sample_two, in_axes=(0, None))(key_array, num_items))\n# [[2 0]\n#  [1 4]\n#  [2 1]\n#  [3 4]\n#  [4 2]\n#  [2 0]\n#  [1 3]\n#  [2 1]\n#  [1 0]\n#  [2 4]]\nFor more information on jax.vmap, see Automatic Vectorization in JAX."
        ],
        "link": "https://stackoverflow.com/questions/71679151/jax-sample-many-observations-from-random-choice-with-replacement-between-them"
    },
    {
        "title": "How to print with JAX",
        "question": "I have a JAX Boolean array and want to print a statement combined with sum of Trues:\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.host_callback import id_print\n\n@jax.jit\ndef overlaps_jax():\n    mask_cp = jnp.array([True, False, False, True, False, True, False, True, True])\n    id_print(jnp.sum(mask_cp))\n\noverlaps_jax()\nThere are 5 Trues in the mask_cp; I want to print as:\nWith jax accelerator\nThere are 5 true bools\nAs this function is jitted, I tried to print this by using id_print, but I couldn't. id_print(jnp.sum(mask_cp)) will print 5, but I couldn't use this with the strings. I have tried the followings:\nid_print(jnp.sum(mask_cp))\n# print:\n# 5\n\nid_print(\"\\nWith jax accelerator\\nThere are \" + jnp.sum(mask_cp) + \" true bools\\n\")\n# error:\n# TypeError: can only concatenate str (not \"DynamicJaxprTracer\") to str\n\nprint(\"\\nWith jax accelerator\\nThere are {} true bools\\n\".format(jnp.sum(mask_cp)))\n# print:\n# With jax accelerator\n# There are Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)> true bools\nHow could I print such statements in this code?",
        "answers": [
            "Please note that id_print is experimental, and its API and capabilities are subject to change. That said, I don't believe id_print has the capability to add text like this, but you can do it via a more general host_callback.call:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.host_callback import call\n\n@jax.jit\ndef overlaps_jax():\n    mask_cp = jnp.array([True, False, False, True, False, True, False, True, True])\n    call(lambda x: print(f\"There are {x} true bools\"), jnp.sum(mask_cp))\n\noverlaps_jax()\nThe output is\nThere are 5 true bools"
        ],
        "link": "https://stackoverflow.com/questions/71548823/how-to-print-with-jax"
    },
    {
        "title": "vmap gives inconsistent shape error when trying to calculate gradient per sample",
        "question": "I am trying to implement a two layer neural network and get the gradient of the second layer per sample.\nMy code looks like this:\npython\nCopy\nx = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.float32)\ny = jnp.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=jnp.float32)\n\nW1 = jnp.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=jnp.float32)\nW2 = random.uniform(key, shape=(10,), minval=1, maxval=2, dtype=jnp.float32)\nb = jnp.linspace(0, -9, 10, dtype=jnp.float32)\nb = jnp.reshape(b, (1,10))\n\ndef predict(W1, W2, b, x):\n  f1 = jnp.einsum('i,j->ji', W1, x)+b\n  f1 = nn.relu(f1)\n  f2 = jnp.einsum('i,ji->j', W2, f1)\n  return f2\n\ndef loss(W1, W2, b, x, y):\n  preds = predict(W1, W2, b, x)\n  return jnp.mean(jnp.square(y-preds))\n\nperex_grads = vmap(grad(loss, argnums=1), in_axes= (0, None, 0, 0, 0))\npers_grads = perex_grads(W1, W2, b, x, y)\nI ran loss and can do grad(loss) just fine. Running vmap is the actual problem.\nThe exact error I get is:\npython\nCopy\nValueError: vmap got inconsistent sizes for array axes to be mapped:\narg 0 has shape (10,) and axis 0 is to be mapped\narg 1 has shape (10,) and axis None is to be mapped\narg 2 has shape (1, 10) and axis 0 is to be mapped\narg 3 has shape (11,) and axis 0 is to be mapped\narg 4 has shape (11,) and axis 0 is to be mapped\nso\narg 0 has an axis to be mapped of size 10\narg 2 has an axis to be mapped of size 1\nargs 3, 4 have axes to be mapped of size 11\nIt is my first time using Jax, and my google search didn't help me resolve the issue, plus the documentation was not very clear to me. I'd appreciate if anyone can help me.",
        "answers": [
            "The issue is exactly what the error message says: in order to vmap an operation over multiple arrays, the dimension of the mapped axes in each array must be equal. In your arrays, the dimensions are not equal: you passed in_axes=(0, None, 0, 0, 0) for arguments W1, W2, b, x, y, but W1.shape[0] = 10, b.shape[0] = 1, x.shape[0] = 11, and y.shape[0] = 11.\nBecause these are not equal, you get this error. To prevent this error, you should only vmap over array axes of the same length.\nFor example, if you want the gradients with respect to W2 computed per pair of W1, W2 inputs, it might look something like this (note the updated predict function and updated in_axes):\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import random, nn, grad, vmap\n\nkey = random.PRNGKey(0)\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=jnp.float32)\ny = jnp.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=jnp.float32)\n\nW1 = jnp.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=jnp.float32)\nW2 = random.uniform(key, shape=(10,), minval=1, maxval=2, dtype=jnp.float32)\nb = jnp.linspace(0, -9, 10, dtype=jnp.float32)\nb = jnp.reshape(b, (1,10))\n\ndef predict(W1, W2, b, x):\n  # Since you're vmapping over W1 and W2, your function needs to\n  # handle scalar values, so we cast to 1D if necessary.\n  W1 = jnp.atleast_1d(W1)\n  W2 = jnp.atleast_1d(W2)\n\n  f1 = jnp.einsum('i,j->ji', W1, x)+b\n  f1 = nn.relu(f1)\n  f2 = jnp.einsum('i,ji->j', W2, f1)\n  return f2\n\ndef loss(W1, W2, b, x, y):\n  preds = predict(W1, W2, b, x)\n  return jnp.mean(jnp.square(y-preds))\n\nperex_grads = vmap(grad(loss, argnums=1), in_axes= (0, 0, None, None, None))\npers_grads = perex_grads(W1, W2, b, x, y)"
        ],
        "link": "https://stackoverflow.com/questions/71168030/vmap-gives-inconsistent-shape-error-when-trying-to-calculate-gradient-per-sample"
    },
    {
        "title": "Merge dataclasses in python",
        "question": "I have a dataclass like:\npython\nCopy\nimport dataclasses\nimport jax.numpy as jnp\n\n@dataclasses.dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\nIn my code, I create multiple instances of it, is there an easy way to merge two of them attribute per attribute? For example if I have:\npython\nCopy\na = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nb = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nI'd like to merge them such as having a single Metric containing:\nscore1=jnp.array([10,10,10,10,10,10]), score2=jnp.array([20,20,20,20,20,20]) and score3=jnp.array([30,30,30,30,30,30])",
        "answers": [
            "It is possible to do so in a \"jax-centric\" manner by registering the class Metric as a pytree_node. google/flax, a neural network library built on top of jax, provides the flax.struct.dataclass helper to do so. Once registered, you can use the jax.tree_util package to manipulate Metric instances:\npython\nCopy\nfrom flax.struct import dataclass as flax_dataclass\nfrom jax.tree_util import tree_multimap\nimport jax.numpy as jnp\n\n@flax_dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\n\na = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\nb = Metric(score1=jnp.array([10,10,10]), score2=jnp.array([20,20,20]), score3=jnp.array([30,30,30]))\n\ntree_multimap(lambda x, y: jnp.concatenate([x, y]), a, b)\nGives:\npython\nCopy\nMetric(score1=DeviceArray([10, 10, 10, 10, 10, 10], dtype=int32), score2=DeviceArray([20, 20, 20, 20, 20, 20], dtype=int32), score3=DeviceArray([30, 30, 30, 30, 30, 30], dtype=int32))",
            "The easiest thing is probably just to define a method:\npython\nCopy\nimport dataclasses\nimport jax.numpy as jnp\n\n\n@dataclasses.dataclass\nclass Metric:\n    score1: jnp.ndarray\n    score2: jnp.ndarray\n    score3: jnp.ndarray\n\n    def concatenate(self, other):\n        return Metric(\n            jnp.concatenate((self.score1, other.score1)),\n            jnp.concatenate((self.score2, other.score2)),\n            jnp.concatenate((self.score3, other.score3)),\n        )\nand then just do a.concatenate(b). You could also instead call the method __add__, which would make it possible just to use a + b. This is neater, but could potentially be confused with element-wise addition."
        ],
        "link": "https://stackoverflow.com/questions/71109587/merge-dataclasses-in-python"
    },
    {
        "title": "Resulting array is zero when jax.numpy is used",
        "question": "I wrote the code below using numpy and got the correct output as shown in program 1. However when I switch to jax.numpy as jnp (in Program 2) the resulting output is an array of zeros. My MWE is shown below. I would like to know where I got the computation wrong? PS: the codes were run in different python files.\npython\nCopy\n#Program 1 (using numpy as np):\nimport numpy as np\n\nnum_rows = 5\nnum_cols = 20\nsmf = np.array([np.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = np.array([1,2,3,4,5])\nlb = np.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = np.array([10, 10, 10, 10, 10])\npar = np.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = np.where(np.isinf(smf), 1, num_cols)\nkvals = np.insert(kvals, 0, 0)\nkvals = np.cumsum(kvals)\n\npar0_col = np.zeros(num_rows*num_cols - (num_cols-1) * np.sum(np.isinf(smf)))\nlb_col = np.zeros(num_rows*num_cols - (num_cols-1) * np.sum(np.isinf(smf)))\nub_col = np.zeros(num_rows*num_cols- (num_cols-1) * np.sum(np.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col[kvals[i]:kvals[i+1]] = par[i, :kvals[i+1]-kvals[i]]\n    lb_col[kvals[i]:kvals[i+1]] = lb[i]\n    ub_col[kvals[i]:kvals[i+1]] = ub[i]\n\narr_1 = np.zeros(shape = (num_rows, num_cols))\narr_2 = np.zeros(shape = (num_rows, num_cols))\n\n\npar_log = np.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\nk = 0\nfor i in range(num_rows):\n\n    arr_1[i, :] = (par_log[kvals[i]:kvals[i+1]])\n    arr_2[i, :] = 10**par_log[kvals[i]:kvals[i+1]]\n  \n\nprint(arr_1)\n\n# [[0.         0.         0.         0.         0.         0.\n#   0.         0.         0.         0.         0.         0.\n#   0.         0.         0.         0.         0.         0.\n#   0.         0.        ]\n#  [0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361\n#   0.37566361 0.37566361]\n#  [0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996 0.61729996 0.61729996 0.61729996 0.61729996\n#   0.61729996 0.61729996]\n#  [0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336 0.81291336 0.81291336 0.81291336 0.81291336\n#   0.81291336 0.81291336]\n#  [0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608 0.99122608 0.99122608 0.99122608 0.99122608\n#   0.99122608 0.99122608]]\n\n# Program 2 (using jax.numpy as jnp):\n\nimport jax\nimport jax.numpy as jnp\njax.config.update(\"jax_enable_x64\", True)\n\nsmf = jnp.array([jnp.inf, 0.1, 0.1, 0.1, 0.1])\npar_init = jnp.array([1.0,2.0,3.0,4.0,5.0])\nlb = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1])\nub = jnp.array([10.0, 10.0, 10.0, 10.0, 10.0])\npar = jnp.broadcast_to(par_init[:,None],(num_rows,num_cols))\n\nkvals = jnp.where(jnp.isinf(smf), 1, num_cols)\nkvals = jnp.insert(kvals, 0, 0)\nkvals = jnp.cumsum(kvals)\n\npar0_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nlb_col = jnp.zeros(num_rows*num_cols - (num_cols-1) * jnp.sum(jnp.isinf(smf)))\nub_col = jnp.zeros(num_rows*num_cols- (num_cols-1) * jnp.sum(jnp.isinf(smf)))\n\n\n\nfor i in range(num_rows):\n    par0_col.at[kvals[i]:kvals[i+1]].set(par[i, :kvals[i+1]-kvals[i]])\n    lb_col.at[kvals[i]:kvals[i+1]].set(lb[i])\n    ub_col.at[kvals[i]:kvals[i+1]].set(ub[i])\n\narr_1 = jnp.zeros(shape = (num_rows, num_cols))\narr_2 = jnp.zeros(shape = (num_rows, num_cols))\n\n\npar_log = jnp.log10((par0_col - lb_col) / (1 - par0_col / ub_col))\n\n\nfor i in range(num_rows):\n \n    arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\n    arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\n  \n\nprint(arr_1)\n\n# #[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]",
        "answers": [
            "The issue is that ndarray.at expressions don't operate in-place, but rather return a modified value.\nSo instead of this:\npython\nCopy\narr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\narr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\nYou should write this:\npython\nCopy\narr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))\narr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])\nRead more at JAX sharp bits: in-place updates.",
            "Oh I already figured it out. I needed to make an explicit assignment."
        ],
        "link": "https://stackoverflow.com/questions/71016283/resulting-array-is-zero-when-jax-numpy-is-used"
    },
    {
        "title": "Compute efficiently Hessian matrices in JAX",
        "question": "In JAX's Quickstart tutorial I found that the Hessian matrix can be computed efficiently for a differentiable function fun using the following lines of code:\nfrom jax import jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\nHowever, one can compute the Hessian also by computing the following:\ndef hessian(fun):\n  return jit(jacrev(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacfwd(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacrev(jacrev(fun)))\nHere is a minimal working example:\nimport jax.numpy as jnp\nfrom jax import jit\nfrom jax import jacfwd, jacrev\n\ndef comp_hessian():\n\n    x = jnp.arange(1.0, 4.0)\n\n    def sum_logistics(x):\n        return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\n    def hessian_1(fun):\n        return jit(jacfwd(jacrev(fun)))\n\n    def hessian_2(fun):\n        return jit(jacrev(jacfwd(fun)))\n\n    def hessian_3(fun):\n        return jit(jacrev(jacrev(fun)))\n\n    def hessian_4(fun):\n        return jit(jacfwd(jacfwd(fun)))\n\n    hessian_fn = hessian_1(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_2(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_3(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_4(sum_logistics)\n    print(hessian_fn(x))\n\n\ndef main():\n    comp_hessian()\n\n\nif __name__ == \"__main__\":\n    main()\nI would like to know which approach is best to use and when? I also would like to know if it is possible to use grad() to compute the Hessian? And how does grad() differ from jacfwd and jacrev?",
        "answers": [
            "The answer to your question is within the JAX documentation; see for example this section: https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev\nTo quote its discussion of jacrev and jacfwd:\nThese two functions compute the same values (up to machine numerics), but differ in their implementation: jacfwd uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while jacrev uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, jacfwd probably has an edge over jacrev.\nand further down,\nTo implement hessian, we could have used jacfwd(jacrev(f)) or jacrev(jacfwd(f)) or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function 𝑓:ℝⁿ→ℝ), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since ∇𝑓:ℝⁿ→ℝⁿ), which is where forward-mode wins out.\nSince your function looks like 𝑓:ℝⁿ→ℝ, then jit(jacfwd(jacrev(fun))) is likely the most efficient approach.\nAs for why you can't implement a hessian with grad, this is because grad is only designed for derivatives of functions with scalar outputs. A hessian by definition is a composition of vector-valued jacobians, not a composition of scalar gradients."
        ],
        "link": "https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax"
    },
    {
        "title": "in_axes keyword in JAX's vmap",
        "question": "I'm trying to understand JAX's auto-vectorization capabilities using vmap and implemented a minimal working example based on JAX's documentation.\nI don't understand how in_axes is used correctly. In the example below I can set in_axes=(None, 0) or in_axes=(None, 1) leading to the same results. Why is that the case?\nAnd why do I have to use in_axes=(None, 0) and not something like in_axes=(0, )?\nimport jax.numpy as jnp\nfrom jax import vmap\n\n\ndef predict(params, input_vec):\n    assert input_vec.ndim == 1\n    activations = input_vec\n    for W, b in params:\n        outputs = jnp.dot(W, activations) + b\n        activations = jnp.tanh(outputs)\n    return outputs\n\n\nif __name__ == \"__main__\":\n\n    # Parameters\n    dims = [2, 3, 5]\n    input_dims = dims[0]\n    batch_size = 2\n\n    # Weights\n    params = list()\n    for dims_in, dims_out in zip(dims, dims[1:]):\n        params.append((jnp.ones((dims_out, dims_in)), jnp.ones((dims_out,))))\n\n    # Input data\n    input_batch = jnp.ones((batch_size, input_dims))\n\n    # With vmap\n    predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n    print(predictions)",
        "answers": [
            "in_axes=(None, 0) means that the first argument (here params) will not be mapped, while the second argument (here input_vec) will be mapped along axis 0.\nIn the example below I can set in_axes=(None, 0) or in_axes=(None, 1) leading to the same results. Why is that the case?\nThis is because input_vec is a 2x2 matrix of ones, so whether you map along axis 0 or axis 1, the input vectors are length-2 vectors of ones. In more general cases, the two specifications are not equivalent, which you can see by either (1) making batch_size differ from input_dims[0], or (2) filling your arrays with non-constant values.\nwhy do I have to use in_axes=(None, 0) and not something like in_axes=(0, )?\nIf you set in_axes=(0, ) for a function with two arguments, you get an error because the length of the in_axes tuple must match the number of arguments passed to the function. That said, it is possible to pass a scalar in_axes=0 as a shorthand for in_axes=(0, 0), but for your function this would lead to a shape error because the leading dimension of the arrays in params does not match the leading dimension of input_vec."
        ],
        "link": "https://stackoverflow.com/questions/70564419/in-axes-keyword-in-jaxs-vmap"
    },
    {
        "title": "Is there a module to convert a tensorflow NN to Jax?",
        "question": "There is a libary to convert Jax functions to Tensorflow functions. Is there a similar library to convert TensorFlow functions to Jax functions?",
        "answers": [
            "No, there is no library supported by the JAX team to convert tensorflow into JAX in a manner similar to how jax.experimental.jax2tf converts JAX code to tensorflow, and I have not seen any such library developed by others.",
            "See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md.\n\"jax2tf.call_tf: for using TensorFlow functions in a JAX context, e.g., to call a TensorFlow library or a SavedModel inside a JAX function.\"\nThat is what you need. So you can call tf function under jax context.\nfor example:\npython\nCopy\n# Compute cos with TF and sin with JAX\ndef cos_tf_sin_jax(x):\n  return jax.numpy.sin(jax2tf.call_tf(cos_tf)(x))",
            "Is https://github.com/google-deepmind/tf2jax what you were looking for? It only works for TF v2 though.",
            "To my knowledge there is no library similar to the one you mentioned to convert TensorFlow functions to Jax functions. I'm sorry"
        ],
        "link": "https://stackoverflow.com/questions/70356126/is-there-a-module-to-convert-a-tensorflow-nn-to-jax"
    },
    {
        "title": "Python - time difference (JAX library)",
        "question": "I'm trying to compare execution times between functions:\nsimpleExponentialSmoothing - which is my implementation of SES in JAX library\nsimpleExponentialSmoothingJax - as above, but boosted with JIT from JAX library\nSimpleExpSmoothing - implementation from Statsmodels library\nI have tried using %timeit, time and writing my own function to measure time using datetime, however I'm quite confused. My function to measure time and %timeit are returning the same exec time, however %time is showing much, much different exec time. I have found that %time checks only single run and is less accurate than %timeit, but how does it apply to asynchronous functions like those in JAX? Although I've blocked them until finishing calculations, I'm not sure if that is enough.\nI need advice about this measure, which should I take as actual execution time?\n%time\npython\nCopy\n%time timeSeriesSes = simpleExponentialSmoothing(params, timeSeries, initState).block_until_ready()\n%time timeSeriesSesJit = simpleExponentialSmoothingJit(params, timeSeries, initState).block_until_ready()\n%time timeSeriesSesSm = SimpleExpSmoothing(timeSeries).fit()\npython\nCopy\nCPU times: user 82.4 ms, sys: 4.03 ms, total: 86.4 ms\nWall time: 97.6 ms\nCPU times: user 199 µs, sys: 0 ns, total: 199 µs\nWall time: 214 µs\nCPU times: user 6.12 ms, sys: 0 ns, total: 6.12 ms\nWall time: 6.2 ms\n%timeit\npython\nCopy\n%timeit timeSeriesSes = simpleExponentialSmoothing(params, timeSeries, initState).block_until_ready()\n%timeit timeSeriesSesJit = simpleExponentialSmoothingJit(params, timeSeries, initState).block_until_ready()\n%timeit timeSeriesSesSm = SimpleExpSmoothing(timeSeries).fit()\npython\nCopy\n48.8 ms ± 904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n15.5 µs ± 222 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n3.4 ms ± 62.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)",
        "answers": [
            "For your JAX-specific question: using block_until_ready() should be enough to account for JAX's asynchronous execution.\nBe careful also about JIT compilation: the first time you call a JIT-compiled function with arguments of a particular shape, the compilation time will affect the speed of execution. After that, the cached compilation will be used.\nAs to your more general question: the difference between %timeit and %time is covered in the IPython docs:\nBy default, timeit() temporarily turns off garbage collection during the timing. The advantage of this approach is that it makes independent timings more comparable. The disadvantage is that GC may be an important component of the performance of the function being measured.\n(From https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit)\nSo if you want to measure performance with Python garbage collection, and with only a single execution, use %time. If you want to measure performance without Python garbage collection, and with multiple executions for more statistical rigor, use %timeit."
        ],
        "link": "https://stackoverflow.com/questions/70161804/python-time-difference-jax-library"
    },
    {
        "title": "JAX: avoid just-in-time recompilation for a function evaluated with a varying number of elements along one axis",
        "question": "Is it possible to avoid recompiling a JIT function when the structure of its input remains essentially unchanged, aside from one axis having a varying number of elements?\npython\nCopy\nimport jax\n\n@jax.jit\ndef f(x):\n    print('recompiling')\n    return (x + 10) * 100\n\na = f(jax.numpy.arange(300000000).reshape((-1, 2, 2)).block_until_ready()) # recompiling\nb = f(jax.numpy.arange(300000000).reshape((-1, 2, 2)).block_until_ready())\nc = f(jax.numpy.arange(450000000).reshape((-1, 2, 2)).block_until_ready()) # recompiling. It would be nice if it weren't\nRequirements: pip install jax, jaxlib",
        "answers": [
            "No, there is no way to avoid recompilation when you call a function with arrays of a different shape. Fundamentally, JAX compiles functions for statically-shaped inputs and outputs, and calling a JIT-compiled function with an array of a new shape will always trigger re-compilation.\nThere is some ongoing work on relaxing this requirement (search \"dynamic shapes\" in JAX's github repository) but no such APIs are available at the moment."
        ],
        "link": "https://stackoverflow.com/questions/70126391/jax-avoid-just-in-time-recompilation-for-a-function-evaluated-with-a-varying-nu"
    },
    {
        "title": "Turn a tf.data.Dataset to a jax.numpy iterator",
        "question": "I am interested about training a neural network using JAX. I had a look on tf.data.Dataset, but it provides exclusively tf tensors. I looked for a way to change the dataset into JAX numpy array and I found a lot of implementations that use Dataset.as_numpy_generator() to turn the tf tensors to numpy arrays. However I wonder if it is a good practice, as numpy arrays are stored in CPU memory and it is not what I want for my training (I use the GPU). So the last idea I found is to manually recast the arrays by calling jnp.array but it is not really elegant (I am afraid about the copy in GPU memory). Does anyone have a better idea for that?\nQuick code to illustrate:\npython\nCopy\nimport os\nimport jax.numpy as jnp\nimport tensorflow as tf\n\ndef generator():\n    for _ in range(2):\n        yield tf.random.uniform((1, ))\n\nds = tf.data.Dataset.from_generator(generator, output_types=tf.float32,\n                                    output_shapes=tf.TensorShape([1]))\n\nds1 = ds.take(1).as_numpy_iterator()\nds2 = ds.skip(1)\n\nfor i, batch in enumerate(ds1):\n    print(type(batch))\n\nfor i, batch in enumerate(ds2):\n    print(type(jnp.array(batch)))\n\n# returns:\n\n<class 'numpy.ndarray'> # not good\n<class 'jaxlib.xla_extension.DeviceArray'> # good but not elegant",
        "answers": [
            "Both tensorflow and JAX have the ability to convert arrays to dlpack tensors without copying memory, so one way you can create a JAX array from a tensorflow array without copying the underlying data buffer is to do it via dlpack:\nimport numpy as np\nimport tensorflow as tf\nimport jax.dlpack\n\ntf_arr = tf.random.uniform((10,))\ndl_arr = tf.experimental.dlpack.to_dlpack(tf_arr)\njax_arr = jax.dlpack.from_dlpack(dl_arr)\n\nnp.testing.assert_array_equal(tf_arr, jax_arr)\nBy doing the round-trip to JAX, you can compare unsafe_buffer_pointer() to ensure that the arrays point at the same buffer, rather than copying the buffer along the way:\ndef tf_to_jax(arr):\n  return jax.dlpack.from_dlpack(tf.experimental.dlpack.to_dlpack(arr))\n\ndef jax_to_tf(arr):\n  return tf.experimental.dlpack.from_dlpack(jax.dlpack.to_dlpack(arr))\n\njax_arr = jnp.arange(20.)\ntf_arr = jax_to_tf(jax_arr)\njax_arr2 = tf_to_jax(tf_arr)\n\nprint(jnp.all(jax_arr == jax_arr2))\n# True\nprint(jax_arr.unsafe_buffer_pointer() == jax_arr2.unsafe_buffer_pointer())\n# True",
            "From Flax example:\nhttps://github.com/google/flax/blob/6ae22681ef6f6c004140c3759e7175533bda55bd/examples/imagenet/train.py#L183\npython\nCopy\ndef prepare_tf_data(xs):\n  local_device_count = jax.local_device_count()\n  def _prepare(x):\n    x = x._numpy() \n    return x.reshape((local_device_count, -1) + x.shape[1:])\n  return jax.tree_util.tree_map(_prepare, xs)\n\nit = map(prepare_tf_data, ds)\nit = jax_utils.prefetch_to_device(it, 2)"
        ],
        "link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator"
    },
    {
        "title": "Jax/Flax (very) slow RNN-forward-pass compared to pyTorch?",
        "question": "I recently implemented a two-layer GRU network in Jax and was disappointed by its performance (it was unusable).\nSo, i tried a little speed comparison with Pytorch.\nMinimal working example\nThis is my minimal working example and the output was created on Google Colab with GPU-runtime. notebook in colab\npython\nCopy\nimport flax.linen as jnn \nimport jax\nimport torch\nimport torch.nn as tnn\nimport numpy as np \nimport jax.numpy as jnp\n\ndef keyGen(seed):\n    key1 = jax.random.PRNGKey(seed)\n    while True:\n        key1, key2 = jax.random.split(key1)\n        yield key2\nkey = keyGen(1)\n\nhidden_size=200\nseq_length = 1000\nin_features = 6\nout_features = 4\nbatch_size = 8\n\nclass RNN_jax(jnn.Module):\n\n    @jnn.compact\n    def __call__(self, x, carry_gru1, carry_gru2):\n        carry_gru1, x = jnn.GRUCell()(carry_gru1, x)\n        carry_gru2, x = jnn.GRUCell()(carry_gru2, x)\n        x = jnn.Dense(4)(x)\n        x = x/jnp.linalg.norm(x)\n        return x, carry_gru1, carry_gru2\n\nclass RNN_torch(tnn.Module):\n    def __init__(self, batch_size, hidden_size, in_features, out_features):\n        super().__init__()\n\n        self.gru = tnn.GRU(\n            input_size=in_features, \n            hidden_size=hidden_size,\n            num_layers=2\n            )\n        \n        self.dense = tnn.Linear(hidden_size, out_features)\n\n        self.init_carry = torch.zeros((2, batch_size, hidden_size))\n\n    def forward(self, X):\n        X, final_carry = self.gru(X, self.init_carry)\n        X = self.dense(X)\n        return X/X.norm(dim=-1).unsqueeze(-1).repeat((1, 1, 4))\n\nrnn_jax = RNN_jax()\nrnn_torch = RNN_torch(batch_size, hidden_size, in_features, out_features)\n\nXj = jax.random.normal(next(key), (seq_length, batch_size, in_features))\nYj = jax.random.normal(next(key), (seq_length, batch_size, out_features))\nXt = torch.from_numpy(np.array(Xj))\nYt = torch.from_numpy(np.array(Yj))\n\ninitial_carry_gru1 = jnp.zeros((batch_size, hidden_size))\ninitial_carry_gru2 = jnp.zeros((batch_size, hidden_size))\n\nparams = rnn_jax.init(next(key), Xj[0], initial_carry_gru1, initial_carry_gru2)\n\ndef forward(params, X):\n    \n    carry_gru1, carry_gru2 = initial_carry_gru1, initial_carry_gru2\n\n    Yhat = []\n    for x in X: # x.shape = (batch_size, in_features)\n        yhat, carry_gru1, carry_gru2 = rnn_jax.apply(params, x, carry_gru1, carry_gru2)\n        Yhat.append(yhat) # y.shape = (batch_size, out_features)\n\n    #return jnp.concatenate(Y, axis=0)\n\njitted_forward = jax.jit(forward)\nResults\npython\nCopy\n# uncompiled jax version\n%time forward(params, Xj)\nCPU times: user 7min 17s, sys: 8.18 s, total: 7min 25s Wall time: 7min 17s\npython\nCopy\n# time for compiling\n%time jitted_forward(params, Xj)\nCPU times: user 8min 9s, sys: 4.46 s, total: 8min 13s Wall time: 8min 12s\npython\nCopy\n# compiled jax version\n%timeit jitted_forward(params, Xj)\nThe slowest run took 204.20 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 115 µs per loop\npython\nCopy\n# torch version\n%timeit lambda: rnn_torch(Xt)\n10000000 loops, best of 5: 65.7 ns per loop\nQuestions\nWhy is my Jax-implementation so slow? What am i doing wrong?\nAlso, why is compiling taking so long? The sequence is not that long..\nThank you :)",
        "answers": [
            "The reason the JAX code compiles slowly is that during JIT compilation JAX unrolls loops. So in terms of XLA compilation, your function is actually very large: you call rnn_jax.apply() 1000 times, and compilation times tend to be roughly quadratic in the number of statements.\nBy contrast, your pytorch function uses no Python loops, and so under the hood it is relying on vectorized operations that run much faster.\nAny time you use a for loop over data in Python, a good bet is that your code will be slow: this is true whether you're using JAX, torch, numpy, pandas, etc. I'd suggest finding an approach to the problem in JAX that relies on vectorized operations rather than relying on slow Python looping."
        ],
        "link": "https://stackoverflow.com/questions/69767707/jax-flax-very-slow-rnn-forward-pass-compared-to-pytorch"
    },
    {
        "title": "how to do curve fitting using google jax?",
        "question": "Extending the examples from http://implicit-layers-tutorial.org/neural_odes/ I am tying to mimic the curve fitting function in scipy , scipy.optimize.curve_fit ,using google jax. The function to be fitted is a first order ODE.\npython\nCopy\n#Generate toy data for first order ode.\n\nimport jax.numpy as jnp\nimport jax\nimport numpy as np\n\n\n#input  data \nu = np.zeros(100)  \nu[10:50] = 1\nt = np.arange(len(u))\nu = jnp.array(u)\n\n#first order ODE\ndef f(y,t,k,tau,u):\n \n  return (k*u[t]-y)/tau\n  \n#Euler integration\ndef odeint_euler(f, y0, t, *args):\n  def step(state, t):\n    y_prev, t_prev = state\n    dt = t - t_prev\n    y = y_prev + dt * f(y_prev, t_prev, *args)\n    return (y, t), y\n  _, ys = jax.lax.scan(step, (y0, t[0]), t[1:])\n  return ys\n\npred = odeint_euler(f, jnp.array([0.0]),t,2.,5.,u) \npred_noise = pred.reshape(-1) +  0.05* np.random.randn(len(pred)) # this is the  data to be fitted\n\n# define loss function \ndef loss_function(params,u,targets):\n  k,tau = params\n  \n  pred = odeint_euler(f, jnp.array([0.0]),t,k,tau,u)\n  return jnp.sum((pred-targets)**2)      \n\n\ndef update(params, u, targets):\n  grads = jax.grad(loss_function)(params,u, targets)\n  return [w - 0.0001 * dw for w,dw  in zip(params, grads)] \n\n\nupdated_params = jnp.array([1.0,2.0]) #initial parameters\nfor i in range(100):\n  updated_params = update(updated_params, u, pred_noise)\nprint(updated_params)\nThe code works fine. However , this runs pretty slow when compared to scipy curve fit. The accuracy of the solution is not good even after 500, 1000 iterations. What is wrong with the above code ? Any idea how to make the code run faster and to get more accurate solution? Is there any better way of doing the curve fitting with jax?",
        "answers": [
            "I see two overall issues with your approach:\nThe reason your code is running slowly is because you are doing your looping in Python, which incurs JAX's dispatch overhead every loop. I'd recommend using JAX's built-in tools for minimization of loss functions; for example:\npython\nCopy\nfrom jax.scipy.optimize import minimize\nresult = minimize(\n    loss_function, x0=jnp.array([1.0,2.0]),\n    method='BFGS', args=(u, pred_noise))\nThe reason your accuracy does not approach that of scipy is likely because JAX defaults to 32-bit computations (See Double (64 bit) Precision). To run your code in 64-bit, you can run this block before any other imports:\npython\nCopy\nfrom jax import config\nconfig.update('jax_enable_x64', True)"
        ],
        "link": "https://stackoverflow.com/questions/69641423/how-to-do-curve-fitting-using-google-jax"
    },
    {
        "title": "How to use grad convolution in google-jax?",
        "question": "Thanks for reading my question!\nI was just learning about custom grad functions in Jax, and I found the approach JAX took with defining custom functions is quite elegant.\nOne thing troubles me though.\nI created a wrapper to make lax convolution look like PyTorch conv2d.\nfrom jax import numpy as jnp\nfrom jax.random import PRNGKey, normal \nfrom jax import lax\nfrom torch.nn.modules.utils import _ntuple\nimport jax\nfrom jax.nn.initializers import normal\nfrom jax import grad\n\ntorch_dims = {0: ('NC', 'OI', 'NC'), 1: ('NCH', 'OIH', 'NCH'), 2: ('NCHW', 'OIHW', 'NCHW'), 3: ('NCHWD', 'OIHWD', 'NCHWD')}\n\ndef conv(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n    n = len(input.shape) - 2\n    if type(stride) == int:\n        stride = _ntuple(n)(stride)\n    if type(padding) == int: \n        padding = [(i, i) for i in _ntuple(n)(padding)]\n    if type(dilation) == int:\n        dilation = _ntuple(n)(dilation)\n    return lax.conv_general_dilated(lhs=input, rhs=weight, window_strides=stride, padding=padding, lhs_dilation=dilation, rhs_dilation=None, dimension_numbers=torch_dims[n], feature_group_count=1, batch_group_count=1, precision=None, preferred_element_type=None)\nThe problem is that I could not find a way to use its grad function:\ninit = normal()\nrng = PRNGKey(42)\nx = init(rng, [128, 3, 224, 224])\nk = init(rng, [64, 3, 3, 3])\ny = conv(x, k)\ngrad(conv)(y, k)\nThis is what I got.\nValueError: conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 64 // 1 != 3.\nPlease help!",
        "answers": [
            "When I run your code with the most recent releases of jax and jaxlib (jax==0.2.22; jaxlib==0.1.72), I see the following error:\nTypeError: Gradient only defined for scalar-output functions. Output had shape: (128, 64, 222, 222).\nIf I create a scalar-output function that uses conv, the gradient seems to work:\npython\nCopy\nresult = grad(lambda x, k: conv(x, k).sum())(x, k)\nprint(result.shape)\n# (128, 3, 224, 224)\nIf you are using an older version of JAX, you might try updating to a more recent version – perhaps the error you're seeing is due to a bug that has already been fixed."
        ],
        "link": "https://stackoverflow.com/questions/69571976/how-to-use-grad-convolution-in-google-jax"
    },
    {
        "title": "Is there a CUDA threadId alike in Jax (google)?",
        "question": "I'm trying to understand the behaviour of jax.vmap/pmap, (jax: https://jax.readthedocs.io/). CUDA has threadId to let you know which thread is executing the code, is there a similar concept in jax? (jax.process_id is not)",
        "answers": [
            "No, there is no real analog to CUDA threadid in JAX. Details about GPU thread assignment are handled at a lower level by the XLA compiler, and I don't know of any straightforward API to plumb this information back to JAX's Python runtime.\nOne case where JAX does offer higher-level handling of device assignment is when using pmap; in this case you can explicitly pass a set of device IDs to the pmapped function if you want logic that depends on the device on which the mapped code is being executed. For example, I ran the following on an 8-device system:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\nnum_devices = jax.device_count()\n\ndef f(device, data):\n  return data + device\n\ndevice_index = jnp.arange(num_devices)\ndata = jnp.zeros((num_devices, 10))\n\njax.pmap(f)(device_index, data)\n\n# ShardedDeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n#                     [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n#                     [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n#                     [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.],\n#                     [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n#                     [5., 5., 5., 5., 5., 5., 5., 5., 5., 5.],\n#                     [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.],\n#                     [7., 7., 7., 7., 7., 7., 7., 7., 7., 7.]], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/69450145/is-there-a-cuda-threadid-alike-in-jax-google"
    },
    {
        "title": "How can I utilize JAX library to speed up my code?",
        "question": "I have written a code that gets some vertex and rearranged them based on some rules. When the input contains big data, the code runs very slowly e.g. for 60000 loops it will take about 15 hours on google colab TPU runtime. I have found JAX is one of the best libraries to do so and trying to use it, but due to lack of experience in dealing with such big data and its related methods such as parallelization, I have faced to some problems. The following small sample is created to show what does the code doing:\npython\nCopy\nimport numpy as np\n\n# <class 'numpy.ma.core.MaskedArray'> <class 'numpy.ma.core.MaskedArray'> (m, 4) <class 'numpy.int64'>\nnodes = np.ma.masked_array(np.array([[0, 1, 2, 3], [4, 0, 5, 1], [6, 4, 7, 5], [8, 6, 9, 7]],\n                                    dtype=np.int64), mask=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# <class 'numpy.ndarray'> <class 'numpy.ndarray'> (n, 3) <class 'numpy.float64'>\nvert = np.array([[0.06944111, -0.12027553, -0.3], [0., -0.13888221, -0.3], [0.05, -0.08660254, -0.3],\n                [0.06944111, -0.12027553, -0.5], [0.06944111, -0.12027553, -0.1], [0., -0.13888221, -0.1],\n                [0.06944111, -0.12027553,  0.1], [0., -0.13888221,  0.1], [0.06944111, -0.12027553,  0.3],\n                [0., -0.13888221,  0.3]])\n\n\ndef ali_sh():\n    mod_array = []\n    mod_idx = []\n\n    for cell in range(len(nodes)):\n        vertex_idx = []\n\n        B_face = sorted(nodes[cell], key=lambda v: [vert[v][0]], reverse=True)\n        if round(vert[B_face[1]][0], 7) == round(vert[B_face[2]][0], 7):\n            if vert[B_face[1]][1] > vert[B_face[2]][1]:\n                B_face[1], B_face[2] = B_face[2], B_face[1]\n\n        mod_array.append(B_face)\n\n        for vertex in B_face:\n            vertex_idx.append(np.where(nodes[cell] == vertex)[0][0])\n\n        mod_idx.append(vertex_idx)\n\n    return mod_idx\n\nmod_idx = ali_sh()\nThe above code is just a view of my code and have some differences e.g. in this code jnp.where run correctly but using the main code and the big data it will stuck and must use np.where instead. In my first try, I have added jax_r = jit(ali_sh) and mod_idx = jax_r().block_until_ready() to the end of the code, but I did not get any better performance. I have used FiPy library and its methods, which where in numpy types e.g. 'fipy.mesh.vertexCoords.T' is a numpy ndarray. I have tried to convert the used fipy numpy arrays to JAX ones by jnp.array(fipy numpy arrays) to check if it could help, but I get errors due to using lambda by sorted command. How can I implement JAX on my code to get a better run time.\nDoes colab need anything to do to get its maximum capability on TPU or GPU for such codes? Does using JAX could have significant effects on my code to speeding up? I would be appreciated if someone could help to find out how to speed up the code.",
        "answers": [
            "Writing efficient JAX code is very similar to writing efficient NumPy code: generally if you are using a for loop over rows of your data, your code will not be very efficient. Instead, you should strive to write your computations in terms of vectorized operations.\nIn your code, it looks like you are relying on many non-JAX elements (e.g. NumPy masked arrays, operations in FiPy, etc.) so it's unlikely that JAX will be able to improve your runtime. I'd focus instead on rewriting your code to make efficient use of NumPy, replacing the for-loop logic with NumPy vectorized operations.\nHere is an example of expressing your function in terms of vectorized operations:\npython\nCopy\ndef ali_sh_vectorized():\n  i_sort = np.argsort(vert[nodes, 0], axis=1)[:, ::-1]\n  B_face = nodes[np.arange(nodes.shape[0])[:, None], i_sort]\n  close = np.isclose(vert[B_face[:, 1],1], vert[B_face[:, 2], 2])\n  larger = np.greater(vert[B_face[:, 1],1], vert[B_face[:, 2], 2])\n  col_1 = np.where(close & larger, B_face[:, 2], B_face[:, 1])\n  col_2 = np.where(close & larger, B_face[:, 1], B_face[:, 2])\n  B_face[:, 1] = col_1\n  B_face[:, 2] = col_2\n  mod_idx = np.where(nodes[:, :, None] == B_face[:, None, :])[2].reshape(nodes.shape)\n  return mod_idx\nThe differences in the output compared to the original function are due to differences in how the Python sort and the NumPy sort handle equivalent elements, but I believe the overall logic is the same."
        ],
        "link": "https://stackoverflow.com/questions/69422078/how-can-i-utilize-jax-library-to-speed-up-my-code"
    },
    {
        "title": "How does one get a parameter from a params (pytree) in haiku? (jax framework)",
        "question": "For example you set up a module and that has params. But if you want do regularize something in a loss what is the pattern?\npython\nCopy\nimport jax.numpy as jnp\nimport jax\ndef loss(params, x, y):\n   l = jnp.sum((y - mlp.apply(params, x)) ** 2)\n   w = hk.get_params(params, 'w') # does not work like this\n   l += jnp.sum(w ** w)\n   return l\nThere is some pattern missing in the examples.",
        "answers": [
            "params is essentially a read-only dictionary, so you can get the value of a parameter by treating it as a dictionary:\npython\nCopy\nprint(params['w'])\nIf you want to update the parameters, you cannot do it in-place, but have to first convert it to a mutable dictionary:\npython\nCopy\nparams_mutable = hk.data_structures.to_mutable_dict(params)\nparams_mutable['w'] = 3.14\nparams_new = hk.data_structures.to_immutable_dict(params_mutable)"
        ],
        "link": "https://stackoverflow.com/questions/69031947/how-does-one-get-a-parameter-from-a-params-pytree-in-haiku-jax-framework"
    },
    {
        "title": "Error message in Python with differentiation",
        "question": "I am computing these derivatives using the Montecarlo approach for a generic call option. I am interested in this combined derivative (with respect to both S and Sigma). Doing this with the algorithmic differentiation, I get an error that can be seen at the end of the page. What could be a possible solution? Just to explain something regarding the code, I am going to attach the formula used to compute the \"X\" in the code below:\npython\nCopy\nfrom jax import jit, grad, vmap\nimport jax.numpy as jnp\nfrom jax import random\nUnderlying_asset = jnp.linspace(1.1,1.4,100)\nvolatilities = jnp.linspace(0.5,0.6,100)\ndef second_derivative_mc(S,vol):\n    N = 100\n    j,T,q,r,k = 10000,1.,0,0,1.\n    S0 = jnp.array([S]).T #(Nx1) vector underlying asset\n    C = jnp.identity(N)*vol    #matrix of volatilities with 0 outside diagonal \n    e = jnp.array([jnp.full(j,1.)])#(1xj) vector of \"1\"\n    Rand = np.random.RandomState()\n    Rand.seed(10)\n    U= Rand.normal(0,1,(N,j)) #Random number for Brownian Motion\n    sigma2 = jnp.array([vol**2]).T #Vector of variance Nx1\n\n    first = jnp.dot(sigma2,e) #First part equation\n    second = jnp.dot(C,U)     #Second part equation\n\n    X = -0.5*first+jnp.sqrt(T)*second\n\n    St = jnp.exp(X)*S0\n\n    P = jnp.maximum(St-k,0)\n    payoff = jnp.average(P, axis=-1)*jnp.exp(-q*T)\n    return payoff \n\n\ngreek = vmap(grad(grad(second_derivative_mc, argnums=1), argnums=0)(Underlying_asset,volatilities)\nThis is the error message:\npython\nCopy\n> UnfilteredStackTrace                      Traceback (most recent call\n> last) <ipython-input-78-0cc1da97ae0c> in <module>()\n>      25 \n> ---> 26 greek = vmap(grad(grad(second_derivative_mc, argnums=1), argnums=0))(Underlying_asset,volatilities)\n> \n> 18 frames UnfilteredStackTrace: TypeError: Gradient only defined for\n> scalar-output functions. Output had shape: (100,).\nThe stack trace below excludes JAX-internal frames. The preceding is the original exception that occurred, unmodified.\nThe above exception was the direct cause of the following exception:\npython\nCopy\n> TypeError                                 Traceback (most recent call\n> last) /usr/local/lib/python3.7/dist-packages/jax/_src/api.py in\n> _check_scalar(x)\n>     894     if isinstance(aval, ShapedArray):\n>     895       if aval.shape != ():\n> --> 896         raise TypeError(msg(f\"had shape: {aval.shape}\"))\n>     897     else:\n>     898       raise TypeError(msg(f\"had abstract value {aval}\"))\n\n> TypeError: Gradient only defined for scalar-output functions. Output had shape: (100,).",
        "answers": [
            "As the error message indicates, gradients can only be computed for functions that return a scalar. Your function returns a vector:\npython\nCopy\nprint(len(second_derivative_mc(1.1, 0.5)))\n# 100\nFor vector-valued functions, you can compute the jacobian (which is similar to a multi-dimensional gradient). Is this what you had in mind?\npython\nCopy\nfrom jax import jacobian\ngreek = vmap(jacobian(jacobian(second_derivative_mc, argnums=1), argnums=0))(Underlying_asset,volatilities)\nAlso, this is not what you asked about, but the function above will probably not work as you intend even if you solve the issue in the question. Numpy RandomState objects are stateful, and thus will generally not work correctly with jax transforms like grad, jit, vmap, etc., which require side-effect-free code (see Stateful Computations In JAX). You might try using jax.random instead; see JAX: Random Numbers for more information."
        ],
        "link": "https://stackoverflow.com/questions/68908160/error-message-in-python-with-differentiation"
    },
    {
        "title": "Websockets messages only sent at the end and not in instances using async / await, yield in nested for loops",
        "question": "I have a computationally heavy process that takes several minutes to complete in the server. So I want to send the results of every iteration to the client via websockets.\nThe overall application works but my problem is that all the messages are arriving at the client in one big chunk after the entire simulation finishes. I must be missing something here as I expect the await websocket.send_json() to send the message during the process and not all of them at the end.\nServer python (FastAPI)\npython\nCopy\n# A very simplified abstraction of the actual app.\n\ndef simulate_intervals(data):\n  for t in range(data.n_intervals):\n    state = interval(data) # returns a JAX NumPy array\n    yield state\n\ndef simulate(data):\n  for key in range(data.n_trials):\n     trial = simulate_intervals(data)\n     yield trial\n\n@app.websocket(\"/ws\")\nasync def socket(websocket: WebSocket):\n\n  await websocket.accept()\n  while True:\n    # Get model inputs from client\n    data = await websocket.receive_text()\n    # Minimal computation\n    nodes = distributions(data)\n\n    nodosJson = json.dumps(nodes, cls=NumpyEncoder)\n    # I expect this message to be sent early on,\n    # but the client gets it at the end with all the other messages. \n    await websocket.send_json({\"tipo\": \"nodos\", \"datos\": json.loads(nodosJson)})\n    \n    # Heavy computation\n    trials = simulate(data)\n\n    for trialI, trial in enumerate(trials):\n      for stateI, state in enumerate(trial):\n        stateString = json.dumps(state, cls=NumpyEncoder)\n\n        await websocket.send_json(\n          {\n            \"tipo\": \"estado\",\n            \"datos\": json.loads(stateString),\n            \"trialI\": trialI,\n            \"stateI\": stateI,\n          }\n        )\n\n    await websocket.send_json({\"tipo\": \"estado\", \"msg\": \"fin\"})\nFor completeness, here is the basic client code.\nClient\njavascript\nCopy\nconst ws = new WebSocket('ws://localhost:8000/ws');\n\nws.onopen = () => {\n  console.log('Conexión exitosa');\n};\n\nws.onmessage = (e) => {\n  const mensaje = JSON.parse(e.data);\n  console.log(mensaje);\n};\n\nbotonEnviarDatos.onclick = () => {\n   ws.send(JSON.stringify({...}));\n}",
        "answers": [
            "I got a similar issue, and was able to resolve it by adding a small await asyncio.sleep(0.1) after sending json messages. I have not dived into asyncios internals yet, but my guess is that websocker.send shedules a message to be sent, but since the async function continues to run it never has a chance to do it in the background. Sleeping the async function makes asyncio pick up other tasks while it is waiting.",
            "I was not able to make it work as posted in my question, still interested in hearing from anyone who understands why it is not possible to send multiple async messages without them getting blocked.\nFor anyone interested, here is my current solution:\nPing pong messages from client and server\nI changed the logic so the server and client are constantly sending each other messages and not trying to stream the data in a single request from the client.\nThis actually works much better than my original attempt because I can detect when a sockets gets disconnected and stop processing in the server. Basically, if the client disconnects, no new requests for data are sent from that client and the server never continues the heavy computation.\nServer\npython\nCopy\n# A very simplified abstraction of the actual app.\n\ndef simulate_intervals(data):\n  for t in range(data.n_intervals):\n    state = interval(data) # returns a JAX NumPy array\n    yield state\n\ndef simulate(data):\n  for key in range(data.n_trials):\n     trial = simulate_intervals(data)\n     yield trial\n\n@app.websocket(\"/ws\")\nasync def socket(websocket: WebSocket):\n\n  await websocket.accept()\n  while True:\n    # Get messages from client\n    data = await websocket.receive_text()\n    \n    # \"tipo\" is basically the type of data being sent from client or server to the other one.\n    # In this case, \"tipo\": \"inicio\" is the client sending inputs and requesting for a certain data in response.\n    if data[\"tipo\"] == \"inicio\":\n      # Minimal computation\n      nodes = distributions(data)\n\n      nodosJson = json.dumps(nodes, cls=NumpyEncoder)\n      # In this first interaction, the client gets the first message without delay. \n      await websocket.send_json({\"tipo\": \"nodos\", \"datos\": json.loads(nodosJson)})\n\n      # Since this is a generator (def returns yield) it does not actually\n      # trigger that actual computationally heavy process. \n      trials = simulate(data)\n      \n      # define some initial variables to count the iterations\n      trialI = 0\n      stateI = 0\n      trialsLen = args.number_trials\n      statesLen = 600\n      \n      # load the first trial (also a generator)\n      # without the for loop used before, the counters and next()\n      # allow us to do the same as being done before in the for loop\n      trial = next(trials)\n\n      # With the use of generators and next() it is possible to keep\n      # this first message light on the server and send the first response\n      # as quickly as possible.\n    \n    # This type of message asks for the next instance of the simluation\n    # without processing the entire model.\n    elif data[\"tipo\"] == \"sim\":\n      # check if we are within the limits (before this was a nested for loop)\n      if trialI < trialsLen and stateI < statesLen:\n        # Trigger the next instance of the simulation\n        state = next(trial)\n        # update counter\n        stateI = stateI + 1\n        \n        # Send the message with 1 instance of the simulation.\n        # \n        stateString = json.dumps(state, cls=NumpyEncoder)\n        await websocket.send_json(\n          {\n             \"tipo\": \"estado\",\n             \"datos\": json.loads(stateString),\n             \"trialI\": trialI,\n             \"stateI\": stateI,\n          }\n        )\n        \n        # Check if the second loop is done\n        if stateI == statesLen:\n          # update counter of first loop\n          trialI = trialI + 1\n          # update counter of second loop\n          stateI = 0\n          \n          # Check if there are more pending trials,\n          # otherwise stop and notify the client we are done.\n          try:\n            trial = next(trials)\n          except StopIteration:\n            await websocket.send_json({\"tipo\": \"fin\"})\nClient\nJust the part that actually changed:\njavascript\nCopy\nws.onmessage = (e) => {\n  const mensaje = JSON.parse(e.data);\n  \n  // Simply check the type of incoming message so it can be processed\n  if (mensaje.tipo === 'fin') {\n    viz.calcularResultados();\n  } else if (mensaje.tipo === 'nodos') {\n    viz.pintarNodos(mensaje.datos);\n  } else if (mensaje.tipo === 'estado') {\n    viz.sumarEstado(mensaje.datos);\n  }\n\n  // After receiving a message, ping the server for the next one \n  ws.send(\n    JSON.stringify({\n      tipo: 'sim',\n    })\n  );\n};\nThis seems like reasonable solution to keep the server and client working together. I am able to show in the client the progress of a long simulation and the user experience is much better than having to wait for a long time for the server to respond. Hope it helps other with a similar problem."
        ],
        "link": "https://stackoverflow.com/questions/68884040/websockets-messages-only-sent-at-the-end-and-not-in-instances-using-async-awai"
    },
    {
        "title": "Is there a way to disable forward evaluation while using VJP in JAX?",
        "question": "I use VJP frequently in my project. It runs the function that is subject to Jacobian computation and returns a primals_out together with the callable vjp function. For example, custom VJP definition in JAX documentation is given like this:\npython\nCopy\nfrom jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n# Returns primal output and residuals to be used in backward pass by f_bwd.\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res # Gets residuals computed in f_fwd\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\nIn this example, we see that evaluation of the forward function is required when using VJP. This is also the case when using regular VJP instead of a custom defined one. However, when the evaluation of function costs highly and since I have already run that function somewhere in my code, I don't want VJP to evaluate that function one more time.\nSo, is there any way to indicate that a function will not be evaluated when computing its VJP?",
        "answers": [
            "I don't think there is any way to explicitly disable forward evaluation in this context, but if you wrap your computation in a jit compilation, the XLA compiler will automatically do dead code elimination and trim unused branches from the computation graph."
        ],
        "link": "https://stackoverflow.com/questions/68491225/is-there-a-way-to-disable-forward-evaluation-while-using-vjp-in-jax"
    },
    {
        "title": "Apply function only on slice of array under jit",
        "question": "I am using JAX, and I want to perform an operation like\npython\nCopy\n@jax.jit\ndef fun(x, index):\n    x[:index] = other_fun(x[:index])\n    return x\nThis cannot be performed under jit. Is there a way of doing this with jax.ops or jax.lax? I thought of using jax.ops.index_update(x, idx, y) but I cannot find a way of computing y without incurring in the same problem again.",
        "answers": [
            "The previous answer by @rvinas using dynamic_slice works well if your index is static, but you can also accomplish this with a dynamic index using jnp.where. For example:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\n\ndef other_fun(x):\n    return x + 1\n\n@jax.jit\ndef fun(x, index):\n  mask = jnp.arange(x.shape[0]) < index\n  return jnp.where(mask, other_fun(x), x)\n\nx = jnp.arange(5)\nprint(fun(x, 3))\n# [1 2 3 3 4]",
            "It seems there are two issues in your implementation. First, the slices are producing dynamically shaped arrays (not allowed in jitted code). Second, unlike numpy arrays, JAX arrays are immutable (i.e. the contents of the array cannot be changed).\nYou can overcome the two problems by combining static_argnums and jax.lax.dynamic_update_slice. Here is an example:\npython\nCopy\ndef other_fun(x):\n    return x + 1\n\n@jax.partial(jax.jit, static_argnums=(1,))\ndef fun(x, index):\n    update = other_fun(x[:index])\n    return jax.lax.dynamic_update_slice(x, update, (0,))\n\nx = jnp.arange(5)\nprint(fun(x, 3))  # prints [1 2 3 3 4]\nEssentially, the example above uses static_argnums to indicate that the function should be recompiled for different index values and jax.lax.dynamic_update_slice creates a copy of x with updated values at :len(update)."
        ],
        "link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit"
    },
    {
        "title": "sum matrix elementwise using vmap (jax)?",
        "question": "I'm trying to understand the in_axes and out_axes options in vmap. For example, I want to sum two matrix and get the output with the same shape.\npython\nCopy\nX = np.arange(9).reshape(3,3)\nY = np.arange(0,-9,-1).reshape(3,3)\ndef sum2(x,y):\n    return x + y\nvmap(sum2,in_axes=((0,1),(0,1)))(X,Y)\nI think I mapped both axes 0 and 1 for X and Y respectively. The output will have the same shape as X,Y. But i get the error,\npython\nCopy\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-403-103694166574> in <module>\n      3 def sum2(x,y):\n      4     return x + y\n----> 5 vmap(sum2,in_axes=((0,1),(0,1)))(X,Y)\n\n    [... skipping hidden 2 frame]\n\n~/anaconda3/lib/python3.8/site-packages/jax/api_util.py in flatten_axes(name, treedef, axis_tree, kws)\n    276       assert treedef_is_leaf(leaf)\n    277       axis_tree, _ = axis_tree\n--> 278     raise ValueError(f\"{name} specification must be a tree prefix of the \"\n    279                      f\"corresponding value, got specification {axis_tree} \"\n    280                      f\"for value tree {treedef}.\") from None\n\nValueError: vmap in_axes specification must be a tree prefix of the corresponding value, got specification ((0, 1), (0, 1)) for value tree PyTreeDef((*, *)).",
        "answers": [
            "First of all, the easiest way to do an element-wise sum is to use the built-in broadcasting of binary operations, and call sum2(X, Y) directly.\nThat said, if you're trying to understand vmap: the issue is that vmap can only map one axis at a time. If you want to map multiple axes, you can nest multiple vmaps. I believe what you intended to do can be expressed this way:\npython\nCopy\nfrom jax import vmap\nimport jax.numpy as np\n\nX = np.arange(9).reshape(3,3)\nY = np.arange(0,-9,-1).reshape(3,3)\n\ndef sum2(x,y):\n    assert x.ndim == y.ndim == 0\n    return x + y\n\nvmap(vmap(sum\n  vmap(sum2, in_axes=(0, 0), out_axes=0),\n  in_axes=(1, 1), out_axes=1\n)(X,Y)\nNote: I added the assertion about number of dimensions to demonstrate that the mapped function is being called on scalar values.\nAlso, notice that when the mapped axes match, e.g. in_axes=(0, 0) can be equivalently written in_axes=0, but I left it as a tuple because it was closer to the syntax you were trying.\nIn fact, a far more concise way to do the same computation with nested vmap would be to use the default arguments: vmap(vmap(sum2))(X, Y) will do the same elementwise sum."
        ],
        "link": "https://stackoverflow.com/questions/68332924/sum-matrix-elementwise-using-vmap-jax"
    },
    {
        "title": "Not able to import python package jax in Google TPU",
        "question": "I am working on linux console and typing python takes me into the python console. When I use the following command in TPU machine\npython\nCopy\nimport jax\nthen it generates following mss and get out of the python prompt.\nbash\nCopy\nparamjeetsingh80@t1v-n-1c883486-w-0:~$ python3\nPython 3.8.5 (default, Jan 27 2021, 15:41:15)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import jax\n2021-07-08 17:41:39.660523: F external/org_tensorflow/tensorflow/core/tpu/tpu_executor_init_fns.inc:110] TpuTransferManager_ReadDynamicShapes not available in this library.\nAborted (core dumped)\nparamjeetsingh80@t1v-n-1c883486-w-0:~$\nThis issue is causing problem in my code so I would like to figure out, what is this issue and how to get rid of this?",
        "answers": [
            "It may be that your system does not have the correct version of libtpu. Try installing the version listed here.\nYou should be able to do this automatically with\npython\nCopy\n$ pip install -U pip  # older pip may not support extra requirements\n$ pip install -U jax  # newer jax required for [tpu] extras declaration\n$ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/jax_releases.html",
            "Above command give some error but I researched and below command worked for me. But your answer give me the direction that it is a package issue.\npython\nCopy\npip install --upgrade pip\npip install \"jax[tpu]>=0.2.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/68306484/not-able-to-import-python-package-jax-in-google-tpu"
    },
    {
        "title": "How to install just XLA?",
        "question": "I want to use XLA as a backend for my project. Is there a recommended way to install it on its own (without the rest of TensorFlow). Jax probably does this, but looking in their repository it's not obvious how.",
        "answers": [
            "There is no supported way to install XLA on its own apart from tensorflow.\nThat said, JAX does extract, build, and bundle XLA separately from tensorflow within the jaxlib package. You can see the relevant build scripts for jaxlib on various platforms here: https://github.com/google/jax/tree/main/build\nIn particular, take a look at build_wheel.py, which contains the scripts that extract relevant pieces of XLA from the tensorflow source as part of the jaxlib build.",
            "XLA has been moved to the OpenXLA GitHub organization. It can be installed on its own from there."
        ],
        "link": "https://stackoverflow.com/questions/68290128/how-to-install-just-xla"
    },
    {
        "title": "Gradient Accumulation with JAX",
        "question": "I made a simple script to try to do gradient accumulation with JAX. The idea is to have large batch size (e.g. 64) that are split in small chunks (e.g. 4) that fit in the GPU's memory. For each chunck, the resulting gradient, stored in a pytree, is added to the current batch gradient. The update is done only when all chunks of the large batch are computed. In this particular example, we simply try to fit random 512-dimensional vectors to random booleans with a linear layer. Here is the script:\npython\nCopy\nimport jax\nimport jax.numpy as jnp\nfrom jax import jit, random\nfrom jax.experimental import optimizers\nfrom functools import partial\nfrom jax.nn.initializers import normal, zeros\nfrom typing import Callable\nfrom dataclasses import dataclass\n\n@dataclass\nclass Jax_model:\n    init_fun: Callable\n    apply_fun: Callable\n\n\ndef Dense(input_size: int, output_size: int, init_kernel=normal(), init_bias=zeros):\n\n    def init_fun(key):\n        key, sub_key1, sub_key2 = jax.random.split(key, 3)\n        params = {\n            'I': init_kernel(sub_key1, (input_size, output_size) ),\n            'I_b': init_bias(sub_key2, (1,output_size) ),\n        }\n        return params\n\n    def apply_fun(params, inputs):\n        I, I_b, = params['I'], params['I_b']\n        logits = inputs @ I + I_b\n        return logits\n\n    return Jax_model(init_fun, apply_fun)\n\n\ndef divide_pytree(pytree, div):\n    for pt in jax.tree_util.tree_leaves(pytree):\n        pt = pt / div\n    return pytree\n\n\ndef add_pytrees(pytree1, pytree2):\n    for pt1, pt2 in zip( jax.tree_util.tree_leaves(pytree1), jax.tree_util.tree_leaves(pytree2) ):\n        pt1 = pt1 + pt2\n    return pytree1\n\n\nrng_key = random.PRNGKey(42)\nbatch_size = 64\naccumulation_size = 4\nmodel_dim = 512\nn_iter = 50\n\nmodel = Dense(model_dim, 1)\nrng_key, sub_key = random.split(rng_key)\ninit_params = model.init_fun(sub_key)\nopt_init, opt_update, get_params = optimizers.adam(0.001)\nopt_state = opt_init(init_params)\n\n@jit\ndef update(i, current_opt_state, current_batch):\n    N = current_batch[0].shape[0]\n    K = accumulation_size\n    num_gradients = N//K\n    accumulation_batch = (current_batch[ib][0:K] for ib in range(len(current_batch)))\n    value, grads = jax.value_and_grad(loss_func)(get_params(current_opt_state), accumulation_batch)\n    value = value / num_gradients\n    grads = divide_pytree(grads, num_gradients)\n    for k in range(K,N,K):\n        accumulation_batch = (current_batch[ib][k:k+K] for ib in range(len(current_batch)))\n        new_value, new_grads = jax.value_and_grad(loss_func)(get_params(current_opt_state), accumulation_batch)\n        value = value + (new_value / num_gradients)\n        grads = add_pytrees(grads, divide_pytree(new_grads, num_gradients))\n    return opt_update(i, grads, current_opt_state), value\n\ndef loss_func(current_params, current_batch):\n    inputs, labels = current_batch\n    predictions = model.apply_fun(current_params, inputs)\n    loss = jnp.square(labels-predictions).sum()\n    return loss\n\nfor i in range(n_iter):\n    rng_key, sub_key1, sub_key2 = random.split(rng_key, 3)\n    inputs = jax.random.uniform(sub_key1, (batch_size, model_dim))\n    labels = jax.random.uniform(sub_key2, (batch_size, 1)) > 0.5\n    batch = inputs, labels\n    opt_state, batch_loss = update(i, opt_state, batch)\n    print(i, batch_loss)\nI have doubts about the divide_pytree and add_pytrees. Does it actually modify the current batch gradient or am I missing something ? Moreover, do you see any speed issue with this code ? In particular, should I use the jax.lax.fori_loop in place of the traditional python for loop ?\nRelated links:\nhttps://github.com/google/jax/issues/1488\nhttps://github.com/google-research/long-range-arena/issues/4",
        "answers": [
            "Regarding the pytree computations: as written your functions are returning the input unmodified. The better approach for this is to use jax.tree_util.tree_map; for example:\npython\nCopy\nfrom jax.tree_util import tree_map\n\ndef divide_pytree(pytree, div):\n  return tree_map(lambda pt: pt / div, pytree)\n\ndef add_pytrees(pytree1, pytree2):\n  return tree_map(lambda pt1, pt2: pt1 + pt2, pytree1, pytree2)\nRegarding performance: anything in the for loop will be flattened when JIT-compiled, with one repeated copy of all XLA instructions per iteration of the loop. If you have 5 iterations, that's not really an issue. If you have 5000, that would significantly slow down compilation times (because XLA needs to analyze & optimize 5000 explicit copies of the instructions in the loop).\nfori_loop can help, but does not lead to optimal code, particularly when running on CPU and GPU.\nBetter would be to use broadcasted or vmapped operations where possible to express the logic of the loops without explicit looping."
        ],
        "link": "https://stackoverflow.com/questions/68016425/gradient-accumulation-with-jax"
    },
    {
        "title": "derivatives by a and b, using using algorithmic differentiation",
        "question": "I've been tasked to find the derivatives by a and b, using jax, for this function\nnow, the reason I'm here is because I don't know enough Python, and this for the course in question, we haven't been thought python either.\nthe assignment is:\npython\nCopy\nreturn a tuple (dfa, dfb) such that dfa is the partial derivatives of f by a,\n           and dfb is the partial derivative of f by b\nnow, I was able to do it the normal way:\npython\nCopy\ndef function(a, b):\n   dfa = sym.diff((2/b)*sym.cos(a)*sym.exp(-a*a/b*b), a)\n   dfb = sym.diff((2/b)*sym.cos(a)*sym.exp(-a*a/b*b), a)\n   return (dfa, dfb)\nbut im not familiar with algorithmic differentiation, using the example we were given, i've tried this:\npython\nCopy\ndef foo():\n\n   x = (2/b)*sym.cos(a)\n   y = sym.exp(-sym.Pow(a/b,2))\n   return (x*y)\n\ndef f_partial_derviatives_algo():\n   return jax.grad(foo)\nbut I'm getting this error:\ncannot unpack non-iterable function object\nIf anyone can help with understanding how i can do something like that, It would be greatly appreciated",
        "answers": [
            "JAX and sympy are not compatible. You should either use one or the other, and not try to combine the two.\nIf you want to compute the partial derivatives of this function at some value using JAX, you can write something like this:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import grad\n\ndef f(a, b):\n  return (2 / b) * jnp.cos(a) * jnp.exp(- a ** 2 / b ** 2)\n\ndf_da = grad(f, argnums=0)\ndf_db = grad(f, argnums=1)\n\nprint(df_da(1.0, 1.0), df_db(1.0, 1.0))\n# -1.4141841 0.3975322"
        ],
        "link": "https://stackoverflow.com/questions/67922307/derivatives-by-a-and-b-using-using-algorithmic-differentiation"
    },
    {
        "title": "Jax and train Neural Networks",
        "question": "I am a beginner in JAX and I am trying to learn how to train a neural network. I saw some blogs, but as I understood there isn't a library that you can train it easily, like 'fit' as in sklearn. I am interested about classification task, could you please reccommend me any blogs in order to adopt his/her algorithm into my problem?",
        "answers": [
            "JAX is an array manipulation library, not a deep learning library: in that respect, you should think of it as more similar to NumPy than similar to scikit-learn. If you want neural networks built on JAX, there are several other good projects available such as haiku and flax."
        ],
        "link": "https://stackoverflow.com/questions/67778124/jax-and-train-neural-networks"
    },
    {
        "title": "is it possible to jit a function which uses jax.numpy.unique?",
        "question": "The following code does not work:\ndef get_unique(arr):\n    return jnp.unique(arr)\n\nget_unique = jit(get_unique)\nget_unique(jnp.ones((10,)))\nThe error message compains about the use of jnp.unique:\nFilteredStackTrace: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=0/1)>\nThe error arose in jnp.unique()\nThe documentation on sharp bits explains that jit doesn't work if the shape of internal arrays depends on argument values. This is exactly the case here.\nAccording to the docs, a potential workaround is to specify static parameters. But this doesn't apply to my case. The parameters will change for almost every function call. I have split up my code into a preprocessing step, which performs calculations such as this jnp.unique, and a computation step which can be jitted.\nBut still I'd like to ask, is there some workaround that I'm not aware of?",
        "answers": [
            "No, for the reasons you mention, there's currently no way to use jnp.unique on a non-static value.\nIn similar cases, JAX sometimes adds extra parameters that can be used to specify a static size for the output (for example, the size parameter in jax.numpy.nonzero) but nothing like that is currently implemented for jnp.unique. If that is something you'd like, it would be worth filing a feature request."
        ],
        "link": "https://stackoverflow.com/questions/67739742/is-it-possible-to-jit-a-function-which-uses-jax-numpy-unique"
    },
    {
        "title": "What is JaxNumpy-compatible equivalent to this Python function?",
        "question": "How do I implement the below in a JAX-compatable way (e.g., using jax.numpy)?\npython\nCopy\ndef actions(state: tuple[int, ...]) -> list[tuple[int, ...]]:\n    l = []\n    iterables = [range(1, i+1) for i in state]\n    ns = list(range(len(iterables)))\n    for i, iterable in enumerate(iterables):\n        for value in iterable:\n            action = tuple(value if n == i else 0 for n in ns)\n            l.append(action)\n    return l\n\n>>> state = (3, 1, 2)\n>>> actions(state)\n[(1, 0, 0), (2, 0, 0), (3, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 2)]",
        "answers": [
            "Jax, like numpy, cannot efficiently operate on Python container types like lists and tuples, so there's not really any JAX-compatible way to create a function with the exact signature you specify above.\nBut if you're alright with the return value being a two-dimensional array, you could do something like this, based on jnp.vstack:\npython\nCopy\nfrom typing import Tuple\nimport jax.numpy as jnp\nfrom jax import jit, partial\n\n@partial(jit, static_argnums=0)\ndef actions(state: Tuple[int, ...]) -> jnp.ndarray:\n  return jnp.vstack([\n    jnp.zeros((val, len(state)), int).at[:, i].set(jnp.arange(1, val + 1))\n    for i, val in enumerate(state)])\npython\nCopy\n>>> state = (3, 1, 2)\n>>> actions(state)\nDeviceArray([[1, 0, 0],\n             [2, 0, 0],\n             [3, 0, 0],\n             [0, 1, 0],\n             [0, 0, 1],\n             [0, 0, 2]], dtype=int32)\nNote that because the size of the output array depends on the content of state, state must be a static quantity, so a tuple is a good option for the input."
        ],
        "link": "https://stackoverflow.com/questions/67290650/what-is-jaxnumpy-compatible-equivalent-to-this-python-function"
    },
    {
        "title": "All pairwise cross products of the rows of two matrices",
        "question": "I would like to efficiently calculate all pairwise cross products of the rows of two matrices, A and B, which are nx3 and mx3 in size. And would ideally like to achieve this in einsum notation.\ni.e. the output Matrix C, would be (n X m x 3),\nwhere\nC[0][0] = cross(n[0],m[0])\nC[0][1] = cross(n[0],m[1])\n...\nC[1][0] = cross(n[1],m[0])\n...\nDue to the approach I am taking, using for loops aren't an option.\nAny help would be much appreciated.",
        "answers": [
            "Looks like cross broadcasts the leading dimensions.\npython\nCopy\nnp.cross(A[:, None,:], B[None, :,:])"
        ],
        "link": "https://stackoverflow.com/questions/67205068/all-pairwise-cross-products-of-the-rows-of-two-matrices"
    },
    {
        "title": "sampling univariate gausssian with specific mean and standard deviation using jax.random.normal",
        "question": "I'm trying to sample from a gaussian with specific standard deviation and mean, I know the following function is sampling from a gaussian with zero mean and standard deviation equals to 1:\npython\nCopy\nimport jax\nfrom jax import random\n\nkey = random.PRNGKey(0)\nmu = 20\nstd = 4\n\nx1 = jax.random.normal(key, (1000,))\nAnd I can adjust the mean by doing: x1 = x1 + mu, but how can I adjust the standard deviation?",
        "answers": [
            "Create your samples this way:\npython\nCopy\nx1 = mu + std * jax.random.normal(key, (1000,))\nIf you do this, the histogram of samples will follow the expected distribution:\npython\nCopy\nimport jax\nfrom jax import random\nfrom jax.scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nkey = random.PRNGKey(0)\nmu = 20\nstd = 4\n\nx1 = mu + std * jax.random.normal(key, (1000,))\nplt.hist(x1, bins=50, density=True)\n\nx = jnp.linspace(5, 35, 100)\ny = norm.pdf(x, loc=mu, scale=std)\nplt.plot(x, y)",
            "This\npython\nCopy\nx1 = std * x1 + mu\nwill give you want you want"
        ],
        "link": "https://stackoverflow.com/questions/66664455/sampling-univariate-gausssian-with-specific-mean-and-standard-deviation-using-ja"
    },
    {
        "title": "JAX vmap behaviour",
        "question": "I'm trying to understand the behaviour of JAX vmap, so I wrote the following code:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import vmap\n\ndef what(a,b,c):\n  z = jnp.dot(a,b)\n  return z + c\n\nv_what = vmap(what, in_axes=(None,0,None))\n\na = jnp.array([1,1,3])\nb = jnp.array([2,2])\nc = 1.0\n\nv_what(a,b,c)\nAnd the output is:\npython\nCopy\nDeviceArray([[3., 3., 7.],\n             [3., 3., 7.]], dtype=float32)\nI understand that the only input that is being altered is b, but Can someone shed some light on why this is the result? And how the dot product behaves after I vectorized the function?",
        "answers": [
            "You have specified that the transformed function should map over the first axis of b, and not map over any axis of a or c. So roughly, you've created a mapped function that does this:\npython\nCopy\ndef v_what(a, b, c):\n  return jnp.stack([what(a, b_i, c) for b_i in b], axis=0)\nFor your inputs, within each row the dot product looks like jnp.dot(a, 2), and the result is equivalent to a * 2."
        ],
        "link": "https://stackoverflow.com/questions/66548897/jax-vmap-behaviour"
    },
    {
        "title": "Understanding JAX argnums parameter in its gradient function",
        "question": "I'm trying to understand the behaviour of argnums in JAX's gradient function. Suppose I have the following function:\npython\nCopy\ndef make_mse(x, t):  \n  def mse(w,b): \n    return np.sum(jnp.power(x.dot(w) + b - t, 2))/2\n  return mse \nAnd I'm taking the gradient in the following way:\npython\nCopy\nw_gradient, b_gradient = grad(make_mse(train_data, y), (0,1))(w,b)\nargnums= (0,1) in this case, but what does it mean? With respect to which variables the gradient is calculated? What will be the difference if I will use argnums=0 instead? Also, can I use the same function to get the Hessian matrix?\nI looked at JAX help section about it, but couldn't figure it out",
        "answers": [
            "When you pass multiple argnums to grad, the result is a function that returns a tuple of gradients, equivalent to if you had computed each separately:\npython\nCopy\ndef f(x, y):\n  return x ** 2 + x * y + y ** 2\n\ndf_dxy = grad(f, argnums=(0, 1))\ndf_dx = grad(f, argnums=0)\ndf_dy = grad(f, argnums=1)\n\nx = 3.0\ny = 4.25\nassert df_dxy(x, y) == (df_dx(x, y), df_dy(x, y))\nIf you want to compute a mixed second derivatives, you can do this by repeatedly applying the gradient:\npython\nCopy\nd2f_dxdy = grad(grad(f, argnums=0), argnums=1)\nassert d2f_dxdy(x, y) == 1"
        ],
        "link": "https://stackoverflow.com/questions/66445754/understanding-jax-argnums-parameter-in-its-gradient-function"
    },
    {
        "title": "Non-hashable static arguments are not supported in Jax when using vmap",
        "question": "This is related to this question. After some work, I managed to change it down to the last error. The code looks like this now.\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, 0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha, T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n\n@jax.partial(jax.jit, static_argnums=(0,2,))    \ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\nThis leads to (what I hope) is the final error, that is\npython\nCopy\nNon-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type <class 'jax.interpreters.batching.BatchTracer'> for function map_row is non-hashable.\nThis is very strange, as I think I have marked every everywhere axis appears to be static, yet it still tells me that it is traced.",
        "answers": [
            "When you write a partial function with positional arguments, those arguments are passed first. So this:\npython\nCopy\njax.partial(map_row, alpha, T)\nis essentially equivalent to this:\npython\nCopy\nlambda z_input: map_row(alpha, T, z_input)\nNotice the incorrect order of the arguments – this is what's causing your error: you're passing z_input, a non-hashable tracer, to an argument that is expected to be static.\nYou can fix this by replacing the partial statement above with:\npython\nCopy\nlambda z: map_row(z, alpha, T)\nand then your code will run correctly."
        ],
        "link": "https://stackoverflow.com/questions/65685211/non-hashable-static-arguments-are-not-supported-in-jax-when-using-vmap"
    },
    {
        "title": "Jax cannot find the static argnums",
        "question": "This is related with this question. I manage to make the most of the code work, except one of the strange thing.\nHere is the modified code.\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, a_min=0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value < 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha=alpha, T=T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n    \n@jax.partial(jax.jit, static_argnums=(0,2,))\ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, axis=0, alpha=1.5, T=20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\nThis code will produce an error as follows:\npython\nCopy\ntuple index out of range\nwhich is cause by this line of code\npython\nCopy\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\nEven if I replace the function body with nothing but an entity function, the error persists. This is a really strange behavior. However, it is very important for me to get this thing to be static as it will help to unrolled loops.",
        "answers": [
            "This error is due to a wart that I hope will be fixed soon in JAX: static arguments cannot be passed by keyword. In other words, you should change this:\npython\nCopy\ndef toy(input, weight):\n    return (weight*entmax(input, axis=0, alpha=1.5, T=20)).sum()\nto this:\npython\nCopy\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\nThe same fix should be applied in calls to max_row.\nAt this point, you end up with a ValueError because of passing traced variables to functions that require static arguments; the solution will be similar to that in How to handle JAX reshape with JIT.\nOne additional note: this static_argnums error has recently been improved, and in the next release will be a bit more clear:\npython\nCopy\nValueError: jitted function has static_argnums=(2,), donate_argnums=() but was called with only 1 positional arguments."
        ],
        "link": "https://stackoverflow.com/questions/65612989/jax-cannot-find-the-static-argnums"
    },
    {
        "title": "Unable to Install Specific JAX jaxlib GPU version",
        "question": "I'm trying to install a particular version of jaxlib to work with my CUDA and cuDNN versions. Following the README, I'm trying\npip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html\nThis returns the following error:\nERROR: Requested jaxlib==0.1.52+cuda101 from https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.52%2Bcuda101-cp37-none-manylinux2010_x86_64.whl has different version in metadata: '0.1.52'\nDoes anyone know what causes this or how to get around the error?",
        "answers": [
            "This error appears to be from a new check in pip version 20.3.X and higher, likely related to the new dependency resolver. I can reproduce this error with pip version 20.3.3, but the package installs correctly with pip version 20.2.4.\nThe easiest way to proceed would probably be to first downgrade pip; i.e.\npip install pip==20.2.4\nand then proceed with your jaxlib install.",
            "Note that both the versions of jax and jaxlib has to match. You can use something like:\n$ pip install --upgrade jax==0.3.2 jaxlib==0.3.2+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nAnother workaround would be to first choose a specific version of jax and jaxlib from the available wheel files and then install those.\n$ pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.76+cuda11.cudnn82-cp39-none-manylinux2010_x86_64.whl",
            "Maybe you need to change the link to: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html. So, pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
        ],
        "link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version"
    },
    {
        "title": "How to resolve ValueError `vector::reserve` in JAX/Python?",
        "question": "EDIT: GitHub issue here: https://github.com/google/jax/issues/5190\nI am trying to optimize the following function using jit:\npython\nCopy\n@partial(jit, static_argnums=(0, 1,))\ndef coocurrence_helper(pairs: np.array, label_map: Dict) -> lil_matrix:\n    uniques = lil_matrix(np.zeros((len(label_map), len(label_map))).astype(\"int32\"))\n    for item in pairs:\n        if item[0]!=item[1]:\n            uniques[label_map[item[0]], label_map[item[1]]] += 1\n    return uniques\nthe routine above is used here:\npython\nCopy\ndef _get_pairwise_frequencies(\n     data: pd.DataFrame, crosstab=False\n    ) -> pd.DataFrame:\n        values = data.stack()\n        values.index = values.index.droplevel(1)\n        values.name = \"vals\"\n        values = optimize(values.to_frame())\n        pair = optimize(values.join(values, rsuffix=\"_2\"))\n        label_map = dict()\n        for lbl, each in enumerate(values.vals.unique()):\n            label_map[each] = lbl\n        if not crosstab:\n            freq = coocurrence_helper(pairs = pair.values, label_map=label_map)\n            return ((freq / freq.sum(1).ravel()).astype(np.float32))\n        else:\n            freq = pd.crosstab(pair[\"vals\"], pair[\"vals_2\"])\n            self.index = freq.index\n            return csr_matrix((freq / freq.sum(1)).astype(np.float32))\nBut i get the following error:\npython\nCopy\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-42-f8e638fc2bb6> in <module>\n----> 1 _get_pairwise_frequencies(data)\n\n<ipython-input-30-43adeb39c76c> in _get_pairwise_frequencies(data, crosstab)\n     25             label_map[each] = lbl\n     26         if not crosstab:\n---> 27             freq = coocurrence_helper(pairs = pair.values, label_map=label_map)\n     28             return csr_matrix((freq / freq.sum(1).ravel()).astype(np.float32))\n     29         else:\n\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/jax/api.py in f_jitted(*args, **kwargs)\n    369         return cache_miss(*args, **kwargs)[0]  # probably won't return\n    370     else:\n--> 371       return cpp_jitted_f(*args, **kwargs)\n    372   f_jitted._cpp_jitted_f = cpp_jitted_f\n    373 \n\nValueError: vector::reserve\nWhat can be the source of the issue here? Without using static_argnums the error message is\npython\nCopy\nRuntimeError: Invalid argument: Unknown NumPy type O size 8\nwith the same traceback.",
        "answers": [
            "The issue is that you are returning a scipy.sparse.lil_matrix, which is not a valid JAX type. The JAX jit decorator cannot be used as a compiler for arbitrary Python code; it is designed to optimize sequences of operations on JAX arrays.\nThe best way to proceed in this case would probably be to remove the @partial(jit, ...) decorator from your function; if you wanted to use JAX jit compilation here, you would first have to rewrite your code to avoid scipy.sparse matrices and use JAX arrays instead."
        ],
        "link": "https://stackoverflow.com/questions/65307334/how-to-resolve-valueerror-vectorreserve-in-jax-python"
    },
    {
        "title": "Jaxlib pip installation failure",
        "question": "From the command line, I've tried following this installation tutorial I'd like to avoid building from source if at all possible. Currently, I'm not sure what the issue is. Could anyone verify that they get the same/different response when trying to install Jaxlib?\nFor awareness, Jax installed fine without any issues, but some supporting components are found in Jaxlib which is installed separately.\npython\nCopy\nC:\\Users\\john.smith>pip install jaxlib\nERROR: Could not find a version that satisfies the requirement jaxlib (from versions: none)\nERROR: No matching distribution found for jaxlib",
        "answers": [
            "It appears you are using Windows. JAX currently does not provide jaxlib builds for Windows (see google/jax#438) though there is work in progress to rectify this; see google/jax#4843.\nThere are some comments in the above issue that suggest a possible approach to build jaxlib for Windows if you wish to build it yourself."
        ],
        "link": "https://stackoverflow.com/questions/64863194/jaxlib-pip-installation-failure"
    },
    {
        "title": "vmap ops.index_update in Jax",
        "question": "I have the following code below and it's using a simple for loop. I was just wondering if there was a way to vmap it? Here is the original code:\npython\nCopy\nimport numpy as np \nimport jax.numpy as jnp\nimport jax.scipy.signal as jscp\nfrom scipy import signal\nimport jax\n\ndata = np.random.rand(192,334)\n\na = [1,-1.086740193996892,0.649914553946275,-0.124948974636730]\nb = [0.054778173164082,0.164334519492245,0.164334519492245,0.054778173164082]\nimpulse = signal.lfilter(b, a, [1] + [0]*99) \nimpulse_20 = impulse[:20]\nimpulse_20 = jnp.asarray(impulse_20)\n\n@jax.jit\ndef filter_jax(y):\n    for ind in range(0, len(y)):\n      y = jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19])\n    return y\n\njnpData = jnp.asarray(data)\n\n%timeit filter_jax(jnpData).block_until_ready()\nAnd here is my attempt at using vmap:\npython\nCopy\ndef paraUpdate(y, ind):\n    return jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19])\n\n@jax.jit\ndef filter_jax2(y):\n  ranger = range(0, len(y))\n  return jax.vmap(paraUpdate, y)(ranger)\nBut I receive the following error:\nTypeError: vmap in_axes must be an int, None, or (nested) container with those types as leaves, but got Traced<ShapedArray(float32[192,334])>with<DynamicJaxprTrace(level=0/1)>.\nI'm a little confused since the range is of type int so I'm not too sure what's going on.\nIn the end, I'm trying to get this little piece optimized as best as possible to get the lowest time.",
        "answers": [
            "jax.vmap can express functionality in which a single operation is independently applied across multiple axes of an input. Your function is a bit different: you have a single operation iteratively applied to a single input.\nFortunately JAX provides lax.scan which can handle this situation. The implementation would look something like this:\npython\nCopy\nfrom jax import lax\n\ndef paraUpdate(y, ind):\n    return jax.ops.index_update(y, jax.ops.index[:, ind], jscp.convolve(impulse_20, y[:,ind])[:-19]), ind\n\n@jax.jit\ndef filter_jax2(y):\n  ranger = jnp.arange(len(y))\n  return lax.scan(paraUpdate, y, ranger)[0]\n\nprint(np.allclose(filter_jax(jnpData), filter_jax2(jnpData)))\n# True\n\n%timeit filter_jax(jnpData).block_until_ready()\n# 10 loops, best of 3: 28.6 ms per loop\n\n%timeit filter_jax2(jnpData).block_until_ready()\n# 1000 loops, best of 3: 519 µs per loop\nIf you change your algorithm so that you'e applying the operation to every column in the array rather than the first N columns, it can be expressed with vmap like this:\npython\nCopy\n@jax.jit\ndef filter_jax3(y):\n  f = lambda col: jscp.convolve(impulse_20, col)[:-19]\n  return jax.vmap(f, in_axes=1, out_axes=1)(y)"
        ],
        "link": "https://stackoverflow.com/questions/64655749/vmap-ops-index-update-in-jax"
    },
    {
        "title": "Alternative to scipy stats zmap function",
        "question": "Is there any alternative to scipy stats module of the zmap function? I'm currently using it to obtain the zmap scores for two really large arrays and it's taking quite some time.\nAre there any libraries or alternatives that could boost its performance? Or even another of obtaining what the zmap function does?\nYour ideas and comments would be appreciated!\nHere's my minimal reproducible code below:\npython\nCopy\nfrom scipy import stats\nimport numpy as np\n\nFeatureData = np.random.rand(483, 1)\ngoodData = np.random.rand(4640, 483)\nFeatureNorm= stats.zmap(FeatureData, goodData)\nAnd here's what the scipy stats.zmap does under the hood:\npython\nCopy\ndef zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\nAny ideas on how to optimize it for my use case? Could I use libraries like numba or JAX to boost this further?",
        "answers": [
            "Fortunately, the zmap code is pretty straightforward. The overhead in numpy, however, will come from the fact that it must instantiate intermediate arrays. If you use a numerical compiler such as that available in numba or jax, it can fuse these operations and do the computation with less overhead.\nUnfortunately, numba doesn't support optional arguments to mean and std, so let's take a look at JAX. For reference, here are benchmarks for scipy and for the raw numpy version of the function, computed on a Google Colab CPU runtime:\npython\nCopy\nimport numpy as np\nfrom scipy import stats\n\nFeatureData = np.random.rand(483, 1)\ngoodData = np.random.rand(4640, 483)\n\n%timeit stats.zmap(FeatureData, goodData)\n# 100 loops, best of 3: 13.9 ms per loop\n\ndef np_zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(np.asanyarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\n%timeit np_zmap(FeatureData, goodData)\n# 100 loops, best of 3: 13.8 ms per loop\nHere is the equivalent code executed in JAX, both eager mode and JIT compiled:\npython\nCopy\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef jnp_zmap(scores, compare, axis=0, ddof=0):\n    scores, compare = map(jnp.asarray, [scores, compare])\n    mns = compare.mean(axis=axis, keepdims=True)\n    sstd = compare.std(axis=axis, ddof=ddof, keepdims=True)\n    return (scores - mns) / sstd\n\njit_jnp_zmap = jit(jnp_zmap)\n\nFeatureData = jnp.array(FeatureData)\ngoodData = jnp.array(goodData)\n%timeit jnp_zmap(FeatureData, goodData).block_until_ready()\n# 100 loops, best of 3: 8.59 ms per loop\n\njit_jnp_zmap(FeatureData, goodData)  # trigger compilation\n%timeit jit_jnp_zmap(FeatureData, goodData).block_until_ready()\n# 100 loops, best of 3: 2.78 ms per loop\nThe JIT-compiled version is about a factor of 5 faster than the scipy or numpy code. On a Colab T4 GPU runtime, the compiled version gains another factor of 10:\npython\nCopy\n%timeit jit_jnp_zmap(FeatureData, goodData).block_until_ready()\n1000 loops, best of 3: 286 µs per loop\nIf this kind of operation is a bottleneck in your analysis, a compiler like JAX might be a good option."
        ],
        "link": "https://stackoverflow.com/questions/64437380/alternative-to-scipy-stats-zmap-function"
    },
    {
        "title": "Efficiently fill an array from a function",
        "question": "I want to construct a 2D array from a function in such a way that I can utilize jax.jit.\nThe way I would normally do this using numpy is to create an empty array, and then fill that array in-place.\npython\nCopy\nxx = jnp.empty((num_a, num_b))\nyy = jnp.empty((num_a, num_b))\nzz = jnp.empty((num_a, num_b))\n\nfor ii_a in range(num_a):\n    for ii_b in range(num_b):\n        a = aa[ii_a, ii_b]\n        b = bb[ii_a, ii_b]\n\n        xyz = self.get_coord(a, b)\n\n        xx[ii_a, ii_b] = xyz[0]\n        yy[ii_a, ii_b] = xyz[1]\n        zz[ii_a, ii_b] = xyz[2]\nTo make this work within jax I have attempted to use the jax.opt.index_update.\npython\nCopy\n        xx = xx.at[ii_a, ii_b].set(xyz[0])\n        yy = yy.at[ii_a, ii_b].set(xyz[1])\n        zz = zz.at[ii_a, ii_b].set(xyz[2])\nThis runs without errors but is very slow when I try to use a @jax.jit decorator (at least an order of magnitude slower than the pure python/numpy version).\nWhat is the best way to fill a multi-dimensional array from a function using jax?",
        "answers": [
            "JAX has a vmap transform that is designed specifically for this kind of application.\nAs long as your get_coords function is compatible with JAX (i.e. is a pure function with no side-effects), you can accomplish this in one line:\npython\nCopy\nfrom jax import vmap\nxx, yy, zz = vmap(vmap(get_coord))(aa, bb)",
            "This can be achieved efficiently by using either the jax.vmap or the jax.numpy.vectorize functions.\nAn example using vectorize:\npython\nCopy\nimport jax.numpy as jnp\n\ndef get_coord(a, b):\n    return jnp.array([a, b, a+b])\n\nf0 = jnp.vectorize(get_coord, signature='(),()->(i)')\nf1 = jnp.vectorize(f0, excluded=(1,), signature='()->(i,j)')\n\nxyz = f1(a,b)\nThe vectorize function uses vmap under the hood, so this should be exactly equivalent to:\npython\nCopy\nf0 = jax.vmap(get_coord, (None, 0))\nf1 = jax.vmap(f0, (0, None)) \nThe advantage of using vectorize is that the code can be still be run in standard numpy. The disadvantage is less concise code and possibly a small amount of overhead because of the wrapper."
        ],
        "link": "https://stackoverflow.com/questions/64221771/efficiently-fill-an-array-from-a-function"
    },
    {
        "title": "Efficient way to compute Jacobian x Jacobian.T",
        "question": "Assume J is the Jacobian of some function f with respect to some parameters. Are there efficient ways (in PyTorch or perhaps Jax) to have a function that takes two inputs (x1 and x2) and computes J(x1)*J(x2).transpose() without instantiating the entire J matrices in memory?\nI have come across something like jvp(f, input, v=vjp(f, input)) but don't quite understand it and not sure is what I want.",
        "answers": [
            "In JAX, you can compute a full jacobian matrix using jax.jacfwd or jax.jacrev, or you can compute a jacobian operator and its transpose using jax.jvp and jax.vjp.\nSo, for example, say you had a function Rᴺ → Rᴹ that looks something like this:\npython\nCopy\nimport jax.numpy as jnp\nimport numpy as np\n\nnp.random.seed(1701)\nN, M = 10000, 5\nf_mat = np.array(np.random.rand(M, N))\ndef f(x):\n  return jnp.sqrt(f_mat @ x / N)\nGiven two vectors x1 and x2, you can evaluate the Jacobian matrix at each using jax.jacfwd\npython\nCopy\nimport jax\nx1 = np.array(np.random.rand(N))\nx2 = np.array(np.random.rand(N))\nJ1 = jax.jacfwd(f)(x1)\nJ2 = jax.jacfwd(f)(x2)\nprint(J1 @ J2.T)\n# [[3.3123782e-05 2.5001222e-05 2.4946943e-05 2.5180108e-05 2.4940484e-05]\n#  [2.5084497e-05 3.3233835e-05 2.4956826e-05 2.5108084e-05 2.5048916e-05]\n#  [2.4969209e-05 2.4896170e-05 3.3232871e-05 2.5006309e-05 2.4947023e-05]\n#  [2.5102483e-05 2.4947576e-05 2.4906987e-05 3.3327218e-05 2.4958186e-05]\n#  [2.4981882e-05 2.5007204e-05 2.4966144e-05 2.5076926e-05 3.3595043e-05]]\nBut, as you note, along the way to computing this 5x5 result, we instantiate two 5x10,000 matrices. How might we get around this?\nThe answer is in jax.jvp and jax.vjp. These have somewhat unintuitive call signatures for the purposes of your question, as they are designed primarily for use in forward-mode and reverse-mode automatic differentiation. But broadly, you can think of them as a way to compute J @ v and J.T @ v for a vector v, without having to actually compute J explicitly.\nFor example, you can use jax.jvp to compute the effect of J1 operating on a vector, without actually computing J1:\npython\nCopy\nJ1_op = lambda v: jax.jvp(f, (x1,), (v,))[1]\n\nvN = np.random.rand(N)\nnp.allclose(J1 @ vN, J1_op(vN))\n# True\nSimilarly, you can use jax.vjp to compute the effect of J2.T operating on a vector, without actually computing J2:\npython\nCopy\nJ2T_op = lambda v: jax.vjp(f, x2)[1](v)[0]\n\nvM = np.random.rand(M)\nnp.allclose(J2.T @ vM, J2T_op(vM))\n# True\nPutting these together and operating on an identity matrix gives you the full jacobian matrix product that you're after:\npython\nCopy\ndef direct(f, x1, x2):\n  J1 = jax.jacfwd(f)(x1)\n  J2 = jax.jacfwd(f)(x2)\n  return J1 @ J2.T\n\ndef indirect(f, x1, x2, M):\n  J1J2T_op = lambda v: jax.jvp(f, (x1,), jax.vjp(f, x2)[1](v))[1]\n  return jax.vmap(J1J2T_op)(jnp.eye(M)).T\n\nnp.allclose(direct(f, x1, x2), indirect(f, x1, x2, M))\n# True\nAlong with the memory savings, this indirect method is also a fair bit faster than the direct method, depending on the sizes of the jacobians involved:\npython\nCopy\n%time direct(f, x1, x2)\n# CPU times: user 1.43 s, sys: 14.9 ms, total: 1.44 s\n# Wall time: 886 ms\n%time indirect(f, x1, x2, M)\n# CPU times: user 311 ms, sys: 0 ns, total: 311 ms\n# Wall time: 158 ms"
        ],
        "link": "https://stackoverflow.com/questions/63559139/efficient-way-to-compute-jacobian-x-jacobian-t"
    },
    {
        "title": "Get and Post API call in java with basic authentication",
        "question": "I want to call GET and POST API in java without using any framework. I need to use basic authentication. Can anybody help me with some tutorial link. In google I found code only in spring framework, But I am not using Spring. I am looking for code to call API with basic authentication.\nI have to add new url with authentication in the below code. What modification is required if API is secured with basic auth and it is POST method. I am new to java so not much aware.\njava\nCopy\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.MalformedURLException;\nimport java.net.Proxy;\nimport java.net.URL;\nimport java.net.URLConnection;\n\npublic class NetClientGet {\n\n    public static void main(String[] args)  {\n        \n        try\n        {\n            System.out.println(\"Inside the main function\");\n             URL weburl=new URL(\"http://dummy.restapiexample.com/api/v1/employees\");\n             HttpURLConnection conn = (HttpURLConnection) weburl.openConnection();\n             conn.setRequestMethod(\"GET\");\n             conn.setRequestProperty(\"Accept\", \"application/json\");\n             System.out.println(\"Output is: \"+conn.getResponseCode());\n             System.out.println(\"Output is: \");\n             System.setProperty(\"http.proxyHost\", null);\n             //conn.setConnectTimeout(60000);\n             if(conn.getResponseCode()!=200)\n             {\n                 System.out.println(conn.getResponseCode());\n                 throw new RuntimeException(\"Failed : HTTP Error Code: \"+conn.getResponseCode());\n             }\n             System.out.println(\"After the 2 call \");\n             InputStreamReader in=new InputStreamReader(conn.getInputStream());\n             BufferedReader br =new BufferedReader(in);\n             String output;\n             while((output=br.readLine())!=null)\n             {\n                 System.out.println(output);\n             }\n             conn.disconnect();\n             \n        }\n        catch(Exception e)\n        {\n            System.out.println(e.getMessage());\n        }\n        \n    }\n}",
        "answers": [
            "Basic Authentication\nSee the RFC #2617 section 2: Basic Authentication Scheme\nAdd Authentication header into the request. Here's an example:\njava\nCopy\nString username = \"john\";\nString password = \"pass\";\n// ...\nURL weburl=new URL(\"http://dummy.restapiexample.com/api/v1/employees\");\nHttpURLConnection conn = (HttpURLConnection) weburl.openConnection();\nconn.setRequestMethod(\"GET\");\nconn.setRequestProperty(\"Accept\", \"application/json\");\n// snippet begins\nconn.setRequestProperty(\"Authorization\",\n  \"Basic \" + Base64.getEncoder().encodeToString(\n    (username + \":\" + password).getBytes()\n  )\n);\n// snippet ends\nSystem.out.println(\"Output is: \"+conn.getResponseCode());\nPOST Method\nSee this answer for more information about using POST method with HttpURLConnection."
        ],
        "link": "https://stackoverflow.com/questions/62564502/get-and-post-api-call-in-java-with-basic-authentication"
    },
    {
        "title": "vmap over a list in jax",
        "question": "Using jax, I try to calculate gradients per sample, process them and then bring them in the normal form to calculate a normal parameter update. My working code looks like\npython\nCopy\ndifferentiate_per_sample = jit(vmap(grad(loss), in_axes=(None, 0, 0)))\ngradients = differentiate_per_sample(params, x, y)\n\n# some code\n\ngradients_summed_over_samples = []\n    for layer in gradients:\n        (dw, db) = layer\n        (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n        gradients_summed_over_samples.append((dw, db))\nwhere gradients is of the form list(tuple(DeviceArray(...), DeviceArray(...)), ...).\nNow I tried to rewrite the loop as vmap (not sure if it brings a speedup in the end)\npython\nCopy\ndef sum_samples(layer):\n    (dw, db) = layer\n    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n\nvmap(sum_samples)(gradients)\nbut sum_samples is called only once and not for each element in the list.\nIs the list the problem or do I understand something else wrong?",
        "answers": [
            "jax.vmap will only be mapped over jax array inputs, not inputs that are lists of arrays or tuples. In addition, vmapped functions cannot modify inputs in-place; the functions should return a value, and this return value will be stacked with other return values to construct the output\nFor example, you could modify the function you defined and use it like this:\npython\nCopy\nimport jax.numpy as np\nfrom jax import random\n\ndef sum_samples(layer):\n    (dw, db) = layer\n    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n    return np.array([dw, db])\n\nkey = random.PRNGKey(1701)\ndata = random.uniform(key, (10, 2, 20))\n\nresult = vmap(sum_samples)(data)\nprint(result.shape)\n# (10, 2)\nSide note: if you're using this approach, the vmapped function above can be more concisely expressed as:\npython\nCopy\ndef sum_samples(layer):\n    return layer.sum(1)"
        ],
        "link": "https://stackoverflow.com/questions/61786831/vmap-over-a-list-in-jax"
    },
    {
        "title": "What's the best way to compute row-wise (or axis-wise) dot products with jax?",
        "question": "I have two numerical arrays of shape (N, M). I'd like to compute a row-wise dot product. I.e. produce an array of shape (N,) such that the nth row is the dot product of the nth row from each array.\nI'm aware of numpy's inner1d method. What would the best way be to do this with jax? jax has jax.numpy.inner, but this does something else.",
        "answers": [
            "You can define your own jit-compiled version of inner1d in a few lines of jax code:\npython\nCopy\nimport jax\n@jax.jit\ndef inner1d(X, Y):\n  return (X * Y).sum(-1)\nTesting it out:\npython\nCopy\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.core import umath_tests\n\n\nX = np.random.rand(5, 10)\nY = np.random.rand(5, 10)\n\nprint(umath_tests.inner1d(X, Y))\nprint(inner1d(jnp.array(X), jnp.array(Y)))\n# [2.23219571 2.1013316  2.70353783 2.14094973 2.62582531]\n# [2.2321959 2.1013315 2.703538  2.1409497 2.6258256]",
            "You can try jax.numpy.einsum. Here the implementaion using numpy einsum\npython\nCopy\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\narr1 = np.random.randint(0,10,[5,5])\narr2 = np.random.randint(0,10,[5,5])\n\narr = np.inner1d(arr1,arr2)\narr\narray([ 87, 200, 229,  81,  53])\nnp.einsum('...i,...i->...',arr1,arr2)\narray([ 87, 200, 229,  81,  53])"
        ],
        "link": "https://stackoverflow.com/questions/61314443/whats-the-best-way-to-compute-row-wise-or-axis-wise-dot-products-with-jax"
    },
    {
        "title": "JAX: time to jit a function grows superlinear with memory accessed by function",
        "question": "Here is a simple example, which numerically integrates the product of two Gaussian pdfs. One of the Gaussians is fixed, with mean always at 0. The other Gaussian varies in its mean:\npython\nCopy\nimport time\n\nimport jax.numpy as np\nfrom jax import jit\nfrom jax.scipy.stats.norm import pdf\n\n# set up evaluation points for numerical integration\nintegr_resolution = 6400\nlower_bound = -100\nupper_bound = 100\nintegr_grid = np.linspace(lower_bound, upper_bound, integr_resolution)\nproba = pdf(integr_grid)\nintegration_weight = (upper_bound - lower_bound) / integr_resolution\n\n\n# integrate with new mean\ndef integrate(mu_new):\n    x_new = integr_grid - mu_new\n\n    proba_new = pdf(x_new)\n    total_proba = sum(proba * proba_new * integration_weight)\n\n    return total_proba\n\n\nprint('starting jit')\nstart = time.perf_counter()\nintegrate = jit(integrate)\nintegrate(1)\nstop = time.perf_counter()\nprint('took: ', stop - start)\nThe function looks seemingly simple, but it doesn't scale at all. The following list contains pairs of (value for integr_resolution, time it took to run the code):\n100 | 0.107s\n200 | 0.23s\n400 | 0.537s\n800 | 1.52s\n1600 | 5.2s\n3200 | 19s\n6400 | 134s\nFor reference, the unjitted function, applied to integr_resolution=6400 takes 0.02s.\nI thought that this might be related to the fact that the function is accessing a global variable. But moving the code to set up the integration points inside of the function has no notable influence on the timing. The following code takes 5.36s to run. It corresponds to the table entry with 1600 which previously took 5.2s:\npython\nCopy\n# integrate with new mean\ndef integrate(mu_new):\n    # set up evaluation points for numerical integration\n    integr_resolution = 1600\n    lower_bound = -100\n    upper_bound = 100\n    integr_grid = np.linspace(lower_bound, upper_bound, integr_resolution)\n    proba = pdf(integr_grid)\n    integration_weight = (upper_bound - lower_bound) / integr_resolution\n\n    x_new = integr_grid - mu_new\n\n    proba_new = pdf(x_new)\n    total_proba = sum(proba * proba_new * integration_weight)\n\n    return total_proba\nWhat is happening here?",
        "answers": [
            "I also answered this at https://github.com/google/jax/issues/1776, but adding the answer here too.\nIt's because the code uses sum where it should use np.sum.\nsum is a Python built-in that extracts each element of a sequence and sums them one by one using the + operator. This has the effect of building a large, unrolled chain of adds which XLA takes a long time to compile.\nIf you use np.sum, then JAX builds a single XLA reduction operator, which is much faster to compile.\nAnd just to show how I figured this out: I used jax.make_jaxpr, which dumps JAX's internal trace representation of a function. Here, it shows:\npython\nCopy\nIn [3]: import jax\n\nIn [4]: jax.make_jaxpr(integrate)(1)\nOut[4]:\n{ lambda b c ;  ; a.\n  let d = convert_element_type[ new_dtype=float32\n                                old_dtype=int32 ] a\n      e = sub c d\n      f = sub e 0.0\n      g = pow f 2.0\n      h = div g 1.0\n      i = add 1.8378770351409912 h\n      j = neg i\n      k = div j 2.0\n      l = exp k\n      m = mul b l\n      n = mul m 2.0\n      o = slice[ start_indices=(0,)\n                 limit_indices=(1,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      p = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] o\n      q = add p 0.0\n      r = slice[ start_indices=(1,)\n                 limit_indices=(2,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      s = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] r\n      t = add q s\n      u = slice[ start_indices=(2,)\n                 limit_indices=(3,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      v = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] u\n      w = add t v\n      x = slice[ start_indices=(3,)\n                 limit_indices=(4,)\n                 strides=(1,)\n                 operand_shape=(100,) ] n\n      y = reshape[ new_sizes=()\n                   dimensions=None\n                   old_sizes=(1,) ] x\n      z = add w y\n... similarly ...\nand it's then obvious why this is slow: the program is very big.\nContrast the np.sum version:\npython\nCopy\nIn [5]: def integrate(mu_new):\n   ...:     x_new = integr_grid - mu_new\n   ...:\n   ...:     proba_new = pdf(x_new)\n   ...:     total_proba = np.sum(proba * proba_new * integration_weight)\n   ...:\n   ...:     return total_proba\n   ...:\n\nIn [6]: jax.make_jaxpr(integrate)(1)\nOut[6]:\n{ lambda b c ;  ; a.\n  let d = convert_element_type[ new_dtype=float32\n                                old_dtype=int32 ] a\n      e = sub c d\n      f = sub e 0.0\n      g = pow f 2.0\n      h = div g 1.0\n      i = add 1.8378770351409912 h\n      j = neg i\n      k = div j 2.0\n      l = exp k\n      m = mul b l\n      n = mul m 2.0\n      o = reduce_sum[ axes=(0,)\n                      input_shape=(100,) ] n\n  in [o] }\nHope that helps!"
        ],
        "link": "https://stackoverflow.com/questions/59068666/jax-time-to-jit-a-function-grows-superlinear-with-memory-accessed-by-function"
    }
]