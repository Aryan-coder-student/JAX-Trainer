[
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Is it expected that vmapping over different input sizes for the same function impacts the accuracy of the result?",
        "question": "I was suprised to see that depending on the size of an input matrix, which is vmapped over inside of a function, the output of the function changes slightly. That is, not only does the size of the output change (which is what I would expect from vmapping) but also the numerics changed slightly. (Note that this only occurs in float32 and only on the GPU)\nI wrote a minimally reproducible example to illustrate the behaviour:\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\ndef equinox_vmap(x, mlp):\n    out = eqx.filter_vmap(mlp.__call__)(x)\n    return out\n\nkey = jax.random.PRNGKey(0)\nkey, network_key = jax.random.split(key, 2)\nmlp = eqx.nn.MLP(2, 2, 10, 2, key=network_key)\n\nkey, key_x = jax.random.split(key, 2)\nx = jax.random.normal(key_x, (10000, 2))\n\nerror_eqx = equinox_vmap(x[:10], mlp) - equinox_vmap(x, mlp)[:10]\nprint(\"eqx error:\", error_eqx)\nWhen running this example I get the output:\neqx error: [[-1.4442205e-04  1.0999292e-04]\n [-5.9515238e-05 -9.1716647e-06]\n [ 1.4841557e-05  5.6132674e-05]\n [ 0.0000000e+00  0.0000000e+00]\n [-9.1642141e-06 -2.5466084e-05]\n [ 3.8832426e-05 -3.3110380e-05]\n [ 3.3825636e-05 -2.4946406e-05]\n [ 4.0918589e-05 -3.2216311e-05]\n [ 1.3601780e-04  8.7693334e-06]\n [ 0.0000000e+00  0.0000000e+00]]\nI understand that the numerics of float32 are not fully accurate and some error is to be expected. However, I was suprised that the result changes depending on how much of the input array is put into the function. I was expecting that the first row of the x array, i.e., x[0,:] would still be filled with the same values and therefore the first row in the output would be the same.\nFurther notes:\nI enabled the use of float64 (jax.config.update(\"jax_enable_x64\", False)) which completely removed this from occuring. I understand that this is a numerical problem, but I am a little bit confused how the vmapping interacts with the example.\nWhen I run the same example on the CPU (using jax.config.update(\"jax_platform_name\", \"cpu\")) this problem also disappears which I also find difficult to understand.\nQuestions:\nIs this to be expected?\nWhere does this \"inconsistency\" come from?\nWhy does it not occur on the CPU and only on the GPU?\nSetup:\nGPU: NVIDIA RTX 6000 Ada Generation 48 GB\nPython 3.11.11 with\nequinox                  0.13.0\njax                      0.7.0\njax-cuda12-pjrt          0.7.0\njax-cuda12-plugin        0.7.0\njaxlib                   0.7.0\njaxtyping                0.3.2\nml_dtypes                0.5.3\nnumpy                    2.3.2\nnvidia-cublas-cu12       12.9.1.4\nnvidia-cuda-cupti-cu12   12.9.79\nnvidia-cuda-nvcc-cu12    12.9.86\nnvidia-cuda-nvrtc-cu12   12.9.86\nnvidia-cuda-runtime-cu12 12.9.79\nnvidia-cudnn-cu12        9.11.0.98\nnvidia-cufft-cu12        11.4.1.4\nnvidia-cusolver-cu12     11.7.5.82\nnvidia-cusparse-cu12     12.5.10.65\nnvidia-nccl-cu12         2.27.6\nnvidia-nvjitlink-cu12    12.9.86\nnvidia-nvshmem-cu12      3.3.9\nopt_einsum               3.4.0\npip                      24.0\nscipy                    1.16.1\nsetuptools               65.5.0\ntyping_extensions        4.14.1\nwadler_lindig            0.1.7\nAny explanations are greatly appreachiated.",
        "answers": [
            "This is behaving as expected. This is not fundamentally about vmap; this is about floating point math. Whenever you're doing floating point operations, you will accumulate rounding errors, and when you do the \"same\" computation in two different ways, you will accumulate rounding errors differently (see Is floating-point math broken? for some discussion of this).\nRunning vmap over different batch sizes results in different sequences of operations, which in turn results in different rounding errors.\nAs for why this differs between CPU and GPU, it's all about how the floating point operations are sequenced. CPU is a serial architecture, so it's likely computing matrix products row-by-row with the same accumulation orders regardless of input size. GPU is a parallel architecture, and will generally distribute and accumulate results differently depending on the size of the inputs."
        ],
        "link": "https://stackoverflow.com/questions/79726091/is-it-expected-that-vmapping-over-different-input-sizes-for-the-same-function-im"
    },
    {
        "title": "Are JAX operations already vectorized?",
        "question": "In the documentation, JAX provides vectorization. However, aren't JAX operations already vectorized? For example, to add two vectors, I thought that the element-wise additions were vectorized internally already.\nMy guess is that vectorization is useful when: it's hard for us to add a dimension for broadcasting, so we resort to a more explicit vectorization.\nEDIT: for example, instead of vectorizing convolution2d with different kernels, I simply stack the kernels, copy and stack the channel, then perform the convolution2d with this stack of kernels.",
        "answers": [
            "I have also raised a similar question here: https://github.com/jax-ml/jax/issues/26212 By now I think there is no universal answer to this and it will remain a matter of taste to a certain degree. However in some cases there is a clearer answer:\nSome operations in JAX are not natively vectorized, such as e.g. jnp.histogram or jnp.bincount, in this case you can use vmap to get a \"batched\" version of that function (for example search for \"batched_histogram\" here http://axeldonath.com/jax-diffusion-models-pydata-boston-2025/). This is really convenient and avoids loops to improve readability as well as performance.\nvmap works over PyTrees. Some libraries (most notably equinox) use this to avoid the need for handling a batch axis in models completely and just finally vmap over the whole parameter tree by convention. This frees developers from thinking about the batch axis at all, but when working with equinox you have to stick to that convention. It also only works if operations are independent across different batches. It does not work for operations such as a \"batch norm\" (see also https://docs.kidger.site/equinox/examples/stateful/)\nIn some cases one introduces a local(!) extra dimension to an array to avoid writing a Python loop and optionally reduce after. This can often be implemented more shortly and with clearer intent using vmap (basically what you said).\nAs broadcasting and batch axes are universally accepted convention in deep learning I mostly stick with them. But I rely on vmap whenever there is no native vectorization, whenever I work with libraries that rely on vmap by convention, or whenever I need to vectorize operations along non-conventional axes (basically everything except batch axis)."
        ],
        "link": "https://stackoverflow.com/questions/79718029/are-jax-operations-already-vectorized"
    },
    {
        "title": "Does vmap correctly split the RNG keys?",
        "question": "In the following code, when I remove the vmap, I have the right randomized behavior. However, with vmap, I don't anymore. Isn't this supposed to be one of the features of nnx.vmap?\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\n@nnx.vmap(\n    in_axes=(None, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((4, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")",
        "answers": [
            "To make dropout work together with vmap in flax, we need to use split_rngs and StateAxes :\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\nbs = 4\n\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=bs, only='dropout')\n@nnx.vmap(\n    in_axes=(state_axes, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\nmodel.train()\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\nOutput with jax: 0.7.0.dev20250704, flax: 0.10.6\nOutput batch:\n[[0.         0.1736668  1.6533196  0.         0.        ]\n [0.         0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.",
            "I'm not sure nnx.vmap and nnx.split_rngs are necessary in vfdev's answer. Also, having a train kwarg is unnecessary in most situations since NNX models can dynamically jump between train=True, train=False with .train() and .eval()\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    x = self.linear(x)\n    x = self.dropout(x)\n    return x\n\nkey = jax.random.PRNGKey(42)\n\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n@nnx.jit\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  return model(x)\n\nbs = 4\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Enable training. This works because Dropout layers have a .deterministic property\n# that can be modified.\nmodel.train()\n\noutput_batch = batched_apply(model, batch_input)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\noutput:\nModel initialized successfully.\nDropout Rate: 0.5\n------------------------------\nInput batch shape: (4, 10)\nInput batch:\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n------------------------------\nRunning the batched model in training mode (dropout is active)...\nOutput batch shape: (4, 5)\n\nOutput batch:\n[[0.         0.1736668  0.         0.         0.        ]\n [0.         0.         1.6533196  1.0752656  0.        ]\n [0.         0.         0.         0.         0.7218913 ]\n [0.09358063 0.         0.         1.0752656  0.        ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.\nand if instead you do model.eval()\nOutput batch:\n[[0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]]\n------------------------------\n❌ Verification failed: All outputs were the same."
        ],
        "link": "https://stackoverflow.com/questions/79698307/does-vmap-correctly-split-the-rng-keys"
    },
    {
        "title": "Configuration options varying between jax installs?",
        "question": "I have a laptop I do work on for a program that includes jax, the program ends up getting run here on a small scale to test it, then it is sent off to a server for batch processing.\nIn the program I have set these flags for jax:\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\n(as well as others but these are the relevant ones)\nThis runs fine on my laptop (using sharding to CPU parallelise), but when running on the server on GPU, I get an error message:\nAttributeError: Unrecognized config option: jax_captured_constants_report_frames\n(and the same for jax_captured_constants_warn_bytes if that were to run first)\nWhy is there this discrepancy? Can I use these flags some other way that is generalised between different jax installs?\npip list | grep jax, on laptop:\njax                       0.6.2\njaxlib                    0.6.2\njaxtyping                 0.3.2\non server:\njax                       0.6.0\njax-cuda12-pjrt           0.6.0\njax-cuda12-plugin         0.6.0\njaxlib                    0.6.0\njaxtyping                 0.3.2\nEDIT: As a side note, what is the scope of jax flags? I have a jax initialisation function to set os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=\" + str(cpu_count()) before the rest of the code runs, if I set jax.config.update(..., ...) options in here, will they hold in files called after it that also import jax? Or do I have to set them again? Is there a function to check the current value of these flags?",
        "answers": [
            "The jax_captured_constants_report_frames and jax_captured_constants_warn_bytes configurations were added in JAX version 0.6.1 (Relevant PR: https://github.com/jax-ml/jax/pull/28157) If you want to use them on your server, you'll have to update JAX to v0.6.1 or later."
        ],
        "link": "https://stackoverflow.com/questions/79693916/configuration-options-varying-between-jax-installs"
    },
    {
        "title": "Unable to set cpu device count for jax parallelisation?",
        "question": "I have been trying to generalise this jax program for solving on both CPU and GPU depending on the machine it's running on (essentially need cpu parallelisation to speed up testing versus gpu for production). I can get jax to parallelise on the GPU, but no matter what I do jax will not detect my cpu_count and thus cannot be sharded across cores (for context am running on 8 core, 16 thread laptop processor).\nI found out that XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT had to be set before jax was initialised (was previously set in the if statement included in the code), but it is still not working. I also tried setting at the very start of my code (this is a snippet from the only file using jax itself, but some other files use jnp as a jax drop in for numpy).\nCan anyone tell me why jax will not pick up on the flag? (Relevant code snippet and jupyter notebook output included below). Thanks.\nRelevant code snippet:\nfrom multiprocessing import cpu_count\ncore_count = cpu_count()\n\n### THIS NEEDS TO BE SET BEFORE JAX IS INITIALISED IN ANY WAY, INCLUDING IMPORTING\n# - XLA_FLAGS are read WHEN jax is IMPORTED\n\n# you can see other ways of setting the environment variable that I've tried here\n\n#jax.config.update('xla_force_host_platform_device_count', core_count)\n#os.environ[\"XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT\"] = '16'#str(core_count)\n#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=' + str(core_count)\nos.environ[\"XLA_FLAGS\"] = f\"--xla_force_host_platform_device_count={cpu_count()}\"\n\nimport jax\n\n# defaults float data types to 64-bit instead of 32 for greater precision\njax.config.update('jax_enable_x64', True)\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\njax.config.update('jax_traceback_filtering', 'off')\n# https://docs.jax.dev/en/latest/gpu_memory_allocation.html\n#jax.config.update('xla_python_client_allocator', '\\\"platform\\\"')\n# can't set via jax.config.update for some reason\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = '\\\"platform\\\"'\n\nprint(\"\\nDefault jax backend:\", jax.default_backend())\n\navailable_devices = jax.devices()\nprint(f\"Available devices: {available_devices}\")\n\nrunning_device = xla_bridge.get_backend().platform\nprint(\"Running device:\", running_device, end='')\n\nif running_device == 'cpu':\n    print(\", with:\", core_count, \"cores.\")\n\n    from jax.sharding import PartitionSpec as P, NamedSharding\n\n    # Create a Sharding object to distribute a value across devices:\n    # Assume core_count is the no. of core devices available\n    mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n\n    # Example matrix shape (9, N), e.g., N = 1e7\n    #x = jax.random.normal(jax.random.key(0), (9, Np))\n\n    # Specify sharding: don't split axis 0 (rows), split axis 1 (columns) across devices\n    # then apply sharding to produce a sharded array from the matrix input\n    # and use jax.device_put to distribute it across devices:\n    s0_sharded = jax.device_put(s0, NamedSharding(mesh, P(None, 'cols')))  # 'None' means don't shard axis 0\n\n    print(s0_sharded.sharding)            # See the sharding spec\n    print(s0_sharded.addressable_shards)  # Check each device's shard\n    jax.debug.visualize_array_sharding(s0_sharded)\nOutput:\nDefault jax backend: cpu\nAvailable devices: [CpuDevice(id=0)]\nRunning device: cpu, with: 16 cores.\n\n...\n\nrelevant line of my code: --> 258 mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n... jax backend trace\nValueError: Number of devices 1 must be >= the product of mesh_shape (16,)",
        "answers": [
            "I tried running your snippet and got a number of errors related to missing imports and undefined names (os is not defined, xla_bridge is not defined, s0 is undefined). This, along with the fact that you're running in Jupyter notebook, makes me think that you've already imported JAX in your runtime before running this cell.\nAs mentioned in your code comments, the XLA device count must be set before JAX is imported in your runtime. You should try restarting the Jupyter kernel, then fix the missing imports and variables and rerun your cell as the first execution in your fresh runtime.\nHere's a simple recipe that should work to set the device count while asserting that you've not already imported JAX in another cell in your notebook:\nimport os\nimport sys\n\nassert \"jax\" not in sys.modules, \"jax already imported: you must restart your runtime\"\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\nIf running this results in an assertion error, then you'll have to restart your kernel/runtime before running it again."
        ],
        "link": "https://stackoverflow.com/questions/79691728/unable-to-set-cpu-device-count-for-jax-parallelisation"
    },
    {
        "title": "Jax vmapping while loop [closed]",
        "question": "Closed. This question needs debugging details. It is not currently accepting answers.\nEdit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\nClosed last month.\nImprove this question\nI have a function that has jax.lax.while_loop. Now, I want to vmap it. However, vmap makes the execution time very slow compared to the original one.\nI understand that in the case of lax.cond, it is transformed into select, which evaluates all branches and thus may decrease the computational speed.\nIs a similar thing happening here? If so, what is the best practice to do do xx while y is true with vmap?",
        "answers": [
            "A while_loop under vmap becomes a single while_loop over a batched body_fun and cond_fun, meaning effectively that every loop in the batch executes for the same number of iterations. If different batches lead to vastly different iteration times, this can result in extra computation compared to executing individual while_loops in sequence."
        ],
        "link": "https://stackoverflow.com/questions/79660448/jax-vmapping-while-loop"
    },
    {
        "title": "Looking for an efficent JAX function to reconstruct an image from patches",
        "question": "I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.\nI am trying to reconstruct the original images from the patches. Here is vanilla python code that works:\nkernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)\n\ndef fwd(x):\n    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')\n\n    # return channels last\n    return jnp.transpose(xcv, [0,2,3,1])\n\npatches = fwd(bfrc)\n\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\n\n# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT\n# H_PATCHES == IMG_WIDTH // PATCH_WIDTH\n\nreconstructed = np.zeros(EXPECTED_IMG_SHAPE)\n\nfor vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):\n    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):\n        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):\n            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):\n                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):\n                    row = vpatch * PATCH_HEIGHT + prow\n                    col = hpatch * PATCH_WIDTH + pcol\n                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]\n\n# This assert passes\nassert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0\nOf course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?",
        "answers": [
            "I'm not sure what happened here:\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\nbut I assume it's some kind of mistake.\nAssuming bfrc has shape of (batch, channels, height, width), and\nV_PATCHES = IMG_HEIGHT // PATCH_HEIGHT\nH_PATCHES = IMG_WIDTH // PATCH_WIDTH\nthen patch_reshaped_pn_c_h_w will have the shape of (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH).\nKeeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.\nV, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape\n\nH_total = V * PH\nW_total = H * PW\n\npatches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)\n\nreconstructed = patches.reshape(V, H, PH, PW, C)\nreconstructed = reconstructed.transpose(0, 2, 1, 3, 4)\nreconstructed = reconstructed.reshape(H_total, W_total, C)\nreconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)\nYou can additionally wrap it into @jax.jit, which should be slightly faster."
        ],
        "link": "https://stackoverflow.com/questions/79647350/looking-for-an-efficent-jax-function-to-reconstruct-an-image-from-patches"
    },
    {
        "title": "Would using lists rather than jax.numpy arrays lead to more accurate numerical transformations?",
        "question": "I am doing a project with RNNs using jax and flax and I have noticed some behavior that I do not really understand.\nMy code is basically an optimization loop where the user provides the initial parameters for the system they want to optimize. This system is divided onto several time steps. He feeds the initial input into the first time step of the the system, gets a certain output, feeds this output into a RNN which returns the parameters for the following time step and so on. Then it is optimized using adam (particularly using optax).\nNow the user inputs his initial parameters as a dict and then there is a function called prepare_parameters_from_dict that basically converts this dict into a list of lists (or a list of jnp arrays for that matter).\nMy question/observation is when I make this function return a list of jnp.arrays instead of a list of lists, the property I am optimizing is an order of magnitude worse!\nFor example, using a list of lists outputs 0.9997 and a list of jnp.arrays outputs 0.998 (the closer to one the better).\nNoting: the RNN output a list of jnp.arrays (it is using flax linnen) and everything in the code remains the same.\nHere are said function:\nOutputing list of lists:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(flat_params)\n        shapes.append(len(flat_params))\n    return res, shapes\nUsing list of jnp.arrays:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(jnp.array(flat_params))\n        shapes.append(jnp.array(flat_params).shape[0])\n    return res, shapes\nand this is an example of the users input initial params:\ninitial_params = {\n    \"param1\": {\n        \"gamma\": 0.1,\n        \"delta\": -3 * jnp.pi / 2,\n    }\n}\nThe rest of the code remains exactly the same for both.\nAfter optimization if for example there were five time steps, this is how the final optimized params for each time step would look like:\nusing list of jnp.arrays:\n[[Array([ 0.1       , -4.71238898], dtype=float64)],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nusing list of lists:\n[[ [0.1       , -4.71238898] ]],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nWould such a difference in behavior be due to how jax handles grad and jit and others with lists compared to jnp.arrays or am I missing something?",
        "answers": [
            "The main operative difference between these two cases is that Python floats are treated as weakly-typed, meaning that the list version of your code could result in operations being performed at a lower precision. For example:\nIn [1]: import jax\n\nIn [2]: import jax.numpy as jnp\n\nIn [3]: jax.config.update('jax_enable_x64', True)\n\nIn [4]: list_values = [0.1, -4.71238898]\n\nIn [5]: array_values = jax.numpy.array(list_values)\n\nIn [6]: x = jax.numpy.float32(1.0)\n\nIn [7]: x + list_values[1]\nOut[7]: Array(-3.712389, dtype=float32)\n\nIn [8]: x + array_values[1]\nOut[8]: Array(-3.71238898, dtype=float64)\nNotice that the array version leads to higher-precision computations in this case. If I had to guess what the main difference is in your two runs, I'd guess something to do with the precision implied by strict vs weak types."
        ],
        "link": "https://stackoverflow.com/questions/79634990/would-using-lists-rather-than-jax-numpy-arrays-lead-to-more-accurate-numerical-t"
    }
]