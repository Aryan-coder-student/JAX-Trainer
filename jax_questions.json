[
    {
        "title": "How is the execution of Jax and non-Jax parts interleaved in a Python program and when does an abstract value become concrete?",
        "question": "I have the following code:\ndef non_jitted_setup():\n    print(\"This code runs once at the beginning of the program.\")\n    return jnp.array([1.0, 2.0, 3.0])\n\nclass A:\n\n    @partial(jax.jit, static_argnums=0)  \n    def my_jitted_function(self, x):\n        print(\"This code runs once during the first trace.\")\n        y = x * 2\n        self.temp = y\n        return y\n\n# Program execution\ndata = non_jitted_setup()\nA = A()\nresult1 = A.my_jitted_function(data) # Tracing happens here\n\nnp.array(result1)\nnp.array(A.temp)\nIf I understand correctly, Jax runs the program line by line and traces the jitted function and runs the Python code inside it once whenever it needs to be compiled and uses the cached version otherwise.\nOnce y is is returned into result1 above, result1 becomes concrete and can be converted to a numpy.array. However, A.temp still seems to an abstract array despite it being assigned y which is what was returned and concretised to result1 in the previous line, because I get the following error when trying to convert it:\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nWhen will the value in A.temp be made concrete? Can we make the value in A.temp be concrete somehow as we know it is used outside the jitted function after it is called?",
        "answers": [
            "When you do this:\nself.temp = y\nYou are mutating a function input, and are violating the requirements of JAX transformations like jit, which are designed to operate on pure functions (see JAX Sharp Bits: Pure Functions).\nWhen will the value in A.temp be made concrete?\nThis will be made concrete when it is returned from the JIT-compiled function. Since you don't return the value, it never has the opportunity to become concrete. Functions like this which break the contract of JAX transformations result in behavior that is not well-defined.\nSide-note: you should not mark self as static when JIT-compiling class methods. In particular, you're modifying self here, so it is definitely not static! For a discussion of the pitfalls here (and recommended solutions), see JAX FAQ: how to use jit with methods."
        ],
        "link": "https://stackoverflow.com/questions/79728690/how-is-the-execution-of-jax-and-non-jax-parts-interleaved-in-a-python-program-an"
    },
    {
        "title": "Is it expected that vmapping over different input sizes for the same function impacts the accuracy of the result?",
        "question": "I was suprised to see that depending on the size of an input matrix, which is vmapped over inside of a function, the output of the function changes slightly. That is, not only does the size of the output change (which is what I would expect from vmapping) but also the numerics changed slightly. (Note that this only occurs in float32 and only on the GPU)\nI wrote a minimally reproducible example to illustrate the behaviour:\nimport jax\nimport jax.numpy as jnp\nimport equinox as eqx\n\ndef equinox_vmap(x, mlp):\n    out = eqx.filter_vmap(mlp.__call__)(x)\n    return out\n\nkey = jax.random.PRNGKey(0)\nkey, network_key = jax.random.split(key, 2)\nmlp = eqx.nn.MLP(2, 2, 10, 2, key=network_key)\n\nkey, key_x = jax.random.split(key, 2)\nx = jax.random.normal(key_x, (10000, 2))\n\nerror_eqx = equinox_vmap(x[:10], mlp) - equinox_vmap(x, mlp)[:10]\nprint(\"eqx error:\", error_eqx)\nWhen running this example I get the output:\neqx error: [[-1.4442205e-04  1.0999292e-04]\n [-5.9515238e-05 -9.1716647e-06]\n [ 1.4841557e-05  5.6132674e-05]\n [ 0.0000000e+00  0.0000000e+00]\n [-9.1642141e-06 -2.5466084e-05]\n [ 3.8832426e-05 -3.3110380e-05]\n [ 3.3825636e-05 -2.4946406e-05]\n [ 4.0918589e-05 -3.2216311e-05]\n [ 1.3601780e-04  8.7693334e-06]\n [ 0.0000000e+00  0.0000000e+00]]\nI understand that the numerics of float32 are not fully accurate and some error is to be expected. However, I was suprised that the result changes depending on how much of the input array is put into the function. I was expecting that the first row of the x array, i.e., x[0,:] would still be filled with the same values and therefore the first row in the output would be the same.\nFurther notes:\nI enabled the use of float64 (jax.config.update(\"jax_enable_x64\", False)) which completely removed this from occuring. I understand that this is a numerical problem, but I am a little bit confused how the vmapping interacts with the example.\nWhen I run the same example on the CPU (using jax.config.update(\"jax_platform_name\", \"cpu\")) this problem also disappears which I also find difficult to understand.\nQuestions:\nIs this to be expected?\nWhere does this \"inconsistency\" come from?\nWhy does it not occur on the CPU and only on the GPU?\nSetup:\nGPU: NVIDIA RTX 6000 Ada Generation 48 GB\nPython 3.11.11 with\nequinox                  0.13.0\njax                      0.7.0\njax-cuda12-pjrt          0.7.0\njax-cuda12-plugin        0.7.0\njaxlib                   0.7.0\njaxtyping                0.3.2\nml_dtypes                0.5.3\nnumpy                    2.3.2\nnvidia-cublas-cu12       12.9.1.4\nnvidia-cuda-cupti-cu12   12.9.79\nnvidia-cuda-nvcc-cu12    12.9.86\nnvidia-cuda-nvrtc-cu12   12.9.86\nnvidia-cuda-runtime-cu12 12.9.79\nnvidia-cudnn-cu12        9.11.0.98\nnvidia-cufft-cu12        11.4.1.4\nnvidia-cusolver-cu12     11.7.5.82\nnvidia-cusparse-cu12     12.5.10.65\nnvidia-nccl-cu12         2.27.6\nnvidia-nvjitlink-cu12    12.9.86\nnvidia-nvshmem-cu12      3.3.9\nopt_einsum               3.4.0\npip                      24.0\nscipy                    1.16.1\nsetuptools               65.5.0\ntyping_extensions        4.14.1\nwadler_lindig            0.1.7\nAny explanations are greatly appreachiated.",
        "answers": [
            "This is behaving as expected. This is not fundamentally about vmap; this is about floating point math. Whenever you're doing floating point operations, you will accumulate rounding errors, and when you do the \"same\" computation in two different ways, you will accumulate rounding errors differently (see Is floating-point math broken? for some discussion of this).\nRunning vmap over different batch sizes results in different sequences of operations, which in turn results in different rounding errors.\nAs for why this differs between CPU and GPU, it's all about how the floating point operations are sequenced. CPU is a serial architecture, so it's likely computing matrix products row-by-row with the same accumulation orders regardless of input size. GPU is a parallel architecture, and will generally distribute and accumulate results differently depending on the size of the inputs."
        ],
        "link": "https://stackoverflow.com/questions/79726091/is-it-expected-that-vmapping-over-different-input-sizes-for-the-same-function-im"
    },
    {
        "title": "Are JAX operations already vectorized?",
        "question": "In the documentation, JAX provides vectorization. However, aren't JAX operations already vectorized? For example, to add two vectors, I thought that the element-wise additions were vectorized internally already.\nMy guess is that vectorization is useful when: it's hard for us to add a dimension for broadcasting, so we resort to a more explicit vectorization.\nEDIT: for example, instead of vectorizing convolution2d with different kernels, I simply stack the kernels, copy and stack the channel, then perform the convolution2d with this stack of kernels.",
        "answers": [
            "I have also raised a similar question here: https://github.com/jax-ml/jax/issues/26212 By now I think there is no universal answer to this and it will remain a matter of taste to a certain degree. However in some cases there is a clearer answer:\nSome operations in JAX are not natively vectorized, such as e.g. jnp.histogram or jnp.bincount, in this case you can use vmap to get a \"batched\" version of that function (for example search for \"batched_histogram\" here http://axeldonath.com/jax-diffusion-models-pydata-boston-2025/). This is really convenient and avoids loops to improve readability as well as performance.\nvmap works over PyTrees. Some libraries (most notably equinox) use this to avoid the need for handling a batch axis in models completely and just finally vmap over the whole parameter tree by convention. This frees developers from thinking about the batch axis at all, but when working with equinox you have to stick to that convention. It also only works if operations are independent across different batches. It does not work for operations such as a \"batch norm\" (see also https://docs.kidger.site/equinox/examples/stateful/)\nIn some cases one introduces a local(!) extra dimension to an array to avoid writing a Python loop and optionally reduce after. This can often be implemented more shortly and with clearer intent using vmap (basically what you said).\nAs broadcasting and batch axes are universally accepted convention in deep learning I mostly stick with them. But I rely on vmap whenever there is no native vectorization, whenever I work with libraries that rely on vmap by convention, or whenever I need to vectorize operations along non-conventional axes (basically everything except batch axis)."
        ],
        "link": "https://stackoverflow.com/questions/79718029/are-jax-operations-already-vectorized"
    },
    {
        "title": "Does vmap correctly split the RNG keys?",
        "question": "In the following code, when I remove the vmap, I have the right randomized behavior. However, with vmap, I don't anymore. Isn't this supposed to be one of the features of nnx.vmap?\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\n@nnx.vmap(\n    in_axes=(None, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((4, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")",
        "answers": [
            "To make dropout work together with vmap in flax, we need to use split_rngs and StateAxes :\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# --- 1. Define a Simple Model with a Stateful Layer (Dropout) ---\n# We use nnx.Dropout because it requires random numbers, making it a stateful\n# operation that benefits from nnx.vmap's automatic RNG splitting.\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    # The dropout layer needs an RNG stream to generate random masks.\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray, *, train: bool) -> jnp.ndarray:\n    \"\"\"Applies the model to a single input.\"\"\"\n    # The `deterministic` flag controls whether dropout is active.\n    # We pass `not train` to it.\n    x = self.linear(x)\n    x = self.dropout(x, deterministic=not train)\n    return x\n\n# --- 2. Initialization ---\n# Create a PRNG key for reproducibility.\nkey = jax.random.PRNGKey(42)\n\n# Instantiate the model. NNX requires an `nnx.Rngs` object to manage\n# different random number streams (e.g., for 'params' and 'dropout').\n# We need to provide an RNG stream for 'params' as well for the Linear layer.\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n\n# --- 3. Define and Transform the Batched Apply Function ---\n# We want to apply our model to a whole batch of data.\n# We compose nnx.vmap and nnx.jit to create an efficient, batched function.\n\n# Define a helper function that takes the model, inputs, and train flag.\n# Apply nnx.vmap and nnx.jit as decorators.\n# Apply vmap first, then jit.\nbs = 4\n\nstate_axes = nnx.StateAxes({'dropout': 0, ...: None})\n\n@nnx.split_rngs(splits=bs, only='dropout')\n@nnx.vmap(\n    in_axes=(state_axes, 0, None), # model is not vmapped, x is vmapped, train is not vmapped\n    out_axes=0 # Output is vmapped\n)\n@nnx.jit(static_argnames=[\"train\"])\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray, train: bool):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  # NNX will handle the state and RNGs of the model instance passed to this function.\n  return model(x, train=train)\n\n\n# --- 4. Run the Demonstration ---\n# Create a dummy batch of 4 identical inputs. Each input is a vector of 10 ones.\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\nmodel.train()\n\n# Run the JIT-compiled, vmapped function.\n# Pass the model instance as the first argument. NNX will handle its state and RNGs.\noutput_batch = batched_apply(model, batch_input, train=True)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\n# --- 5. Verification ---\n# Because dropout is random and nnx.vmap correctly split the RNG keys,\n# each row in the output batch should be different, even though the inputs were identical.\n# We verify that not all outputs are the same.\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\nOutput with jax: 0.7.0.dev20250704, flax: 0.10.6\nOutput batch:\n[[0.         0.1736668  1.6533196  0.         0.        ]\n [0.         0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]\n [0.09358063 0.         1.6533196  0.         0.7218913 ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.",
            "I'm not sure nnx.vmap and nnx.split_rngs are necessary in vfdev's answer. Also, having a train kwarg is unnecessary in most situations since NNX models can dynamically jump between train=True, train=False with .train() and .eval()\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass SimpleDropoutModel(nnx.Module):\n  def __init__(self, *, rngs: nnx.Rngs):\n    \"\"\"Intializes the model.\"\"\"\n    self.dropout = nnx.Dropout(rate=0.5, rngs=rngs)\n    self.linear = nnx.Linear(in_features=10, out_features=5, rngs=rngs)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    x = self.linear(x)\n    x = self.dropout(x)\n    return x\n\nkey = jax.random.PRNGKey(42)\n\nmodel = SimpleDropoutModel(rngs=nnx.Rngs(params=key, dropout=key))\n\nprint(\"Model initialized successfully.\")\nprint(\"Dropout Rate:\", model.dropout.rate)\nprint(\"-\" * 30)\n\n@nnx.jit\ndef batched_apply(model: SimpleDropoutModel, x: jnp.ndarray):\n  \"\"\"Applies the model to a batch of inputs.\"\"\"\n  return model(x)\n\nbs = 4\nbatch_input = jnp.ones((bs, 10))\n\nprint(f\"Input batch shape: {batch_input.shape}\")\nprint(\"Input batch:\")\nprint(batch_input)\nprint(\"-\" * 30)\nprint(\"Running the batched model in training mode (dropout is active)...\")\n\n# Enable training. This works because Dropout layers have a .deterministic property\n# that can be modified.\nmodel.train()\n\noutput_batch = batched_apply(model, batch_input)\n\nprint(f\"Output batch shape: {output_batch.shape}\\n\")\nprint(\"Output batch:\")\nprint(output_batch)\nprint(\"-\" * 30)\n\nfirst_output = output_batch[0]\nall_same = jnp.all(jnp.all(output_batch == first_output, axis=1))\n\nif not all_same:\n    print(\"✅ Verification successful: The outputs are different for each sample in the batch.\")\n    print(\"This proves nnx.vmap correctly split the 'dropout' RNG stream.\")\nelse:\n    print(\"❌ Verification failed: All outputs were the same.\")\noutput:\nModel initialized successfully.\nDropout Rate: 0.5\n------------------------------\nInput batch shape: (4, 10)\nInput batch:\n[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n------------------------------\nRunning the batched model in training mode (dropout is active)...\nOutput batch shape: (4, 5)\n\nOutput batch:\n[[0.         0.1736668  0.         0.         0.        ]\n [0.         0.         1.6533196  1.0752656  0.        ]\n [0.         0.         0.         0.         0.7218913 ]\n [0.09358063 0.         0.         1.0752656  0.        ]]\n------------------------------\n✅ Verification successful: The outputs are different for each sample in the batch.\nThis proves nnx.vmap correctly split the 'dropout' RNG stream.\nand if instead you do model.eval()\nOutput batch:\n[[0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]\n [0.04679031 0.0868334  0.8266598  0.5376328  0.36094564]]\n------------------------------\n❌ Verification failed: All outputs were the same."
        ],
        "link": "https://stackoverflow.com/questions/79698307/does-vmap-correctly-split-the-rng-keys"
    },
    {
        "title": "Configuration options varying between jax installs?",
        "question": "I have a laptop I do work on for a program that includes jax, the program ends up getting run here on a small scale to test it, then it is sent off to a server for batch processing.\nIn the program I have set these flags for jax:\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\n(as well as others but these are the relevant ones)\nThis runs fine on my laptop (using sharding to CPU parallelise), but when running on the server on GPU, I get an error message:\nAttributeError: Unrecognized config option: jax_captured_constants_report_frames\n(and the same for jax_captured_constants_warn_bytes if that were to run first)\nWhy is there this discrepancy? Can I use these flags some other way that is generalised between different jax installs?\npip list | grep jax, on laptop:\njax                       0.6.2\njaxlib                    0.6.2\njaxtyping                 0.3.2\non server:\njax                       0.6.0\njax-cuda12-pjrt           0.6.0\njax-cuda12-plugin         0.6.0\njaxlib                    0.6.0\njaxtyping                 0.3.2\nEDIT: As a side note, what is the scope of jax flags? I have a jax initialisation function to set os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=\" + str(cpu_count()) before the rest of the code runs, if I set jax.config.update(..., ...) options in here, will they hold in files called after it that also import jax? Or do I have to set them again? Is there a function to check the current value of these flags?",
        "answers": [
            "The jax_captured_constants_report_frames and jax_captured_constants_warn_bytes configurations were added in JAX version 0.6.1 (Relevant PR: https://github.com/jax-ml/jax/pull/28157) If you want to use them on your server, you'll have to update JAX to v0.6.1 or later."
        ],
        "link": "https://stackoverflow.com/questions/79693916/configuration-options-varying-between-jax-installs"
    },
    {
        "title": "Unable to set cpu device count for jax parallelisation?",
        "question": "I have been trying to generalise this jax program for solving on both CPU and GPU depending on the machine it's running on (essentially need cpu parallelisation to speed up testing versus gpu for production). I can get jax to parallelise on the GPU, but no matter what I do jax will not detect my cpu_count and thus cannot be sharded across cores (for context am running on 8 core, 16 thread laptop processor).\nI found out that XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT had to be set before jax was initialised (was previously set in the if statement included in the code), but it is still not working. I also tried setting at the very start of my code (this is a snippet from the only file using jax itself, but some other files use jnp as a jax drop in for numpy).\nCan anyone tell me why jax will not pick up on the flag? (Relevant code snippet and jupyter notebook output included below). Thanks.\nRelevant code snippet:\nfrom multiprocessing import cpu_count\ncore_count = cpu_count()\n\n### THIS NEEDS TO BE SET BEFORE JAX IS INITIALISED IN ANY WAY, INCLUDING IMPORTING\n# - XLA_FLAGS are read WHEN jax is IMPORTED\n\n# you can see other ways of setting the environment variable that I've tried here\n\n#jax.config.update('xla_force_host_platform_device_count', core_count)\n#os.environ[\"XLA_FORCE_HOST_PLATFORM_DEVICE_COUNT\"] = '16'#str(core_count)\n#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=' + str(core_count)\nos.environ[\"XLA_FLAGS\"] = f\"--xla_force_host_platform_device_count={cpu_count()}\"\n\nimport jax\n\n# defaults float data types to 64-bit instead of 32 for greater precision\njax.config.update('jax_enable_x64', True)\njax.config.update('jax_captured_constants_report_frames', -1)\njax.config.update('jax_captured_constants_warn_bytes', 128 * 1024 ** 2)\njax.config.update('jax_traceback_filtering', 'off')\n# https://docs.jax.dev/en/latest/gpu_memory_allocation.html\n#jax.config.update('xla_python_client_allocator', '\\\"platform\\\"')\n# can't set via jax.config.update for some reason\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = '\\\"platform\\\"'\n\nprint(\"\\nDefault jax backend:\", jax.default_backend())\n\navailable_devices = jax.devices()\nprint(f\"Available devices: {available_devices}\")\n\nrunning_device = xla_bridge.get_backend().platform\nprint(\"Running device:\", running_device, end='')\n\nif running_device == 'cpu':\n    print(\", with:\", core_count, \"cores.\")\n\n    from jax.sharding import PartitionSpec as P, NamedSharding\n\n    # Create a Sharding object to distribute a value across devices:\n    # Assume core_count is the no. of core devices available\n    mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n\n    # Example matrix shape (9, N), e.g., N = 1e7\n    #x = jax.random.normal(jax.random.key(0), (9, Np))\n\n    # Specify sharding: don't split axis 0 (rows), split axis 1 (columns) across devices\n    # then apply sharding to produce a sharded array from the matrix input\n    # and use jax.device_put to distribute it across devices:\n    s0_sharded = jax.device_put(s0, NamedSharding(mesh, P(None, 'cols')))  # 'None' means don't shard axis 0\n\n    print(s0_sharded.sharding)            # See the sharding spec\n    print(s0_sharded.addressable_shards)  # Check each device's shard\n    jax.debug.visualize_array_sharding(s0_sharded)\nOutput:\nDefault jax backend: cpu\nAvailable devices: [CpuDevice(id=0)]\nRunning device: cpu, with: 16 cores.\n\n...\n\nrelevant line of my code: --> 258 mesh = jax.make_mesh((core_count,), ('cols',))  # 1D mesh for columns\n... jax backend trace\nValueError: Number of devices 1 must be >= the product of mesh_shape (16,)",
        "answers": [
            "I tried running your snippet and got a number of errors related to missing imports and undefined names (os is not defined, xla_bridge is not defined, s0 is undefined). This, along with the fact that you're running in Jupyter notebook, makes me think that you've already imported JAX in your runtime before running this cell.\nAs mentioned in your code comments, the XLA device count must be set before JAX is imported in your runtime. You should try restarting the Jupyter kernel, then fix the missing imports and variables and rerun your cell as the first execution in your fresh runtime.\nHere's a simple recipe that should work to set the device count while asserting that you've not already imported JAX in another cell in your notebook:\nimport os\nimport sys\n\nassert \"jax\" not in sys.modules, \"jax already imported: you must restart your runtime\"\nos.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"\n\nimport jax\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\nIf running this results in an assertion error, then you'll have to restart your kernel/runtime before running it again."
        ],
        "link": "https://stackoverflow.com/questions/79691728/unable-to-set-cpu-device-count-for-jax-parallelisation"
    },
    {
        "title": "Jax vmapping while loop [closed]",
        "question": "Closed. This question needs debugging details. It is not currently accepting answers.\nEdit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\nClosed 2 months ago.\nImprove this question\nI have a function that has jax.lax.while_loop. Now, I want to vmap it. However, vmap makes the execution time very slow compared to the original one.\nI understand that in the case of lax.cond, it is transformed into select, which evaluates all branches and thus may decrease the computational speed.\nIs a similar thing happening here? If so, what is the best practice to do do xx while y is true with vmap?",
        "answers": [
            "A while_loop under vmap becomes a single while_loop over a batched body_fun and cond_fun, meaning effectively that every loop in the batch executes for the same number of iterations. If different batches lead to vastly different iteration times, this can result in extra computation compared to executing individual while_loops in sequence."
        ],
        "link": "https://stackoverflow.com/questions/79660448/jax-vmapping-while-loop"
    },
    {
        "title": "Looking for an efficent JAX function to reconstruct an image from patches",
        "question": "I have a set of images in (c, h, w) jax arrays. These arrays have been converted to (patch_index, patch_dim) arrays where patch_dim == c * h * w.\nI am trying to reconstruct the original images from the patches. Here is vanilla python code that works:\nkernel = jnp.ones((PATCH_DIM, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH), dtype=jnp.float32)\n\ndef fwd(x):\n    xcv = lax.conv_general_dilated_patches(x, (PATCH_HEIGHT, PATCH_WIDTH), (PATCH_HEIGHT, PATCH_WIDTH), padding='VALID')\n\n    # return channels last\n    return jnp.transpose(xcv, [0,2,3,1])\n\npatches = fwd(bfrc)\n\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\n\n# V_PATCHES == IMG_HEIGHT // PATCH_HEIGHT\n# H_PATCHES == IMG_WIDTH // PATCH_WIDTH\n\nreconstructed = np.zeros(EXPECTED_IMG_SHAPE)\n\nfor vpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[0]):\n    for hpatch in range(0, patch_reshaped_ph_pw_c_h_w.shape[1]):\n        for ch in range(0, patch_reshaped_ph_pw_c_h_w.shape[2]):\n            for prow in range(0, patch_reshaped_ph_pw_c_h_w.shape[3]):\n                for pcol in range(0, patch_reshaped_ph_pw_c_h_w.shape[4]):\n                    row = vpatch * PATCH_HEIGHT + prow\n                    col = hpatch * PATCH_WIDTH + pcol\n                    reconstructed[0, ch, row , col] = patch_reshaped_ph_pw_c_h_w[vpatch, hpatch, ch, prow, pcol]\n\n# This assert passes\nassert jnp.max(jnp.abs(reconstructed - bfrc[0])) == 0\nOf course this vanilla python code is very inefficient. How can I convert the for loops into efficient JAX code?",
        "answers": [
            "I'm not sure what happened here:\npatch_reshaped_pn_c_h_w = patch_reshaped_ph_pw_c_h_w = jnp.reshape(patches, (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH))\nbut I assume it's some kind of mistake.\nAssuming bfrc has shape of (batch, channels, height, width), and\nV_PATCHES = IMG_HEIGHT // PATCH_HEIGHT\nH_PATCHES = IMG_WIDTH // PATCH_WIDTH\nthen patch_reshaped_pn_c_h_w will have the shape of (V_PATCHES, H_PATCHES, IMG_CHANNELS, PATCH_HEIGHT, PATCH_WIDTH).\nKeeping this in mind, you can simply reconstruct the image via simply transposing and reshaping, which is quite cheaper than these nested loops.\nV, H, C, PH, PW = patch_reshaped_ph_pw_c_h_w.shape\n\nH_total = V * PH\nW_total = H * PW\n\npatches = jnp.transpose(patch_reshaped_ph_pw_c_h_w, (0, 1, 3, 4, 2))  # (V, H, PH, PW, C)\n\nreconstructed = patches.reshape(V, H, PH, PW, C)\nreconstructed = reconstructed.transpose(0, 2, 1, 3, 4)\nreconstructed = reconstructed.reshape(H_total, W_total, C)\nreconstructed = jnp.transpose(reconstructed, (2, 0, 1))[jnp.newaxis, ...] # (1, C, H, W)\nYou can additionally wrap it into @jax.jit, which should be slightly faster."
        ],
        "link": "https://stackoverflow.com/questions/79647350/looking-for-an-efficent-jax-function-to-reconstruct-an-image-from-patches"
    },
    {
        "title": "Would using lists rather than jax.numpy arrays lead to more accurate numerical transformations?",
        "question": "I am doing a project with RNNs using jax and flax and I have noticed some behavior that I do not really understand.\nMy code is basically an optimization loop where the user provides the initial parameters for the system they want to optimize. This system is divided onto several time steps. He feeds the initial input into the first time step of the the system, gets a certain output, feeds this output into a RNN which returns the parameters for the following time step and so on. Then it is optimized using adam (particularly using optax).\nNow the user inputs his initial parameters as a dict and then there is a function called prepare_parameters_from_dict that basically converts this dict into a list of lists (or a list of jnp arrays for that matter).\nMy question/observation is when I make this function return a list of jnp.arrays instead of a list of lists, the property I am optimizing is an order of magnitude worse!\nFor example, using a list of lists outputs 0.9997 and a list of jnp.arrays outputs 0.998 (the closer to one the better).\nNoting: the RNN output a list of jnp.arrays (it is using flax linnen) and everything in the code remains the same.\nHere are said function:\nOutputing list of lists:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(flat_params)\n        shapes.append(len(flat_params))\n    return res, shapes\nUsing list of jnp.arrays:\ndef prepare_parameters_from_dict(params_dict):\n    \"\"\"\n    Convert a nested dictionary of parameters to a flat list and record shapes.\n\n    Args:\n        params_dict: Nested dictionary of parameters.\n\n    Returns:\n        tuple: Flattened parameters list and list of shapes.\n    \"\"\"\n    res = []\n    shapes = []\n    for value in params_dict.values():\n        flat_params = jax.tree_util.tree_leaves(value)\n        res.append(jnp.array(flat_params))\n        shapes.append(jnp.array(flat_params).shape[0])\n    return res, shapes\nand this is an example of the users input initial params:\ninitial_params = {\n    \"param1\": {\n        \"gamma\": 0.1,\n        \"delta\": -3 * jnp.pi / 2,\n    }\n}\nThe rest of the code remains exactly the same for both.\nAfter optimization if for example there were five time steps, this is how the final optimized params for each time step would look like:\nusing list of jnp.arrays:\n[[Array([ 0.1       , -4.71238898], dtype=float64)],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nusing list of lists:\n[[ [0.1       , -4.71238898] ]],\n [Array([-0.97106537, -0.03807388], dtype=float64)],\n [Array([-1.17050792, -0.01463591], dtype=float64)],\n [Array([-0.77229875, -0.0124556 ], dtype=float64)],\n [Array([-1.56113376, -0.01103598], dtype=float64)]]\nWould such a difference in behavior be due to how jax handles grad and jit and others with lists compared to jnp.arrays or am I missing something?",
        "answers": [
            "The main operative difference between these two cases is that Python floats are treated as weakly-typed, meaning that the list version of your code could result in operations being performed at a lower precision. For example:\nIn [1]: import jax\n\nIn [2]: import jax.numpy as jnp\n\nIn [3]: jax.config.update('jax_enable_x64', True)\n\nIn [4]: list_values = [0.1, -4.71238898]\n\nIn [5]: array_values = jax.numpy.array(list_values)\n\nIn [6]: x = jax.numpy.float32(1.0)\n\nIn [7]: x + list_values[1]\nOut[7]: Array(-3.712389, dtype=float32)\n\nIn [8]: x + array_values[1]\nOut[8]: Array(-3.71238898, dtype=float64)\nNotice that the array version leads to higher-precision computations in this case. If I had to guess what the main difference is in your two runs, I'd guess something to do with the precision implied by strict vs weak types."
        ],
        "link": "https://stackoverflow.com/questions/79634990/would-using-lists-rather-than-jax-numpy-arrays-lead-to-more-accurate-numerical-t"
    },
    {
        "title": "How to select between using a `jax.lax.scan` vs a `for` loop when using JAX?",
        "question": "I am a JAX beginner and someone experienced with JAX told me that if we have repeated calls to a scan/for loop (e.g. when these are themselves wrapped by another for loop), it might be better to leave the loop as a for instead of converting it to a scan because the for loop is unrolled completely and only has the 1-time huge compilation cost while the scan is not unrolled by default and even though its compilation cost will be small, the fact that it is rolled will mean that the cost of repeatedly running this loop will end up making the scan more expensive than the for. This did not strike me immediately when I started writing my code, but made sense upon thinking about it.\nSo, I tested this assumption using code based on the following pseudo-code (the full code is really long and I hope these relevant parts I provide here are easier to understand):\nfor i in range(num_train_steps):  # Currently fixed to be a for loop\n  for j in range(num_env_steps):  # Currently fixed to be a for loop\n    act()\n\ndef act():\n  for k in range(num_algo_iters):  # Currently playing around with making this one either a scan or a for loop\n    jax.lax.scan(rollout_func)  # Currently fixed to be a scan\nThe only loop in the above code that I tested switching between scan and for was the k loop and then I varied the variable num_env_steps to be 1, 100, 1000 and 10000 to see whether increasing the number of times the act() (and thus the k loop) was executed made a difference to the timing. (The testing was done with 5 iterations for the k for loop and 2 iterations for the innermost scan although these are variable in general, if that matters.) The times taken for act() for the different repeats were 1.5, 11.3,, 99.0, 956.2 seconds for the scan version and 5.1, 14.5, 103.6, 972.7 seconds for the for version. So the for version never ended up faster for the number of repeats I tried.\nSo, now I am wondering if for any number of repeats (i.e. num_env_steps), the unrolling of the for actually makes the program faster than with scan. My questions:\nWould maybe increasing the repeats even more by setting num_env_steps to 100k or 1 million make it faster or can we always just replace a for with a scan? I have this question because I wonder if I am trying to over-optimise my code by converting every for to a scan.\nIf I set unroll = True for the scan, would it then always be fine to replace all fors with scans and expect speed-ups?\nIs there a rule of thumb that can help me decide when to use for and when to use scan if I am only interested in such speed-ups?\nact was jitted by the way.",
        "answers": [
            "scan vs for loop is essentially a tradeoff between compilation cost and runtime cost.\nJAX unrolls Python control flow, meaning that a for loop with 100 iterations leads to a single linear program with 100 copies of the loop body. The benefit of this is that it leaves the compiler free to optimize code across loop iterations, e.g. fusing operations between one iteration and the next; or noticing that one output is unused and eliding every computation in its graph. The downside is that compilation cost grows super-linearly with the size of the program, so for loops with large loop bodies and/or many iterations can lead to very long compilation times.\nWith scan or fori_loop on the other hand, the looping logic is pushed into the HLO, and the loop body is only parsed and compiled once. This results in much more efficient compilation, but may leave some runtime performance on the table compared to a for loop, because the compiler has fewer degrees of freedom to work with.\nThe best option will depend on the details of your program, and the relative importance of runtime and compile time costs in your particular application. Speaking very generally, though: for a smaller loop body with fewer iterations, for loops are often the better choice. For a larger loop body with more iterations, scan / fori_loop is likely better.\nNote also that scan has an unroll parameter that gives you the ability to tune the tradeoff between these extremes: unroll=True is effectively equivalent to a for loop, while unroll=n for 1 < n < N_iterations effectively puts a small for loop within each step of the larger scan."
        ],
        "link": "https://stackoverflow.com/questions/79633608/how-to-select-between-using-a-jax-lax-scan-vs-a-for-loop-when-using-jax"
    },
    {
        "title": "JAX Point Cloud Processing: Slow index_points_3d operation causing extreme XLA fusion loops in backpropagation",
        "question": "I'm trying to use JAX for implementing point cloud processing. However, I found that training becomes extremely slow due to my implementation of the following index_points_3d operation, which performs selection of features based on 3D indices.\nHere's my current implementation:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    features_expanded = features[..., None, :]\n    idx_expanded = indices[..., None]\n    return jnp.take_along_axis(features_expanded, idx_expanded, axis=1)\nWhen I traced the profiler, I found that this operation triggers extreme repetitions of loop_dynamic_update_slice_fusion, loop_add_fusion, input_reduce_fusion, and loop_select_fusion in the backpropagation stage as in following.\nThe forward pass is not a problem since the learning went fast when I stopped the gradient of the output features.\nI've tried different implementations such as using vmap on the batch dimension, but failed to achieve any performance gains.\nI'm not deeply familiar with JAX's low-level operations, so I'm unsure if this is a fundamental limitation of JAX/XLA or if there's a more efficient approach. Any help or guidance on optimizing this operation would be greatly appreciated!",
        "answers": [
            "Thanks to jakevdp's comment, I got a significant speedup using one-hot matrix multiplication. I changed to the following code:\n@jax.jit\ndef index_points_3d(features, indices):\n    \"\"\"\n    Args:\n        features: shape (B, N, C)\n        indices: shape (B, npoint, nsample)\n    \n    Returns:\n        shape (B, npoint, nsample, C)\n    \"\"\"\n    B, N, C = features.shape\n    _, S, K = indices.shape\n    one_hot = jax.nn.one_hot(indices, num_classes=N, dtype=features.dtype)\n    return jnp.einsum('bskn,bnc->bskc', one_hot, features)"
        ],
        "link": "https://stackoverflow.com/questions/79631678/jax-point-cloud-processing-slow-index-points-3d-operation-causing-extreme-xla-f"
    },
    {
        "title": "Why some nested python functions are defined as `def _():`",
        "question": "I understand internal functions are prefixed with '_' to indicate they are helper/internal functions. It also helps with tooling etc. But I find some functions with just '_' as their name. Can't even find where they are called from. e.g., from\nhttps://github.com/jax-ml/jax/blob/7412adec21c534f8e4bcc627552f28d162decc86/jax/_src/pallas/mosaic/helpers.py#L72\ndef run_on_first_core(core_axis_name: str):\n  \"\"\"Runs a function on the first core in a given axis.\"\"\"\n  num_cores = jax.lax.axis_size(core_axis_name)\n  if num_cores == 1:\n    return lambda f: f()\n\n  def wrapped(f):\n    core_id = jax.lax.axis_index(core_axis_name)\n\n    @pl_helpers.when(core_id == 0)\n    @functools.wraps(f)\n    def _(): ## How is this called?\n      return f()\n\n  return wrapped\nThere are several of them in an internal code base but here are some references\nhttps://github.com/search?q=repo%3Ajax-ml%2Fjax%20def%20_()%3A&type=code\nhttps://github.com/jax-ml/jax/blob/7412adec21c534f8e4bcc627552f28d162decc86/docs/pallas/tpu/distributed.ipynb#L1125",
        "answers": [
            "A name of _ is different from a name prefixed with _. A name that is only _ means, by convention, \"I need to supply a name to satisfy the syntax, but I don't actually need to use the name\"*. That would be the case here, since the _ is never actually used anywhere.\nIn terms of how this function is actually called, the when decorator appears to be here:\ndef when(condition):\n  def _wrapped(f):\n    if isinstance(condition, bool):\n      if condition:\n        f()\n    else:\n      jax.lax.cond(condition, f, lambda: None)\n  return _wrapped\nYou can see that the decorator has a handle on the function via f, and calls it internally if condition is satisfied.\n* I could have sworn that this convention comes from PEP8, but I've skimmed the document twice now, and can't find where it says it."
        ],
        "link": "https://stackoverflow.com/questions/79625948/why-some-nested-python-functions-are-defined-as-def"
    },
    {
        "title": "Why is array manipulation in JAX much slower?",
        "question": "I'm working on converting a transformation-heavy numerical pipeline from NumPy to JAX to take advantage of JIT acceleration. However, I’ve found that some basic operations like broadcast_to and moveaxis are significantly slower in JAX—even without JIT—compared to NumPy, and even for large batch sizes like 3,000,000 where I would expect JAX to be much quicker.\n### Benchmark: moveaxis + broadcast_to ###\nNumPy: moveaxis + broadcast_to → 0.000116 s\nJAX: moveaxis + broadcast_to → 0.204249 s\nJAX JIT: moveaxis + broadcast_to → 0.054713 s\n\n### Benchmark: broadcast_to only ###\nNumPy: broadcast_to → 0.000059 s\nJAX: broadcast_to → 0.062167 s\nJAX JIT: broadcast_to → 0.057625 s\nAm I doing something wrong? Are there better ways of performing these kind of manipulations?\nHere's a minimal benchmark ChatGPT generated, comparing broadcast_to and moveaxis in NumPy, JAX, and JAX with JIT:\nimport timeit\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit\n\n# Base transformation matrix\nM_np = np.array([[1, 0, 0, 0.5],\n                 [0, 1, 0, 0],\n                 [0, 0, 1, 0],\n                 [0, 0, 0, 1]])\n\nM_jax = jnp.array(M_np)\n\n# Batch size\nn = 1_000_000\n\nprint(\"### Benchmark: moveaxis + broadcast_to ###\")\n\n# NumPy\nt_numpy = timeit.timeit(\n    lambda: np.moveaxis(np.broadcast_to(M_np[:, :, None], (4, 4, n)), 2, 0),\n    number=10\n)\nprint(f\"NumPy: moveaxis + broadcast_to → {t_numpy:.6f} s\")\n\n# JAX\nt_jax = timeit.timeit(\n    lambda: jnp.moveaxis(jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)), 2, 0).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: moveaxis + broadcast_to → {t_jax:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_and_move_jax(M):\n    return jnp.moveaxis(jnp.broadcast_to(M[:, :, None], (4, 4, n)), 2, 0)\n\n# Warm-up\nbroadcast_and_move_jax(M_jax).block_until_ready()\n\nt_jit = timeit.timeit(\n    lambda: broadcast_and_move_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: moveaxis + broadcast_to → {t_jit:.6f} s\")\n\nprint(\"\\n### Benchmark: broadcast_to only ###\")\n\n# NumPy\nt_numpy_b = timeit.timeit(\n    lambda: np.broadcast_to(M_np[:, :, None], (4, 4, n)),\n    number=10\n)\nprint(f\"NumPy: broadcast_to → {t_numpy_b:.6f} s\")\n\n# JAX\nt_jax_b = timeit.timeit(\n    lambda: jnp.broadcast_to(M_jax[:, :, None], (4, 4, n)).block_until_ready(),\n    number=10\n)\nprint(f\"JAX: broadcast_to → {t_jax_b:.6f} s\")\n\n# JAX JIT\n@jit\ndef broadcast_only_jax(M):\n    return jnp.broadcast_to(M[:, :, None], (4, 4, n))\n\nbroadcast_only_jax(M_jax).block_until_ready()\n\nt_jit_b = timeit.timeit(\n    lambda: broadcast_only_jax(M_jax).block_until_ready(),\n    number=10\n)\nprint(f\"JAX JIT: broadcast_to → {t_jit_b:.6f} s\")",
        "answers": [
            "There are a couple things happening here that come from the different execution models of NumPy and JAX.\nFirst, NumPy operations like broadcasting, transposing, reshaping, slicing, etc. typically return views of the original buffer. In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies. I suspect this is the largest contribution to the timing difference here.\nSecond, NumPy tends to have very fast dispatch time for individual operations. JAX has much slower dispatch time for individual operations, and this can become important when the operation itself is very cheap (like \"return a view of the array with different strides/shape\")\nYou might wonder given these points how JAX could ever be faster than NumPy. The key is JIT compilation of sequences of operations: within JIT-compiled code, sequences of operations are fused so that the output of each individual operation need not be allocated (or indeed, need not even exist at all as a buffer of intermediate values). Additionally, for JIT compiled sequences of operations the dispatch overhead is paid only once for the whole program. Compare this to NumPy where there's no way to fuse operations or to avoid paying the dispatch cost of each and every operation.\nSo in microbenchmarks like this, you can expect JAX to be slower than NumPy. But for real-world sequences of operations wrapped in JIT, you should often find that JAX is faster, even when executing on CPU.\nThis type of question comes up enough that there's a section devoted to it in JAX's FAQ: FAQ: is JAX faster than NumPy?\nAnswering the followup question:\nIs the statement \"In JAX, it is not possible for two array objects to share memory, and so the equivalent operations return copies\", within a jitted environment?\nThis question is not really well-formulated, because in a jitted environment, array objects do not necessarily correspond to buffers of values. Let's make this more concrete with a simple example:\nimport jax\n\n@jax.jit\ndef f(x):\n  y = x[::2]\n  return y.sum()\nYou might ask: in this program, is y a copy or a view of x? The answer is neither, because y is never explicitly created. Instead, JIT fuses the slice and the sum into a single operation: the array x is the input, and the array y.sum() is the output, and the intermediate array y is never actually created.\nYou can see this by printing the compiled HLO for this function:\nx = jax.numpy.arange(10)\nprint(f.lower(x).compile().as_text())\nHloModule jit_f, is_scheduled=true, entry_computation_layout={(s32[10]{0})->s32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\n%region_0.9 (Arg_0.10: s32[], Arg_1.11: s32[]) -> s32[] {\n  %Arg_0.10 = s32[] parameter(0), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  %Arg_1.11 = s32[] parameter(1), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\"}\n  ROOT %add.12 = s32[] add(s32[] %Arg_0.10, s32[] %Arg_1.11), metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\n%fused_computation (param_0.2: s32[10]) -> s32[] {\n  %param_0.2 = s32[10]{0} parameter(0)\n  %iota.0 = s32[5]{0} iota(), iota_dimension=0, metadata={op_name=\"jit(f)/jit(main)/iota\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.1 = s32[] constant(2)\n  %broadcast.0 = s32[5]{0} broadcast(s32[] %constant.1), dimensions={}\n  %multiply.0 = s32[5]{0} multiply(s32[5]{0} %iota.0, s32[5]{0} %broadcast.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %bitcast.1 = s32[5,1]{1,0} bitcast(s32[5]{0} %multiply.0), metadata={op_name=\"jit(f)/jit(main)/mul\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %gather.0 = s32[5]{0} gather(s32[10]{0} %param_0.2, s32[5,1]{1,0} %bitcast.1), offset_dims={}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1}, indices_are_sorted=true, metadata={op_name=\"jit(f)/jit(main)/gather\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=4}\n  %constant.0 = s32[] constant(0)\n  ROOT %reduce.0 = s32[] reduce(s32[5]{0} %gather.0, s32[] %constant.0), dimensions={0}, to_apply=%region_0.9, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\n\nENTRY %main.14 (Arg_0.1: s32[10]) -> s32[] {\n  %Arg_0.1 = s32[10]{0} parameter(0), metadata={op_name=\"x\"}\n  ROOT %gather_reduce_fusion = s32[] fusion(s32[10]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/reduce_sum\" source_file=\"<ipython-input-1-9ea6c70efef5>\" source_line=5}\n}\nThe output is complicated, but the main thing to look at here is the ENTRY %main section, which is the \"main\" program generated by compilation. It consists of two steps: %Arg0.1 identifies the input argument, and ROOT %gather_reduce_fusion is essentially a single compiled kernel that sums every second element of the input. No intermediate arrays are generated. The blocks above this (e.g. the %fused_computation (param_0.2: s32[10]) -> s32[] definition) give you information about what operations are done within this kernel, but represent a single fused operation.\nNotice that the sliced array represented by y in the Python code never actually appears in the main function block, so questions about its memory layout cannot be answered except by saying \"y doesn't exist in the compiled program\".",
            "According to the Jax Docs (emphasis mine):\nif you’re doing microbenchmarks of individual array operations on CPU, you can generally expect NumPy to outperform JAX due to its lower per-operation dispatch overhead"
        ],
        "link": "https://stackoverflow.com/questions/79615872/why-is-array-manipulation-in-jax-much-slower"
    },
    {
        "title": "Freezing filtered parameter collections with Flax.nnx",
        "question": "I'm trying to work out how to do transfer learning with flax.nnx. Below is my attempt to freeze the kernel of my nnx.Linear instance and optimize the bias. I think maybe I'm not correctly setting up the 'wrt' argument to my optimizer.\nfrom jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\ndef f(x,m=2.234,b=-1.123):\n    return m*x+b\n\ndef compute_loss(model, inputs, obs):\n    prediction = model(inputs)\n    error = obs - prediction\n    loss = jnp.mean(error ** 2)\n    mae = jnp.mean(jnp.abs(error ) )\n    return loss, mae\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    model.kernel.value = jnp.array([[2.0]]) #load pretrained kernel  \n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 ) \n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n    \n    loss_grad = nnx.value_and_grad(compute_loss, has_aux = True)\n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None,),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( optimizer, x, obs ):\n        (loss,mae), grads = loss_grad( optimizer.model, x, obs )        \n        optimizer.update( grads )\n        return optimizer, (loss,mae)\n\n    transfer_params = nnx.All(nnx.PathContains(\"bias\"))\n    optimizer_transfer = nnx.Optimizer(model, optax.adam(learning_rate=1e-3), wrt = transfer_params)\n\n    optimizer, (losses,maes) = optimizer_scan( optimizer_transfer, x1, obs1 )\n\n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = optimizer.model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', optimizer.model.kernel.value )\n    print( 'b approximation:', optimizer.model.bias.value )\nAnd this results in the following error:\nValueError: Mismatch custom node data: ('bias', 'kernel') != ('bias',); value: State({\n  'bias': VariableState(\n    type=Param,\n    value=Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace(level=1/0)>\n  )\n}).",
        "answers": [
            "The missing link for me was nnx.DiffState. For clarification on DiffState, see the documentation for nnx.grad() on the nnx \"transforms\" page:\nhttps://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html\nAnyway, effectively the only changes that need be made for the code to work as intended are:\nMove the declaration of transfer_params to before the value_and_grad call,\nCreate an nnx.DiffState object diff_state = nnx.DiffState(0,transfer_params)\nGive diff_state as the argnums keyword for nnx.value_and_grad.\nAnd that does it!\nAnother helpful example of how to use nnx.DiffState with parameter filtering can be found here:\nhttps://github.com/google/flax/issues/4167\nAnd lastly here is the complete fixed example:\nfrom jax import numpy as jnp\nfrom jax import random\nfrom flax import nnx\nimport optax\nfrom matplotlib import pyplot as plt\n\ndef f(x,m=2.234,b=-1.123):\n    return m*x+b\n\ndef compute_loss(model, inputs, obs):\n    prediction = model(inputs)\n    error = obs - prediction\n    loss = jnp.mean(error ** 2)\n    mae = jnp.mean(jnp.abs(error ) )\n    return loss, mae\n\nif __name__ == '__main__':\n    shape = (2,55,1)\n    epochs = 123\n\n    rngs = nnx.Rngs(123)\n    model = nnx.Linear( 1, 1, rngs=rngs )\n\n    model.kernel.value = jnp.array([[2.0]]) #load pretrained kernel\n\n    skey = rngs.params()\n    xx = random.uniform( skey, shape, minval=-10, maxval=10 ) \n    obs1,obs2 = f(xx)\n    x1,x2 = xx\n\n    transfer_params = nnx.All(nnx.PathContains(\"bias\"))\n    diff_state = nnx.DiffState(0,transfer_params)\n    \n    loss_grad = nnx.value_and_grad(compute_loss, argnums = diff_state, has_aux = True)\n    @nnx.scan(\n        in_axes=(nnx.Carry,None,None,),\n        out_axes=(nnx.Carry,0),\n        length=epochs\n    )\n    def optimizer_scan( optimizer, x, obs ):\n        (loss,mae), grads = loss_grad( optimizer.model, x, obs )        \n        optimizer.update( grads )\n        return optimizer, (loss,mae)\n\n    optimizer_transfer = nnx.Optimizer(model, optax.adamw(learning_rate = 1e-3), wrt = transfer_params)\n\n    optimizer, (losses,maes) = optimizer_scan( optimizer_transfer, x1, obs1 )\n\n    print( ' AFTER TRAINING' )\n    print( 'training loss:', losses[-1] )\n\n    y1,y2 = optimizer.model(xx)\n    error = obs2-y2\n    loss = jnp.mean( error*error )\n    print( 'test loss:',loss )\n    print( 'm approximation:', optimizer.model.kernel.value )\n    print( 'b approximation:', optimizer.model.bias.value )"
        ],
        "link": "https://stackoverflow.com/questions/79580101/freezing-filtered-parameter-collections-with-flax-nnx"
    },
    {
        "title": "DIfference in variable values in jax non-jit runtime and jit transformed runtime",
        "question": "I have a deep learning mode which I am running in the jit transformed manner by:\nmy_function_checked = checkify.checkify(model.apply)\n    model_jitted = jax.jit(my_function_checked)\n    err, pred = model_jitted({\"params\": params}, batch, training=training, rng=rng)\n    err.throw()\nThe code is compiling fine, but now I want to debug the intermediate values after every few steps, save the arrays, and then compare them with pytorch tensors. For this, I need to repeatedly save the arrays. The easiest way to do this is to use any IDE's inbuilt debugger and evaluate the save expression after every few steps. But jax.jit transformed code doesn't allow external debuggers. But, I can do this after disabling the jit. Should I be expecting any discrepancies between the two runs? Can I assume that the values in jit and non-jit runs will remain same?",
        "answers": [
            "In general when comparing the same JAX operation with and without JIT, you should expect equivalence up to typical floating point rounding errors, but you should not expect bitwise equivalence, as the compiler may fuse operations in a way that leads to differing float error accumulation."
        ],
        "link": "https://stackoverflow.com/questions/79571227/difference-in-variable-values-in-jax-non-jit-runtime-and-jit-transformed-runtime"
    },
    {
        "title": "Reproducibility of JAX calculations",
        "question": "I am using JAX in running Reinforcement Learning (RL) & Multi-Agent Reinforcement Learning (MARL) calculations. I have noticed the following behaviour:\nIn RL, my results are always fully reproducible.\nIn MARL, where computations become significantly heavier, my results are reprodicible when running on CPU.\nHowever, when running MARL in GPU I encounter a different behaviour. I have noticed that repeating the same calculation within a script execution leads to identical results. However, executing the same script twice leads to different results. The latter problem is only mitigated when I use:\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_deterministic_ops\"\nos.environ[\"JAX_DISABLE_MOST_FASTER_PATHS\"] = \"1\"\nUnfortunately, this measure significantly reduces the computation speed.\nAny idea about dealing with this issue?",
        "answers": [
            "On an accelerator like GPU, there will generally be a tradeoff between strict bit-wise reproducibility and speed of computation.\nWhy is this? Fundamentally, this is because of the fact that floating point arithmetic only approximates real arithmetic, and so the order in which operations are executed can change the results, and order of operations is a degree of freedom that the GPU can exploit to execute code faster.\nAs a simple example, consider summing the same array in different orders:\nIn [1]: import numpy as np\n\nIn [2]: rng = np.random.default_rng(0)\n\nIn [3]: x = rng.normal(size=10000).astype('float32')\n\nIn [4]: x.sum()\nOut[4]: np.float32(63.11888)\n\nIn [5]: x[::-1].sum()\nOut[5]: np.float32(63.118877)\nThe results differ slightly.\nThis is relevant to your question because of the way a GPU works: GPUs do fast vector operations by automatically running them in parallel. So, for example, to compute a sum, it might chunk the array across N cores, sum each chunk individually, and then accumulate the intermediate sums to get the final result.\nIf you only care mainly about speed, you can sacrifice reproducibility and accumulate those intermediate sums in the order they're ready, which might vary from run to run, and therefore produce slightly different results. If you care mainly about reproducibility, then you have to sacrifice some speed by ensuring that you accumulate those intermediate sums in exactly the same order every time, which may leave the process waiting for a slower chunk even if a faster chunk is already ready. This is a simplistic example but the same principal applies for any computation parallelized on a GPU.\nSo fundamentally speaking, there will always be a tradeoff between bitwise reproducibility and speed of computation. You've already discovered the primary flags for controlling this tradeoff (XLA_FLAGS=\"--xla_gpu_deterministic_ops\" and JAX_DISABLE_MOST_FASTER_PATHS=1 ). Your question seems to be \"can I somehow get both speed and strict bitwise reproducibility at once\": the answer to that question is No."
        ],
        "link": "https://stackoverflow.com/questions/79563698/reproducibility-of-jax-calculations"
    },
    {
        "title": "Flax nnx / jax: tree.map for layers of incongruent size",
        "question": "I am trying to figure out how to use nnx.split_rngs. Can somebody give a version of the code below that uses nnx.split_rngs with jax.tree.map to produce an arbitrary number of Linear layers with different out_features?\nimport jax\nfrom flax import nnx\nfrom functools import partial\n\nif __name__ == '__main__':\n\n    session_sizes = {\n        'a':2,\n        'b':3,\n        'c':4,\n        'd':5,\n        'e':6,\n    }\n    dz = 2\n\n    rngs = nnx.Rngs(0)\n    \n    my_linear = partial(\n        nnx.Linear,\n        use_bias = False,\n        in_features = dz,\n        rngs=rngs )\n    \n    def my_linear_wrapper(a):\n        return my_linear( out_features=a )\n\n    q_s = jax.tree.map(my_linear_wrapper, session_sizes)\n\n    for k in session_sizes.keys():\n        print(q_s[k].kernel)\nSo in this case, we would need a tree of layers that will take our 2 in_features into spaces of 2, ..., 6 out_features.\nThe function my_linear_wrapper is sort of a workaround for the original solution we had in mind, which is to map in very much the same fashion as we're doing, but instead use (something like) the @nnx.split_rngs function decorator.\nIs there a way to use nnx.split_rngs on my_linear in order to map over the rng argument to nnx.Linear?",
        "answers": [
            "split_rngs is mostly useful when you are going to pass the Rngs through a transform like vmap, here you want to produce variable sized Modules so the current solution is the way to go. Because of how partial works you can simplify this to:\ndin = 2\nrngs = nnx.Rngs(0)\n\nmy_linear = functools.partial(\n  nnx.Linear, din, use_bias=False, rngs=rngs\n)\n\nq_s = jax.tree.map(my_linear, session_sizes)\n\nfor k in session_sizes.keys():\n  print(q_s[k].kernel)"
        ],
        "link": "https://stackoverflow.com/questions/79551198/flax-nnx-jax-tree-map-for-layers-of-incongruent-size"
    },
    {
        "title": "Why is Jax treating floating point values as tracers rather than concretizing them when nesting jitted functions?",
        "question": "I am doing some physics simulations using jax, and this involves a function called the Hamiltonian defined as follows:\n# Constructing the Hamiltonian\n@partial(jit, static_argnames=['n', 'omega'])\ndef hamiltonian(n: int, omega: float):\n    \"\"\"Construct the Hamiltonian for the system.\"\"\"\n    H = omega *  create(n) @ annhilate(n)\n    return H \nand then a bigger function def solve_diff(n, omega, kappa, alpha0): that is defined as follows:\n@partial(jit, static_argnames=['n', 'omega'])\ndef solve_diff(n, omega, kappa, alpha0):\n    # Some functionality that uses kappa and alpha0\n    \n    H = hamiltonian(n, omega)\n\n    # returns an expectation value\nWhen I try to compute the gradient of this function using jax.grad\nn = 16   \nomega = 1.0   \nkappa = 0.1  \nalpha0 = 1.0 \n\n# Compute gradients with respect to omega, kappa, and alpha0\ngrad_population = grad(solve_diff, argnums=(1, 2, 3))\ngrads = grad_population(n, omega, kappa, alpha0)\n\nprint(f\"Gradient w.r.t. omega: {grads[0]}\")\nprint(f\"Gradient w.r.t. kappa: {grads[1]}\")\nprint(f\"Gradient w.r.t. alpha0: {grads[2]}\")\nit outputs the following error:\nValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'jax._src.interpreters.ad.JVPTracer'>, Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n  primal = 1.0\n  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n    pval = (ShapedArray(float32[], weak_type=True), None)\n    recipe = LambdaBinding(). The error was:\nTypeError: unhashable type: 'JVPTracer'\nThough, running solve_diff(16,1.0,0.1,1.0) on its own works as expected.\nNow if I remove omega from the list of static variables for both the hamiltonian function and the solve_diff, the grad is output as expected.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls, both n and omega are constants and indeed should not change between function calls.",
        "answers": [
            "The fundamental issue is that you cannot differentiate with respect to a static variable, and if you try to do so you will get the error you observed.\nThis is confusing me, because I no longer know what qualifies as static or dynamic variables anymore, from the definition that static variables does not change between function calls\nIn JAX, the term \"static\" does not have to do with whether the variable is changed between function calls. Rather, a static variable is a variable that does not participate in tracing, which is the mechanism used to compute transformations like vmap, grad, jit, etc. When you differentiate with respect to a variable, it is no longer static because it is participating in the autodiff transformation, and trying to treat it as static later in the computation will lead to an error.\nFor a discussion of transformations, tracing, and related concepts, I'd start with JAX Key Concepts: transformations."
        ],
        "link": "https://stackoverflow.com/questions/79550040/why-is-jax-treating-floating-point-values-as-tracers-rather-than-concretizing-th"
    },
    {
        "title": "General way to define JAX functions with non-differentiable arguments",
        "question": "For a particular JAX function func, one can define non-differentiable arguments by using the decorator @partial(jax.custom_jvp, nondiff_argnums=...). However, in order to make it work, one must also explicitly define the differentiation rules in a custom jvp function by using the decorator @func.defjvp. I'm wondering if there is a generic way to define non-differentiable arguments for any given func, without defining a custom jvp (or vjp) function? This will be useful when the differentiation rules are too complicated to write out.",
        "answers": [
            "In JAX's design, non-differentiated arguments are a property of the gradient transformation being used, not a property of the function being differentiated. custom_jvp is fundamentally about customizing the gradient behavior, and using it to mark non-differentiable arguments without actually customizing the gradient is not an intended use.\nThe way to ensure that arguments do not participate in an autodiff transformation is to specify the arguments you want to differentiate against when you call the jax.grad, jax.jacobian, or other autodiff transformation; e.g.\njax.grad(func, argnums=(0,))  # differentiate with respect to argument 0.\nRegardless of what func is, this will attempt to differentiate with respect to the 0th argument, and if that argument is either explicitly or implicitly not differentiable due to how func is defined, an error will be raised."
        ],
        "link": "https://stackoverflow.com/questions/79516990/general-way-to-define-jax-functions-with-non-differentiable-arguments"
    },
    {
        "title": "How can I apply member functions of a list of objects across slices of a JAX array using vmap?",
        "question": "I have a list of a objects, each of which has a function to be applied on a slice of a jax.numpy.array. There are n objects and n corresponding slices. How can I vectorise this using vmap?\nFor example, for the following code snippet:\nimport jax\nimport jax.numpy as jnp\n\nclass Obj:\n    def __init__(self, i):\n        self.i = i\n\n    def f1(self, x): return (x - self.i)\n\nx = jnp.arange(9).reshape(3, 3).astype(jnp.float32)\n\nfunctions_obj = [Obj(1).f1, Obj(2).f1, Obj(3).f1]\nhow would I apply the functions in functions_obj to slices of x?\nMore details, probably not relevant: My specific use-case is running the member functions of a lot of Reinforcement Learning Gym environment objects on slices of an actions array, but I believe my problem is more general and I formulated it as above. (P.S.: I know about AsyncVectorEnv by the way but that does not solve my problem as I am not trying to run the step function).",
        "answers": [
            "Use jax.lax.switch to select between the functions in the list and map over the desired axis of x at the same time:\ndef apply_func_obj(i, x_slice):\n    return jax.lax.switch(i, functions_obj, x_slice)\n\nindices = jnp.arange(len(functions_obj)) \n# Use vmap to apply the function element-wise\nresults = jax.vmap(apply_func_obj, in_axes=(0, 0))(indices, x)"
        ],
        "link": "https://stackoverflow.com/questions/79499056/how-can-i-apply-member-functions-of-a-list-of-objects-across-slices-of-a-jax-arr"
    },
    {
        "title": "Why does JAX's grad not always print inside the cost function?",
        "question": "I am new to JAX and trying to use it with PennyLane and optax to optimize a simple quantum circuit. However, I noticed that my print statement inside the cost function does not execute in every iteration. Specifically, it prints only once at the beginning and then stops appearing.\nThe quantum circuit itself does not make sense; I just wanted to simplify the example as much as possible. I believe the circuit is not actually relevant to the question, but it's included as an example.\nHere is my code:\nimport pennylane as qml\nimport jax\nimport jax.numpy as jnp\nimport optax\n\njax.config.update(\"jax_enable_x64\", True)\n\ndevice = qml.device(\"default.qubit\", wires=1)\n\n\n@qml.qnode(device, interface='jax')\ndef circuit(params):\n    qml.RX(params, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(params):\n    print('Evaluating')\n    return circuit(params)\n\n# Define optimizer\nparams = jnp.array(0.1)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\n\n# JIT the gradient function\ngrad = jax.jit(jax.grad(cost))\n\nfor epoch in range(5):\n    print(f'{epoch = }')\n    grad_value = grad(params)\n    updates, opt_state = opt.update(grad_value, opt_state)\n    params = optax.apply_updates(params, updates)\nExpected output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nEvaluating\nepoch = 3\nEvaluating\nepoch = 4\nEvaluating\nActual output:\nepoch = 0\nEvaluating\nepoch = 1\nEvaluating\nepoch = 2\nepoch = 3\nepoch = 4\nQuestion:\nWhy is the print statement inside cost not executed after the first iteration? Is JAX caching the function call or optimizing it in a way that skips execution? How can I ensure that cost is evaluated in every iteration?",
        "answers": [
            "When working with JAX it is important to understand the difference between \"trace time\" and \"runtime\". For JIT compilation JAX does an abstract evaluation of the function when it is called first. This is used to \"trace\" the computational graph of the function and then create a fully compiled replacement, which is cached and then invoked on the next calls (\"runtime\") of the function. Now, Python's print statements are only evaluated at trace time and not at runtime, because the code of the function has been effectively replaced by a compiled version.\nFor the case of printing during runtime, JAX has a special jax.debug.print function, you can use:\ndef cost(params):\n    jax.debug.print('Evaluating')\n    return circuit(params)\nMore on the jax.debug utilities: https://docs.jax.dev/en/latest/debugging/index.html\nAnd JIT compilation: https://docs.jax.dev/en/latest/jit-compilation.html"
        ],
        "link": "https://stackoverflow.com/questions/79498911/why-does-jaxs-grad-not-always-print-inside-the-cost-function"
    },
    {
        "title": "Jax numpy extracting non-nan values gives NonConcreteBooleanIndexError",
        "question": "I have a jax 2d array with some nan-values\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\nand want to get an array which contains for each row only the non-nan values. The resulting array has thus the same number of rows, and either less columns or the same number but with nan values padded at the end. So in this case, the result should be\narray_2d = jnp.array([\n    [1,   2,      3],\n    [10  20,jnp.nan]\n    ])\nThe order (among non-nan values) should stay the same.\nTo make things easier, I know that each row has at most k (in this case 3) non-nan values. Getting the indices for the non-nan values is very easy, but ``moving them to the front'' is harder.\nI tried to work on a row-by-row basis; the following function works indeed:\n# we want to vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\nHowever, I can't vmap this. Even though I payed attention that the returned array always has the same size, the line ret_vals = row_vals[row_mask] makes problems, since this has a dynamic size. Does anyone know how to circumvent this? I believe that functions like `jnp.where' etc don't help either.\nHere is the full MWE:\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n# we want to get -- efficiently -- all non-nan values per row.\n# we know that each row has at most 3 non-nan values\n\n# we will vmap this over each row\ndef get_non_nan_values(row_vals):\n    ret_arr = jnp.zeros(3) # there are at most 3 non-nan values per row\n    row_mask = ~jnp.isnan(row_vals)\n    ret_vals = row_vals[row_mask] # this gets all (at most 3) non-nan values. However, the size here is dynamically. This throws after vmapping NonConcreteBooleanIndexError error.\n    ret_arr = ret_arr.at[:ret_vals.shape[0]].set(ret_vals) # this returns a FIXED SIZE array\n    return ret_arr\n\n# the following works:\nget_non_nan_values(array_2d[0,:]) # should return [1,2,3]\n\n# we now vmap\nnon_nan_vals = jax.vmap(get_non_nan_values)(array_2d) # this gives error: NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[5])\nNB: The array will be very large in practice and have many nan values, while k (the number of non-nan values) is on the order of 10 or 100.\nThank you very much!",
        "answers": [
            "By padding the array with a fill value at the end of each row first, you can rely on jnp.nonzero and its size and fill_value arguments, which define a fixed output size and fill value index, when the size requirement is not met. Here is a minimal example:\nimport jax.numpy as jnp\nimport jax\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n    ])\n\n\n@jax.vmap\ndef get_non_nan_values(row_vals, size=3):\n    padded = jnp.pad(row_vals, (0, 1), constant_values=jnp.nan)\n    non_nan = jnp.nonzero(~jnp.isnan(padded), size=size, fill_value=-1)\n    return padded[non_nan]\n\nget_non_nan_values(array_2d)\nWhich returns:\nArray([[ 1.,  2.,  3.],\n       [10., 20., nan]], dtype=float32)\nI think this solution is a bit more compact and clearer in intend, however I have not checked the performance.\nI hope this helps!",
            "I think you can do what you want with this function, which rather than sorting the array (as I commented), sorts and masks the indices of the non-nan values:\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.jit, static_argnums=(1,))\ndef func(array, k=3):\n    m, n = array.shape[-2:]\n    indices = jnp.broadcast_to(jnp.arange(n)[None, :], (m, n))\n    sorted_masked_indices = jnp.sort(jnp.where(jnp.isnan(array), jnp.nan, indices))\n    array_rearranged = array[jnp.arange(m)[:, None], sorted_masked_indices.astype(int)]\n    return jnp.where(jnp.isnan(sorted_masked_indices), jnp.nan, array_rearranged)[:, :k]\nTest:\nimport numpy as np\nrng = np.random.default_rng(0)\nk = 3\n\na = rng.random((12, 6))\na[np.arange(12)[:, None], rng.integers(0, 6, (12, 6))] = np.nan\n\nprint(a)\nprint(func(a, k=k))\nGives:\n[[0.63696169        nan        nan 0.01652764 0.81327024        nan]\n [       nan 0.72949656        nan        nan 0.81585355        nan]\n [       nan 0.03358558        nan        nan        nan        nan]\n [0.29971189        nan        nan        nan        nan 0.64718951]\n [       nan        nan        nan 0.98083534        nan 0.65045928]\n [       nan        nan 0.13509651 0.72148834        nan        nan]\n [       nan 0.88948783 0.93404352 0.3577952         nan        nan]\n [       nan 0.33791123 0.391619   0.89027435        nan        nan]\n [       nan 0.83264415        nan        nan 0.87648423        nan]\n [0.33611706        nan        nan 0.79632427        nan 0.0520213 ]\n [       nan        nan 0.09075305 0.58033239        nan        nan]\n [       nan 0.94211311        nan        nan 0.62910815        nan]]\n[[0.6369617  0.01652764 0.8132702 ]\n [0.72949654 0.81585354        nan]\n [0.03358557        nan        nan]\n [0.29971188 0.6471895         nan]\n [0.9808353  0.6504593         nan]\n [0.1350965  0.72148836        nan]\n [0.88948786 0.9340435  0.3577952 ]\n [0.33791122 0.391619   0.89027435]\n [0.83264416 0.8764842         nan]\n [0.33611706 0.79632425 0.0520213 ]\n [0.09075305 0.5803324         nan]\n [0.9421131  0.62910813        nan]]",
            "With the stable=True option, argsort on a boolean array is guaranteed to preserve the relative order between True and False elements. So this should do the trick:\ndef get_non_nan_values(row_vals):\n    return row_vals[jnp.argsort(jnp.isnan(rowvals), stable=True)[:3]]\nHowever, for wide rows, sorting the entire row seems unnecessary when we already know there are only at most 3 non-nan values. So another simple approach using jax.lax.top_k:\ndef get_top_3_non_nan(row_vals):\n  return row_vals[jax.lax.top_k(~jnp.isnan(row_vals), 3)[1]]",
            "I would do this using vmap of argsort of isnan:\nimport jax\nimport jax.numpy as jnp\n\narray_2d = jnp.array([\n    [jnp.nan,        1,       2,   jnp.nan,    3],\n    [10     ,jnp.nan,   jnp.nan,        20,jnp.nan]\n])\n\nresult = jax.vmap(lambda x: x[jnp.argsort(jnp.isnan(x))])(array_2d)\nprint(result)\n# [[ 1.  2.  3. nan nan]\n#  [10. 20. nan nan nan]]\nThis approach uses static shapes, and thus will be compatible with jit."
        ],
        "link": "https://stackoverflow.com/questions/79443943/jax-numpy-extracting-non-nan-values-gives-nonconcretebooleanindexerror"
    },
    {
        "title": "Problems when boolean indexing in Jax, getting NonConcreteBooleanIndexError",
        "question": "I'm currently trying to create a CustomProblem inheriting from the BaseProblem class in TensorNEAT which is a Jax based library. In trying to implement the evaluate function of this class, I'm using a boolean mask, but I have problems getting it to work. My code results in jax.errors.NonConcreteBooleanIndexError: Array boolean indices must be concrete; got ShapedArray(bool[n,n]) which I think is due to some of my arrays not having a definite shape. How do I circumvent this?\nConsider this example in np:\nimport numpy as np\n\nran_int = np.random.randint(1, 5, size=(2, 2))\nprint(ran_int)\n\nran_bool = np.random.randint(0,2, size=(2,2), dtype=bool)\nprint(ran_bool)\n\na = (ran_int[ran_bool]>0).astype(int)\nprint(a)\nIt could give an output like this:\n[[2 2]\n [3 4]]\n[[ True False]\n [ True  True]]\n[1 1 1] #Is 1D and has less elements than before boolean mask was applied!\nBut in Jax, the same way of thinking results in the NonConcreteBooleanIndexError error I got.\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #ERROR HAPPENS HERE\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #WOULD HAPPEN HERE TOO IF THIS PART WAS FIRST\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) #want this to print a 1D array that potentially has less elements than n*n depending on the boolean mask\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI was considering using jnp.where to solve the issue, but the resulting pairwise_labels and pairwise_predictions have a different shape than what I expect (namely (n, n)) as seen in the code below:\n#NB! len(labels) = len(inputs) = n\ndef evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (n, 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (n, n)\n        pairwise_predictions = predict - predict.T  # shape (n, n)\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold \n        print(pairs_to_keep.shape) #this prints (n, n)\n\n\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape) # shape (n, n)\n\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #one problem is that now I have -inf instead of discarding the element entirely\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape) # shape (n, n)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (n ,n)\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\nI fear that the differing shapes of pairwise_predictions and pairwise_labels after using jnp.where will result in a different loss than if I had just used the boolean mask as I would in np. There is also the fact that I get another error that happens later in the pipeline with the output ValueError: max() iterable argument is empty from line 143 in the pipeline.py file of TensorNeat. This is curiously circumvented by changing pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold to pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold, which probably also results in some loss that is incorrect.\nBelow is some code that should be enough to setup a minimal running example that is similar to my setup:\nfrom tensorneat import algorithm, genome, common\nfrom tensorneat.pipeline import Pipeline\nfrom tensorneat.genome.gene.node import DefaultNode\nfrom tensorneat.genome.gene.conn import DefaultConn\nfrom tensorneat.genome.operations import mutation\nimport jax, jax.numpy as jnp\nfrom tensorneat.problem import BaseProblem\n\ndef binary_cross_entropy(prediction, target):\n    return -(target * jnp.log(prediction) + (1 - target) * jnp.log(1 - prediction))\n\n# Define the custom Problem\nclass CustomProblem(BaseProblem):\n\n    jitable = True  # necessary\n\n    def __init__(self, inputs, labels, threshold):\n        self.inputs = jnp.array(inputs) #nb! already has shape (n, 768)\n        self.labels = jnp.array(labels).reshape((-1,1)) #nb! has shape (n), must be transformed to have shape (n, 1) \n        self.threshold = threshold\n\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        #finding which pairs to keep\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold #this is the thing I actually want\n        #pairs_to_keep = jnp.abs(pairwise_labels - pairwise_predictions) > self.threshold #weird fix to circumvent ValueError: max() iterable argument is empty when using jnp.where for pairwise_labels and pairwise_predictions\n        print(pairs_to_keep.shape)\n\n        pairwise_labels = pairwise_labels[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n        print(pairwise_labels.shape)\n\n        pairwise_predictions = pairwise_predictions[pairs_to_keep] #normal boolean mask that doesnt work\n        #pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, -jnp.inf) #using jnp.where to circumvent NonConcreteBooleanIndexError, but gives different shape than I want\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n        print(pairwise_predictions.shape)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # reduce loss to a scalar\n        loss = jnp.mean(loss)\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss\n        return -loss\n\n    @property\n    def input_shape(self):\n        # the input shape that the act_func expects\n        return (self.inputs.shape[1],)\n\n    @property\n    def output_shape(self):\n        # the output shape that the act_func returns\n        return (1,)\n\n    def show(self, state, randkey, act_func, params, *args, **kwargs):\n        # showcase the performance of one individual\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(state, params, self.inputs)\n\n        loss = jnp.mean(jnp.square(predict - self.labels))\n\n        n_elements = 5\n        if n_elements > len(self.inputs):\n            n_elements = len(self.inputs)\n\n        msg = f\"Looking at {n_elements} first elements of input\\n\"\n        for i in range(n_elements):\n            msg += f\"for input i: {i}, target: {self.labels[i]}, predict: {predict[i]}\\n\"\n        msg += f\"total loss: {loss}\\n\"\n        print(msg)\n\nalgorithm = algorithm.NEAT(\n    pop_size=10,\n    survival_threshold=0.2,\n    min_species_size=2,\n    compatibility_threshold=3.0,  \n    species_elitism=2,  \n    genome=genome.DefaultGenome(\n        num_inputs=768,\n        num_outputs=1,\n        max_nodes=769,  # must at least be same as inputs and outputs\n        max_conns=768,  # must be 768 connections for the network to be fully connected\n        output_transform=common.ACT.sigmoid,\n        mutation=mutation.DefaultMutation(\n            # no allowing adding or deleting nodes\n            node_add=0.0,\n            node_delete=0.0,\n            # set mutation rates for edges to 0.5\n            conn_add=0.5,\n            conn_delete=0.5,\n        ),\n        node_gene=DefaultNode(),\n        conn_gene=DefaultConn(),\n    ),\n)\n\n\nINPUTS = jax.random.uniform(jax.random.PRNGKey(0), (100, 768)) #the input data x\nLABELS = jax.random.uniform(jax.random.PRNGKey(0), (100)) #the annotated labels y\n\nproblem = CustomProblem(INPUTS, LABELS, 0.25)\n\nprint(\"Setting up pipeline and running it\")\nprint(\"-----------------------------------------------------------------------\")\npipeline = Pipeline(\n    algorithm,\n    problem,\n    generation_limit=1,\n    fitness_target=1,\n    seed=42,\n)\n\nstate = pipeline.setup()\n# run until termination\nstate, best = pipeline.auto_run(state)\n# show results\npipeline.show(state, best)",
        "answers": [
            "The solution I got from the authors of TensorNEAT was to update the evaluate() function to use jnp.nan instead of -jnp.inf in the first jnp.where() calls used on pairwise_labels and pairwise_predictions. I also had to make the loss take into consideration the nan values that would be present in the loss after running the bce. The new evaluate() function that has the same behavior as boolean indexing is pasted below.\n    def evaluate(self, state, randkey, act_func, params):\n        # do batch forward for all inputs (using jax.vamp).\n        predict = jax.vmap(act_func, in_axes=(None, None, 0))(\n            state, params, self.inputs\n        )  # should be shape (len(labels), 1)\n\n        #calculating pairwise labels and predictions\n        pairwise_labels = self.labels - self.labels.T # shape (len(labels), len(labels))\n        pairwise_predictions = predict - predict.T  # shape (len(inputs), len(inputs))\n\n        pairs_to_keep = jnp.abs(pairwise_labels) > self.threshold\n\n        #finding only the labels to keep\n        pairwise_labels = jnp.where(pairs_to_keep, pairwise_labels, jnp.nan) #use jnp.nan here\n        pairwise_labels = jnp.where(pairwise_labels > 0, True, False)\n\n        #finding only the predictions to keep\n        pairwise_predictions = jnp.where(pairs_to_keep, pairwise_predictions, jnp.nan) #use jnp.nan here\n        pairwise_predictions = jax.nn.sigmoid(pairwise_predictions)\n\n        # calculate loss\n        loss = binary_cross_entropy(pairwise_predictions, pairwise_labels)  # shape (len(labels), len(labels))\n\n        # loss with shape (len(labels), len(labels)), we need to reduce it to a scalar\n        loss = jnp.mean(loss, where=~jnp.isnan(loss)) #only use number values in loss\n\n        # return negative loss as fitness\n        # TensorNEAT maximizes fitness, equivalent to minimizing loss        \n        return -loss",
            "Yes, the mask operation makes the shape of the resulting array dependent on the content of the array. And jax only supports static shapes. The workaround you propose looks reasonable, with using the value -inf as a placeholder. The missing part is ignoring the zero entries in the mean. This you could achieve by a custom “masked” mean function along the lines of:\nfrom jax import numpy as jnp\nfrom jax import random\nimport jax\n\nkey = random.PRNGKey(0)\n\nx = random.normal(key, (4, 4))\n\nkey, subkey = random.split(key)\nmask = random.bernoulli(key, 0.5, (4, 4))\n\n@jax.jit\ndef masked_mean(x, mask):\n    return jnp.sum(jnp.where(mask, x, 0), axis=0) / jnp.sum(mask, axis=0)\n\n\nmasked_mean(x, mask)\nI have not checked other parts of the code in detail, but e.g. the statement jnp.where(pairwise_labels > 0, True, False) has no effect. And with the masked mean you might not need the placeholder values at all.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79423352/problems-when-boolean-indexing-in-jax-getting-nonconcretebooleanindexerror"
    },
    {
        "title": "How to use jax.vmap with a tuple of flax TrainStates as input?",
        "question": "I am setting up a Deep MARL framework and I need to assess my actor policies. Ideally, this would entail using jax.vmap over a tuple of actor flax TrainStates. I have tried the following:\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom flax.linen.initializers import constant, orthogonal\nfrom flax.training.train_state import TrainState\nimport optax\nimport distrax\n\nclass PGActor_1(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 4\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(128, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0))(x)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nclass PGActor_2(nn.Module):\n\n   @nn.compact\n   def __call__(self, x):\n       action_dim = 2\n       activation = nn.tanh\n\n       actor_mean = nn.Dense(64, kernel_init=orthogonal(jnp.sqrt(2)), bias_init=constant(0.0)) (actor_mean)\n       actor_mean = activation(actor_mean)\n       actor_mean = nn.Dense(action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(actor_mean)\n       pi = distrax.Categorical(logits=actor_mean)\n\n    return pi\n\nstate= jnp.zeros((1, 5))\n\nnetwork_1 = PGActor_1()\nnetwork_1_init_rng = jax.random.PRNGKey(42)\nparams_1 = network_1.init(network_1_init_rng, state)\n\nnetwork_2 = PGActor_2()\nnetwork_2_init_rng = jax.random.PRNGKey(42)\nparams_2 = network_2.init(network_2_init_rng, state)\n\ntx = optax.chain(\noptax.clip_by_global_norm(1),\noptax.adam(lr=1e-3)\n)\nactor_trainstates= (\n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_1),             \n TrainState.create(apply_fn=network_1.apply, tx=tx, params=params_2)\n )\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(actor_trainstates)\nbut I recieve the following error:\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nDoes anybody have any idea how to make this work?\nThank you in advance.",
        "answers": [
            "This is quite similar to other questions (e.g. Jax - vmap over batch of dataclasses). The key point is that JAX transformations like vmap require data in a struct of arrays pattern, whereas you are using an array of structs pattern.\nTo work directly with an array of structs pattern in JAX, you can use Python's built-in map function – due to JAX's asynchronous dispatch, the resulting operations will be executed in parallel where possible:\npis = map(lambda x: x.apply_fn(x.params, state), actor_trainstates)\nHowever, this doesn't take advantage of the automatic vectorization done by vmap. In order to do this, you can convert your data from an array of structs to a struct of arrays, although this requires that all entries have the same structure.\nFor compatible cases, the solution would look something like this, however it errors for your data:\ntrain_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\npis = jax.vmap(lambda x: x.apply_fn(x.params, state))(train_states_soa)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-36-da904fa40b9c> in <cell line: 0>()\n----> 1 train_states_soa = jax.tree.map(lambda *args: jnp.stack(args), *actor_trainstates)\n\nValueError: Dict key mismatch; expected keys: ['Dense_0', 'Dense_1', 'Dense_2']\nThe problem is that your two train states do not have matching structure, and so they cannot be transformed into a single struct of arrays. You can see the difference in structure by inspecting the params:\nprint(actor_trainstates[0].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1', 'Dense_2'])\nprint(actor_trainstates[1].params['params'].keys())  # dict_keys(['Dense_0', 'Dense_1'])\nThere is no way to use vmap in a context where your inputs have different structure, so you'll either have to change the problem to ensure the same structure, or stick with the map approach."
        ],
        "link": "https://stackoverflow.com/questions/79405049/how-to-use-jax-vmap-with-a-tuple-of-flax-trainstates-as-input"
    },
    {
        "title": "Serialization in JAX",
        "question": "What is the recommended way to do serialization/deserialization in JAX?\nIn the context of reinforcement learning my starting point in terms of data might be e.g. match replays that have to be pre-processed to obtain tuples of JAX arrays. This is a process that I would like to do just once, save to disk, then wrap around that a data loading interface like grain.DataLoader.\nBut I have no idea how to do the actual serialization.\nRight now I'm doing\ndef save_jax(path, x: jnp.array):\n    y = np.array(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.array(x)\n    return y\nIt works. My hope is there won't be any copies in wrapping jnp->np or the other way around, and then hopefully this will be mmapped.\nIs this approach bad for performance? What is a better way?",
        "answers": [
            "Both Jax and Numpy support the Python buffer protocol, allowing for zero-copy data sharing between the different types. That being said when using jnp.array or np.array, the default is to copy (see linked docs). So right now you do an unnecessary copy of the data. So I would suggest to use np.asarray instead (and the Jax equivalent), which only copies when needed. So your code would look like:\ndef save_jax(path, x: jnp.array):\n    y = np.asarray(x)\n    np.save(path, y)\n\ndef load_jax(path):\n    with open(path, \"br\") as f:\n        x = np.load(f)\n    y = jnp.asarray(x)\n    return y\nAside from the copy the native Numpy format might not be the best choice of format in neither I/O performance nor associated meta data. For a lightweight alternative you might want to look for example into safetensors.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/79387670/serialization-in-jax"
    },
    {
        "title": "How to do jittable masked get?",
        "question": "How do I do a jax get from a masked index?\nThe code below works without jit.\nx = jnp.arange(25).reshape((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n    [1,2],\n])\ncoords_mask = jnp.array([True, True, False, True])\n\n@jax.jit\ndef masked_gather(x, coords, coords_mask):\n    coords_masked = coords[coords_mask]\n    return x.at[coords_masked[:, 0], coords_masked[:, 1]].get()\n\nmasked_gather(x, coords, coords_mask)\nFails with NonConcreteBooleanIndexError.\nShould return Array([ 7, 13,  7], dtype=int32)",
        "answers": [
            "There is no way to execute this function in a JIT-compatible way, because JAX does not support compilation of programs with dynamic shapes. In your case, the size of the returned array depends on the number of True elements in coords_mask, and so the shape is dynamic by definition.\nSee JAX Sharp Bits: Dynamic Shapes for more information.\nDepending on what you are doing with the resulting value, there are a number of available approaches to work around this: for example, if the shape is truly unknown, you could return an array padded with zeros; it might look something like this:\n@jax.jit\ndef masked_gather_padded(x, coords, coords_mask, fill_value=0):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='fill', fill_value=fill_value)\n  return result[order]\n\nmasked_gather_padded(x, coords, coords_mask)\n# Array([ 7, 13,  7,  0], dtype=int32)\nAlternatively, if the number of True entries in the mask is known a priori, you could modify the function to accept a static size argument and use that to construct an appropriate output. It might look something like this:\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['size'])\ndef masked_gather_with_size(x, coords, coords_mask, *, size):\n  coords_masked = jnp.where(coords_mask[:, None], coords, max(x.shape))\n  order = jnp.argsort(~coords_mask)\n  result = x.at[coords_masked[:, 0], coords_masked[:, 1]].get(mode='drop')\n  return result[order[:size]]\n\nmasked_gather_with_size(x, coords, coords_mask, size=3)\n# Array([ 7, 13,  7], dtype=int32)\nThe best approach will depend on your application."
        ],
        "link": "https://stackoverflow.com/questions/79375141/how-to-do-jittable-masked-get"
    },
    {
        "title": "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?",
        "question": "Is it possible to use vmap for auto-batching if your function isn't jittable?\nI have a function that's not jittable:\ndef testfunc(model, x1, x2, x2_mask):\n    ( ... non-jittable stuff with masks ... )\nI'm trying to wrap it in vmap so I can benefit from auto-batching as explained here.\nSo I do:\ntestfunc_batched = jax.vmap(testfunc, in_axes=(None, 0, 0, 0))\nThe intention is that in batched mode, each of x1, x2, and x2_mask will have an additional outter dimension, the batching dimension. The model shouldn't be treated differently in batched mode hence the None. Let me know if the syntax isn't right.\nI create batches of size one just to test, schematically:\nx1s = x1.reshape(1, ...)\nx2s = x2.reshape(1, ...)\nx2_masks = x2_mask.reshape(1, ...)\n\ntestfunc_batched(model, x1s, x2s, x2_masks)\nThe last line fails with ConcretizationTypeError.\nI've recently learned that stuff with masks makes functions not jittable. But does that mean that I also can't use vmap? Or am I doing something wrong?\n(There is further context in How to JIT code involving masked arrays without NonConcreteBooleanIndexError?, but you don't have to read that question to understand this one.)",
        "answers": [
            "Is it possible to use jax.vmap for auto-batching if your function isn't jittable?\nNo. In general, functions which are incompatible with jit will also be incompatible with vmap, because both jit and vmap use the same JAX tracing mechanism to transform the program."
        ],
        "link": "https://stackoverflow.com/questions/79374152/is-it-possible-to-use-jax-vmap-for-auto-batching-if-your-function-isnt-jittable"
    },
    {
        "title": "Count onto 2D JAX coordinates of another 2D array",
        "question": "I have\nx = jnp.zeros((5,5))\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\nI would like to count onto x how many times each of the individual (x,y) coordinates appear in coords. In other words, obtain the output:\nArray([[0., 0., 0., 0., 0.],\n       [0., 0., 2., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI've tried x.at[coords].add(1) and this gives me:\nArray([[0., 0., 0., 0., 0.],\n       [2., 2., 2., 2., 2.],\n       [3., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)\nI understand what it's doing, but not how to make it do the thing I want.\nThere's this related question[1], but I haven't been able to use it to solve my problem.\n[1] Update JAX array based on values in another array",
        "answers": [
            "For multiple indices, you should pass a tuple of index arrays:\nx = x.at[coords[:, 0], coords[:, 1]].add(1)\nprint(x)\n[[0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]",
            "The generalized operation is basically computing a histogram, especially when the coordinate arrays are float values. So depending on the context the code is used, the following alternative might communicate the intent a bit more clearly:\nfrom jax import numpy as jnp\n\ncoords = jnp.array([\n    [1,2],\n    [2,3],\n    [1,2],\n])\n\nbins = jnp.arange(5 + 1) - 0.5 \nx, _ = jnp.histogramdd(coords, bins=(bins, bins))\nIt will also handle if coordinates are out of bounds. But I presume under the hood, it does the same operation as at[...].add(1). So I would not expect any relevant difference in performance."
        ],
        "link": "https://stackoverflow.com/questions/79370053/count-onto-2d-jax-coordinates-of-another-2d-array"
    },
    {
        "title": "Return a different class based on an optional flag in the arguments without factory",
        "question": "I am implementing a series of classes in Equinox to enable taking derivatives with respect to the class parameters. Most of the time, the user will be instantiating class A and using the fn function to generate some data, the details of which are unimportant. However, in cases where we are interested in gradients, it is beneficial to represent param_c in terms of a sigmoid function to ensure that it remains clamped in the range (0,1). However, I don't want the user to notice a difference in how the class behaves if they do this. As such, I implement another class A_sigmoid that has param_c as a property and use A_abstract to ensure that both classes inherit the fn method, which will call param_c in its logic. While I could simply have the user instantiate an A_sigmoid object with a _param_c_sigmoid instead of param_c I don't want to force the user to have to make this distinction. Rather, I would want them to pass in the same kwargs dictionary no matter the class and have conversion happen behind the scenes. I also wanted to make it so that when making a new A one could simply pass an optional flag to direct the program to use the sigmoid version of the code. To do so, I implemented the following MWE:\nclass A_abstract(eqx.Module):\n    param_a: jax.Array\n    param_b: jax.Array\n    param_c: eqx.AbstractVar[jax.Array]\n    \n    def fn(self,*args,**kwargs):\n        pass\n\nclass A_sigmoid(A_abstract):\n    _param_c_sigmoid: jax.Array\n\n    @property\n    def param_c(self):\n        return 1 / (1 + jnp.exp(-self._param_c_sigmoid))\n\nclass A(A_abstract):\n    param_c: jax.Array\n\n    def __new__(cls, **kwargs):\n        sigmoid_flag = kwargs.pop('use_sigmoid_c',False)\n        if sigmoid_flag == True:\n            param_c = kwargs.pop('param_c')\n            _param_c_sigmoid = jnp.log(param_c / (1 - param_c))\n            kwargs['_param_c_sigmoid'] = _param_c_sigmoid\n            instance = A_sigmoid.__new__(A_sigmoid)\n            instance.__init__(**kwargs)\n            print(type(instance))\n            return instance\n        else:\n            return super(A,cls).__new__(cls)\n\nclassA = A(param_a = 1.,param_b = 2.,param_c = 0.5,use_sigmoid_c=True)\nprint(type(classA))\nThe code correctly says that instance has type A_sigmoid when print is called in the __new__ method. However, when I print type(classA), it is of type A and has no attribute param_c, though it does have a value for _param_c_sigmoid. Why is this the case? Am I missing something in my use of __new__ that is causing this error? While I know that in principle a factory would be the best way to do this, there are other classes of types B, C, etc. that don't have this need for a sigmoid implementation and that I would like to behave exactly the same way as A to enable them to be easily swapped. Thus, I don't want some custom method to instantiate A that would be different from calling the default constructor on the other classes.\nI am running this on a Jupyter notebook with the following package versions:\nPython           : 3.12.4\nIPython          : 8.30.0\nipykernel        : 6.29.5\njupyter_client   : 8.6.3\njupyter_core     : 5.7.2",
        "answers": [
            "If you were using a normal class, what you did is perfectly reasonable:\nclass A_abstract:\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A_sigmoid'>\nHowever, eqx.Module includes a bunch of metaclass logic that overrides how __new__ works, and this seems to collide with the __new__ overrides that you're making. Notice here the only difference is that A_abstract inherits from eqx.Module, and the result is A rather than A_sigmoid:\nimport equinox as eqx\n\nclass A_abstract(eqx.Module):\n  pass\n\nclass A_sigmoid(A_abstract):\n  pass\n\nclass A(A_abstract):\n  def __new__(cls, flag, **kwds):\n    if flag:\n      instance = A_sigmoid.__new__(A_sigmoid)\n    else:\n      instance = super().__new__(cls)\n    instance.__init__(**kwds)\n    return instance\n\nprint(type(A(True))) # <class '__main__.A'>\nI dug-in for a few minutes to try and find the exact cause of this change, but wasn't able to pin it down.\nIf you're trying to do metaprogramming during class construction, you'll have to modify it to work within the construction-time metaprogramming that equinox is already doing."
        ],
        "link": "https://stackoverflow.com/questions/79359839/return-a-different-class-based-on-an-optional-flag-in-the-arguments-without-fact"
    },
    {
        "title": "Trying to install an older version of Jax",
        "question": "Trying to add a specific version of jax and jaxlib\npip install -U jaxlib==0.4.10          \nERROR: Ignored the following yanked versions: 0.4.32\nERROR: Could not find a version that satisfies the requirement jaxlib==0.4.10 (from versions: 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.24, 0.4.25, 0.4.26, 0.4.27, 0.4.28, 0.4.29, 0.4.30, 0.4.31, 0.4.33, 0.4.34, 0.4.35, 0.4.36, 0.4.38)\nERROR: No matching distribution found for jaxlib==0.4.10\nLooks like my old app needs jax to be '<=0.4.10'\nNot sure how to move forward",
        "answers": [
            "From the error message you're seeing, I suspect you're using Python 3.12. The first jaxlib release to support Python 3.12 was v0.4.17. If you want to use an older jaxlib version, you'll have to install an older version of Python.\njaxlib v0.4.10 supports Python v3.8-3.11; one way to see this is to look at the available wheel files on PyPI for this version: https://pypi.org/project/jaxlib/0.4.10/#files"
        ],
        "link": "https://stackoverflow.com/questions/79333553/trying-to-install-an-older-version-of-jax"
    },
    {
        "title": "How Can I Use GPU to Accelerate Image Augmentation?",
        "question": "When setting up image augmentation pipelines using keras.layers.Random* or other augmentation or processing methods, we often integrate these pipelines with a data loader, such as the tf.data API, which operates mainly on the CPU. But heavy augmentation operations on the CPU can become a significant bottleneck, as these processes take longer to execute, leaving the GPU underutilized. This inefficiency can impact the overall training performance.\nTo address this, is it possible to offload augmentation processing to the GPU, enabling faster execution and better resource utilization? If so, how can this be implemented effectively?",
        "answers": [
            "We can speed up processing and improve resource usage by offloading data augmentation to the GPU. I'll demonstrate how to do this in keras. Note that the approach might differ slightly depending on the task, such as classification, detection, or segmentation.\nClassification\nLet’s take a classification task as an example. If we use the tf.data API to apply an augmentation pipeline, the processing will run on the CPU. Here's how it can be done.\nimport numpy as np\nfrom keras import layers\n\na = np.ones((4, 224, 224, 3)).astype(np.float32)\nb = np.ones((4, 2)).astype(np.float32)\n\naugmentation_layers = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.2),\n    ]\n)\n\ndataset = tf.data.Dataset.from_tensor_slices((a, b))\ndataset = dataset.batch(3, drop_remainder=True)\ndataset = dataset.map(\n    lambda x, y: (augmentation_layers(x), y), \n    num_parallel_calls=tf.data.AUTOTUNE\n)\nx.shape, y.shape\n(TensorShape([3, 224, 224, 3]), TensorShape([3, 2]))\nBut for heavy augmentation pipelines, it's better to include them inside the model to take advantage of GPU acceleration.\ninputs = keras.Input(shape=(224, 224, 3))\nprocessed = augmentation_layers(inputs)\nbackbone = keras.applications.EfficientNetB0(\n    include_top=True, pooling='avg'\n)(processed)\noutput = keras.layers.Dense(10)(backbone)\nmodel = keras.Model(inputs, output)\nmodel.count_params() / 1e6\n5.340581\nHere, we set the augmentation pipeline right after keras.Input. Note that these model-with-augmentations don't affect the target vector. So, for augmentations like cutmix or mixup, this approach won't work. For such cases, I'll explore another solution while testing with a segmentation task.\nSegmentation\nI'll use this dataset for comparing execution times. It's a binary segmentation task. Additionally, I'll run it using keras-3, which might allow for multi-backend support.\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # torch, jax\n\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nkeras.__version__ # 3.4.1\n# ref https://keras.io/examples/vision/oxford_pets_image_segmentation/\n# u-net model\ndef get_model(img_size, num_classes, classifier_activation):\n    ...\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(\n        num_classes, \n        3, \n        activation=classifier_activation, \n        padding=\"same\", \n        dtype='float32'\n    )(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\nimg_size = (224, 224)\nnum_classes = 1\nclassifier_activation = 'sigmoid'\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nLet's define the augmentation pipelines.\naugmentation_layers = [\n    layers.RandomFlip(\"horizontal_and_vertical\")\n]\n\ndef augment_data(images, masks):\n    combined = tf.concat([images, tf.cast(masks, tf.float32)], axis=-1)\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = tf.cast(combined[..., 3:], tf.int32)\n    return images_augmented, masks_augmented\nLet’s define the tf.data API to build the dataloader. First, I’ll run the model with a dataloader that includes augmentation pipelines. These augmentations will run on the CPU, and I’ll record the execution time.\ndef read_image(image_path, mask=False):\n    image = tf.io.read_file(image_path)\n    \n    if mask:\n        image = tf.image.decode_png(image, channels=1)\n        image.set_shape([None, None, 1])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = tf.cast(image, tf.int32)\n    else:\n        image = tf.image.decode_png(image, channels=3)\n        image.set_shape([None, None, 3])\n        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n        image = image / 255.\n        \n    return image\n\ndef load_data(image_list, mask_list):\n    image = read_image(image_list)\n    mask  = read_image(mask_list, mask=True)\n    return image, mask\n\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE) \n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n    # Augmenting on CPU\n    dataset = dataset.map(\n        augment_data, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nprint(\"Train Dataset:\", train_dataset)\nTrain Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(16, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16, 224, 224, 1), dtype=tf.int32, name=None))>\nNow, let's compile it and run it.\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nmodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n\n%%time\nepochs = 2\nmodel.fit(\n    train_dataset, \n    epochs=epochs, \n)\nEpoch 1/2\n318/318 ━ 65s 140ms/step - accuracy: 0.9519 - loss: 0.2087\nEpoch 2/2\n318/318 ━ 44s 139ms/step - accuracy: 0.9860 - loss: 0.0338\nCPU times: user 5min 38s, sys: 14.2 s, total: 5min 52s\nWall time: 1min 48s\nNext, we will remove the augmentation layers from the dataloader.\ndef data_generator(image_list, mask_list):\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.shuffle(8*BATCH_SIZE)\n    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nIMAGE_SIZE = 224\nBATCH_SIZE = 16\n\ntrain_dataset = data_generator(images, masks)\nTo offload augmentation to the GPU, we’ll create a custom model class, override the train_step, and use the augment_data method that we defined earlier. Here's how to structure it:\nclass ExtendedModel(keras.Model):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = model\n\n    def train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def call(self, inputs):\n        return self.model(inputs)\n\n    def save(\n        self, filepath, \n        overwrite=True, \n        include_optimizer=True, \n        save_format=None, \n        add_loss=None, \n    ):\n        # Overriding this method will allow us to use the `ModelCheckpoint`\n        self.model.save(\n            filepath=filepath,\n            overwrite=overwrite,\n            save_format=save_format,\n            include_optimizer=include_optimizer,\n        )\nNow that we’ve defined the custom model with GPU-accelerated augmentation, let’s compile and run the model. It should be faster compared to using CPU for augmentations.\nmodel = get_model(\n    img_size, \n    num_classes=num_classes, \n    classifier_activation=classifier_activation\n)\nemodel = ExtendedModel(model)\noptim = keras.optimizers.Adam(0.001)\nbce   = keras.losses.BinaryCrossentropy()\nmetrics = [\"accuracy\"]\nemodel.compile(\n    optimizer=optim, \n    loss=bce, \n    metrics=metrics\n)\n%%time\nepochs = 2\nemodel.fit(\n    train_dataset, \n    epochs=epochs, \n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            filepath='model.{epoch:02d}-{loss:.3f}.keras',\n            monitor='loss',\n            mode='min',\n            save_best_only=True\n        )\n    ]\n)\nEpoch 1/2\n318/318 ━ 54s 111ms/step - accuracy: 0.8885 - loss: 0.2748\nEpoch 2/2\n318/318 ━ 35s 111ms/step - accuracy: 0.9754 - loss: 0.0585\nCPU times: user 4min 43s, sys: 3.81 s, total: 4min 47s\nWall time: 1min 29s\nSo, augmentation processing on CPU took total 65+44 = 109 seconds and processing on GPU took total 54+35 = 89 seconds. Around 18.35% improvements.This approach can be applied to object detection tasks as well, where both image manipulation and bounding box adjustments are needed.\nAs shown in the ExtendedModel class above, we override the save method, allowing the callbacks.ModelCheckpoint to save the full model. Inference can then be performed as shown below.\nloaded_model = keras.saving.load_model(\n    \"/kaggle/working/model.02-0.0585.keras\"\n)\nx, y = next(iter(train_dataset))\noutput = loaded_model.predict(x)\n1/1 ━━━━━━━━━━━━━━━━━━━━ 2s 2s/step\nUpdate\nIn order to run the above code with multiple backends (i.e., tensorflow, torch, and jax), we need to esnure that the augment_data that is used in ExtendedModel use the following backend agnostic keras.ops functions.\ndef augment_data(images, masks):\n    combined = keras.ops.concatenate(\n        [images, keras.ops.cast(masks, 'float32')], axis=-1\n    )\n    for layer in augmentation_layers:\n        combined = layer(combined)\n    images_augmented = combined[..., :3]\n    masks_augmented = keras.ops.cast(combined[..., 3:], 'int32')\n    return images_augmented, masks_augmented\nAdditionally, to make the pipeline flexible for all backend, we can update the ExtendedModel as follows. Now, this code can run with tensorflow, jax, and torch backends.\nclass ExtendedModel(keras.Model):\n    ...\n\n    def train_step(self, *args, **kwargs):\n        if keras.backend.backend() == \"jax\":\n            return self._jax_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"tensorflow\":\n            return self._tensorflow_train_step(*args, **kwargs)\n        elif keras.backend.backend() == \"torch\":\n            return self._torch_train_step(*args, **kwargs)\n\n    def _jax_train_step(self, state, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step(state, (x, y))\n\n    def _tensorflow_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    def _torch_train_step(self, data):\n        x, y = data\n        x, y = augment_data(x, y)\n        return super().train_step((x, y))\n\n    ..."
        ],
        "link": "https://stackoverflow.com/questions/79327723/how-can-i-use-gpu-to-accelerate-image-augmentation"
    },
    {
        "title": "jax and flax not playing nicely with each other",
        "question": "I want to implement a neural network with multiple LSTM gates stacked one after the other.I set the hidden states to 0, as suggested here. When I try to run the code, I get\nJaxTransformError: Jax transforms and Flax models cannot be mixed. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.JaxTransformError)\nWhen I try to replace jax.lax.scan by flax.linen.scan, it gives another error. Not quite sure how to proceed or what's actually going wrong here. Code attached below. Thanks!\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom typing import Sequence\n\n\nclass LSTMModel(nn.Module):\nlstm_hidden_size: int\nnum_lstm_layers: int\nlinear_layer_sizes: Sequence[int]\nmean_aggregation: bool\n\ndef initialize_carry(self, batch_size, feature_size=1):\n    \"\"\"Initialize carry states with zeros for all LSTM layers.\"\"\"\n    return [\n        (\n            # Hidden state (h)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n            # Cell state (c)\n            jnp.zeros((batch_size, self.lstm_hidden_size)),\n        )\n        for _ in range(self.num_lstm_layers)\n    ]\n\n@nn.compact\ndef __call__(self, x, carry=None):\n    if carry is None:\n        raise ValueError(\n            \"Carry must be initialized explicitly using `initialize_carry`.\"\n        )\n\n    # Expand 2D input to 3D (if necessary)\n    if x.ndim == 2:\n        # [batch_size, sequence_length] -> [batch_size, sequence_length, 1]\n        x = jnp.expand_dims(x, axis=-1)\n\n    # Process through LSTM layers\n    for i in range(self.num_lstm_layers):\n        lstm_cell = nn.LSTMCell(\n            features=self.lstm_hidden_size, name=f'lstm_cell_{i}')\n\n        def step_fn(carry, xt):\n            new_carry, yt = lstm_cell(carry, xt)\n            return new_carry, yt\n\n        # Use lax.scan to process the sequence\n        carry[i], outputs = jax.lax.scan(step_fn, carry[i], x)\n        x = outputs  # Update x for the next layer\n\n    # Aggregate outputs\n    if self.mean_aggregation:\n        x = jnp.mean(x, axis=1)  # Average over the sequence\n    else:\n        x = x[:, -1, :]  # Use the last output\n\n    # Pass through linear layers\n    for size in self.linear_layer_sizes:\n        x = nn.Dense(features=size)(x)\n        x = nn.elu(x)\n\n    # Final output layer\n    x = nn.Dense(features=1)(x)\n    return x\n\n\n# Model hyperparameters\nlstm_hidden_size = 64\nnum_lstm_layers = 2\nlinear_layer_sizes = [32, 16]\nmean_aggregation = False\n\n# Initialize model\nmodel = LSTMModel(\n    lstm_hidden_size=lstm_hidden_size,\n    num_lstm_layers=num_lstm_layers,\n    linear_layer_sizes=linear_layer_sizes,\n    mean_aggregation=mean_aggregation\n)\n\n# Dummy input: batch of sequences with 10 timesteps\nkey = jax.random.PRNGKey(0)\n# [batch_size, sequence_length, feature_size]\ndummy_input = jax.random.normal(key, (32, 10, 1))\n\n# Initialize carry states\ncarry = model.initialize_carry(\n    batch_size=dummy_input.shape[0], feature_size=dummy_input.shape[-1])\n\n# Initialize parameters\nparams = model.init(key, dummy_input, carry)\n\n# Apply the model\noutputs = model.apply(params, dummy_input, carry)\n\n# Should print: [batch_size, 1]\nprint(\"Model output shape:\", outputs.shape)",
        "answers": [
            "Consider using nn.RNN to simplify your code:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  name=f'lstm_cell_{i}'\n)\noutputs = lstm(x)\nRNN will handle the carries for you. If you really want to handle the carries yourself you could use return_carry and initial_carry:\nlstm = nn.RNN(\n  nn.LSTMCell(features=self.lstm_hidden_size),\n  return_carry=True, \n  name=f'lstm_cell_{i}'\n)\ncarry[i], outputs = lstm(x, initial_carry=carry[i])"
        ],
        "link": "https://stackoverflow.com/questions/79266328/jax-and-flax-not-playing-nicely-with-each-other"
    },
    {
        "title": "Efficiently custom array creation routines in JAX",
        "question": "I'm still getting a handle of best practices in jax. My broad question is the following:\nWhat are best practices for the implementation of custom array creation routines in jax?\nFor instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):\nimport numpy as np\nimport jax.numpy as jnp\n\ndef ones_at_col(shape_mat, idx):\n    idxs = jnp.arange(shape_mat[1])[None,:]\n    mat = jnp.where(idx==idxs, 1, 0)\n    mat = jnp.repeat(mat, shape_mat[0], axis=0)\n    return mat\n\nshape_mat = (5,10)\n\nprint(ones_at_col(shape_mat, 5))\n\n%timeit np.zeros(shape_mat)\n\n%timeit jnp.zeros(shape_mat)\n\n%timeit ones_at_col(shape_mat, 5)\nThe output is\n[[0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0]]\n127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nMy function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.\nBut then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?\nIs there something better I can do or am I pushing the limits of what is possible in jax?",
        "answers": [
            "The best way to do this is probably something like this:\nmat = jnp.zeros(shape_mat).at[:, 5].set(1)\nRegarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.\nIf you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):\n@partial(jax.jit, static_argnames=['shape_mat', 'idx'])\ndef ones_at_col(shape_mat, idx):\n  return jnp.zeros(shape_mat).at[:, idx].set(1)\nYou could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.\nAnother side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects."
        ],
        "link": "https://stackoverflow.com/questions/79256001/efficiently-custom-array-creation-routines-in-jax"
    },
    {
        "title": "How to handle PRNG splitting in a jax.vmap context?",
        "question": "I have a function which simulates a stochastic differential equation. Currently, without stochastic noise, my invokation of simulating the process up to time t looks like this (and, yeah, I need to use jax):\ndef evolve(u, t):\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\n\ndef simulate(x, t):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt), u)\nNow, the pain comes with the noise. I'm a C++-guy who only occasionally needs to use Python for research/scientific work. And I really don't understand how I need (or should) implement PRNG splitting here. I guess I would change evolve to\ndef evolve(u, t, key):\n    noise = jax.random.multivariate_normal(key, jax.numpy.zeros(d), covariance_matrix, shape = (n,))\n    # return u + dt * b(t, u) + sigma(t, u) * sqrt_dt * noise\nBut that will not work properly I guess. If I got it right, I need to use jax.random.split to split the key. Cause if I don't, I end up with correlated samples. But how and where do I need to split?\nAlso: I guess I would need to modify simulate to def simulate(x, t, key). But then, should simulate also return the modified key?\nAnd to make it even more complicated: I actually wrap simulate into a batch_simulate function which uses jax.vmap to process a whole batch of x's and t's. How do I pass the PRNG to that batch_simulate function, how do I pass it (and broadcast it) to jax.vmap and what should batch_forward return? At first glance, it seems to me that it would take a single PRNG and split it into many (due to the vmap). But what does the caller of batch_forward do then ...\nCompletely lost on this. Any help is highly appreciated!",
        "answers": [
            "If I understand your setup correctly, you should make both evolve and simulate accept a key, and within simulate, use fold_in to generate unique keys for the loop:\ndef evolve(u, t, key):\n    ...\n\ndef simulate(x, t, key):\n    k = jax.numpy.floor(t / dt).astype(int)\n    u = jax.lax.fori_loop(0, k, lambda i, u : evolve(u, i * dt, jax.random.fold_in(key, i)), u)\nThen if you want to vmap over simulate, you can split the key and map over it:\nx_batch = ...  # your batched x inputs\nt_batch = ...  # your batched t inputs\nkey_batch = jax.random.split(key, x_batch.shape[0])\n\nbatch_result = jax.vmap(simulate)(x_batch, t_batch, key_batch)"
        ],
        "link": "https://stackoverflow.com/questions/79238188/how-to-handle-prng-splitting-in-a-jax-vmap-context"
    },
    {
        "title": "Hello World for jaxtyping?",
        "question": "I can't find any instructions or tutorials for getting started with jaxtyping. I tried the simplest possible program and it fails to parse. I'm on Python 3.11. I don't see anything on GitHub jaxtyping project about an upper bound (lower bound is Python 3.9) and it looks like it's actively maintained (last commit was 8 hours ago). What step am I missing?\njaxtyping==0.2.36\nnumpy==2.1.3\ntorch==2.5.1\ntypeguard==4.4.1\n(It seems like numpy is required for some reason even though I'm not using it)\nfrom typeguard import typechecked\nfrom jaxtyping import Float\nfrom torch import Tensor\n\n\n@typechecked\ndef matmul(a: Float[Tensor, \"m n\"], b: Float[Tensor, \"n p\"]) -> Float[Tensor, \"m p\"]:\n    \"\"\"\n    Matrix multiplication of two 2D arrays.\n    \"\"\"\n    raise NotImplementedError(\"This function is not implemented yet.\")\n(venv) dspyz@dspyz-desktop:~/helloworld$ python matmul.py \nTraceback (most recent call last):\n  File \"/home/dspyz/helloworld/matmul.py\", line 6, in <module>\n    @typechecked\n     ^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 221, in typechecked\n    retval = instrument(target)\n             ^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_decorators.py\", line 72, in instrument\n    instrumentor.visit(module_ast)\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 598, in visit_Module\n    self.generic_visit(node)\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 498, in generic_visit\n    node = super().generic_visit(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 494, in generic_visit\n    value = self.visit(value)\n            ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 672, in visit_FunctionDef\n    with self._use_memo(node):\n  File \"/usr/lib/python3.11/contextlib.py\", line 137, in __enter__\n    return next(self.gen)\n           ^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 556, in _use_memo\n    new_memo.return_annotation = self._convert_annotation(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 582, in _convert_annotation\n    new_annotation = cast(expr, AnnotationTransformer(self).visit(annotation))\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in visit_Subscript\n    [self.visit(item) for item in node.slice.elts],\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 421, in <listcomp>\n    [self.visit(item) for item in node.slice.elts],\n     ^^^^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 355, in visit\n    new_node = super().visit(node)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 418, in visit\n    return visitor(node)\n           ^^^^^^^^^^^^^\n  File \"/home/dspyz/helloworld/venv/lib/python3.11/site-packages/typeguard/_transformer.py\", line 474, in visit_Constant\n    expression = ast.parse(node.value, mode=\"eval\")\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n    return compile(source, filename, mode, flags,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<unknown>\", line 1\n    m p\n      ^\nSyntaxError: invalid syntax",
        "answers": [
            "(jaxtyping author here)\nSadly this is a known bug in typeguard v4. It's been around forever and hasn't been fixed. (At a technical level: typeguard v4 attempts to load and reparse the source code of your function, but it doesn't properly parse all type annotations.)\nI use typeguard==2.13.3 myself, which seems to be pretty robust.\nEDIT: removed some other suggested workarounds. These turned out not to, well, work. For now I just recommend pinning to that earlier version of typeguard.",
            "You are running into the issue reported here: https://github.com/patrick-kidger/jaxtyping/issues/80\nYou can work around this by installing typeguard version 3.0, but given how long this bug has remained open without any real fix, I suspect the best conclusion is that jaxtyping should no longer be considered compatible with typeguard."
        ],
        "link": "https://stackoverflow.com/questions/79201839/hello-world-for-jaxtyping"
    },
    {
        "title": "Why is JAX's jit compilation slower on the second run in my example?",
        "question": "I am new to using JAX, and I’m still getting familiar with how it works. From what I understand, when using Just-In-Time (JIT) compilation (jax.jit), the first execution of a function might be slower due to the compilation overhead, but subsequent executions should be faster. However, I am seeing the opposite behavior.\nIn the following code snippet:\nfrom icecream import ic\nimport jax\nfrom time import time\nimport numpy as np\n\n\n@jax.jit\ndef my_function(x, y):\n    return x @ y\n\n\nvectorized_function = jax.vmap(my_function, in_axes=(0, None))\n\nshape = (1_000_000, 1_000)\n\nx = np.ones(shape)\ny = np.ones(shape[1])\n\nstart = time()\nvectorized_function(x, y)\nt_1 = time() - start\n\nstart = time()\nvectorized_function(x, y)\nt_2 = time() - start\n\nprint(f'{t_1 = }\\n{t_2 = }')\nI get the following results:\nt_1 = 13.106784582138062\nt_2 = 15.664098024368286\nAs you can see, the second run (t_2) is actually slower than the first one (t_1), which seems counterintuitive to me. I expected the second run to be faster due to JAX’s JIT caching.\nHas anyone encountered a similar situation or have any insights into why this might be happening?\nPS: I know I could have done x @ y directly without invoking vmap, but this is an easy example just to test its behaviour. My actual code is more complex, and the difference in runtime is even bigger (around 8x slower). I hope this simple example works similar.",
        "answers": [
            "For general tips on running JAX microbenchmarks effectively, see FAQ: Benchmarking JAX code.\nI cannot reproduce the timings from your snippet, but in your more complicated case, I suspect you are getting fooled by JAX's Asynchronous dispatch, which means that the timing method you're using will not actually reflect the time taken by the underlying computation. To address this, you can wrap your results in jax.block_until_ready:\nstart = time()\nvectorized_function(x, y).block_until_ready()\nt_1 = time() - start"
        ],
        "link": "https://stackoverflow.com/questions/79192549/why-is-jaxs-jit-compilation-slower-on-the-second-run-in-my-example"
    },
    {
        "title": "Restoring flax model checkpoints using orbax throws ValueError",
        "question": "The following code blocks are being utlized to save the train state of the model during training and to restore the state back into memory.\nfrom flax.training import orbax_utils\nimport orbax.checkpoint\n\ndirectory_gen_path = \"checkpoints_loc\"\norbax_checkpointer_gen = orbax.checkpoint.PyTreeCheckpointer()\ngen_options = orbax.checkpoint.CheckpointManagerOptions(save_interval_steps=5, create=True)\ngen_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n    directory_gen_path, orbax_checkpointer_gen, gen_options\n)\n\ndef save_model_checkpoints(step_, generator_state, generator_batch_stats):\n\n    gen_ckpt = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    save_args_gen = orbax_utils.save_args_from_target(gen_ckpt)\n    gen_checkpoint_manager.save(step_, gen_ckpt, save_kwargs={\"save_args\": save_args_gen})\n\ndef load_model_checkpoints(generator_state, generator_batch_stats):\n    gen_target = {\n        \"model\": generator_state,\n        \"batch_stats\": generator_batch_stats,\n    }\n\n    latest_step = gen_checkpoint_manager.latest_step()\n    gen_ckpt = gen_checkpoint_manager.restore(latest_step, items=gen_target)\n    generator_state = gen_ckpt[\"model\"]\n    generator_batch_stats = gen_ckpt[\"batch_stats\"]\n\n    return generator_state, generator_batch_stats\nThe training of the model was done on a GPU and loading the state onto GPU device works fine, however, when trying to load the model to cpu, the following error is being thrown by the orbax checkpoint manager's restore method\nValueError: SingleDeviceSharding with Device=cuda:0 was not found in jax.local_devices().\nI'm not quite sure what could be the reason, any thoughts folks?\nUpdate: Updated to the latest version of orbax-checkpoint, 0.8.0 traceback changed to the following error\nValueError: sharding passed to deserialization should be specified, concrete and an instance of `jax.sharding.Sharding`. Got None",
        "answers": [
            "What version of orbax.checkpoint are you using?\nIt looks like this issue was fixed in https://github.com/google/orbax/issues/678 – you should update to the most recent version of orbax-checkpoint, and try running your code again. If that doesn't work, I'd suggest reporting the problem at https://github.com/google/orbax/issues/new"
        ],
        "link": "https://stackoverflow.com/questions/79162665/restoring-flax-model-checkpoints-using-orbax-throws-valueerror"
    },
    {
        "title": "Storing and jax.vmap() over Pytrees",
        "question": "I've ran into an issue with Jax that will make me rewrite an entire 20000-line application if I don't solve it.\nI have a non-ML application which relies on pytrees to store data, and the pytrees are deep - about 6-7 layers of data storage (class1 stores class2, and that stores an array of class3 etc.)\nI've used python lists to store pytrees and hoped to vmap over them, but turns out jax can't vmap over lists.\n(So one solution is to rewrite literally every single dataclass to be a structured array and work from there, possibly putting all 6-7 layers of data into one mega-array)\nIs there a way to avoid the rewrite? Is there a way to store pytree classes in a vmappable state so that everything works as before?\nI have my classes marked with flax.struct.dataclass if that helps.",
        "answers": [
            "jax.vmap is designed to work with a struct-of-arrays pattern, and it sounds like you have an array-of-structs pattern. From your description, it sounds like you have a sequence of nested structs that look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom flax.struct import dataclass\n\n@dataclass\nclass Params:\n  x: jax.Array\n  y: jax.Array\n\n\n@dataclass\nclass AllParams:\n  p: list[Params]\n\n\nparams_list = [AllParams([Params(4, 2), Params(4, 3)]),\n               AllParams([Params(3, 5), Params(2, 4)]),\n               AllParams([Params(3, 2), Params(6, 3)])]\nThen you have a function that you want to apply to each element of the list; something like this:\ndef some_func(params):\n  a, b = params.p\n  return a.x * b.y - b.x * a.y\n\n[some_func(params) for params in params_list]\n[4, 2, -3]\nBut as you found, if you try to do this with vmap, you get an error:\njax.vmap(some_func)(params_list)\nValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nThe issue is that vmap operates separately over each entry of the list or pytree you pass to it, not over the elements of the list.\nTo address this, you can often transform your data structure from an array-of-structs into a struct-of-arrays, and then apply vmap over this. For example:\nparams_array = jax.tree.map(lambda *vals: jnp.array(vals), *params_list)\nprint(params_array)\nAllParams(p=[\n  Params(x=Array([4, 3, 3], dtype=int32), y=Array([2, 5, 2], dtype=int32)),\n  Params(x=Array([4, 2, 6], dtype=int32), y=Array([3, 4, 3], dtype=int32))\n])\nNotice that rather than a list of structures, this is now a single structure with the batching pushed all the way down to the leaves. This is the \"struct-of-arrays\" pattern that vmap is designed to work with, and so vmap will work correctly:\njax.vmap(some_func)(params_array)\nArray([ 4,  2, -3], dtype=int32)\nNow, this assumes that every dataclass in your list has identical structure: if not, then vmap will not be applicable, because by design it must map over computations with identical structure."
        ],
        "link": "https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees"
    },
    {
        "title": "JIT: partial or with static argnums? Non hashable input, but hashable partial",
        "question": "I am a bit lost on what exactly going on and what option to choose. Let's go trough an example:\nimport jax\nfrom functools import partial\nfrom typing import List\n\ndef dummy(a: int, b: List[str]):\n    return a + 1\nAs b argument is mutable, jitting with static argnames will be failed:\nj_dummy = jax.jit(dummy, static_argnames=['b'])\nj_dummy(2, ['kek'])\nValueError: Non-hashable static arguments are not supported\nHowever, if we do partial: jp_dummy = jax.jit(partial(dummy, b=['kek'])), we aim the goal. Somehow, partial object is indeed has __hash__ method, so we can check it with hash(partial(dummy, b=['kek'])).\nSo, I am a bit lost here: how I should proceed in a bigger picture? Should I produce partial functions with whatever arguments and then jit them or should I try to maintain my arguments hashable? What are situations when one approach is better than other? Is there any drawbacks?",
        "answers": [
            "When you use static_argnames, the static values passed to the function become part of the cache key, so if the value changes the function is re-compiled:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, s):\n  return x * len(s)\n\nf_jit = jax.jit(f, static_argnames=['s'])\n\nprint(f_jit(2, \"abc\"))  # 6\nprint(f_jit(2, \"abcd\"))  # 8\nThis is why the static arguments must be hashable: their hash is used as the JIT cache key.\nOn the other hand, when you wrap a static argument via closure, its value does not affect the cache key, and so it need not be hashable. But since it's not part of the cache key, if the global value changes, it does not trigger a recompilation and so you may get unexpected results:\nf_closure = jax.jit(lambda x: f(x, s))\n\ns = \"abc\"\nprint(f_closure(2))  # 6\ns = \"abcd\"\nprint(f_closure(2))  # 6\nFor this reason, explicit static arguments can be safer. In your case, it may be best to change your list into a tuple, as tuples are hashable and can be used as explicit static arguments."
        ],
        "link": "https://stackoverflow.com/questions/79114391/jit-partial-or-with-static-argnums-non-hashable-input-but-hashable-partial"
    },
    {
        "title": "precision of JAX",
        "question": "I have a question regarding the precision of float in JAX. For the following code,\nimport numpy as np\nimport jax.numpy as jnp\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is:','%.60f' % np.arctan(10))\n\njnp.arctan(10) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\n\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is:','%.60f' % np.arctan(10+1e-7))\n\njnp.arctan(10+1e-7) is: 1.471127629280090332031250000000000000000000000000000000000000\nnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\njnp gave identical results for arctan(x) for a small change of input variable (1e-7), but np did not. My question is how to let jax.numpy get the right number for a small change of x?\nAny comments are appreciated.",
        "answers": [
            "JAX defaults to float32 computation, which has a relative precision of about 1E-7. This means that your two inputs are effectively identical:\n>>> np.float32(10) == np.float32(10 + 1E-7)\nTrue\nIf you want 64-bit precision like NumPy, you can enable it as discussed at JAX sharp bits: double precision, and then the results will match to 64-bit precision:\nimport jax\njax.config.update('jax_enable_x64', True)\n\nimport jax.numpy as jnp\nimport numpy as np\n\nprint('jnp.arctan(10) is:','%.60f' % jnp.arctan(10))\nprint('np.arctan(10) is: ','%.60f' % np.arctan(10))\n\nprint('jnp.arctan(10+1e-7) is:','%.60f' % jnp.arctan(10+1e-7))\nprint('np.arctan(10+1e-7) is: ','%.60f' % np.arctan(10+1e-7))\njnp.arctan(10) is: 1.471127674303734700345103192375972867012023925781250000000000\nnp.arctan(10) is:  1.471127674303734700345103192375972867012023925781250000000000\njnp.arctan(10+1e-7) is: 1.471127675293833592107262120407540351152420043945312500000000\nnp.arctan(10+1e-7) is:  1.471127675293833592107262120407540351152420043945312500000000\n(but please note that even the 64-bit precision used by Python and NumPy is only accurate to about one part in 10^16, so most of the digits in the representation you printed are inaccurate compared to the true arctan value)."
        ],
        "link": "https://stackoverflow.com/questions/79098013/precision-of-jax"
    },
    {
        "title": "jax register_pytree_node_class and register_dataclass returns non consistent datatype: list and tuple accordingly",
        "question": "I am writing custom class, which is basically a wrapper around list, with custom setitem method. I would like this class participate in jax.jit code, so during that I found a following problem: during jitting List field converted to tuple. However, this is case only when using\nregister_pytree_node_class When use register_dataclas , then List keep being list.\nI simplify example to highlight only this problem.\nimport jax\nfrom jax.tree_util import register_dataclass\nfrom jax.tree_util import register_pytree_node_class\nfrom functools import partial\nfrom dataclasses import dataclass\nfrom typing import List\n\n@partial(register_dataclass,\n         data_fields=['data'],\n         meta_fields=['shift'])\n@dataclass\nclass DecoratorFlatten:\n    data: List[int]\n    shift: int = 5\n\n@register_pytree_node_class\n@dataclass\nclass CustomFlatten:\n    data: List[int]\n    shift: int = 5\n\n    def tree_flatten(self):\n            children = self.data\n            aux_data = self.shift\n            return (children, aux_data)\n    \n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        obj = object.__new__(cls)\n        obj.data = children\n        setattr(obj, 'shift', aux_data)\n        return obj\nNow let's call a simple as this function over instances of this two class:\n@jax.jit\ndef get_value(a):\n    return a.data\ndf = DecoratorFlatten([0,1,2])\ncf = CustomFlatten([0,1,3])\nget_value(df), get_value(cf)\nIn first case we get list as output, but in second tuple. I thought maybe this is because of my implementation of the tree_flatten method, however:\ncf.tree_flatten()\nLeads to ([0, 1, 3], 5) as desirable.",
        "answers": [
            "In tree_unflatten, children is a tuple, and you are assigning this directly to obj.data. If you want it to be a list, you should use obj.data = list(children)."
        ],
        "link": "https://stackoverflow.com/questions/79093341/jax-register-pytree-node-class-and-register-dataclass-returns-non-consistent-dat"
    },
    {
        "title": "Batched matrix multiplication with JAX on GPU faster with larger matrices",
        "question": "I'm trying to perform batched matrix multiplication with JAX on GPU, and noticed that it is ~3x faster to multiply shapes (1000, 1000, 3, 35) @ (1000, 1000, 35, 1) than it is to multiply (1000, 1000, 3, 25) @ (1000, 1000, 25, 1) with f64 and ~5x with f32.\nWhat explains this difference, considering that on cpu neither JAX or NumPy show this behaviour, and on GPU CuPy doesn't show this behaviour?\nI'm running this with JAX: 0.4.32 on an NVIDIA RTX A5000 (and get similar results on a Tesla T4), code to reproduce:\nimport numpy as np\nimport cupy as cp\nfrom cupyx.profiler import benchmark\nfrom jax import config\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng()\n\nx = np.arange(5, 55, 5)\nGPU timings:\ndtype = cp.float64\ntimings_cp = []\nfor i in range(5, 55, 5):\n    a = cp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n    b = cp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n    timings_cp.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ndtype = jnp.float64\ntimings_jax_gpu = []\nwith jax.default_device(jax.devices('gpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)), dtype=dtype)\n        b = jnp.array(rng.random((1000, 1000, i, 1)), dtype=dtype)\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_gpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.gpu_times.mean() for i in timings_cp], label=\"CuPy\")\nplt.plot(x, [i.gpu_times.mean() for i in timings_jax_gpu], label=\"JAX GPU\")\nplt.legend()\nTimings with those specific shapes:\ndtype = jnp.float64\nwith jax.default_device(jax.devices('gpu')[0]):\n    a = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\n    func = jax.jit(lambda a, b: a@b)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\n\n    a = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\n    b = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\n    print(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=1000, n_warmup=10).gpu_times.mean())\nGives\nf64:\n0.01453789699935913\n0.004859122595310211\n\nf32:\n\n0.005860503035545349\n0.001209742688536644\nCPU timings:\ntimings_np = []\nfor i in range(5, 55, 5):\n    a = rng.random((1000, 1000, 3, i))\n    b = rng.random((1000, 1000, i, 1))\n    timings_np.append(benchmark(lambda a, b: a@b, (a, b), n_repeat=10, n_warmup=10))\n\ntimings_jax_cpu = []\nwith jax.default_device(jax.devices('cpu')[0]):\n    for i in range(5, 55, 5):\n        a = jnp.array(rng.random((1000, 1000, 3, i)))\n        b = jnp.array(rng.random((1000, 1000, i, 1)))\n        func = jax.jit(lambda a, b: a@b)\n        timings_jax_cpu.append(benchmark(lambda a, b: func(a, b).block_until_ready(), (a, b), n_repeat=10, n_warmup=10))\n\nplt.figure()\nplt.plot(x, [i.cpu_times.mean() for i in timings_np], label=\"NumPy\")\nplt.plot(x, [i.cpu_times.mean() for i in timings_jax_cpu], label=\"JAX CPU\")\nplt.legend()",
        "answers": [
            "The difference seems to come from the compiler emitting a kLoop fusion for smaller sizes, and a kInput fusion for larger sizes. You can read about the effect of these in this source comment: https://github.com/openxla/xla/blob/e6b6e61b29cc439350a6ad2f9d39535cb06011e5/xla/hlo/ir/hlo_instruction.h#L639-L656\nThe compiler likely uses some heuristic to choose between the two, and it appears that this heuristic is suboptimal at the boundary for your particular problem. You can see this by outputting the compiled HLO for your operation:\na = jnp.array(rng.random((1000, 1000, 3, 25)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 25, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\nHloModule jit__lambda_, is_scheduled=true, entry_computation_layout={(f64[1000,1000,3,25]{3,2,1,0}, f64[1000,1000,25,1]{3,2,1,0})->f64[1000,1000,3,1]{3,2,1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}, frontend_attributes={fingerprint_before_lhs=\"a02cbfe0fda9d44e2bd23462363b6cc0\"}\n\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.7: f64[1000,1000,3,25], param_1.6: f64[1000,1000,25,1]) -> f64[1000,1000,3] {\n  %param_0.7 = f64[1000,1000,3,25]{3,2,1,0} parameter(0)\n  %param_1.6 = f64[1000,1000,25,1]{3,2,1,0} parameter(1)\n  %bitcast.28.5 = f64[1000,1000,25]{2,1,0} bitcast(f64[1000,1000,25,1]{3,2,1,0} %param_1.6)\n  %broadcast.2.5 = f64[1000,1000,3,25]{3,2,1,0} broadcast(f64[1000,1000,25]{2,1,0} %bitcast.28.5), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  %multiply.2.3 = f64[1000,1000,3,25]{3,2,1,0} multiply(f64[1000,1000,3,25]{3,2,1,0} %param_0.7, f64[1000,1000,3,25]{3,2,1,0} %broadcast.2.5)\n  %constant_4 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,25]{3,2,1,0} %multiply.2.3, f64[] %constant_4), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,25], Arg_1.2.0: f64[1000,1000,25,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,25,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,25]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %loop_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,25]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,25,1]{3,2,1,0} %Arg_1.2.0), kind=kLoop, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %loop_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-4-68f2557428ff>\" source_line=3}\n}\na = jnp.array(rng.random((1000, 1000, 3, 35)), dtype=dtype)\nb = jnp.array(rng.random((1000, 1000, 35, 1)), dtype=dtype)\nprint(jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text())\n%scalar_add_computation (scalar_lhs: f64[], scalar_rhs: f64[]) -> f64[] {\n  %scalar_rhs = f64[] parameter(1)\n  %scalar_lhs = f64[] parameter(0)\n  ROOT %add.2 = f64[] add(f64[] %scalar_lhs, f64[] %scalar_rhs)\n}\n\n%fused_reduce (param_0.5: f64[1000,1000,3,35], param_1.2: f64[1000,1000,35,1]) -> f64[1000,1000,3] {\n  %param_0.5 = f64[1000,1000,3,35]{3,2,1,0} parameter(0)\n  %param_1.2 = f64[1000,1000,35,1]{3,2,1,0} parameter(1)\n  %bitcast.28.3 = f64[1000,1000,35]{2,1,0} bitcast(f64[1000,1000,35,1]{3,2,1,0} %param_1.2)\n  %broadcast.2.3 = f64[1000,1000,3,35]{3,2,1,0} broadcast(f64[1000,1000,35]{2,1,0} %bitcast.28.3), dimensions={0,1,3}, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  %multiply.2.1 = f64[1000,1000,3,35]{3,2,1,0} multiply(f64[1000,1000,3,35]{3,2,1,0} %param_0.5, f64[1000,1000,3,35]{3,2,1,0} %broadcast.2.3)\n  %constant_3 = f64[] constant(0)\n  ROOT %reduce.2 = f64[1000,1000,3]{2,1,0} reduce(f64[1000,1000,3,35]{3,2,1,0} %multiply.2.1, f64[] %constant_3), dimensions={3}, to_apply=%scalar_add_computation, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\n\nENTRY %main.4 (Arg_0.1.0: f64[1000,1000,3,35], Arg_1.2.0: f64[1000,1000,35,1]) -> f64[1000,1000,3,1] {\n  %Arg_1.2.0 = f64[1000,1000,35,1]{3,2,1,0} parameter(1), metadata={op_name=\"b\"}\n  %Arg_0.1.0 = f64[1000,1000,3,35]{3,2,1,0} parameter(0), metadata={op_name=\"a\"}\n  %input_reduce_fusion = f64[1000,1000,3]{2,1,0} fusion(f64[1000,1000,3,35]{3,2,1,0} %Arg_0.1.0, f64[1000,1000,35,1]{3,2,1,0} %Arg_1.2.0), kind=kInput, calls=%fused_reduce, metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n  ROOT %bitcast.1.0 = f64[1000,1000,3,1]{3,2,1,0} bitcast(f64[1000,1000,3]{2,1,0} %input_reduce_fusion), metadata={op_name=\"jit(<lambda>)/jit(main)/dot_general\" source_file=\"<ipython-input-3-eb3ac06eae7a>\" source_line=4}\n}\nHere's a script to observe this compiler decision with respect to size:\nfor size in range(10, 55, 5):\n  a = jnp.array(rng.random((1000, 1000, 3, size)), dtype=dtype)\n  b = jnp.array(rng.random((1000, 1000, size, 1)), dtype=dtype)\n  hlo_text = jax.jit(lambda a, b: a @ b).lower(a, b).compile().as_text()\n  print(f\"{size=} {'kLoop' in hlo_text=}\")\nsize=10 'kLoop' in hlo_text=True\nsize=15 'kLoop' in hlo_text=True\nsize=20 'kLoop' in hlo_text=True\nsize=25 'kLoop' in hlo_text=True\nsize=30 'kLoop' in hlo_text=True\nsize=35 'kLoop' in hlo_text=False\nsize=40 'kLoop' in hlo_text=False\nsize=45 'kLoop' in hlo_text=False\nsize=50 'kLoop' in hlo_text=False\nI don't have any suggestion beyond perhaps reporting this at https://github.com/openxla/xla; it may be that the compiler heuristic for choosing to emit kLoop vs. kInput needs some additional logic."
        ],
        "link": "https://stackoverflow.com/questions/79085795/batched-matrix-multiplication-with-jax-on-gpu-faster-with-larger-matrices"
    },
    {
        "title": "Computing gradient using JAX of a function that outputs a list of arrays",
        "question": "I have a function which returns a list of arrays, and I need to find its derivative with respect to a single parameter. For instance, let's say we have\ndef fun(x):\n...\nreturn [a,b,c]\nwhere a,b,c and d are multi-dimensional arrays (for example, 2 by 2 by 2 real arrays). Now I want to obtain [da/dx, db/dx, dc/dx]. By db/dx I mean I want to obtain derivative of each element in the a:222 array with respect to x, so da/dx, db/dx, dc/dx are all 222 arrays.\nThis is me using JAX differentiation for the first time, and most of the examples I find online are about functions that has scalar output.\nFrom my search, I understand one way to find this is basically get the gradient of each scalar in all these arrays one at a time (probably making it faster using vmap). Is there any other way that is faster? I think JAX.jacobian might do the trick, but I am having hard time finding its documentation to see what does the function does exactly. Any help is very much appreciated.\nNow, I have tried JAX.jacobian with simple examples, and it does give me the answer that I expect. This assures me a bit, but I would like to find official documentation or assurance from others that is the right way to do it, and it is doing what I expect it.",
        "answers": [
            "You can use jax.jacobian for what you describe. Here is an example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  a = jnp.full((2, 2), 2) * x\n  b = jnp.full((2, 2), 3) * x\n  c = jnp.full((2, 2), 4) * x\n  return [a, b, c]\n\nda_dx, db_dx, dc_dx = jax.jacobian(f)(1.0)\n\nprint(da_dx)\n# [[2. 2.]\n#  [2. 2.]]\n\nprint(db_dx)\n# [[3. 3.]\n#  [3. 3.]]\n\nprint(dc_dx)\n# [[4. 4.]\n#  [4. 4.]]\njax.jacobian is an alias of jax.jacrev, and you can find the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.jacrev.html"
        ],
        "link": "https://stackoverflow.com/questions/79025241/computing-gradient-using-jax-of-a-function-that-outputs-a-list-of-arrays"
    },
    {
        "title": "Modifying multiple dimensions of Jax array simultaneously",
        "question": "When using the jax_array.at[idx] function, I wish to be able to set values at both a set of specified rows and columns within the jax_array to another jax_array containing values in the same shape. For example, given a 5x5 jax array, I might want to set the values, jax_array.at[[0,3],:][:,[1,2]] to some 2x2 array of values. However, I am coming across an issue where the _IndexUpdateRef' object is not subscriptable. I understand the idea of the error (and I get a similar one when using 2 chained .at[]s), but I want to know if there is anyway to achieve the desired functionality within 1 line.",
        "answers": [
            "JAX follows the indexing semantics of NumPy, and NumPy's indexing semantics allow you to do this via broadcasted arrays of indices (this is discussed in Integer array indexing in the NumPy docs).\nSo for example, you could do something like this:\nimport jax.numpy as jnp\n\nx = jnp.zeros((4, 6), dtype=int)\ny = jnp.array([[1, 2],\n               [3, 4]])\ni = jnp.array([0, 3])\nj = jnp.array([1, 2])\n\n# reshape indices so they broadcast \ni = i[:, jnp.newaxis]\nj = j[jnp.newaxis, :]\n\nx = x.at[i, j].set(y)\nprint(x)\n[[0 1 2 0 0 0]\n [0 0 0 0 0 0]\n [0 0 0 0 0 0]\n [0 3 4 0 0 0]]\nHere the i index has shape (2, 1), and the j index has shape (1, 2), and via broadcasting rules they index a 2x2 noncontiguous subgrid of the array x, which you can then set to the contents of y in a single statement."
        ],
        "link": "https://stackoverflow.com/questions/78985089/modifying-multiple-dimensions-of-jax-array-simultaneously"
    },
    {
        "title": "Mapping Over Arrays of Functions in JAX",
        "question": "What is the most performant, idiomatic way of mapping over arrays of functions in JAX?\nContext: This GitHub issue shows a way to apply vmap to several functions using lax.switch. The example is reproduced below:\nfrom jax import lax, vmap\nimport jax.numpy as jnp\n\ndef func1(x):\n  return 2 * x\n\ndef func2(x):\n  return -2 * x\n\ndef func3(x):\n  return 0 * x\n\nfunctions = [func1, func2, func3]\nindex = jnp.arange(len(functions))\nx = jnp.ones((3, 5))\n\nvmap_functions = vmap(lambda i, x: lax.switch(i, functions, x))\nvmap_functions(index, x)\n# DeviceArray([[ 2.,  2.,  2.,  2.,  2.],\n#              [-2., -2., -2., -2., -2.],\n#              [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)\nMy specific questions are:\nIs this (currently) the most idiomatic way of mapping over arrays of functions in JAX?\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)",
        "answers": [
            "For the kind of operation you're doing, where the functions are applied over full axes of an array in a way that's known statically, you'll probably get the best performance via a simple Python loop:\ndef map_functions(functions: list[Callable[[Array], Array], x: Array) -> Array:\n  assert len(functions) == x.shape[0]\n  return jnp.array([f(row) for f, row in zip(functions, x)])\nThe method based on switch is designed for the more general case where the structure of the indices is not known statically.\nWhat performance penalties, if any, does this method incur? (This refers to both runtime and/or compile-time performance.)\nvmap of switch is implemented via select, which will compute the output of each function for the full input array before selecting just the pieces needed to construct the output, so if the functions are expensive to compute, it may lead to longer runtimes."
        ],
        "link": "https://stackoverflow.com/questions/78980521/mapping-over-arrays-of-functions-in-jax"
    },
    {
        "title": "JAX TypeError: 'Device' object is not callable",
        "question": "I found a piece of JAX codes from few years ago.\nimport jax\nimport jax.random as rand\n\ndevice_cpu = None\n\ndef do_on_cpu(f):\n    global device_cpu\n    if device_cpu is None:\n        device_cpu = jax.devices('cpu')[0]\n\n    def inner(*args, **kwargs):\n        with jax.default_device(device_cpu):\n            return f(*args, **kwargs)\n    return inner\n\nseed2key = do_on_cpu(rand.PRNGKey)\nseed2key.__doc__ = '''Same as `jax.random.PRNGKey`, but always produces the result on CPU.'''\nand I call it with:\nkey = seed2key(42)\nBut it results in TypeError:\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 14\n---> 14 key = seed2key(42)\n\nFile ~/bert-tokenizer-cantonese/lib/seed2key.py:12, in do_on_cpu.<locals>.inner(*args, **kwargs)\n     11 def inner(*args, **kwargs):\n---> 12     with jax.default_device(device_cpu):\n     13         return f(*args, **kwargs)\n\nTypeError: 'Device' object is not callable\nI think the function has breaking changes after version upgrade.\nCurrent versions:\njax 0.4.31\njaxlib 0.4.31\n(latest version at the moment of writing)\nHow can I change the codes to avoid the error? Thanks.",
        "answers": [
            "This code works fine in all recent versions of JAX: jax.default_device is a configuration function designed to be used as a context manager.\nI can reproduce the error you're seeing if I add this to the top of your script:\njax.default_device = jax.devices('cpu')[0]  # wrong!\nI suspect you inadvertently executed something similar to this at some point earlier in your notebook session. Try restarting your notebook runtime and rerunning just your valid code."
        ],
        "link": "https://stackoverflow.com/questions/78951225/jax-typeerror-device-object-is-not-callable"
    },
    {
        "title": "Using Jax Jit on a method as decorator versus applying jit function directly",
        "question": "I guess most people familiar with jax have seen this example in the documentation and know that it does not work:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @jit  # <---- How to do this correctly?\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\nc.calc(3)  \n3 workarounds are mentioned, but it appears that applying jit as a function directly, rather than a decorator works fine as well. That is, JAX does not complain about not knowing how to deal with the CustomClass type of self:\nimport jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  # No decorator here !\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\n6 # works fine!\nAlthough not documented (which it maybe should be?), this appears to function identical to marking self as static via @partial(jax.jit, static_argnums=0), in that changing self does nothing for subsequent calls, i.e.:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\n6\n6 # no update\nSo I originally assumed that decorators in general might just deal with self as a static parameter when applying them directly. Because the method might be saved to another variable with a specific instance (copy) of self. As a sanity check, I checked if non-jit decorators indeed do this as well, but this appears not to be the case, as the below non-jit \"decorated\" function happily deals with changes to self:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\n\ncustom = CustomClass(2, True)\ndecorated_calc = decorator(custom.calc)\nprint(decorated_calc(3))\ncustom.mul = False\nprint(decorated_calc(3))\n6\n3\nI saw some other questions about applying decorators directly as functions versus decorator style (e.g. here and here), and there it is mentioned there is a slight difference in the two versions, but this should almost never matter. I am left wondering what it is about the jit decorator that makes these versions behave so differently, in that JAX.jit cán deal with the self type if not in decorated style. If anyone has an answer, that would be much appreciated.",
        "answers": [
            "Decorators have nothing to do with static arguments: static arguments are a concept specific to jax.jit.\nBacking up, you should keep in mind that whenever jax.jit compiles a function, it caches the compilation artifact based on several quantites, including:\nthe ID of the function or callable being compiled\nthe static attributes of any non-static arguments, such as shape and dtype\nthe hash of any arguments marked static via static_argnums or static_argnames\nthe value of any global configurations that would affect outputs\nWith this in mind, let's examine this snippet:\nc = CustomClass(2, True)\njitted_calc = jit(c.calc)\nprint(jitted_calc(3))\nc.mul = False \nprint(jitted_calc(3))\nthe reason that jitted_calc doesn't update when you update attributes of c is because nothing related to the cache key has changed: (1) the function ID is the same, (2) the shape and dtype of the argument is unchanged, (3) there are no static arguments, (4) no global configurations have changed. Thus the previous cached compilation artifact (with the previous value of mul) is executed again. This is the primary reason I didn't mention this strategy in the doc you linked to: it's rarely the behavior that users would want.\nThis approach of wrapping the bound method in JIT is incidentally similar to wrapping the method definition with @partial(jit, static_argnums=0), but the details are not the same: in the static_argnums version, self is marked as a static argument, and so its hash becomes part of the JIT cache. The default __hash__ method for a class is simply based on the ID of the instance, and so changing c.mul does not change the hash, and does not trigger re-compilation. You can see an example of how to rectify this under Strategy 2 in the doc you linked to: basically, define appropriate __hash__ and __eq__ methods for the class:\nclass CustomClass:\n  def __init__(self, x: jnp.ndarray, mul: bool):\n    self.x = x\n    self.mul = mul\n\n  @partial(jit, static_argnums=0)\n  def calc(self, y):\n    if self.mul:\n      return self.x * y\n    return y\n\n  def __hash__(self):\n    return hash((self.x, self.mul))\n\n  def __eq__(self, other):\n    return (isinstance(other, CustomClass) and\n            (self.x, self.mul) == (other.x, other.mul))\nIn your last example, you define this:\ndef decorator(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        return x\n    return wrapper\nThis code does not use jax.jit at all. The fact that changes to c.mul lead to changes in outputs has nothing to do with decorator syntax, but rather has to do with the fact that there is no JIT cache in play here.\nI hope that's all clear!"
        ],
        "link": "https://stackoverflow.com/questions/78918066/using-jax-jit-on-a-method-as-decorator-versus-applying-jit-function-directly"
    },
    {
        "title": "Zero length error of non-zero length array",
        "question": "I'm writing environment for rl agent training.\nMy env.step method takes as action array with length 3\n    def scan(self, f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n\n    def step_env(\n        self,\n        key: chex.PRNGKey,\n        state: EnvState,\n        action: Union[int, float, chex.Array],\n        params: EnvParams,\n    ) -> Tuple[chex.Array, EnvState, jnp.ndarray, jnp.ndarray, Dict[Any, Any]]:\n        \n        c_action = jnp.clip(action,\n                          params.min_action, \n                          params.max_action)\n        \n        _, m1 = self.scan(self.Rx, 0, action[0])\n        _, m2 = self.scan(self.Rx, 0, action[1])\n        _, m3 = self.scan(self.Rx, 0, action[2])\nI vectorize the env.step using and then call it\nobsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(rng_step,\n                                                                                          env_state,\n                                                                                          action,\n                                                                                          env_params)\nBut I got error\nCell In[9], line 65, in PCJ1_0.scan(self, f, init, xs, length)\n     63 ys = []\n     64 print(xs)\n---> 65 for x in xs:\n     66     carry, y = f(carry, x)\n     67     ys.append(y)\n\n    [... skipping hidden 1 frame]\n\nFile ~/anaconda3/envs/jax/lib/python3.10/site-packages/jax/_src/lax/lax.py:1592, in _iter(tracer)\n   1590 def _iter(tracer):\n   1591   if tracer.ndim == 0:\n-> 1592     raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\n   1593   else:\n   1594     n = int(tracer.shape[0])\n\nTypeError: iteration over a 0-d array\nHow is it possible? If I plot the action array in the scan function I got array with length 5 (I vectored env.step for 5 envs), the length!=0\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0",
        "answers": [
            "When you print your value, it gives this:\nTraced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([-0.25605989, -0.27983692, -1.0055736 , -0.4460616 , -0.8323701 ],      dtype=float32)\n  batch_dim = 0\nHere float32[] tells you that this is a tracer with dtype float32 and shape []: that is, your array is zero-dimensional within the context of the vmapped function.\nThe purpose of vmap is to efficiently map a function over an axis of an array, so that within the function evaluation the array has one less dimension than it does outside the vmapped context. You can see that this way:\n>>> import jax\n\n>>> def f(x):\n...  print(f\"{x.shape=}\")\n...  print(f\"{x=}\")\n...\n>>> x = jax.numpy.arange(4.0)\n\n>>> f(x)\nx.shape=(4,)\nx=Array([0., 1., 2., 3.], dtype=float32)\n\n>>> jax.vmap(f)(x)\nx.shape=()\nx=Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0., 1., 2., 3.], dtype=float32)\n  batch_dim = 0\nIf you're passing a 1D input into your function and you want to manipulate the full 1D array within your function (instead of evaluating the function element-by-element), then it sounds like you should remove the vmap."
        ],
        "link": "https://stackoverflow.com/questions/78838517/zero-length-error-of-non-zero-length-array"
    },
    {
        "title": "Jax jitting of kd-tree code taking an intractably long amount of time",
        "question": "I've written myself into a corner with the following situation:\nI'm running an optimiser which requires smooth gradients to work, and I'm using Jax for automatic differentiation. Since this code is Jax jitted, this means that anything connected to it has to be Jax jit traceable.\nI need to interpolate a function to use with the optimiser, but can't use the Scipy library as it isn't compatable with Jax (there's a jax.scipy.interpolate.RegularGridInterpolator implementation, but this isn't smooth - it only supports linear and nearest neighbour interpolation).\nThis means that I'm having to write my own Jax-compatible smooth interpolator, which I'm basing off the Scipy RBFInterpolator code. The implementation of this is very nice - it uses a kd-tree to find the nearest neighbours of a queried point in space, and then uses these to construct a local interpolation. This means that I also need to write a Jax-compatable kd-tree class (the Scipy one also isn't compatible with Jax), which I've done.\nThe problem comes with jit-compiling the kd-tree code. I've written it in the 'standard way', using objects for the tree nodes with left and right node fields for the children. At the leaf nodes, these fields have None values to signify the absense of children.\nThe code runs and is functionally correct, however jit-compiling it takes a long time: 72 seconds for a tree of 64 coordinates, 131 seconds for 343 coordinates, ... and my intended dataset has over 14 million points. I think internally Jax is tracing every single possible path through the tree, which is why it's taking so long. The results are that it's blazingly quick: 0.0075s for kd-tree 10-point retrieval vs 0.4s for a brute force search over all of the points (for 343 points). These are the kind of speeds I'm hoping to obtain for use in the optimiser (without jitting it will be too slow). However it doesn't seem possible if the compilation times are going to continue to grow as experienced.\nI thought that the problem might lie in the structure of the tree, with lots of different objects to be stored, so have also implemented a kd-tree search algorithm where the tree is represented by a set of Jax-numpy arrays (e.g. coord, value, left and right; where each index corresponds to a point in the tree) and iteration rather than recursion is used to do the tree search (this was a challenge but it works!). However, converting this to work with jit (changing if-statements for jax.lax.cond) is going to be complicated, and before I start I was wondering if it's going to be worth it - surely I'll have the same problem: Jax will trace all branches of the tree until the 'null terminators' (-1 values in the left and right arrays) are reached, and it will still take a very long time to compile. I've been investigating structures like jax.lax.while_loop, in case they might help?\n(I've also written a hybrid of the two approaches, with an array-based tree and a recursion-based algorithm. In this case the tracing goes into an infinite loop, I think because of the fact that the null-terminator is -1 rather than None. But the arrays should be known statically (they don't change after construction, and belong to an object which is marked as a static input), so maybe the solution lies in this and I'm doing something wrong.)\nI was wondering if I'm doing anything which is obviously wrong (or if my understanding is wrong), and if there is anything I can do to speed it up? Is it just to be expected that the compile time would be so high when there are so many code paths to trace? I don't suppose I could even build the jitted function only once and then save it?\nI'm concerned that the only solution may be to rewrite the optimiser code so that it doesn't use Jax (e.g. if I hard-code the derivatives, and rewrite some of the code so that it operates on arrays directly instead of being vectorised across the inputs).\nThe code is available here: https://github.com/FluffyCodeMonster/jax_kd_tree\nAll three varieties described are given: the node-based tree with recursion, the array-based tree with iteration, and the array-based tree with recursion. The former works, but is very slow to jit compile as the number of points in the tree increases; the second also works, but is not written in a jit-able way yet. The last is written to be jitted, but can't jit compile as it gets into an infinite recursion.\nI really need to get this working urgently so that I can obtain the optimisation results.",
        "answers": [
            "All python-level control flow, including if statements, for and while loops, and recursion, is traced in full and flattened into a linear set of commands that is then sent to the compiler. If you are attempting a tree traversal via Python-level control flow, you're going to end up with very large programs that take a very long time to compile. This issue is discussed broadly at JAX sharp bits: control flow.\nIf you want to traverse a KD tree under JIT without the long compilation, you'll have to use an iterative approach with XLA control-flow operators such as jax.lax.fori_loop and jax.lax.while_loop.\nAlternatively, you might think about instead using jax.pure_callback in order to run neighbors queries using scipy on the host. There is some discussion of this at Exploring pure_callback. It's not super efficient—each call will incur some host synchronization and data movement overhead—but it can be a pretty effective solution for things like this, particularly if you're running on CPU."
        ],
        "link": "https://stackoverflow.com/questions/78791013/jax-jitting-of-kd-tree-code-taking-an-intractably-long-amount-of-time"
    },
    {
        "title": "Execution of conditional branches causing errors in Jax (kd-tree implementation)",
        "question": "I'm writing a kd-tree in Jax, and using custom written Node objects for the tree elements. Each Node is very simple, with a single data field (for holding numeric values) and left and right fields which are references to other Nodes. A leaf Node is identified as one for which the left and right fields are None.\nThe code performs conditional checks on the values of left and right as part of the tree traversal process - e.g. it will only try to traverse down the left or right branch of a node's subtree if it actually exists. Doing checks like if (current_node.left is not None) (or does it have to be jax.numpy.logical_not(current_node.left is None) in Jax - I've tried both?) was fine for this, but since converting the if statements to jax.lax.cond(...) I've been getting the error AttributeError: 'NoneType' object has no attribute 'left'.\nI think the situation might be like in the following minimum working example:\nimport jax\nimport jax.numpy as jnp\n\ndef my_func(val):\n    return 2*val\n\n@jax.jit\ndef test_fn(a):\n    return jax.lax.cond(a is not None,\n                lambda: my_func(a),\n                lambda: 0)\n\nprint(test_fn(2))       # Prints 4\n# in test_fn(), a has type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>\nprint(test_fn(None))    # TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n# in test_fn(), a has type <class 'NoneType'>\nIn this code, if the Jax cond statement were a regular if statement, my_func() wouldn't even be called when a is None, and no error would be raised. To the best of my understanding, Jax tries to trace the function, meaning that all branches are executed, and this leads to my_func() being called with None (when a is None), causing the error. I believe a similar situation is arising in my tree code, where conditional branches are being executed even though .left and /or .right are None, and a traditional if statement wouldn't lead to execution of the code branches.\nIs my understanding correct, and what could I do about this issue? Strangely, the minimum working example code also has the problem when the @jax.jit decorator is omitted, suggesting that both branches are still being traced.\nAs a related point, is the tree structure 'baked into' the Jax/XLA code? I have noticed that when using larger trees the code takes longer to be jit-compiled, which makes me concerned that this might not be a valid approach with the very large number of points I need to represent (about 14,000,000). I would use the regular Scipy kd-tree implementation, but this isn't compatible with Jax unfortunately, and the rest of my code requires it. I might ask this as a separate question for clarity.",
        "answers": [
            "If you are using jax.lax.cond, the input must have a valid type for both branches. When a is None, the first branch is invalid because None * 2 results in an error.\nIn this case, the condition a is not None is known statically, so rather than using lax.cond you can use a regular if statement:\n@jax.jit\ndef test_fn(a):\n  return my_func(a) if a is not None else 0"
        ],
        "link": "https://stackoverflow.com/questions/78784486/execution-of-conditional-branches-causing-errors-in-jax-kd-tree-implementation"
    },
    {
        "title": "JAX/Equinox pipeline slows down after adding an integer argument to a loss function",
        "question": "I have the following training pipeline in JAX and Equinox. I want to pass a batch index to the loss function in order to apply different logic depending on index. Without batch index training loop works for about 15 sec, but if I pass an index, then it slows down for about an hour. Could you explain, why this happens? I'm new to JAX, sorry.\ndef fit_cv(model: eqx.Module, \n           dataloader: jdl.DataLoader, \n           optimizer: optax.GradientTransformation, \n           loss: tp.Callable, \n           n_steps: int = 1000):\n    \n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    dloss = eqx.filter_jit(eqx.filter_value_and_grad(loss))\n    \n    @eqx.filter_jit\n    def step(model, data, opt_state, batch_index):\n        loss_score, grads = dloss(model, data, batch_index)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state, loss_score\n    \n    loss_history = []\n    for batch_index, batch in tqdm(zip(range(n_steps), dataloader), total=n_steps):\n        if batch_index >= n_steps:\n            break\n        batch = batch[0] # dataloader returns tuple of size (1,)\n        model, opt_state, loss_score = step(model, batch, opt_state, batch_index)\n        loss_history.append(loss_score)\n    return model, loss_history\nLoss function has the following signature\ndef loss(self, model: eqx.Module, data: jnp.ndarray, batch_index: int):\nIn particular, I want to switch between two loss functions after N steps. So, probably, I need to know the concrete value of a batch index.\nSolution:\nTo use jax.lax.cond\n        condition = (batch_index // self.switch_steps) % 2 == 1\n        ...\n        loss_value = jax.lax.cond(\n            jnp.all(condition),\n            lambda: loss1(inputs),\n            lambda: loss2(inputs),\n        )\n        return loss_value",
        "answers": [
            "I suspect the issue is excessive recompilation. You are using filter_jit, which according to the docs has the following property:\nAll JAX and NumPy arrays are traced, and all other types are held static.\nEach time a static argument to a JIT-compiled function changes, it triggers a re-compilation. This means that if batch_index is a Python int, then each time you call your function with a new value, the function will be recompiled.\nAs a fix, I would recommend using regular old jax.jit, which requires you to explicitly specify static arguments, instead of the function trying to make the choice for you (potential surprises like this are one of the reasons why JAX has made this design choice - as the Zen of Python says, explicit is better than implicit). If you use jax.jit and don't mark batch_index as static, you shouldn't see this recompilation penalty.\nAlternatively, if you want to keep using filter_jit, then you could change your step call to this:\nstep(model, batch, opt_state, jnp.asarray(batch_index))\nWith this change, filter_jit will no longer decide to make the batch index static. Of course, either of these suggestions would require that that loss is compatible with dynamic batch_index, which can't be determined from the information included in your question."
        ],
        "link": "https://stackoverflow.com/questions/78775635/jax-equinox-pipeline-slows-down-after-adding-an-integer-argument-to-a-loss-funct"
    },
    {
        "title": "Dictionary indexing with Numpy/Jax",
        "question": "I'm writing an interpolation routine and have a dictionary which stores the function values at the fitting points. Ideally, the dictionary keys would be 2D Numpy arrays of the fitting point coordinates, np.array([x, y]), but since Numpy arrays aren't hashable these are converted to tuples for the keys.\n# fit_pt_coords: (n_pts, n_dims) array\n# fn_vals: (n_pts,) array\ndef fit(fit_pt_coords, fn_vals):\n    pt_map = {tuple(k): v for k, v in zip(fit_pt_coords, fn_vals)}\n    ...\nLater in the code I need to get the function values using coordinates as keys in order to do the interpolation fitting. I'd like this to be within @jax.jited code, but the coordinate values are of type <class 'jax._src.interpreters.partial_eval.DynamicJaxprTracer'>, which can't be converted to a tuple. I've tried other things, like creating a dictionary key as (x + y, x - y), but again this requires concrete values, and calling .item() results in an ConcretizationTypeError.\nAt the moment I've @jax.jited all of the code I can, and have just left this code un-jitted. It would be great if I could jit this code as well however. Are there any better ways to do the dictionary indexing (or better Jax-compatible data structures) which would allow all of the code to be jitted? I am new to Jax and still understading how it works, so I'm sure there must be better ways of doing it...",
        "answers": [
            "There is no way to use traced JAX values as dictionary keys. The problem is that the key values will not be known until runtime within the XLA compiler, and XLA has no dictionary-like data structure that such lookups can be lowered to.\nThere are imperfect solutions, such as keeping the dictionary on the host and using something like io_callback to do the dict lookups on host, but this approach comes with performance penalties that will likely make it impractical.\nUnfortunately, your best approach for doing this efficiently under JIT would probably be to switch to a different interpolation algorithm that doesn't depend on hash table lookups.",
            "I agree with @jakevdp that this might not be the best solution. Python is not the quickest when built-ins are looped over.\nPython can do anything... Except for-loops. We use numpy for that.\nMaybe a pandas.DataFrame with columns [\"x\", \"y\", \"v\"] would be a way to go.\nCan you not use scipy.interpolate's functions?"
        ],
        "link": "https://stackoverflow.com/questions/78767142/dictionary-indexing-with-numpy-jax"
    },
    {
        "title": "Taking derivatives with multiple inputs in JAX",
        "question": "I am trying to take first and second derivatives of functions in JAX however, my ways of doing that give me the wrong number or zeros. I have an array with two columns for each variable and two rows for each input\nimport jax.numpy as jnp\nimport jax\n\nrng = rng = jax.random.PRNGKey(1234)\narray = jax.random.normal(rng, (2,2))\nTwo test functions\ndef F1(arr):\n    return 1/arr\n\ndef F2(arr):\n    return jnp.array([arr[0]**2 + arr[1]**3])\nand two methods of taking first and second derivatives, one using jax.grad()\ndef dF_m1(arr, F):\n    return jax.grad(lambda arr: F(arr)[0])(arr)\n\ndef ddF_m1(arr, F, dF):\n    return jax.grad(lambda arr: dF(arr, F)[0])(arr)\nand another using jax.jacobian()\ndef dF_m2(arr, F):\n    jac = jax.jacobian(lambda arr: F(arr))(arr)\n    return jnp.diag(jac)\n\ndef ddF_m2(arr, F, dF):\n    hess = jax.jacobian(lambda arr: dF(arr, F))(arr)\n    return jnp.diag(hess)\nComputing the first and second derivative (and error) of each function using both methods gives the following\nexact_dF1  = (-1/array**2)\nexact_ddF1 = (2/array**3)\n\nprint(\"Function 1 using all grad()\")\ndF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)\nddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)\nprint(dF1_m1  - exact_dF1,\"\\n\")\nprint(ddF1_m1 - exact_ddF1,\"\\n\")\n\nprint(\"Function 1 using all jacobian()\")\ndF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)\nddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)\nprint(dF1_m2  - exact_dF1,\"\\n\")\nprint(ddF1_m2 - exact_ddF1,\"\\n\")\nOutput\nFunction 1 using all grad()\n[[ 0.         48.43877   ]\n [ 0.          0.62903005]] \n\n[[  0.        674.248    ]\n [  0.          0.9977852]] \n\nFunction 1 using all jacobian()\n[[0. 0.]\n [0. 0.]] \n\n[[0. 0.]\n [0. 0.]] \nand\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\nprint(\"Function 2 using all grad()\")\ndF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)\nddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)\nprint(dF2_m1  - exact_dF2,\"\\n\")\nprint(ddF2_m1 - exact_ddF2,\"\\n\")\n\nprint(\"Function 2 using all jacobian()\")\ndF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)\nddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)\nprint(dF2_m2  - exact_dF2,\"\\n\")\nprint(ddF2_m2 - exact_ddF2,\"\\n\")\nOutput\nFunction 2 using all grad()\n[[0. 0.]\n [0. 0.]] \n\n[[0.         0.86209416]\n [0.         7.5651155 ]] \n\nFunction 2 using all jacobian()\n[[ 0.         -0.10149619]\n [ 0.         -6.925739  ]] \n\n[[0.        2.8620942]\n [0.        9.565115 ]] \nI would prefer only to use jax.grad() for something like F1 but it seems right now that only jax.jacobian is working. The whole reason for this is that I need to calculate higher-order derivatives of a neural network with respect to its inputs. Thank you for any help.",
        "answers": [
            "Assuming exact_* is what you're attempting to compute, you're going about it in the wrong way. Your indexing within the differentiated functions (i.e. ...[0]) is removing some of the elements that you're trying to compute.\nWhat exact_dF1 and exact_ddF1 are computing is element-wise first and second derivatives for 2D inputs. You can compute this using either grad or jacobian by applying vmap twice (once for each input dimension). For example:\nexact_dF1  = (-1/array**2)\ngrad_dF1 = jax.vmap(jax.vmap(jax.grad(F1)))(array)\njac_dF1 = jax.vmap(jax.vmap(jax.jacobian(F1)))(array)\nprint(jnp.allclose(exact_dF1, grad_dF1))  # True\nprint(jnp.allclose(exact_dF1, jac_dF1))  # True\n\nexact_ddF1 = (2/array**3)\ngrad_ddF1 = jax.vmap(jax.vmap(jax.grad(jax.grad(F1))))(array)\njac_ddF1 = jax.vmap(jax.vmap(jax.jacobian(jax.jacobian(F1))))(array)\nprint(jnp.allclose(exact_ddF1, grad_ddF1))  # True\nprint(jnp.allclose(exact_ddF1, jac_ddF1))  # True\nWhat exact_dF2 and exact_ddF2 are computing is a row-wise jacobian and hessian of a 2D->1D mapping. By its nature, this is difficult to compute using jax.grad, which is meant for functions with scalar output, but you can compute it using the jacobian this way:\nexact_dF2  = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))\nexact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))\n\njac_dF2 = jax.vmap(jax.jacobian(lambda a: F2(a)[0]))(array)\njac_ddF2_full = jax.vmap(jax.jacobian(jax.jacobian(lambda a: F2(a)[0])))(array)\njac_ddF2 = jax.vmap(jnp.diagonal)(jac_ddF2_full)\nprint(jnp.allclose(exact_dF2, jac_dF2))  # True\nprint(jnp.allclose(exact_ddF2, jac_ddF2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax"
    },
    {
        "title": "weird shape when indexing a jax array",
        "question": "I am experiencing a weird issue when indexing a Jax array using a list. If I place a debugger in the middle of my code, I have the following:\nThis array are created by convering a numpy array.\nHowever, when I try this in a new instance of Python, I have the correct behavior: [\nWhat is it happening?",
        "answers": [
            "This is working as expected. JAX follows the semantics of NumPy indexing, and in the case of advanced indexing with multiple scalars and integer arrays separated by slices, the indexed dimensions are combined via broadcasting and moved to the front of the output array. You can read more about the details of this kind of indexing in the NumPy documentation: https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing. In particular:\nTwo cases of index combination need to be distinguished:\nThe advanced indices are separated by a slice, Ellipsis or newaxis. For example x[arr1, :, arr2].\nThe advanced indices are all next to each other. For example x[..., arr1, arr2, :] but not x[arr1, :, 1] since 1 is an advanced index in this regard.\nIn the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array\nThe code in your program falls under the first case, while the code in your separate interpreter falls under the second case. This is why you're seeing different results.\nHere's a concise example of this difference:\n>>> import numpy as np\n>>> x = np.zeros((3, 4, 5))\n\n>>> x[0, :, [1, 2]].shape  # size-2 dimension moved to front\n(2, 4)\n\n>>> x[:, 0, [1, 2]].shape  # size-2 dimension not moved to front\n(3, 2)"
        ],
        "link": "https://stackoverflow.com/questions/78741064/weird-shape-when-indexing-a-jax-array"
    },
    {
        "title": "Jax vmap with lax scan having different sequence length in batch dimension",
        "question": "I have this following code , where my sim_timestep is in batch I am not able to run this since the lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) ) requires the concrete array , but since I have vmapped the state_predictor function the sim_timestep is being as a tracedArray . Any help would be greatly appreciated . Thanks all\nfrom jax import random\nfrom jax import lax\nimport jax\nimport jax.numpy as jnp\nimport pdb\n\n\ndef fwd_dynamics(x_u, xs):\n    x0,uk =  x_u\n    Delta_T = 0.001\n    lwb = 1.2\n    psi0=x0[2][0]\n    v0= x0[3][0]\n    vdot0 = uk[0][0]\n    delta0 = uk[1][0]\n    thetadot0 = uk[2][0]\n        \n    xdot= jnp.asarray([[v0*jnp.cos(psi0) ],\n        [v0*jnp.sin(psi0)] ,\n        [v0*jnp.tan(delta0)/(lwb)],\n        [vdot0],\n        [thetadot0]])\n    x_next = x0 + xdot*Delta_T\n    return (x_next,uk), x_next  # (\"carryover\", \"accumulated\")\n\n\ndef state_predictor( xk,uk ,sim_timestep):\n    (x_next,_), _ = lax.scan(fwd_dynamics, (xk,uk) ,jnp.arange(sim_timestep) )\n    return x_next\n\nlow = 0  # Adjust minimum value as needed\nhigh = 100  # Adjust maximum value as needed\nkey = jax.random.PRNGKey(44)\n\nsim_time = jax.random.randint(key, shape=(10, 1), minval=low, maxval=high)\n\nxk = jax.random.uniform(key, shape=(10,5, 1))\nuk = jax.random.uniform(key, shape=(10,2, 1))\n\nstate_predictor_vmap = jax.jit(jax.vmap(state_predictor,in_axes= 0 ,out_axes=0 ))\nx_next = state_predictor_vmap( xk,uk ,sim_time)\nprint(x_next.shape)\nI tried to solve it by above code , hoping to get alternative way to achieve the same functionality.",
        "answers": [
            "What you're asking to do is impossible: scan lengths must be static, and vmapped values are non-static by definition.\nWhat you can do instead is replace your scan with a fori_loop or a while_loop, and then the loop boundary does not need to be static. For example, if you implement your function this way and leave the rest of your code unchanged, it should work:\ndef state_predictor(xk, uk, sim_timestep):\n  body_fun = lambda i, x_u: fwd_dynamics(x_u, i)[0]\n  x_next, _ = lax.fori_loop(0, sim_timestep[0], body_fun, (xk, uk))\n  return x_next"
        ],
        "link": "https://stackoverflow.com/questions/78713478/jax-vmap-with-lax-scan-having-different-sequence-length-in-batch-dimension"
    },
    {
        "title": "Colab, Jax, and GPU: why does cell execution take 60 seconds when %%timeit says it only takes 70 ms?",
        "question": "As the basis for a project on fractals, I'm trying to use GPU computation on Google Colab using the Jax library.\nI'm using Mandelbrot on all accelerators as a model, and I'm encountering a problem.\nWhen I use the %%timeit command to measure how long it takes to calculate my GPU function (same as in the model notebook), the times are entirely reasonable, and in line with expected results -- 70 to 80 ms.\nBut actually running %%timeit takes something like a full minute. (By default, it runs the function 7 times in a row and reports the average -- but even that should take less than a second.)\nSimilarly, when I run the function in a cell and output the results (a 6 megapixel image), it takes around 60 seconds for the cell to finish -- to execute a function that supposedly only takes 70-80 ms.\nIt seems like something is producing a massive amount of overhead, that also seems to scale with the amount of computation -- e.g. when the function contains 1,000 iterative calculations %%timeit says it takes 71 ms while in reality it takes 60 seconds, but with just 20 iterations %%timeit says it takes 10 ms while in reality it takes about 10 seconds.\nI am pasting the code below, but here is a link to the Colab notebook itself -- anyone can make a copy, connect to a \"T4 GPU\" instance, and run it themselves to see.\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax\n\nassert len(jax.devices(\"gpu\")) == 1\n\ndef run_jax_kernel(c, fractal):\n    z = c\n    for i in range(1000):\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n    return fractal\n\nrun_jax_gpu_kernel = jax.jit(run_jax_kernel, backend=\"gpu\")\n\ndef run_jax_gpu(height, width):\n\n    mx = -0.69291874321833995150613818345974774914923989808007473759199\n    my = 0.36963080032727980808623018005116209090839988898368679237704\n    zw = 4 / 1e3\n\n    y, x = jax.numpy.ogrid[(my-zw/2):(my+zw/2):height*1j, (mx-zw/2):(mx+zw/2):width*1j]\n    c = x + y*1j\n    fractal = jax.numpy.full(c.shape, 1000, dtype=np.int32)\n    return np.asarray(run_jax_gpu_kernel(c, fractal).block_until_ready())\nTakes about a minute to produce an image:\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nax.imshow(run_jax_gpu(2000, 3000));\nTakes about a minute to report that the function only takes 70-80 ms to execute:\n%%timeit -o\nrun_jax_gpu(2000, 3000)",
        "answers": [
            "The first thing to realize is that %timeit will execute your code multiple times, and then return an average of the times for each run. The number of times it will execute is determined dynamically by the time of the first run.\nThe second thing to realize is that JAX code is just-in-time (JIT) compiled, meaning that on the first execution of any particular function, you will incur a one-time compilation cost. Many things affect compilation cost, but functions that use large for loops (say, 1000 or more repetitions) tend to compile very slowly, because JAX unrolls those loops before passing the operations to XLA for compilation, and XLA compilation scales approximately quadratically with the number of unrolled operations (there is some discussion of this at JAX Sharp Bits: Control Flow).\nPut these together, and you'll see why you're observing the timings that you are: under %timeit, your first run results in a very long compilation, and subsequent runs are very fast. The resulting average time is printed, and is very short compared to the first run, and to the overall time.\nWhen you run your code a single time to plot the results, you are mainly seeing the compilation time. Because it is not amortized away by multiple calls to your function, that compilation time is long.\nThe solution would be to avoid writing Python for loops in your function in order to avoid the long compilation time: one possibility would be to use lax.fori_loop, which allows you to write iterative computations without the huge compilation time penalty, though it will incur a runtime penalty on GPU compared to the for loop solution because the operations are executed sequentially rather than being parallelized by the compiler. In your case it might look like this:\ndef run_jax_kernel(c, fractal):\n    z = c\n    def body_fun(i, carry):\n        z, fractal = carry\n        z = z**2 + c\n        diverged = jax.numpy.absolute(z) > 2\n        diverging_now = diverged & (fractal == 1000)\n        fractal = jax.numpy.where(diverging_now, i, fractal)\n        return (z, fractal)\n    z, fractal = jax.lax.fori_loop(0, 1000, body_fun, (z, fractal))\n    return fractal"
        ],
        "link": "https://stackoverflow.com/questions/78708817/colab-jax-and-gpu-why-does-cell-execution-take-60-seconds-when-timeit-says"
    },
    {
        "title": "Implementing a vectorized function over LinkedLists using Jax’s vmap function",
        "question": "Trying to implement a vectorized version of an algorithm (from computational geometry) using Jax. I have made the minimum working example using a LinkedList to particularly express my query (I am using a DCEL otherwise).\nThe idea is that this vectorized algorithm will be checking certain criteria over a DCEL. I have substituted this “criteria checking procedure” with a simple summation algorithm for the sake simplicity.\nimport jax\nfrom jax import vmap\nimport jax.numpy as jnp\n\nclass Node: \n  \n    # Constructor to initialize the node object \n    def __init__(self, data): \n        self.data = data \n        self.next = None\n\nclass LinkedList: \n  \n    def __init__(self): \n        self.head = None\n  \n    def push(self, new_data): \n        new_node = Node(new_data) \n        new_node.next = self.head \n        self.head = new_node \n\n    def printList(self): \n        temp = self.head \n        while(temp): \n            print (temp.data,end=\" \") \n            temp = temp.next\n\ndef summate(list) :\n    prev = None\n    current = list.head\n    sum = 0\n    while(current is not None): \n        sum += current.data\n        next = current.next\n        current = next\n    return sum\n\nlist1 = LinkedList() \nlist1.push(20) \nlist1.push(4) \nlist1.push(15) \nlist1.push(85) \n\nlist2 = LinkedList() \nlist2.push(19)\nlist2.push(13)\nlist2.push(2)\nlist2.push(13)\n\n#list(map(summate, ([list1, list2])))\n\nvmap(summate)(jnp.array([list1, list2]))\nI get the following error.\n TypeError: Value '<__main__.LinkedList object at 0x1193799d0>' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.\nThe objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion. I have implemented what I want in basic Python, but I want to do it in Jax as there is a larger probabilistic function which I will be using this subprocedure for (it’s a Markov Chain).\nIt might be the case that I am completely unable to work over such data structures over Jax as the error suggests that only numeric types are supported. Can I use pytrees in some way to mitigate this constraint?\nIt will be tempting to suggest I use a simple list from jnp, but I am using Linkedlist just as an example of a simple(st) data structure. As mentioned earlier, am actually working over a DCEL.\nPS : the Linkedlist code was taken from GeeksForGeeks, as I wanted to come up with a minimum working example quickly.",
        "answers": [
            "The objective is, if I have a set of say, 10,000 Linkedlists, I should be able to apply this summate function over each LinkedList in a vectorized fashion.\nThis goal is not feasible using JAX. You could register your class as a custom Pytree to make it work with JAX functions (see Extending pytrees), but this won't mean you can vectorize an operation over a list of such objects.\nJAX transformations like vmap and jit work for data stored with a struct-of-arrays pattern (e.g. a single LinkedList object containing arrays that represent multiple batched linked lists) not an array-of-structs pattern (e.g. a list of multiple LinkedList objects).\nFurther, the algorithm you're using, based on a while loop, is not compatible with JAX transformations (See JAX sharp bits: control flow), and the dynamically sized tree of nodes will not fit into the static shape constraints of JAX programs.\nI'd love to point you in the right direction, but I think you either need to give up on using JAX, or give up on using dynamic linked lists. You won't be able to do both."
        ],
        "link": "https://stackoverflow.com/questions/78677115/implementing-a-vectorized-function-over-linkedlists-using-jax-s-vmap-function"
    },
    {
        "title": "Simplest equivalent implementation of numpy.ma.notmasked_edges() for use in JAX",
        "question": "I have a square numpy.ndarray and a numpy boolean mask of the same shape. I want to find the first element in each row of the array that is not masked.\nMy code currently relies on numpy.ma.notmasked_edges(), which does exactly what I need. However, I now need to migrate my code to JAX, which has not implemented numpy.ma within jax.numpy.\nWhat would be the simplest way to find the index of the first unmasked element in each row, calling only numpy functions that have been implemented in JAX (which exclude numpy.ma)?\nThe code I'm trying to reproduce is something like:\nimport numpy as np\nmy_array = np.random.rand(5,5)\nmask = (my_array < 0.5)\nmy_masked_array = np.ma.masked_array(my_array, mask=mask)\nnp.ma.notmasked_edges(my_masked_array, axis=1)[0]\nI'm sure there are many ways to do this, but I'm looking for the least unwieldy way.",
        "answers": [
            "Here's a JAX implementation of nonmasked_edges, which takes a boolean mask and returns the same indices returned by the numpy.ma function:\nimport jax.numpy as jnp\n\ndef notmasked_edges(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  alltrue = mask.all(axis=axis).ravel()\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind)[~alltrue] for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel()[~alltrue])\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel()[~alltrue])\n  \n  return [tuple(first), tuple(last)]\nThis will not be compatible with JIT, because the size of the output arrays depend on the values of the mask (rows which have no unmasked value are left out).\nIf you want a JIT-compatible version, you can remove the [~alltrue] indexing, and the first/last index will be returned for rows that have no unmasked value:\ndef notmasked_edges_v2(mask, axis=None):\n  mask = jnp.asarray(mask)\n  assert mask.dtype == bool\n  if axis is None:\n    mask = mask.ravel()\n    axis = 0\n  shape = list(mask.shape)\n  del shape[axis]\n  indices = jnp.meshgrid(*(jnp.arange(n) for n in shape), indexing='ij')\n  indices = [jnp.ravel(ind) for ind in indices]\n\n  first = indices.copy()\n  first.insert(axis, jnp.argmin(mask, axis=axis).ravel())\n\n  last = indices.copy()\n  last.insert(axis, mask.shape[axis] - 1 - jnp.argmin(jnp.flip(mask, axis=axis), axis=axis).ravel())\n\n  return [tuple(first), tuple(last)]\nHere's an example:\nimport numpy as np\nmask = np.array([[True, False, False, True],\n                 [False, False, True, True],\n                 [True, True, True, True]])\n\narr = np.ma.masked_array(np.ones_like(mask), mask=mask)\nprint(np.ma.notmasked_edges(arr, axis=1))\n# [(array([0, 1]), array([1, 0])), (array([0, 1]), array([2, 1]))]\n\nprint(notmasked_edges(mask, axis=1))\n# [(Array([0, 1], dtype=int32), Array([1, 0], dtype=int32)),\n#  (Array([0, 1], dtype=int32), Array([2, 1], dtype=int32))]\n\nprint(notmasked_edges_v2(mask, axis=1))\n# [(Array([0, 1, 2], dtype=int32), Array([1, 0, 0], dtype=int32)),\n#  (Array([0, 1, 2], dtype=int32), Array([2, 1, 3], dtype=int32))]"
        ],
        "link": "https://stackoverflow.com/questions/78660344/simplest-equivalent-implementation-of-numpy-ma-notmasked-edges-for-use-in-jax"
    },
    {
        "title": "Why is Flax Linear layer not identical to matrix multiplication?",
        "question": "Due to the novelty of Flax, NNX, and JAX, there’s not a lot of resources available. I’m running into the following peculiarity:\nx = jnp.random.normal((1,512), key=KEY)\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = layer.kernel@x.squeeze() + layer.bias\nprint(y1==y2) # returns all False\nMy understanding is that matrix multiplication should be identical to a linear / fully connected layer. The discrepancy demonstrated here hinders the inspection of certain behavior (and the implementation of invertible dense layers using jnp.tensorsolve).\nDoes anyone know what causes this discrepancy?",
        "answers": [
            "The matmul should be transposed; also floating point equality checks should be done via approximate rather than exact comparison, because different ways of computing the same result may lead to different floating point rounding errors:\nimport jax\nfrom flax import nnx\n\nKEY = jax.random.key(0)\nx = jax.random.normal(KEY, (1,512))\nlayer = nnx.Linear(512, 512, rngs=nnx.Rngs(KEY))\ny1 = layer(x)\ny2 = x @ layer.kernel + layer.bias\nprint(jax.numpy.allclose(y1, y2))  # True"
        ],
        "link": "https://stackoverflow.com/questions/78659890/why-is-flax-linear-layer-not-identical-to-matrix-multiplication"
    },
    {
        "title": "Using JAX ndarray.at apply(ufunc) with arguments",
        "question": "Can arguments be passed to a jax.numpy.ufunc within a jax.numpy.ndarray.at call?\nThe following is an attempt to replicate jax.numpy.ndarray.at[...].add(...)\nimport jax.numpy as jnp\n\ndef myadd(a,b=1):\n    return a+b\n\numyadd = jnp.frompyfunc(myadd,2,1,identity=0)\n\nx = jnp.arange(4)\n\n# call jnp.add(x,x)\nx.at[:].add(x)\n# [0 2 4 6]\n\n# call umyadd.at\numyadd.at(x, np.arange(x.size), x, inplace=False)\n# [0 2 4 6]\n\n# Default b=1 (can b be passed here?)\nx.at[:].apply(umyadd)\n# [1 2 3 4]",
        "answers": [
            "arr.at[...].apply() only accepts unary functions that map a scalar to a scalar. So you could pass b via closure, as long as it's a scalar; for example:\nx.at[:].apply(lambda a: umyadd(a, 2))\n# [2, 3, 4, 5]\nBut there is no way to pass b=jnp.arange(4) within apply(), because then the applied function no longer maps a scalar to a scalar."
        ],
        "link": "https://stackoverflow.com/questions/78642505/using-jax-ndarray-at-applyufunc-with-arguments"
    },
    {
        "title": "how to log activation values using jax",
        "question": "I am following a jax tutorial that trains mnist using mlp network. I am trying to add an additional code that saves the activation patterns at every layer except the last. Here is the modified code:\nfrom collections import defaultdict\n# this is my activation pattern logger\nclass ActivationLogger:\n    def __init__(self, epoch):\n       self.reset(epoch)\n\n    def __call__(self, layer, activations):\n        D = activations.shape[0]\n        for i in range(D):\n            self.activations[(layer, i)].append(\n                    jax.lax.stop_gradient(activations[i]))\n\n    def reset(self, epoch):\n        self.epoch = epoch\n        self.activations = defaultdict(list)\n\nactivation_logger = ActivationLogger(epoch=1)\n\n...\n\ndef predict(params, image):\n    # per-example predictions\n    activations = image\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        activation_logger(l+1, activations) # <- this was added\n\n    final_w, final_b = params[-1]\n    logits = jnp.dot(final_w, activations) + final_b\n    return logits - logsumexp(logits)\n\nbatched_predict = jax.vmap(\n        predict, \n        in_axes=(None, 0), \n        out_axes=0)\n\n@jax.jit\ndef loss(params, images, targets):\n  preds = batched_predict(params, images)\n  return -jnp.mean(preds * targets)\nWhen I run my training code, I keep getting the following error message:\nTracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[].\nThis BatchTracer with object id 7541955024 was created on line:\n  /var/folders/km/3nj8tmq56s16dsgc9_63530r0000gn/T/ipykernel_73750/3494160804.py:12:20 (ActivationLogger.__call__)\nAny suggestions on how to fix this?",
        "answers": [
            "Python functions in JAX code are executed at trace-time, not runtime, and so as written you're not logging concrete runtime values, but rather their abstract trace-time representations.\nIf you want to log runtime values, the best tool is probably jax.debug.callback; for info on using this, I'd suggest starting with External Callbacks in JAX.\nUsing it in your case would look something like this:\n    for l, (w, b) in enumerate(params[:-1]):\n        outputs = jnp.dot(w, activations) + b\n        activations = jnp.maximum(0, outputs)\n        jax.debug.callback(activation_logger, l+1, activations)\nFor more background on JAX's execution model, and why your function didn't work as expected when executed directly a trace-time, a good place to start is How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78611094/how-to-log-activation-values-using-jax"
    },
    {
        "title": "jax complaining about static start/stop/step",
        "question": "Here is a very simple computation in jax which errors out with complaints about static indices:\ndef get_slice(ar, k, I):\n  return ar[i:i+k]\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2,3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-32-6c60650ce6b7> in <cell line: 1>()\n----> 1 vec_get_slice(arr, 2, jnp.arange(3))\n\n    [... skipping hidden 3 frame]\n\n4 frames\n<ipython-input-29-9528369725c2> in get_slice(ar, k, i)\n      1 def get_slice(ar, k, i):\n----> 2   return ar[i:i+k]\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/array.py in __getitem__(self, idx)\n    346           return out\n    347 \n--> 348     return lax_numpy._rewriting_take(self, idx)\n    349 \n    350   def __iter__(self):\n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _rewriting_take(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4602 \n   4603   treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape)\n-> 4604   return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n   4605                  unique_indices, mode, fill_value)\n   4606 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\n   4611             unique_indices, mode, fill_value):\n   4612   idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n-> 4613   indexer = _index_to_gather(shape(arr), idx)  # shared with _scatter_update\n   4614   y = arr\n   4615 \n\n/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py in _index_to_gather(x_shape, idx, normalize_indices)\n   4854                \"dynamic_update_slice (JAX does not support dynamically sized \"\n   4855                \"arrays within JIT compiled functions).\")\n-> 4856         raise IndexError(msg)\n   4857 \n   4858       start, step, slice_size = _preprocess_slice(i, x_shape[x_axis])\n\nHorrible error output below. I am obviously missing something simple, but what?\n\n\nIndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([0, 1, 2], dtype=int32)\n  batch_dim = 0, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)> with\n  val = Array([2, 3, 4], dtype=int32)\n  batch_dim = 0, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).",
        "answers": [
            "Indices passed to slices in JAX must be static. Values that are mapped over in vmap are not static: because you're mapping over the start indices, your indices are not static and you see this error.\nThere is good news though: the size of your subarray is controlled by k, which is unmapped in your code and therefore static; it's only the location of the slice (given by I) that is dynamic. This is exactly the situation that jax.lax.dynamic_slicewas designed for, and so you can rewrite your code like this:\nimport jax\nimport jax.numpy as jnp\n\ndef get_slice(ar, k, I):\n  return jax.lax.dynamic_slice(ar, (I,), (k,))\n\nvec_get_slice = jax.vmap(get_slice, in_axes=(None, None, 0))\n\narr = jnp.array([1, 2, 3, 4, 5])\n\nvec_get_slice(arr, 2, jnp.arange(3))\n# Array([[1, 2],\n#        [2, 3],\n#        [3, 4]], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78588301/jax-complaining-about-static-start-stop-step"
    },
    {
        "title": "TypeError: unhashable type: 'ArrayImpl' when trying to use Equinox module with jax.lax.scan",
        "question": "I'm new to Equinox and JAX but wanted to use them to simulate a dynamical system.\nBut when I pass my system model as an Equinox module to jax.lax.scan I get the unhashable type error in the title. I understand that jax expects the function argument to be a pure function but I thought an Equinox Module would emulate that.\nHere is a test script to reproduce the error\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\n\n\nclass EqxModel(eqx.Module):\n    A: jax.Array\n    B: jax.Array\n    C: jax.Array\n    D: jax.Array\n\n    def __call__(self, states, inputs):\n        x = states.reshape(-1, 1)\n        u = inputs.reshape(-1, 1)\n        x_next = self.A @ x + self.B @ u\n        y = self.C @ x + self.D @ u\n        return x_next.reshape(-1), y.reshape(-1)\n\n\ndef simulate(model, inputs, x0):\n    xk = x0\n    outputs = []\n    for uk in inputs:\n        xk, yk = model(xk, uk)\n        outputs.append(yk)\n    outputs = jnp.stack(outputs)\n    return xk, outputs\n\n\nA = jnp.array([[0.7, 1.0], [0.0, 1.0]])\nB = jnp.array([[0.0], [1.0]])\nC = jnp.array([[0.3, 0.0]])\nD = jnp.array([[0.0]])\nmodel = EqxModel(A, B, C, D)\n\n# Test simulation\ninputs = jnp.array([[0.0], [1.0], [1.0], [1.0]])\nx0 = jnp.zeros(2)\nxk, outputs = simulate(model, inputs, x0)\nassert jnp.allclose(xk, jnp.array([2.7, 3.0]))\nassert jnp.allclose(outputs, jnp.array([[0.0], [0.0], [0.0], [0.3]]))\n\n# This raises TypeError\nxk, outputs = jax.lax.scan(model, x0, inputs)\nWhat is unhashable type: 'ArrayImpl' referring to? Is it the arrays A, B, C, and D? In this model, these matrices are parameters and therefore should be static for the duration of the simulation.\nI just found this issue thread that might be related:\nlax.scan for equinox Modules",
        "answers": [
            "Owen Lockwood (lockwo) has provided an explanation and answer in this issue thread, which I will re-iterate below.\nI believe your issue is happening because jax tries to hash the function you are scanning over, but it can't hash the arrays that are in the module. There are probably a number of things that you could do to solve this, the simplest being to just curry the model, e.g. xk, outputs = jax.lax.scan(lambda carry, y: model(carry, y), x0, inputs) works fine\nOr, re-written in terms of the variable names I am using:\nxk, outputs = jax.lax.scan(lambda xk, uk: model(xk, uk), x0, inputs)"
        ],
        "link": "https://stackoverflow.com/questions/78583009/typeerror-unhashable-type-arrayimpl-when-trying-to-use-equinox-module-with-j"
    },
    {
        "title": "Multiplying chains of matrices in JAX",
        "question": "Suppose I have a vector of parameters p which parameterizes a set of matrices A_1(p), A_2(p),...,A_N(p). I have a computation in which for some list of indices q of length M, I have to compute A_{q_M} * ... * A_{q_2} * A_{q_1} * v for several different q s. Each q has a different length, but crucially doesn't change! What changes, and what I wish to take gradients against is p.\nI'm trying to figure out how to convert this to performant JAX. One way to do it is to have some large matrix Q which contains all the different qs on each row, padded out with identity matrices such that each multiplication chain is the same length, and then scan over a function that switch es between N different functions doing matrix-vector multiplications by A_n(p).\nHowever -- I don't particularly like the idea of this padding. Also, since Q here is fixed, is there potentially a smarter way to do this? The distribution of lengths of q s has a very long tail, so Q will be dominated by padding.\nEDIT: Here's a (edit 2: functional) minimal example\nsigma0 = jnp.eye(2)\nsigmax = jnp.array([[0, 1], [1, 0]])\nsigmay = jnp.array([[0, -1j], [1j, 0]])\nsigmaz = jnp.array([[1, 0], [0, -1]])\nsigma = jnp.array([sigmax, sigmay, sigmaz])\n\ndef gates_func(params):\n    theta = params[\"theta\"]\n    epsilon = params[\"epsilon\"]\n\n    n = jnp.array([jnp.cos(theta), 0, jnp.sin(theta)])\n    omega = jnp.pi / 2 * (1 + epsilon)\n    X90 = expm(-1j * omega * jnp.einsum(\"i,ijk->jk\", n, sigma) / 2)\n\n    return {\n        \"Z90\": expm(-1j * jnp.pi / 2 * sigmaz / 2),\n        \"X90\": X90\n    }\n\ndef multiply_out(params):\n    gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n\n    gates = gates_func(params)\n    out = jnp.zeros(len(gate_lists)) \n    \n    for i, gate_list in enumerate(gate_lists):\n        init = jnp.array([1.0,0.0], dtype=jnp.complex128)\n        for g in gate_list:\n            init = gates[g] @ init\n        out = out.at[i].set(jnp.abs(init[0]))\n\n    return out\n\nparams = dict(theta=-0.0, epsilon=0.001)\nmultiply_out(params)",
        "answers": [
            "The main issue here is that JAX does not support string inputs. But you can use NumPy to manipulate string arrays and turn them into integer categorical arrays that can then be used by jax.jit and jax.vmap. The solution might look something like this:\nimport numpy as np\n\ndef gates_func_int(params, gate_list_vals):\n  g = gates_func(params)\n  identity = jnp.eye(*list(g.values())[0].shape)\n  return jnp.stack([g.get(val, identity) for val in gate_list_vals])\n\n@jax.jit\ndef multiply_out_2(params):\n  # compile-time pre-processing\n  gate_lists = [[\"X90\", \"X90\"], [\"X90\",\"Z90\"], [\"Z90\", \"X90\"], [\"X90\",\"Z90\",\"X90\"]]\n  max_size = max(map(len, gate_lists))\n  gate_array = np.array([gates + [''] * (max_size - len(gates))\n                        for gates in gate_lists])\n  gate_list_vals, gate_list_ints = np.unique(gate_array, return_inverse=True)\n  gate_list_ints = gate_list_ints.reshape(gate_array.shape)\n\n  # runtime computation\n  gates = gates_func_int(params, gate_list_vals)[gate_list_ints]\n  initial = jnp.array([[1.0],[0.0]], dtype=jnp.complex128)\n  return jax.vmap(lambda g: jnp.abs(jnp.linalg.multi_dot([*g, initial]))[0])(gates).ravel()\n\nmultiply_out_2(params)"
        ],
        "link": "https://stackoverflow.com/questions/78562406/multiplying-chains-of-matrices-in-jax"
    },
    {
        "title": "How to set a new learning rate manually in optax optimizer?",
        "question": "I have the following optimizer being create using optax:\ndef create_optimizer(learning_rate=6.25e-2, beta1=0.4, beta2=0.999,\n                     eps=2e-4, centered=False):\n\n  Returns:\n    An optax optimizer.\n  \"\"\"\n \n    return optax.adam(learning_rate, b1=beta1, b2=beta2, eps=eps)\nHow during training update this learning rate manually?\nI couldn't find any documentation about that.",
        "answers": [
            "Disclaimer. Usually, you would use a schedule to adapt the learning rate during training. This answer provides a solution to obtain direct control over the learning rate.\nIn general, you can put any optimizer's hyperparmeters (such as the learning rate) into the optimizer's state and then directly mutate the state. Moving the hyperparameters into the state is necessary as optax optimizers are pure functions. Especially, the only way to dynamically change their behaviour is by changing their input.\nSetup. I am using a stochastic gradient descent optimizer to highlight the effect of the learning rate on the update suggested by the optimizer.\nimport jax.numpy as jnp\nimport optax\n\n# Define example parameters and gradients.\nparams, grads = jnp.array([0.0, 0.0]), jnp.array([1.0, 2.0])\n\n# Ensure the learning rate is part of the optimizer's state.\nopt = optax.inject_hyperparams(optax.sgd)(learning_rate=1e-2)\nopt_state = opt.init(params)\nUpdate computation.\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.01, -0.02], dtype=float32)\nDirectly setting the learning rate.\nopt_state.hyperparams['learning_rate'] = 3e-4\nSame update computation as before (with new learning rate).\nupdates, _ = opt.update(grads, opt_state)\nupdates\nArray([-0.0003, -0.0006], dtype=float32)\nSee this discussion for more information."
        ],
        "link": "https://stackoverflow.com/questions/78527164/how-to-set-a-new-learning-rate-manually-in-optax-optimizer"
    },
    {
        "title": "Why JAX is considering same list as different data structure depending on appending a new array inside function?",
        "question": "I am very new to JAX. Please excuse me if this something obvious or I am making some stupid mistake. I am trying to implement a function which does the following. All these functions will be called from other JIT-ed function. So, removing JIT may not be possible.\nget_elements function takes a JAX array( call it state (1D)). Looks at each element in it and calls a function get_condition.\nget_condition returns a tuple depending on the element at the given position of state. The tuple may be (1,0),(0,1) or (0,0)\nHere I want to call update_state only if the tuple received from get_conn is (0,1) or (1,0). In that case update_state_vec will get called and add a new vector of same length as state will get appended to the list.\nBut, I couldn't make jax.lax.cond work here. So, I tried to call update_state for each case, but I want the list to remain unchanged if the codition is (0,0).\nIn update_state_vec, no_update_state should return the same array\nthat it receives withourt appending anything\nHere, is the entire code:\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import lax\nimport copy\nfrom copy import deepcopy\n\nimport numpy as np\n\n\ndef get_condition(state, x, y):\n   L = (jnp.sqrt(len(jnp.asarray(state)))).astype(int)\n   state = jnp.reshape(state, (L,L), order=\"F\")\n   s1 = state[x, y]\n\n   branches = [lambda : (0,1), lambda : (1,0), lambda : (0,0)]\n   conditions = jnp.array([s1==2, s1==4, True])\n   result = lax.switch(jnp.argmax(conditions), branches)\n   return tuple(x for x in result)\n\n\n\n\ndef update_state_vec(state, x, y, condition, list_scattered_states):\n   L = (jnp.sqrt(len(state))).astype(int)   \n   def update_state_4(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(4)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n   def update_state_2(list_scattered_states):\n       state1 = jnp.array( jnp.reshape(deepcopy(state), (L, L), order=\"F\"))\n       state1 = state1.at[x, y].set(2)\n       list_scattered_states.append(jnp.ravel(state1, order=\"F\"))\n       return list_scattered_states\n\n\n   def no_update_state (list_scattered_states):\n       #state1 = jnp.ravel(state, order=\"F\")\n       #list_scattered_states.append(jnp.ravel(state, order=\"F\"))\n       #This doesn't work---------------------------------\n       return list_scattered_states\n\n\n\n   conditions = jnp.array([condition == (1, 0), condition == (0, 1), condition == (0, 0)])\n   print(conditions)\n   branches = [update_state_4, update_state_2,no_update_state]\n\n   return(lax.switch(jnp.argmax(conditions), branches, operand=list_scattered_states))\n           \n\n\ndef get_elements(state):\n\n   L = (jnp.sqrt(len(state))).astype(int)\n   list_scattered_states = []\n   for x in range(L):\n       for y in range(L):\n           condition=get_condition(state, x, y)\n           print(condition)\n           list_scattered_states = update_state_vec(state, x, y, condition, list_scattered_states)\n\n\n   return list_scattered_states\nWe can take an example input as follows,\narr=jnp.asarray([2., 1., 3., 4., 1., 2., 3., 4., 4., 1., 2., 3., 4., 2., 1., 3.])\nget_elements(arr)\nI get an error message as below:\n    print(conditions)\n 41 branches = [update_state_4, update_state_2,no_update_state]\n ---> 43 return(lax.switch(jnp.argmax(conditions), branches, \n operand=list_scattered_states))\n TypeError: branch 0 and 2 outputs must have same type structure, got PyTreeDef([*]) \n and PyTreeDef([]).\nSo, the error is coming from the face that no_update_state is returning something that doesn't match with return type of update_state_4 or update_state_2. I am quite clueless at this point. Any help will be much appreciated.",
        "answers": [
            "The root of the issue here is that under transformations like jit, vmap, switch, etc. JAX requires the shape of outputs to be known statically, i.e. at compile time (see JAX sharp bits: dynamic shapes). In your case, the functions you are passing to switch return outputs of different shapes, and since jnp.argmax(conditions) is not known at compile time, there's no way for the compiler to know what memory to allocate for the result of this function.\nSince you're not JIT-compiling or otherwise transforming your code, the easiest way to address this would be to replace the lax.switch statement with this:\n  if condition == (1, 0):\n    list_scattered_states = update_state_4(list_scattered_states)\n  elif condition == (0, 1):\n    list_scattered_states = update_state_2(list_scattered_states)\n  return list_scattered_states\nIf you do want your function to be compatible with jit or other JAX transformations, you'll have to re-write the logic so that the size of list_scattered_states remains constant, e.g. by padding it to the expected size from the beginning."
        ],
        "link": "https://stackoverflow.com/questions/78512663/why-jax-is-considering-same-list-as-different-data-structure-depending-on-append"
    },
    {
        "title": "jax parallel multiplication of pairs of matrix with different shapes",
        "question": "Task: I have two lists of matrices A,B with length N. For each pair of elements A[i], B[i] shapes are such that matrix product is well-defined, however for each i in $0,\\dots, N-1$ shapes can be different. Hence, I can not stack them in array. Shapes are static.\nI would like to do achieve same result as following :\nout = [None] * length(A)\nfor i, a, b in enumerate(zip(A,B)):\n   out[i] = a @ b\nHowever, I would like to do this in parallel with jax. The best option will be vmap, but it is impossible as shapes are different.\nHere I will discuss solutions that I know and why they are not satisfactory.\nWrite for loop and then jit it. This will grow compilation time super linear over length N. This is not good, as I know all shapes of input and output before running computation, so I would expect to constant compilation time (provided say list of shapes).\nUse fori_loop primitive from jax. In documentation, there is following:\nThe semantics of fori_loop are given by this Python implementation:\ndef fori_loop(lower, upper, body_fun, init_val):\n  val = init_val\n  for i in range(lower, upper):\n    val = body_fun(i, val)\n  return val\nHowever, my case is easier: I don't need to care val across iterations. This means that fori is sequential. While my case is parallel. Hence, it should be possible to do better.\nPad with zeros, use vmap, read result. I don't control distribution of shapes, so it can lead to blowing memory if only one shape is big.\nUse lax.map Here (What are the tradeoffs between jax.lax.map and jax.vmap?) I read following:\nThe lax.map solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.\nSo I don't know what to do. Thanks!\nUpd after answer:\nN = 100\nd = 1000\nkey = jrandom.key(0)\nAjnp = jrandom.normal(key, (N, d, d))\nBjnp = jrandom.normal(key, (N, d, d))\n\nAnp = list(np.random.randn(N,d,d))\nBnp = list(np.random.randn(N,d,d))\n\nvmatmul = vmap(jnp.matmul, (0,0))\n\ndef lmatmul(A,B):\n    return [a @ b for a, b in zip(A,B)]\n%timeit vmatmul(Ajnp, Bjnp).block_until_ready()  # jax vmap over arrays\n6.59 ms ± 73.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%timeit block_until_ready(lmatmul(list(Ajnp), list(Bjnp))) # jax loop over lists\n13 ms ± 221 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n%timeit lmatmul(Anp, Bnp) # numpy loop over lists\n1.28 s ± 13.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)",
        "answers": [
            "I think your best approach will be something like your original formulation, though you can avoid pre-allocating the out list:\nout = [a @ b for a, b in zip(A, B)]\nBecause of JAX's Asynchronous dispatch, if you run this on an accelerator like GPU the operations will be executed in parallel to the extent possible.\nAll of your other proposed solutions either won't work due to static shape limitations, will force sequential computation, or will incur overhead that will make them worse in practice than this more straightforward approach."
        ],
        "link": "https://stackoverflow.com/questions/78502704/jax-parallel-multiplication-of-pairs-of-matrix-with-different-shapes"
    },
    {
        "title": "Jax dynamic slicing tracer array",
        "question": "To make this brief: I wrote the following codes:\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef body_func(carry,x):\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nbut got\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nHere's the full situation: I'm trying to develop a model to do phase recognition tasks, and I would like to normalize my logits phase by phase using jax. For example, suppose I have the phase labels and logits:\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\nI would like to normalize the first 4 elements in logits where in the phase labels they all belong to phase 0. Then the next 4 elements, because in the phase labels they all belong to phase 1. So the normalized logits should look like:\nnormalized_logits=[0,0.33,0.66,1.0,0,0.33,0.66,1.0,0,0.33,0.66,1.0]\nHere's what tried:\nimport jax\nimport jax.numpy as np\n\nlabels=np.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits=np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\ndef min_max_normalization(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndef body_func(carry,x):\n    jax.debug.print(\"carry is {}\",carry)\n    jax.debug.print(\"x is {}\",x)\n    start_idx,arr=carry\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(x, start_idx+1)]))\n    print(min_max_normalization(jax.lax.dynamic_slice(arr, [start_idx], [jax.lax.tie_in(x, x-start_idx+1)])))\n    print(jax.lax.dynamic_slice(arr, [x+1], [jax.lax.tie_in(x, len(arr)-x-1)]))\n    carry=(start_idx,arr)\n    return carry, carry\n\nslices,=np.where(np.diff(labels)!=0)\nprint(jax.lax.scan(body_func,(0,logits),np.array(slices)))\nBasically, this is a debug version, the actual return value should concatenate three dynamically sliced array together. But I'm getting the error below:\nTraceback (most recent call last):\n  File \"/path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 250, in scan\n    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n                                                                ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/loops.py\", line 236, in _create_jaxpr\n    jaxpr, consts, out_tree = _initial_style_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 64, in _initial_style_jaxpr\n    jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/control_flow/common.py\", line 58, in _initial_style_open_jaxpr\n    jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2155, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/slicing.py\", line 110, in dynamic_slice\n    static_sizes = core.canonicalize_shape(slice_sizes)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/core.py\", line 2086, in canonicalize_shape\n    raise _invalid_shape_error(shape, context)\njax._src.traceback_util.UnfilteredStackTrace: TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /Users/wuhaoyang/Documents/Research/Project_Surgical_Robot/Code/SSM_Med/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"path/test.py\", line 21, in <module>\n    print(jax.lax.scan(body_func,(0,b),np.array(c)))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"path/test.py\", line 14, in body_func\n    print(jax.lax.dynamic_slice(arr, [0], [jax.lax.tie_in(int(1), start_idx+1)]))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_func at /path/test.py:10 for scan. This concrete value was not available in Python because it depends on the value of the argument carry[0].\nThe reason why I'm not simply using a for loop is that I'm later going to wrap this function into another one that uses jit compile, so I want to do this with pure jax API. Any help is appreciated, please tell me if you need more information.",
        "answers": [
            "JAX arrays used in transformations like jit, vmap, and scan must always be statically-shaped (see Sharp bits: Dynamic Shapes for some discussion of this).\ndynamic_slice allows you to slice a static length at a dynamic position, while you're trying to use it to slice a dynamic length at a static position, and thus you're seeing this concretization error.\nTo solve your problem, I would avoid scan and instead use JAX's segment_min and segment_max functions to compute the output in a vectorized rather than iterative manner:\nimport jax\nimport jax.numpy as jnp\n\nlabels = jnp.array([0,0,0,0,1,1,1,1,2,2,2,2])\nlogits = jnp.array([1,2,3,4,5,6,7,8,9,10,11,12])\n\nl_min = jax.ops.segment_min(logits, labels)[labels]\nl_max = jax.ops.segment_max(logits, labels)[labels]\n\nnormalized_logits = (logits - l_min) / (l_max - l_min)\nprint(normalized_logits)\n# [0.         0.33333334 0.6666667  1.         0.         0.33333334\n#  0.6666667  1.         0.         0.33333334 0.6666667  1.        ]\nIf you want this to be compatible with jit and other transformations, you'll need to pass a static num_segments argument to your segment reductions to specify an upper-bound for the number of segments present:\nl_min = jax.ops.segment_min(logits, labels, num_segments=3)[labels]\nl_max = jax.ops.segment_max(logits, labels, num_segments=3)[labels]"
        ],
        "link": "https://stackoverflow.com/questions/78496911/jax-dynamic-slicing-tracer-array"
    },
    {
        "title": "Finite basis physics-informed neural networks (FBPINNs) JAX problem",
        "question": "I am trying to modify Ben Moseley's code available on github https://github.com/benmoseley/FBPINNs. My intention is to insert a vector of values into the loss fn that is dependent on x y coordinates, and I need the original vector Z to be interpolated as a function of x and y, and then the values at the same coordinates with which the algorithm samples x and y are extracted, so that the values match. The problem I have encountered is that within loss fn I cannot use libraries other than JAX and to my knowledge there are no functions within JAX to interpolate in 2D.\nI'm trying to get around the problem in every way but I'm not succeeding, one of my ideas was to extrapolate the x,y points sampled by the algorithm but I'm not succeeding, the code is really very articulated. Would anyone be able to give me any advice/help on this?\nThere would be the function jax.scipy.ndimage.map_coordinates but it doesn't work properly and the points it extrapolates are meaningless.",
        "answers": [
            "If linear or nearest-neighbor interpolation is sufficient, you may be able to do what you need with jax.scipy.interpolate.RegularGridInterpolator\nIf you need something more sophisticated, like spline interpolation, there is nothing included in jax itself. That said, you may be able to find downstream implementations that work for you. One I came across that might be worth trying is in the jax_cosmo project: https://jax-cosmo.readthedocs.io/en/latest/_modules/jax_cosmo/scipy/interpolate.html."
        ],
        "link": "https://stackoverflow.com/questions/78494686/finite-basis-physics-informed-neural-networks-fbpinns-jax-problem"
    },
    {
        "title": "Why is custom pytree 'aux_data' traced after jax.jit() for jnp.array but not for np.array?",
        "question": "I am trying to understand how pytrees work and registered my own class as a pytree. I noticed that if the aux_data in the pytree is a jax.numpy.ndarray the auxilliary data is subsequently traced and returned as a Traced<ShapedArray(...)>.... However, if the aux_data is a numpy.ndarray (i.e. not JAX array), then it is not traced and returns an array from a jit tranformed function.\nNow, I am aware of the tracing that happens during the jax.jit() transformation, but I do not understand why, on the level of pytrees, this results in the behaviour described above.\nHere is an example to reproduce this behaviour (multiplying both the aux_data and the tree leaves by two, which may be a problem in itself after JIT transformation...?). I have used the custom pytree implementations of accepted libraries (equinox and simple_pytree) for comparison, and they all give the same result, so that I am very sure that this is not a bug but a feature that I am trying to understand.\nimport jax\nfrom jax.tree_util import tree_structure, tree_leaves\nimport numpy as np\n\ndef get_pytree_impl(base):\n    if base == \"equinox\":\n        import equinox as eqx\n        Module = eqx.Module\n        static_field = eqx.static_field\n    elif base == \"simple_pytree\":\n        from simple_pytree import Pytree, static_field\n        Module = Pytree\n    elif base == \"dataclasses\":\n        from dataclasses import dataclass, field\n        @dataclass\n        class Module():\n            pass\n        static_field = field\n    \n    class PytreeImpl(Module):\n        x: jax.numpy.ndarray\n        y: jax.numpy.ndarray = static_field()\n\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n\n    if base == 'dataclasses':\n        from jax.tree_util import register_pytree_node\n        \n        def flatten(ptree):\n            return ((ptree.x,), ptree.y)\n        \n        def unflatten(aux_data, children):\n            return PytreeImpl(*children, aux_data)\n\n        register_pytree_node(PytreeImpl, flatten, unflatten)\n        \n    return PytreeImpl\n\ndef times_two(ptree):\n    return type(ptree)(ptree.x*2, ptree.y*2)\n\ntimes_two_jitted = jax.jit(times_two)\n\nbases = ['dataclasses', 'equinox', 'simple_pytree']\nfor base in bases:\n    print(\"========  \" + base + \"  ========\")\n    for lib_name, array_lib in zip(['jnp', 'np'], [jax.numpy, np]):\n        print(\"====  \" + lib_name)\n        PytreeImpl = get_pytree_impl(base)\n        x = jax.numpy.array([1,2])\n        y = array_lib.array([3,4])\n        input_tree = PytreeImpl(x, y)\n        for tag, pytree in zip([\"input\", \"no_jit\", \"jit\"],[input_tree, times_two(input_tree), times_two_jitted(input_tree)]):\n            print(f' {tag}:')\n            print(f'\\t Structure: {tree_structure(pytree)}')\n            print(f'\\t Leaves: {tree_leaves(pytree)}')\nThis produces the follwing, where dataclasses is my naive custom implementation of a pytree:\n========  dataclasses  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[3 4]], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[[6 8]], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  equinox  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([3, 4], dtype=int32),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Array([6, 8], dtype=int32),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>,)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([3, 4]),)], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[('x',), ('y',), (array([6, 8]),)], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n========  simple_pytree  ========\n====  jnp\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([3, 4], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Array([6, 8], dtype=int32), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': Traced<ShapedArray(int32[2])>with<DynamicJaxprTrace(level=1/0)>, '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n====  np\n input:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([3, 4]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([1, 2], dtype=int32)]\n no_jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\n jit:\n     Structure: PyTreeDef(CustomNode(PytreeImpl[(('x',), {'y': array([6, 8]), '_pytree__initialized': True})], [*]))\n     Leaves: [Array([2, 4], dtype=int32)]\nI ran this example using Python 3.12.1 with equinox 0.11.4 jax 0.4.28 jaxlib 0.4.28 simple-pytree 0.1.5",
        "answers": [
            "From the JAX docs:\nWhen defining unflattening functions, in general children should contain all the dynamic elements of the data structure (arrays, dynamic scalars, and pytrees), while aux_data should contain all the static elements that will be rolled into the treedef structure.\naux_data in a pytree flattening must contain static elements, and static elements must be hashable and immutable. Neither np.ndarray nor jax.Array satisfy this, so they should not be included in aux_data. If you do include such values in aux_data, you'll get unsupported, poorly-defined behavior.\nWith that background: the answer to your question of why you're seeing the results you're seeing is that you are defining your pytrees incorrectly. If you define aux_data to only contain static (i.e. hashable and immutable) attributes, you will no longer see this behavior."
        ],
        "link": "https://stackoverflow.com/questions/78485445/why-is-custom-pytree-aux-data-traced-after-jax-jit-for-jnp-array-but-not-for"
    },
    {
        "title": "What is an efficient method to calculate multiple \"offset-traces\" in JAX?",
        "question": "Given a matrix m with shape (n, n), I need to compute the series of \"offset traces\" [np.trace(m, offset=i) for i in range(q)] in JAX. For my application, n >> q, and q is a static parameter.\nThe obvious JAX approach using vmap does not work, possibly because although the trace has fixed output size, each offset diagonal has a different length?\nI came up with two other approaches using JAX which work but are about 100x slower than NumPy. get_traces_jax_1 is the more efficient of the two. But it does a lot of extra work when I only need a few diagonals, and I don't think that extra work gets compiled away.\nIs there a more efficient way to do this in JAX with similar performance to NumPy? I want to use JAX because:\nI need to vmap this across many matrices;\nIt is part of a larger algorithm, other parts of which are significantly sped up by JAX jit.\nBelow are the methods I explored and timings on my computer.\nimport numpy as np\nfrom numpy import random\nimport jax\njax.config.update(\"jax_enable_x64\", True) # default is float32\nfrom jax import numpy as jnp\nfrom functools import partial\n\nn, q = 1000, 5\n\n# check the methods produce the same result\ndef distance(u, v):\n    return jnp.max(jnp.abs(u - v))\n\n# numpy - this is what I want\ndef get_traces_np(mat, q):\n    return np.array([np.trace(mat, offset=i) for i in range(q)])\n\n# jax\n# !! This does not work\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_broken(mat, q):\n    return jax.vmap(lambda i: jnp.trace(mat, offset=i))(jnp.arange(q)) # !! does not work\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_0(mat, q):\n    return jnp.array([jnp.trace(mat, offset=i) for i in range(q)])\n\n@partial(jax.jit, static_argnums=(1,))\ndef get_traces_jax_1(mat, q):\n    n = mat.shape[0]\n    padded = jnp.pad(mat, ((0, 0), (0, n-1)), 'constant')\n    shifts = jax.vmap(lambda v, i: jnp.roll(v, -i))(padded, jnp.arange(n))[:, :n]\n    return jnp.sum(shifts, axis=0)[:q]\n\nmat = random.uniform(size=(n, n))\n# Check they produce the same result and precompile\nd0 = distance(get_traces_np(mat, q), get_traces_jax_0(mat, q))\nd1 = distance(get_traces_np(mat, q), get_traces_jax_1(mat, q))\nprint(f'Errors: {d0}, {d1}')\n\nmat = jnp.array(mat)\nprint('Numpy:')\n%timeit get_traces_np(mat, q) # 7.43 microseconds\nprint('Jax 0:')\n%timeit get_traces_jax_0(mat, q) # 4.82ms\nprint('Jax 1:')\n%timeit get_traces_jax_1(mat, q) # 1.22ms",
        "answers": [
            "vmapping jnp.trace across offset doesn't work because as currently implemented, the offset parameter of jnp.trace must be static, and vmapped parameters are not static. You could address this by constructing your own version of the trace function that does not require a static parameter; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef dynamic_trace(x, offset):\n  assert x.ndim == 2\n  i = jnp.arange(x.shape[0])[:, None]\n  j = jnp.arange(x.shape[1])\n  return jnp.sum(jnp.where(i + offset == j, x, 0))\n\nx = jnp.arange(12).reshape(3, 4)\n\noffset = jnp.arange(-2, 3)\n\njax.vmap(dynamic_trace, in_axes=(None, 0))(x, offset)\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\n\njnp.array([jnp.trace(x, int(o)) for o in offset])\n# Array([ 8, 13, 15, 18,  9], dtype=int32)\nIn terms of benchmark comparisons, you should first make sure you're measuring what you think you're measuring. For example, your benchmark is written in such a way that it doesn't just measure runtime, but also device transfer and JIT compilation costs. It also ignores potentially confounding effects of asynchronous dispatch. For a discussion of these issues, see JAX FAQ: Benchmarking JAX code.\nEven accounting for this, however, I would not expect JAX to be faster than NumPy here for the reasons discussed at JAX FAQ: Is JAX Faster than NumPy?. Namely, this benchmark is doing a short sequence of relatively small array operations on CPU: this is a regime in which NumPy will always beat JAX due to its lower per-operation dispatch overhead. But not to worry: when this operation is used as part of a larger JIT-compiled function, those dispatch costs are amortized away."
        ],
        "link": "https://stackoverflow.com/questions/78479150/what-is-an-efficient-method-to-calculate-multiple-offset-traces-in-jax"
    },
    {
        "title": "Iterators in jit JAX functions",
        "question": "I'm new to JAX and reading the docs i found that jitted functions should not contain iterators (section on pure functions)\nand they bring this example:\nimport jax.numpy as jnp\nimport jax.lax as lax\nfrom jax import jit\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\ntrying to fiddling with it a little bit in order to see if i can get directly an error instead of undefined behaviour i wrote\n@jit\ndef f(x, arr):\n    for i in range(10):\n        x += arr[i]\n    return x\n\n@jit\ndef f1(x, arr):\n    it = iter(arr)\n    for i in range(10):\n        x += next(it)\n    return x\n\nprint(f(0,array)) # 45 as expected\nprint(f1(0,array)) # still 45 \nIs it a \"chance\" that the jitted function f1() now shows the correct behaviour?",
        "answers": [
            "Your code works because of the way that JAX's tracing model works. When JAX's tracing encounters Python control flow, like for loops, the loop is fully evaluated at trace-time (There's some exploration of this in JAX Sharp Bits: Control Flow).\nBecause of this, your use of an iterator in this context is fine, because every iteration is evaluated at trace-time, and so next(it) is re-evaluated at every iteration.\nIn contrast, when using lax.fori_loop, next(iterator) is only executed a single time and its output is treated as a trace-time constant that will not change during the runtime iterations."
        ],
        "link": "https://stackoverflow.com/questions/78403517/iterators-in-jit-jax-functions"
    },
    {
        "title": "how to vmap over multiple Dense instances in flax model? trying to avoid looping over a list of Dense instances",
        "question": "from jax import random,vmap\nfrom jax import numpy as jnp\nimport pprint\n\ndef f(s,layers,do,dx):\n    x = jnp.zeros((do,dx))\n    for i,layer in enumerate(layers):\n        x=x.at[i].set( layer( s[i] ) )\n    return x\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layers = [ nn.Dense( self.dx, use_bias=False )\n                        for _ in range(self.do) ]\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None,None))(s,self.layers,self.do,self.dx)\n        return x\n\nif __name__ == '__main__':\n    seed = 123\n    key = random.PRNGKey( seed )\n    key,subkey = random.split( key )\n    outer_batches = 4\n    s_observations = 5 # AKA the inner batch\n    x_features = 2\n    s_features = 3\n    s_shape = (outer_batches,s_observations, s_features)\n    s = random.uniform( subkey, s_shape )\n\n    key,subkey = random.split( key )    \n    model = net(x_features,s_observations)\n    p = model.init( subkey, s )\n    x = model.apply( p, s )    \n\n    params = p['params']\n    pkernels = jnp.array([params[key]['kernel'] for key in params.keys()])\n    x_=jnp.zeros((outer_batches,s_observations,x_features))\n    \n    g = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\n    \n    x_=g(s,pkernels)\n    print('s shape:',s.shape)\n    print('p shape:',pkernels.shape)\n    print('x shape:',x.shape)\n    print('x_ shape:',x_.shape)\n    print('sum of difference:',jnp.sum(x-x_))\nHi. I need some \"batch-specific\" parameters in my model. Here, there is an \"inner batch\" of length do such that there is a flax.linen.Dense instance for each element in that batch. The outer batch just passes multiple data instances into those layers. I accomplish this by creating a list of flax.linen.Dense instances in the setup method. Then in the __call__ method, I iterate over those layers to fill up an array. This iteration is encapsulated in a function f, and that function is wrapped in jax.vmap.\nI have also included some equivalent logic written as matrix multiplication (see the function g) to make it explicit what operation I was hoping to capture with this class.\nI would like to replace the for-loop in the __call__ method with a call to jax.vmap. I ofc get an error when I pass a list to vmap, and I ofc get an error when I try to put multiple Dense instances in a jax array. Is there an alternative to using a list to contain my multiple Dense instances? A constraint is that I should be able to create an arbitrary number of Dense instances at the time of model initialization.",
        "answers": [
            "vmap can be used to map a single function over batches of data. You are attempting to use it to map multiple functions over batches of data, which it cannot do.\nUpdated answer based on updated question:\nSince each layer is identical aside from the parameters fit to the input data, it sounds like what you want is to map a single dense layer against a batch of data. It might look something like this:\nkeys = vmap(random.fold_in, in_axes=(None, 0))(subkey, jnp.arange(s_observations))\nmodel = nn.Dense(x_features, use_bias=False)\np = vmap(model.init, in_axes=(0, 1))(keys, s)\nx = vmap(model.apply, in_axes=(0, 1), out_axes=1)(p, s)\n\npkernels = p['params']['kernel']\ng = vmap(vmap(lambda a,b: a@b),in_axes=(0,None))\nx_=g(s,pkernels)\n\nprint('sum of difference:',jnp.sum(x-x_))\n# sum of difference: 0.0\nPrevious answer\nIn general, the fix would be to define a single parameterized layer that you can pass to vmap. In the example you gave, every layer is identical, and so to achieve the result you're looking for you could write something like this:\ndef f(s,layer,dx):\n  return layer(s)\n\nclass net(nn.Module):\n    dx: int \n    do: int \n    def setup(self):\n        self.layer = nn.Dense( self.dx, use_bias=False )\n    def __call__(self, s):\n        x = vmap(f,in_axes=(0,None,None))(s,self.layer,self.dx)\n        return x\nIf you had different parameterization per layer, then you could achieve this within vmap by passing those parameters to vmap as well."
        ],
        "link": "https://stackoverflow.com/questions/78385261/how-to-vmap-over-multiple-dense-instances-in-flax-model-trying-to-avoid-looping"
    },
    {
        "title": "How to restore a orbax checkpoint with jax/flax?",
        "question": "I saved a orbax checkpoint with the code below:\ncheck_options = ocp.CheckpointManagerOptions(max_to_keep=5, create=True)\ncheck_path = Path(os.getcwd(), out_dir, 'checkpoint')\ncheckpoint_manager = ocp.CheckpointManager(check_path, options=check_options, item_names=('state', 'metadata'))\ncheckpoint_manager.save(\n                    step=iter_num,\n                    args=ocp.args.Composite(\n                        state=ocp.args.StandardSave(state),\n                        metadata=ocp.args.JsonSave((model_args, iter_num, best_val_loss, losses['val'].item(), config))))\nWhen I try to resume from the saved checkpoints, I used the code below to recover the state variable:\nstate, lr_schedule = init_train_state(model, params['params'], learning_rate, weight_decay, beta1, beta2, decay_lr, warmup_iters, \n                     lr_decay_iters, min_lr)  # Here state is the initialied state variable with type Train_state.\nstate = checkpoint_manager.restore(checkpoint_manager.latest_step(), items={'state': state})\nBut when I try to use the recovered state in the training loop, I got this error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:584, in shaped_abstractify(x)\n    583 try:\n--> 584   return _shaped_abstractify_handlers[type(x)](x)\n    585 except KeyError:\n\nKeyError: <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'>\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 37\n     34 if iter_num == 0 and eval_only:\n     35     break\n---> 37 state, loss = train_step(state, get_batch('train'))\n     39 # timing and logging\n     40 t1 = time.time()\n\n    [... skipping hidden 6 frame]\n\nFile /opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/api_util.py:575, in _shaped_abstractify_slow(x)\n    573   dtype = dtypes.canonicalize_dtype(x.dtype, allow_extended_dtype=True)\n    574 else:\n--> 575   raise TypeError(\n    576       f\"Cannot interpret value of type {type(x)} as an abstract array; it \"\n    577       \"does not have a dtype attribute\")\n    578 return core.ShapedArray(np.shape(x), dtype, weak_type=weak_type,\n    579                         named_shape=named_shape)\n\nTypeError: Cannot interpret value of type <class 'orbax.checkpoint.composite_checkpoint_handler.CompositeArgs'> as an abstract array; it does not have a dtype attribute\nSo, how should I correctly recover the state checkpoint and use it in the training loop?\nThanks!",
        "answers": [
            "You're mixing the old and new APIs in a way that is not allowed. Apologies that an error to that effect is not being raised, I can look into that.\nYour saving is correct, but I'd recommend that it look more like the following:\nwith ocp.CheckpointManager(path, options=options, item_names=('state', 'metadata')) as mngr:\n  mngr.save(\n      step, \n      args=ocp.args.Composite(\n          state=ocp.args.StandardSave(state),\n          metadata=ocp.args.JsonSave(...),\n      )\n  )\nWhen restoring, you're currently using items which is part of the old API, and the usage is inconsistent with the CheckpointManager's definition, which is done based on the new API.\nitem_names and args are hallmarks of the new API.\nYou should do:\nwith ocp.CheckpointManager(...) as mngr:\n  mngr.restore(\n      mngr.latest_step(), \n      args=ocp.args.Composite(\n          state=ocp.args.StandardRestore(abstract_state),\n      )\n  )\nLet me know if there's any unexpected issues with that."
        ],
        "link": "https://stackoverflow.com/questions/78376465/how-to-restore-a-orbax-checkpoint-with-jax-flax"
    },
    {
        "title": "acme error - AttributeError: module 'jax' has no attribute 'linear_util'",
        "question": "I am using acme framework to run some experiments, and I installed acme based on documentation. However, I have attribute error that raised likely from JAX, HAIKU, and when I looked into github issue, there was no solution given at this time. Can anyone take a look what package dependecy caused this issue?\nmy venv spec:\nhere is my venv spec\ndm-acme                      0.4.0\ndm-control                   0.0.364896371\ndm-env                       1.6\ndm-haiku                     0.0.10\ndm-launchpad                 0.5.0\ndm-reverb                    0.7.0\ndm-tree                      0.1.8\nacme                         2.10.0\ndm-acme                      0.4.0\njax                          0.4.26\njaxlib                       0.4.26+cuda12.cudnn89\npython -V                    Python 3.9.5\nerror details:\nFile \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in from acme.agents.jax import dqn File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/init.py\", line 18, in from acme.agents.jax.dqn.actor import behavior_policy File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in from acme.agents.jax import actor_core as actor_core_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in from acme.jax import networks as networks_lib File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/init.py\", line 18, in from acme.jax.networks.atari import AtariTorso File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/atari.py\", line 29, in from acme.jax.networks import base File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/base.py\", line 24, in import haiku as hk File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/init.py\", line 20, in from haiku import experimental File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/experimental/init.py\", line 34, in from haiku._src.dot import abstract_to_dot File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/haiku/_src/dot.py\", line 163, in @jax.linear_util.transformation File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 54, in getattr raise AttributeError(f\"module {module!r} has no attribute {name!r}\") AttributeError: module 'jax' has no attribute 'linear_util'\nseems it raised from haiku and JAX, how this can be fixed? any quick thoughts?\nupdated attempt\nbased on @jakevdp suggestion, I reinstalled jax, jaxlib, but now I am getting this error again:\nTraceback (most recent call last):\n  File \"/data/acme/examples/baselines/rl_discrete/run_dqn.py\", line 18, in <module>\n    from acme.agents.jax import dqn\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/__init__.py\", line 18, in <module>\n    from acme.agents.jax.dqn.actor import behavior_policy\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/dqn/actor.py\", line 20, in <module>\n    from acme.agents.jax import actor_core as actor_core_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/agents/jax/actor_core.py\", line 22, in <module>\n    from acme.jax import networks as networks_lib\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/__init__.py\", line 45, in <module>\n    from acme.jax.networks.multiplexers import CriticMultiplexer\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/networks/multiplexers.py\", line 20, in <module>\n    from acme.jax import utils\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/acme/jax/utils.py\", line 190, in <module>\n    devices: Optional[Sequence[jax.xla.Device]] = None,\n  File \"/data/acme/acme_venv_new/lib/python3.9/site-packages/jax/_src/deprecations.py\", line 53, in getattr\n    raise AttributeError(f\"module {module!r} has no attribute {name!r}\")\nAttributeError: module 'jax' has no attribute 'xla'\nhere is my pip freeze list on this public gist: acme pip list\nI looked into this github issue: jax xla attribute issue\n@jakevdp, any updated comment or possible workaround for this jax.xla issue? thanks",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt sounds like you have too new a JAX version for the framework code you are using. I'd try installing an older version; e.g.\npip install --upgrade \"jax[cuda12_pip]<0.4.24\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSee JAX installation for more installation options.\nIf you're hoping to update the framework code for compatibility with more recent JAX versions, you might find replacements for previous functionality in jax.extend.linear_util."
        ],
        "link": "https://stackoverflow.com/questions/78372618/acme-error-attributeerror-module-jax-has-no-attribute-linear-util"
    },
    {
        "title": "How can we cast a `ctypes.POINTER(ctypes.c_float)` to `int`? [duplicate]",
        "question": "This question already has answers here:\nGet the memory address pointed to by a ctypes pointer (2 answers)\nClosed last year.\nI think this is a simple task, but I could not find a solution on the web to this. I have a external C++ library, which I'm using in my Python code, returning a ctypes.POINTER(ctypes.c_float) to me. I want to pass an array of these pointers to a jax.vmap function. The problem is that jax does not accept the ctypes.POINTER(ctypes.c_float) type. So, can I somehow cast this pointer to an ordinary int. Technically, this is clearly possible. But how do I do this in Python?\nHere is an example:\nlib = ctypes.cdll.LoadLibrary(lib_path)\nlib.foo.argtypes = None\nlib.foo.restype = ctypes.POINTER(ctypes.c_float)\n\nbar = jax.vmap(lambda : dummy lib.foo())(jax.numpy.empty(16))\n\nx = jax.numpy.empty(16, 256, 256, 1)\ny = jax.vmap(lib.bar, in_axes = (0, 1))(x, bar)\nSo, I want to invoke lib.foo 16-times so that I have an array bar containing all the pointers. Then I want to invoke another library function lib.bar which expects bar together with another (batched) parameter x.\nThe problem is that jax claims that ctypes.POINTER(ctypes.c_float) is not a valid jax type. This is why I think the solution is to cast the pointers to ints and store those ints in bar instead.",
        "answers": [
            "Listing:\n[SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer) - a common pitfall when working with CTypes (calling functions)\n[Python.Docs]: ctypes - A foreign function library for Python\nHere's a piece of code exemplifying how to handle pointers and their addresses. The trick is to use ctypes.addressof (documented in the 2nd URL).\ncode00.py:\n#!/usr/bin/env python\n\nimport ctypes as cts\nimport sys\n\n\nCType = cts.c_float\nCTypePtr = cts.POINTER(CType)\n\n\ndef ctype_pointer(seq):  # Helper\n    CTypeArr = (CType * len(seq))\n    ctype_arr = CTypeArr(*seq)\n    return cts.cast(ctype_arr, CTypePtr)\n\n\ndef pointer_elements(addr, count):  # Helper\n    return tuple(CType.from_address(addr + i * cts.sizeof(CType)).value for i in range(count))\n\n\ndef main(*argv):\n    seq = (2.718182, -3.141593, 1.618034, -0.618034, 0)\n    ptr = ctype_pointer(seq)\n    print(f\"Pointer: {ptr}\")\n    print(f\"\\nPointer elements: {tuple(ptr[i] for i in range(len(seq)))}\")  # Check if pointer has correct data\n    ptr_addr = cts.addressof(ptr.contents)  # @TODO - cfati: Straightforward\n    print(f\"\\nAddress: {ptr_addr} (0x{ptr_addr:016X})\\nElements from address: {pointer_elements(ptr_addr, len(seq))}\")\n    ptr_addr0 = cts.cast(ptr, cts.c_void_p).value  # @TODO - cfati: Alternative\n    print(f\"\\nAddresses match: {ptr_addr == ptr_addr0}\")\n\n\nif __name__ == \"__main__\":\n    print(\n        \"Python {:s} {:03d}bit on {:s}\\n\".format(\n            \" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n            64 if sys.maxsize > 0x100000000 else 32,\n            sys.platform,\n        )\n    )\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\nNotes:\nAlthough it adds a bit of complexity, I introduced the CType \"layer\" to show that it should work with any type, not just float (as long as the values in the sequence are of that type)\nThe only truly relevant lines are those marked with @TODO\nOutput:\n(py_pc064_03.08_test0_lancer) [cfati@cfati-5510-0:/mnt/e/Work/Dev/StackExchange/StackOverflow/q078366208]> python ./code00.py \nPython 3.8.19 (default, Apr  6 2024, 17:58:10) [GCC 11.4.0] 064bit on linux\n\nPointer: <__main__.LP_c_float object at 0x7203e97e7d40>\n\nPointer elements: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddress: 125361127594576 (0x00007203E97A9A50)\nElements from address: (2.71818208694458, -3.1415929794311523, 1.6180340051651, -0.6180340051651001, 0.0)\n\nAddresses match: True\n\nDone."
        ],
        "link": "https://stackoverflow.com/questions/78366208/how-can-we-cast-a-ctypes-pointerctypes-c-float-to-int"
    },
    {
        "title": "Simultaneously going over different kinds of data with Keras training",
        "question": "In a regression task I'm given the following data:\nInput vectors with a known label. MSE loss should be used between the precidtion and the label.\nPairs of input vectors without a label, for which it is known that the model should give similar results. MSE loss should be used between the two predictions.\nWhat's the right way to fit a Keras model with these two kinds of data simultaneously?\nIdeally, I'd like the train loop to iterate the two kinds in an interleaved way - a superivsed (1) batch and then a self-supervised (2) batch, then supervised again etc.\nIf it matters, I'm using the Jax backend. Keras version 3.2.1.",
        "answers": [
            "I eventually found a trick that solved it for my case without too many customizations.\nBut if you do need to pass different kinds of data for training, I don't think there's an easy answer as for today.\nIt should be possible though to write your own training loop, and use any structure that you want for the data and labels. In this case you might also want to use the trainer pattern, implementing a custom version of keras.src.backend.jax.trainer.JAXTrainer."
        ],
        "link": "https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training"
    },
    {
        "title": "Flax neural network with nans in the outputs",
        "question": "I am training a neural network using Flax. My training data has a significant number of nans in the outputs. I want to ignore these and only use the non-nan values for training. To achieve this, I have tried to use jnp.nanmean to compute the losses, i.e.:\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    return jnp.nanmean((pred - targets) ** 2)\n\ndef train_step(state, inputs, targets):\n    loss, grads = jax.value_and_grad(nanloss)(state.params, inputs, targets)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\nHowever, after one training step the loss is nan.\nIs what I am trying to achieve possible? If so, how can I fix this?",
        "answers": [
            "I suspect you are hitting the issue discussed here: JAX FAQ: gradients contain NaN where using where. You've handled the NaNs in the computation itself, but they're still sneaking into the gradient due to how autodiff is implemented.\nIf this is in fact the issue, you can fix this by filtering the values before computing the loss; for example like this:\ndef nanloss(params, inputs, targets):\n    pred = model.apply(params, inputs)\n    mask = jnp.isnan(pred) | jnp.isnan(targets)\n    pred = jnp.where(mask, 0, pred)\n    targets = jnp.where(mask, 0, targets)\n    return jnp.mean((pred - targets) ** 2, where=~mask)"
        ],
        "link": "https://stackoverflow.com/questions/78332120/flax-neural-network-with-nans-in-the-outputs"
    },
    {
        "title": "Jax ValueError: Incompatible shapes for broadcasting: shapes",
        "question": "I'm trying to write a weighted cross-entropy loss to train my model with Jax. However, I think there are some issues with my input dimension. Here are my codes:\nimport jax.numpy as np\nfrom functools import partial\nimport jax\n\n@partial(np.vectorize, signature=\"(c),(),()->()\")\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[0])\n    return -np.sum(weights* logits*one_hot_label)\n\nlogits=np.array([[1,2,3,4,5,6,7],[2,3,4,5,6,7,8]])\nlabels=np.array([1,2])\nweights=np.array([1,2,3,4,5,6,7])\nprint(weighted_cross_entropy_loss(logits,label,weights))\nHere are my error messages:\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 147, in broadcast_shapes\n    return _broadcast_shapes_cached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 284, in wrapper\n    return cached(config._trace_context(), *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/util.py\", line 277, in cached\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 153, in _broadcast_shapes_cached\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/PATH/test.py\", line 15, in <module>\n    print(weighted_cross_entropy_loss(a,label,weights))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 274, in wrapped\n    broadcast_shape, dim_sizes = _parse_input_dimensions(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/numpy/vectorize.py\", line 123, in _parse_input_dimensions\n    broadcast_shape = lax.broadcast_shapes(*shapes)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 149, in broadcast_shapes\n    return _broadcast_shapes_uncached(*shapes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/jax/_src/lax/lax.py\", line 169, in _broadcast_shapes_uncached\n    raise ValueError(f\"Incompatible shapes for broadcasting: shapes={list(shapes)}\")\nValueError: Incompatible shapes for broadcasting: shapes=[(2,), (2,), (7,)]\nI'm expecting a single number that represents the cross-entropy loss between logits and labels.\nI'm fairly new to this, can somebody tell me what is going on? Any help is appreciated.",
        "answers": [
            "label is length 2, and weights is length 7, which means they cannot be broadcast together.\nIt's not clear to me from your question what your expected outcome was, but you can read more about how broadcasting works in NumPy (and in JAX, which implements NumPy's semantics) at https://numpy.org/doc/stable/user/basics.broadcasting.html.\nEdit: it looks like this is the operation you were aiming for:\ndef weighted_cross_entropy_loss(logits, label, weights):\n    one_hot_label = jax.nn.one_hot(label, num_classes=logits.shape[1])\n    return -np.sum(weights * logits * one_hot_label)\nSince you want a single scalar output, I don't think vectorize is the right mechanism to use here."
        ],
        "link": "https://stackoverflow.com/questions/78323919/jax-valueerror-incompatible-shapes-for-broadcasting-shapes"
    },
    {
        "title": "Stable diffusion: AttributeError: module 'jax.random' has no attribute 'KeyArray'",
        "question": "When I run the stable diffusion on colab https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\nwith no modification, it fails on the line\nfrom diffusers import StableDiffusionPipeline \nThe error log is\nAttributeError: module 'jax.random' has no attribute 'KeyArray'\nHow can I fix this or any clue ?\nThe import should work, the ipynb should run with no error.",
        "answers": [
            "jax.random.KeyArray was deprecated in JAX v0.4.16 and removed in JAX v0.4.24. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier.\nYou can install JAX v0.4.23 with GPU support like this:\npip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nor, if you prefer targeting a local CUDA installation, like this:\npip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nFor more information on GPU installation, see JAX Installation: NVIDIA GPU.\nFrom the colab tutorial, update the second segment into:\n!pip install \"jax[cuda12_local]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n!pip install diffusers==0.11.1\n!pip install transformers scipy ftfy accelerate",
            "# Change this\n# !pip install diffusers==0.11.1\n\n# To just\n!pip install diffusers \nIf you've already run pip install in your Colab runtime, you'll need to either disconnect and open a new runtime (my recommendation) or use  --upgrade.\nDiffusers v0.11.1 is now over 18 months old, and the notebook works with current v0.29.0 without any other changes. Instead of using an old version of diffusers, requiring an old version of jax, we can use the latest versions.",
            "In the end, we need to downgrade the jax, Try each from the lateset to ealier, and luckily it works for\njax==0.4.23 jaxlib==0.4.23"
        ],
        "link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray"
    },
    {
        "title": "AttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'",
        "question": "I'm trying to run a model written in jax, https://github.com/lindermanlab/S5. However, I ran into some error that says\n   Traceback (most recent call last):\n  File \"/Path/run_train.py\", line 101, in <module>\n    train(parser.parse_args())\n  File \"/Path/train.py\", line 144, in train\n    state = create_train_state(model_cls,\n  File \"/Path/train_helpers.py\", line 135, in create_train_state\n    params = variables[\"params\"].unfreeze()\nAttributeError: 'dict' object has no attribute 'unfreeze'\nI tried to replicate this error by\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax import linen as nn\n\nmodel = nn.Dense(features=3)\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1, 2)))\nparams_unfrozen = flax.traverse_util.unfreeze(params)\nAnd the error reads:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: module 'flax.traverse_util' has no attribute 'unfreeze'\nI'm using:\nflax 0.7.4\njax 0.4.13\njaxlib 0.4.13+cuda12.cudnn89\nI think this is an issue relating to the version of flax, but does anyone know what exactly is going on? Any help is appreciated. Let me know if you need any further information",
        "answers": [
            "unfreeze is a method of Flax's FrozenDict class: (See FrozenDict.unfreeze). It appears that you have passed a Python dict where a FrozenDict is expected.\nTo fix this, you should ensure that variables['params'] is a FrozenDict, not a dict.\nRegarding the error in your attempted replication: flax.traverse_util does not define an unfreeze function, but this seems unrelated to the original problem."
        ],
        "link": "https://stackoverflow.com/questions/78256559/attributeerror-module-flax-traverse-util-has-no-attribute-unfreeze"
    },
    {
        "title": "jax: How do we solve the error: pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0?",
        "question": "I'm trying to run this simple introduction to score-based generative modeling. The code is using flax.optim, which seems to be moved to optax meanwhile (https://flax.readthedocs.io/en/latest/guides/converting_and_upgrading/optax_update_guide.html).\nI've made a copy of the colab code with the changes I think needed to be made (I'm only unsure how I need to replace optimizer = flax.jax_utils.replicate(optimizer)).\nNow, in the training section, I get the error\npmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\nat the line loss, params, opt_state = train_step_fn(step_rng, x, params, opt_state). This obviously comes from the return jax.pmap(step_fn, axis_name='device') in the \"Define the loss function\" section.\nHow can I fix this error? I've googled it, but have no idea what's going wrong here.",
        "answers": [
            "This happens because you are passing a scalar argument to a pmapped function. For example:\nimport jax\nfunc = lambda x: x ** 2\npfunc = jax.pmap(func)\n\npfunc(1.0)\n# ValueError: pmap was requested to map its argument along axis 0, which implies\n# that its rank should be at least 1, but is only 0 (its shape is ())\nIf you want to operate on a scalar, you should use the function without wrapping it in pmap:\nfunc(1.0)\n# 1.0\nAlternatively, if you want to use pmap, you should operate on an array whose leading dimension matches the number of devices:\nnum_devices = len(jax.devices())\nx = jax.numpy.arange(num_devices)\npfunc(x)\n# Array([ 0,  1,  4,  9, 16, 25, 36, 49], dtype=int32)"
        ],
        "link": "https://stackoverflow.com/questions/78244620/jax-how-do-we-solve-the-error-pmap-was-requested-to-map-its-argument-along-axi"
    },
    {
        "title": "what are the numbers in the operation names when profiling an application",
        "question": "What are the numbers in \"fusion_2\", \"fusion_4\"? Where do they come from? Thank you!",
        "answers": [
            "These numbers exist to de-duplicate the names of generated HLO operations. The first fusion operation created by the compiler is called fusion, the next is fusion_2, then fusion_3, and so on.\nNote that the order of creation does not necessarily match the order of execution."
        ],
        "link": "https://stackoverflow.com/questions/78236312/what-are-the-numbers-in-the-operation-names-when-profiling-an-application"
    },
    {
        "title": "Cannot import name 'linear_util' from 'jax'",
        "question": "I'm trying to reproduce the experiments of the S5 model, https://github.com/lindermanlab/S5, but I encountered some issues when solving the environment. When I'm running the shell script./run_lra_cifar.sh, I get the following error\nTraceback (most recent call last):\n  File \"/Path/S5/run_train.py\", line 3, in <module>\n    from s5.train import train\n  File \"/Path/S5/s5/train.py\", line 7, in <module>\n    from .train_helpers import create_train_state, reduce_lr_on_plateau,\\\n  File \"/Path/train_helpers.py\", line 6, in <module>\n    from flax.training import train_state\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py\", line 19, in <module>\n    from . import core\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py\", line 15, in <module>\n    from .axes_scan import broadcast\n  File \"/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py\", line 22, in <module>\n    from jax import linear_util as lu\nImportError: cannot import name 'linear_util' from 'jax' (/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py)\nI'm running this on an RTX4090 and my CUDA version is 11.8. My jax version is 0.4.25 and jaxlib version is 0.4.25+cuda11.cudnn86\nI first tried to install the dependencies using the author's\npip install -r requirements_gpu.txt\nHowever, this doesn't seem to work in my case since I can't evenimport jax. So I installed jax according to the instructions on https://jax.readthedocs.io/en/latest/installation.html by typing\npip install --upgrade pip\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nSo far I've tried:\nUsing a older GPU(3060 and 2070)\nDowngrading python to 3.9\nDoes anyone know what could be wrong? Any help is appreciated",
        "answers": [
            "jax.linear_util was deprecated in JAX v0.4.16 and removed in JAX v0.4.24.\nIt appears that flax is the source of the linear_util import, meaning that you are using an older flax version with a newer jax version.\nTo fix your issue, you'll either need to install an older version of JAX which still has jax.linear_util, or update to a newer version of flax which is compatible with more recent JAX versions."
        ],
        "link": "https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax"
    },
    {
        "title": "Numpyro AR(1) mean switching model sampling incongrouencies",
        "question": "I'm trying to estimate an AR(1) process y with a switching mean according to a latent state S =0,1 that evolves as a markov process with fixed transition probabilities (as in here). In short, it takes the form:\ny_t - mu_{0/1} = phi * (y_{t-1} - mu_{0/1})+ epsilon_t\nwhere mu_0 would be used if state_t = 0 and mu_1 if state_t =1. I'm using jax/numpyro with DiscreteHMCGibbs (although normal NUTS with latent state enumeration yields the same result) but I can't seem to have the sampler work properly. From all diagnostics I run, it seems that all hyperparameters are stuck at initialization value, and summary returns accordingly with all std==0. Here below I have a MWE that reproduces my problem. Is there an obvious mistake I am making in the implementation?\nMWE:\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.contrib.control_flow import scan\nfrom numpyro.infer import MCMC, NUTS,DiscreteHMCGibbs\nfrom jax import random, pure_callback\nimport jax\nimport numpy as np\n\ndef generate_synthetic_data(T=100, mu=[0, 5], phi=0.5, sigma=1.0, p=np.array([[0.95, 0.05], [0.1, 0.9]])):\n    states = np.zeros(T, dtype=np.int32)\n    y = np.zeros(T)\n    current_state = np.random.choice([0, 1], p=[0.5, 0.5])\n    states[0] = current_state\n    y[0] = np.random.normal(mu[current_state], sigma)\n\n    for t in range(1, T):\n        current_state = np.random.choice([0, 1], p=p[current_state,:])\n        states[t] = current_state\n        y[t] = np.random.normal(mu[current_state] + phi * (y[t-1] - mu[current_state]), sigma)\n\n    return y, states\n\n\ndef mean_switching_AR1_model(y):\n    T = len(y)\n    phi = numpyro.sample('phi', dist.Normal(0, 1))\n    sigma = numpyro.sample('sigma', dist.Exponential(1))\n    \n    \n    with numpyro.plate('state_plate', 2):\n        mu = numpyro.sample('mu', dist.Normal(0, 5))\n        p = numpyro.sample('p', dist.Dirichlet(jnp.ones(2)))\n\n    probs_init = numpyro.sample('probs_init', dist.Dirichlet(jnp.ones(2)))\n    s_0 = numpyro.sample('s_0', dist.Categorical(probs_init))\n\n    def transition_fn(carry, y_t):\n        prev_state = carry\n        state_probs = p[prev_state]\n        state = numpyro.sample('state', dist.Categorical(state_probs))\n\n        mu_state = mu[state]\n        y_mean = mu_state + phi * (y_t - mu_state)\n        y_next = numpyro.sample('y_next', dist.Normal(y_mean, sigma), obs=y_t)\n        return state, (state, y_next)\n\n    _ , (signal, y)=scan(transition_fn, s_0, y[:-1], length=T-1)\n    return (signal, y)\n\n# Synthetic data generation\nT = 1000\nmu_true = [0, 3]\nphi_true = 0.5\nsigma_true = 0.25\ntransition_matrix_true = np.array([[0.95, 0.05], [0.1, 0.9]])\ny, states_true = generate_synthetic_data(T, mu=mu_true, phi=phi_true, sigma=sigma_true, p=transition_matrix_true)\n\n\nrng_key = random.PRNGKey(0)\nnuts_kernel = NUTS(mean_switching_AR1_model)\ngibbs_kernel = DiscreteHMCGibbs(nuts_kernel, modified=True)\n\n# Run MCMC\nmcmc = MCMC(gibbs_kernel, num_samples=1000, num_warmup=1000)\nmcmc.run(rng_key, y=y)\nmcmc.print_summary()",
        "answers": [
            "So it turns out there was indeed a pretty obvious mistake in the sense that I was not correctly carrying down y_{t-1} as part of the state variables. The following corrected transition functions yield the intended result without problems.\ndef transition_fn(carry, y_curr):\n    prev_state, y_prev = carry\n    state_probs = p[prev_state]\n    state = numpyro.sample('state', dist.Categorical(state_probs))\n\n    mu_state = mu[state]\n    y_mean = mu_state + phi * (y_prev - mu_state)\n    y_curr = numpyro.sample('y_curr', dist.Normal(y_mean, sigma), obs=y_curr)\n    return (state, y_curr), (state, y_curr)\n\n_, (signal, y) = scan(transition_fn, (s_0, y[0]), y[1:], length=T-1)"
        ],
        "link": "https://stackoverflow.com/questions/78209454/numpyro-ar1-mean-switching-model-sampling-incongrouencies"
    },
    {
        "title": "Slow JAX Optimization with ScipyBoundedMinimize and Optax - Seeking Speedup Strategies",
        "question": "I'm working on optimizing a model in jax that involves fitting a large observational dataset (4800 data points) with a complex model containing interpolation. The current optimization process using jaxopt.ScipyBoundedMinimize takes around 30 seconds for 100 iterations, with most of the time spent seemingly during or before the first iteration starts. You can find the relevant code snippet below. you can find the necessary data for the relevant code at the following link.\nnecessary data (idc, sg and cpcs)\nimport jax.numpy as jnp\nimport time as ela_time\nfrom jaxopt import ScipyBoundedMinimize\nimport optax\nimport jax\nimport pickle\n\n\nfile1 = open('idc.pkl', 'rb')\nidc = pickle.load(file1)\nfile1.close()\n\nfile2 = open('sg.pkl', 'rb')\nsg = pickle.load(file2)\nfile2.close()\n\nfile3 = open('cpcs.pkl', 'rb')\ncpcs = pickle.load(file3)\nfile3.close()\n\n\ndef model(fssc, fssh, time, rv, amp):\n\n    fssp = 1.0 - (fssc + fssh)\n\n    ivis = cpcs['common'][time]['ivis']\n    areas = cpcs['common'][time]['areas']\n    mus = cpcs['common'][time]['mus']\n\n    vels = idc['vels'].copy()\n\n    ldfs_phot = cpcs['line'][time]['ldfs_phot']\n    ldfs_cool = cpcs['line'][time]['ldfs_cool']\n    ldfs_hot = cpcs['line'][time]['ldfs_hot']\n\n    lps_phot = cpcs['line'][time]['lps_phot']\n    lps_cool = cpcs['line'][time]['lps_cool']\n    lps_hot = cpcs['line'][time]['lps_hot']\n\n    lis_phot = cpcs['line'][time]['lis_phot']\n    lis_cool = cpcs['line'][time]['lis_cool']\n    lis_hot = cpcs['line'][time]['lis_hot']\n\n    coeffs_phot = lis_phot * ldfs_phot * areas * mus\n    wgt_phot = coeffs_phot * fssp[ivis]\n    wgtn_phot = jnp.sum(wgt_phot)\n\n    coeffs_cool = lis_cool * ldfs_cool * areas * mus\n    wgt_cool = coeffs_cool * fssc[ivis]\n    wgtn_cool = jnp.sum(wgt_cool)\n\n    coeffs_hot = lis_hot * ldfs_hot * areas * mus\n    wgt_hot = coeffs_hot * fssh[ivis]\n    wgtn_hot = jnp.sum(wgt_hot)\n\n    prf = jnp.sum(wgt_phot[:, None] * lps_phot + wgt_cool[:, None] * lps_cool + wgt_hot[:, None] * lps_hot, axis=0)\n    prf /= wgtn_phot + wgtn_cool + wgtn_hot\n\n    prf = jnp.interp(vels, vels + rv, prf)\n\n    prf = prf + amp\n\n    avg = jnp.mean(prf)\n\n    prf = prf / avg\n\n    return prf\n\n\ndef loss(x0s, lmbd):\n\n    noes = sg['noes']\n\n    noo = len(idc['times'])\n\n    fssc = x0s[:noes]\n    fssh = x0s[noes: 2 * noes]\n    fssp = 1.0 - (fssc + fssh)\n    rv = x0s[2 * noes: 2 * noes + noo]\n    amp = x0s[2 * noes + noo: 2 * noes + 2 * noo]\n\n    chisq = 0\n    for i, itime in enumerate(idc['times']):\n        oprf = idc['data'][itime]['prf']\n        oprf_errs = idc['data'][itime]['errs']\n\n        nop = len(oprf)\n\n        sprf = model(fssc=fssc, fssh=fssh, time=itime, rv=rv[i], amp=amp[i])\n\n        chisq += jnp.sum(((oprf - sprf) / oprf_errs) ** 2) / (noo * nop)\n\n    wp = sg['grid_areas'] / jnp.max(sg['grid_areas'])\n\n    mem = jnp.sum(wp * (fssc * jnp.log(fssc / 1e-5) + fssh * jnp.log(fssh / 1e-5) +\n                    (1.0 - fssp) * jnp.log((1.0 - fssp) / (1.0 - 1e-5)))) / sg['noes']\n\n    ftot = chisq + lmbd * mem\n\n    return ftot\n\n\nif __name__ == '__main__':\n\n    # idc: a dictionary containing observational data (150 x 32)\n    # sg and cpcs: dictionaries with related coefficients\n\n    noes = sg['noes']\n    lmbd = 1.0\n    maxiter = 1000\n    tol = 1e-5\n\n    fss = jnp.ones(2 * noes) * 1e-5\n    x0s = jnp.hstack((fss, jnp.zeros(len(idc['times']) * 2)))\n\n    minx0s = [1e-5] * (2 * noes) + [-jnp.inf] * len(idc['times']) * 2\n    maxx0s = [1.0 - 1e-5] * (2 * noes) + [jnp.inf] * len(idc['times']) * 2\n\n    bounds = (minx0s, maxx0s)\n\n    start = ela_time.time()\n\n    optimizer = ScipyBoundedMinimize(fun=loss, maxiter=maxiter, tol=tol, method='L-BFGS-B',\n                                 options={'disp': True})\n    x0s, info = optimizer.run(x0s, bounds,  lmbd)\n\n    # optimizer = optax.adam(learning_rate=0.1)\n    # optimizer_state = optimizer.init(x0s)\n    #\n    # for i in range(1, maxiter + 1):\n    #\n    #     print('ITERATION -->', i)\n    #\n    #     gradients = jax.grad(loss)(x0s, lmbd)\n    #     updates, optimizer_state = optimizer.update(gradients, optimizer_state, x0s)\n    #     x0s = optax.apply_updates(x0s, updates)\n    #     x0s = jnp.clip(x0s, jnp.array(minx0s), jnp.array(maxx0s))\n    #     print('Objective function: {:.3E}'.format(loss(x0s, lmbd)))\n\n    end = ela_time.time()\n\n    print(end - start)   # total elapsed time: ~30 seconds\nHere's a breakdown of the relevant aspects:\nNumber of free parameters (x0s): 5263\nData: Observational data stored in idc dictionary (4800 data points)\nModel: Defined in model function, also utilizes interpolation\nOptimization methods tried:\njaxopt.ScipyBoundedMinimize with L-BFGS-B method (slow ~30 seconds, with most of the time spent during or just before the first iteration)\noptax.adam (too slow ~200 seconds)\nAttempted parallelization: I attempted to parallelize optax.adam, yet due to the inherent nature of the modeling, I couldn't succeed as the x0s couldn't be divided. (assuming I understood parallelization correctly)\nQuestions:\nWhat are potential reasons for the slowness before or during the first iteration in ScipyBoundedMinimize ?\nAre there alternative optimization algorithms in jax that might be faster for my scenario (large number of free parameters and data points, complex model with interpolation)?\nDid I misunderstand parallelization with optax.adam? Are there any strategies for potential parallelization in this case?\nAre there any code optimizations within the provided snippet that could improve performance (e.g., vectorization)?\nAdditional Information:\nHardware: Intel® Core™ i7-9750H CPU @ 2.60GHz × 12, 16 GiB RAM (laptop)\nSoftware: OS Ubuntu 22.04, Python 3.10.12, JAX 0.4.25, optax 0.2.1\nI'd appreciate any insights or suggestions to improve the optimization performance.",
        "answers": [
            "JAX code is Just-in-time (JIT) compiled, meaning that the long duration of the first step is likely related to compilation costs. The longer your code is, the more time it will take to compile.\nOne common issue leading to long compile times is the use of Python control flow such as for loops. JAX's tracing machinery essentially flattens out these loops (see JAX Sharp Bits: Control Flow). In your case, you loop over 4800 entries in your data structure, and thus are creating a very long and inefficient program.\nThe typical solution in a case like this is to rewrite your program using jax.vmap. Like most JAX constructs, this works best with a struct-of-arrays pattern rather than the array-of-structs pattern used in your data. So the first step to using vmap is to restructure your data in a way that JAX can use; it might look something like this:\nitimes = jnp.arange(len(idc['times']))\nprf = jnp.array([idc['data'][i]['prf'] for i in itimes])\nerrs = jnp.array([idc['data'][i]['errs'] for i in itimes])\n\nsprf = jax.vmap(model, in_axes=[None, None, 0, 0, 0])(fssc, fssh, itimes, rv, amp)\nchi2 = jnp.sum((oprf - sprf) / oprf_errs) ** 2) / len(times) / sprf.shape[1]\nThis will not work directly: you'll also have to restructure the data used by your model function into the struct-of-arrays style, but hopefully this gives you the general idea.\nNote also that this assumes that every entry of idc['data'][i]['prf'] and idc['data'][i]['errs'] has the same shape. If that's not the case, then I'm afraid your problem is not particularly well-suited to JAX's SPMD programming model, and there's not an easy way to work around the need for long compilations."
        ],
        "link": "https://stackoverflow.com/questions/78174997/slow-jax-optimization-with-scipyboundedminimize-and-optax-seeking-speedup-stra"
    },
    {
        "title": "Equivalent of `jax.lax.cond` for multiple boolean conditions",
        "question": "Currently jax.lax.cond works for one boolean condition. Is there a way to extend it to multiple boolean conditions?\nAs an example, below is an untraceable function:\ndef func(x):\n    if x < 0: return x\n    elif (x >= 0) & (x < 1): return 2*x\n    else: return 3*x\nHow to write this function in JAX in a traceable way?",
        "answers": [
            "One compact way to write something like this is using jnp.select:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef func(x):\n  return jnp.select([x < 0, x < 1], [x, 2 * x], default=3 * x)\n\nx = jnp.array([-0.5, 0.5, 1.5])\nprint(func(x))\n# [-0.5  1.   4.5]"
        ],
        "link": "https://stackoverflow.com/questions/78122820/equivalent-of-jax-lax-cond-for-multiple-boolean-conditions"
    },
    {
        "title": "\"The truth value of an array with more than one element is ambiguous\" when trying to train a new JAX+Equinox model a second time",
        "question": "TL;DR: I create a new instance of my equinox.Module model and fit it using Optax. Everything works fine. When I create a new instance of the same model and try to fit it from scratch, using the same code, same initial values, same everything, I get:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n...somewhere deep in Optax code. My code doesn't compare any arrays. The error message doesn't show where exactly the comparison happens. What's wrong?\nCode\n# 1. Import dependencies.\nimport jax; jax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as np, jax.random as rnd, equinox as eqx\nimport optax\n\n# 2. Define loss function. I'm fairly confident this is correct.\ndef npdf(x, var):\n    return np.exp(-0.5 * x**2 / var) / np.sqrt(2 * np.pi * var)\n\ndef mixpdf(x, ps, vars):\n    return ps.dot(npdf(x, vars))\n\ndef loss(model, series):\n    weights, condvars = model(series)\n    return -jax.vmap(\n        lambda x, vars: np.log(mixpdf(x, weights, vars))\n    )(series[1:], condvars[:-1]).mean()\n\n# 3. Define recurrent neural network.\nclass RNNCell(eqx.Module):\n    bias: np.ndarray\n    Wx: np.ndarray\n    Wh: np.ndarray\n    def __init__(self, ncomp: int, n_in: int=1, *, key: np.ndarray):\n        k1, k2, k3 = rnd.split(key, 3)\n        self.bias = rnd.uniform(k1, (ncomp, ))\n        self.Wx = rnd.uniform(k2, (ncomp, n_in))\n        self.Wh = 0.9 * rnd.uniform(k3, (ncomp, ))\n\n    def __call__(self, vars_prev, obs):\n        vars_new = self.bias + self.Wx @ obs + self.Wh * vars_prev\n        return vars_new, vars_new\n\nclass RNN(eqx.Module):\n    cell: RNNCell\n    logits: np.ndarray\n    vars0: np.ndarray = eqx.field(static=True)\n\n    def __init__(self, vars0: np.ndarray, n_in=1, *, key: np.ndarray):\n        self.vars0 = np.array(vars0)\n        K = len(self.vars0)\n        self.cell = RNNCell(K, n_in, key=key)\n        self.logits = np.zeros(K)\n\n    def __call__(self, series: np.ndarray):\n        _, hist = jax.lax.scan(self.cell.__call__, self.vars0, series**2)\n        return jax.nn.softmax(self.logits), abs(hist)\n\n    def condvar(self, series):\n        weights, variances = self(series)\n        return variances @ weights\n\n    def predict(self, series: np.ndarray):\n        return self.condvar(series).flatten()[-1]\n\n# 4. Training/fitting code.\ndef fit(model, logret, nepochs: int, optimizer, loss):\n    loss_and_grad = eqx.filter_value_and_grad(loss)\n    \n    @eqx.filter_jit\n    def make_step(model, opt_state):\n        loss_val, grads = loss_and_grad(model, logret)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss_val, model, opt_state\n\n    opt_state = optimizer.init(model)\n    for epoch in range(nepochs):\n        loss_val, model, opt_state = make_step(model, opt_state)\n    print(\"Works!\")\n    return model\n\ndef experiment():\n    series = rnd.normal(rnd.PRNGKey(8), (100, 1))\n    model = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\n    return fit(model, series, 100, optax.adam(0.01), loss)\n\n# 5. Run the exact same code twice.\nexperiment() # 1st call, works\nexperiment() # 2nd call, error\nError message\n> python my_RNN.py\nWorks!\nTraceback (most recent call last):\n  File \"/Users/forcebru/test/my_RNN.py\", line 75, in <module>\n    experiment() # 2nd call, error\n    ^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 72, in experiment\n    return fit(model, series, 100, optax.adam(0.01), loss)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 65, in fit\n    loss_val, model, opt_state = make_step(model, opt_state)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 206, in __call__\n    return self._call(False, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_module.py\", line 935, in __call__\n    return self.__func__(self.__self__, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 200, in _call\n    out = self._cached(dynamic_donate, dynamic_nodonate, static)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 495, in common_infer_params\n    jaxpr, consts, out_shardings, out_layouts_flat, attrs_tracked = _pjit_jaxpr(\n                                                                    ^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1150, in _pjit_jaxpr\n    jaxpr, final_consts, out_type, attrs_tracked = _create_pjit_jaxpr(\n                                                   ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 350, in memoized_fun\n    ans = call(fun, *args)\n          ^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 1089, in _create_pjit_jaxpr\n    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/profiler.py\", line 336, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2314, in trace_to_jaxpr_dynamic\n    jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/interpreters/partial_eval.py\", line 2336, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/equinox/_jit.py\", line 49, in fun_wrapped\n    out = fun(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/test/my_RNN.py\", line 59, in make_step\n    updates, opt_state = optimizer.update(grads, opt_state)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/combine.py\", line 59, in update_fn\n    updates, new_s = fn(updates, s, params, **extra_args)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/base.py\", line 337, in update\n    return tx.update(updates, state, params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optax/_src/transform.py\", line 369, in update_fn\n    mu_hat = bias_correction(mu, b1, count_inc)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 248, in cache_miss\n    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n                                                                ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 136, in _python_pjit_helper\n    infer_params_fn(*args, **kwargs)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api.py\", line 325, in infer_params\n    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/pjit.py\", line 491, in common_infer_params\n    canonicalized_in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 4, in __eq__\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 745, in __bool__\n    check_bool_conversion(self)\n  File \"/Users/forcebru/.pyenv/versions/3.12.1/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py\", line 662, in check_bool_conversion\n    raise ValueError(\"The truth value of an array with more than one element is \"\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nProblem\nThe error message says File \"<string>\", line 4, in __eq__, which doesn't help.\nIt refers to the line mu_hat = bias_correction(mu, b1, count_inc) in Optax code, but as far as I understand, it doesn't compare any arrays.\nIt also refers to JAX code that's supposedly responsible for JIT compilation, but this seems outside my control.\nIs there a bug in my model definition (RNNCell or RNN)? Did I implement the training loop wrong? I basically copied it straight from Equinox docs, so it should be fine. Why does it work when I call experiment() the first time, but not the second?",
        "answers": [
            "It appears this is a bug in equinox. The function _process_in_axis_resources is decorated in functools.lru_cache, meaning that all inputs are checked for equality with arguments from the previous call. On the second run, this triggers a call to equinox.Module.__eq__, which raises the error. You can see this problem by doing the equality check directly:\nmodel = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel2 = RNN([0.4, 0.6, 0.8], key=rnd.PRNGKey(8))\nmodel == model2\n# ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nI would suggest reporting this bug at https://github.com/patrick-kidger/equinox/issues\nYou could probably work around this issue by not storing a numpy array (vars0) as a static attribute. I suspect that equinox assumes that all static attributes are hashable, and numpy arrays are not.\nEdit: I just checked, and changing this:\nvars0: np.ndarray = eqx.field(static=True)\nto this:\nvars0: np.ndarray\nresolves the issue.\nEdit 2: Indeed it looks like static fields in equinox must be hashable, so this is not an equinox bug but rather a usage error (see the discussion at https://github.com/patrick-kidger/equinox/issues/154#issuecomment-1561735995). You might try storing vars0 as a tuple (which is hashable) rather than an array (which isn't)."
        ],
        "link": "https://stackoverflow.com/questions/78074623/the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous-when-tryin"
    },
    {
        "title": "Jax scan with dynamic number of iterations",
        "question": "I wanted to perform a scan with a dynamic number of iterations. To accomplish that, I want to recompile the function each time when iters_to_do changes.\nTo avoid a huge slowdown, I'll be using a recompilation_cache but that's beside the point.\nHowever, when I mark the argument in @partial(jax.jit) I'm still obtaining a concretization error:\n@partial(jax.jit, static_argnums=(3))\ndef iterate_for_steps(self,\n                        interim_thought: Array, \n                        mask: Array,\n                        iters_to_do: int, \n                        input_arr: Array, \n                        key: PRNGKeyArray) -> Array:\n\n    # These are constants\n    input_arr = input_arr.astype(jnp.bfloat16)\n    interim_thought = interim_thought.astype(jnp.bfloat16)\n    \n    def body_fun(i: int, thought: Array) -> Array:\n        latent = jnp.concatenate([thought, input_arr], axis=-1).astype(jnp.bfloat16)\n        latent = self.main_block(latent, input_arr, mask, key).astype(jnp.bfloat16)\n        latent = jax.vmap(self.post_ln)(latent).astype(jnp.bfloat16)  # LN to keep scales tidy\n\n        return latent\n    \n    iters_to_do = iters_to_do.astype(int).item()\n    final_val = jax.lax.scan(body_fun, interim_thought, xs=None, length=iters_to_do)\n    \n    return final_val\nFull traceback is here.\nI've tried marking multiple arguments with @partial but to no avail.\nI'm not sure how to approach debugging this - with a python debugger, I'm getting no help apart from the fact that its definitely a tracer.\nMRE\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\n\ninit = jnp.ones((5,))\niterations = jnp.array([1, 2, 3])\n\n@partial(jax.jit, static_argnums=(0,))\ndef iterate_for_steps(iters: int):\n    def body_fun(carry):\n        return carry * 2\n    \n    iters = iters.astype(int)\n    output = jax.lax.scan(body_fun, init, xs=None, length=iters)\n    \n    return output\n\nprint(jax.vmap(iterate_for_steps)(iterations))",
        "answers": [
            "First of all, the number of iterations in a scan must be static. If you want something similar to scan that allows a dynamic number of iterations, you can take a look at while_loop.\nRegarding your code: in isolation, your fix of marking iters_to_do as static using static_argnums is probably roughly the right idea, so long as you are passing a static int in this position when you call the function.\nBut the fact that you are calling the astype array method in your function (in iters_to_do.astype(int).item()) and getting a ConcretizationError rather than an AttributeError makes me think that the error you linked to is not coming from the code as pasted in your question.\nTo help address this discrepancy, I'd suggest trying to construct a minimal reproducible example of the problem you're having. Without that, any answer to your question is going to require too much guesswork regarding what code you're actually executing.",
            "One can use equinox's (internal as of right now) while_loop implementation which would also be able to handle a dynamic amount of iterations with checkpointing to reduce memory usage.\nNote that this can be used as a drop-in replacement to jax's native while_loop. One can also use equinox's eqx.internal.scan if they wish to leverage similar checkpointing with scan."
        ],
        "link": "https://stackoverflow.com/questions/78070050/jax-scan-with-dynamic-number-of-iterations"
    },
    {
        "title": "How to train a model using gradient descent with multioutput (vector-valued) loss function in JAX?",
        "question": "I am trying to train a model that has two outputs with gradient descent. My cost function therefore returns two errors. What is the typical way to deal with this problem?\nI've seen mentions here and there of this problem, but I haven't come up with a satisfactory solution.\nThis is a toy example that reproduces my problem:\nfrom jax import jit, random, grad\nimport optax\n\n\n@jit\ndef my_model(forz, params):\n    a, b = params\n\n    a_vect = a + forz**b\n    b_vect = b + forz**a\n\n    return a_vect, b_vect*50.\n\n\n@jit\ndef rmse(predictions, targets):\n\n    rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n    return rmse\n\n\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a, loss_b\n\n\ngrad_myloss = jit(grad(my_loss, argnums=1))\n\n# synthetic true data\nkey = random.PRNGKey(758493)\nforz = random.uniform(key, shape=(1000,))\n\ntrue_params = [8.9, 6.6]\ntrue_a, true_b = my_model(forz, true_params)\n\n# Train\nmodel_params = random.uniform(key, shape=(2,))\noptimizer = optax.adabelief(1e-1)\nopt_state = optimizer.init(model_params)\n\nfor i in range(1000):\n\n    grads = grad_myloss(forz, model_params, true_a, true_b)  # this fails\n    updates, opt_state = optimizer.update(grads, opt_state)\n    model_params = optax.apply_updates(model_params, updates)\nI understand that either the two errors has to be somehow aggregated to a single one implementing some kind of normalization to the losses (my output vectors have non-comparable units),\n@jit\ndef normalized_rmse(predictions, targets):\n   std_dev_targets = jnp.std(targets)\n   rmse = jnp.sqrt(jnp.mean((predictions - targets) ** 2))\n   return rmse/std_dev_targets\n\n\n@jit\ndef my_loss_single(forz, params, true_a, true_b):\n\n   sim_a, sim_b = my_model(forz, params)\n\n   loss_a = normalized_rmse(sim_a, true_a)\n   loss_b = normalized_rmse(sim_b, true_b)\n\n   return jnp.sqrt((loss_a ** 2) + (loss_b * 2)) \nor I should use the Jacobian matrix (jacrev) somehow?",
        "answers": [
            "optax, like most optimization frameworks, is only able to optimize a single-valued loss function. You should decide what single-valued loss makes sense for your particular problem. A good option given the RMS form of your individual losses might be the square sum:\n@jit\ndef my_loss(forz, params, true_a, true_b):\n\n    sim_a, sim_b = my_model(forz, params)\n\n    loss_a = rmse(sim_a, true_a)\n    loss_b = rmse(sim_b, true_b)\n\n    return loss_a ** 2 + loss_b ** 2\nWith this change, your code executes without an error."
        ],
        "link": "https://stackoverflow.com/questions/78044014/how-to-train-a-model-using-gradient-descent-with-multioutput-vector-valued-los"
    },
    {
        "title": "Using Orbax to checkpoint flax `TrainState` with new `CheckpointManager` API",
        "question": "Context\nThe Flax docs describe how to checkpoint a flax.training.train_state.TrainState with orbax. In a nutshell, you set up a orbax.checkpoint.CheckpointManager which keeps track of checkpoints. Next, you use the CheckpointManager to save the state to disk. Summarising the code snippets from the Flax docs:\nimport orbax\n\n# <-- Code building an empty and a full chkpt. -->.\nabstract_chkpt = ...\nchkpt = ...\n\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\nsave_args = orbax_utils.save_args_from_target(ckpt)\n\noptions = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_manager = orbax.checkpoint.CheckpointManager(\n    '/tmp/flax_ckpt/orbax/managed', orbax_checkpointer, options)\n\n# Save and restore a checkpoint.\ncheckpoint_manager.save(step, ckpt, save_kwargs={'save_args': save_args})\ncheckpoint_manager.restore(1, items=abstract_ckpt)\nThe notebook provided by the Flax docs does what I want: periodically track TrainState, which can then be restored. However, when executing the code provided by the Flax docs warn that this orbax checkpoint API is deprecated:\nWARNING:absl:Configured CheckpointManager using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\nThe link indicated by the error message gives some pointers how to use the new orbax.checkpoint.CheckpointManager.\nQuestion\nHow do I save and restore a Flax TrainState with the new orbax.checkpoint.CheckpointManager API?\nHere is my failed attempt (based on the Orbax migration instructions) at saving and restoring a trivial flax.training.train_state.TrainState:\nimport orbax.checkpoint as obc\nfrom flax.training.train_state import TrainState\n\nabstract_ckpt = TrainState(step=0, apply_fn=lambda _: None, params={}, tx={}, opt_state={})\nckpt = abstract_ckpt.replace(step=1)\n\n# Set up the checkpointer.\noptions = obc.CheckpointManagerOptions(max_to_keep=2, create=True)\ncheckpoint_dir = obc.test_utils.create_empty('/tmp/checkpoint_manager')\ncheckpoint_manager = obc.CheckpointManager(checkpoint_dir, options=options)\nsave_args = obc.args.StandardSave(abstract_ckpt)\n\n# Do actual checkpointing.\ncheckpoint_manager.save(1, ckpt, args=save_args)\n\n# Restore checkpoint.\nrestore_args = obc.args.StandardRestore(abstract_ckpt)\nrestored_ckpt = checkpoint_manager.restore(1, args=restore_args)\n\n# Verify if it is correctly restored.\nassert ckpt.step == restored_ckpt.step  # AssertionError\nMy guess would be that the problem relates to save_args, but I haven't managed to pinpoint the problem and figure out a fix. Any suggestions how to correctly restore the checkpoint using the new CheckpointManager API?",
        "answers": [
            "You created save_args = ocp.args.StandardSave(abstract_ckpt) instead of save_args = ocp.args.StandardSave(ckpt), so you're just saving the wrong thing.\nAlso note that checkpoint_dir = ocp.test_utils.create_empty('/tmp/checkpoint_manager') is a bit unnecessary - it's just a test utility for deleting a directory if it already exists - makes running our colabs a bit easier. Probably you shouldn't need to use it in real life, as the create option in CheckpointManager will create the directory for you."
        ],
        "link": "https://stackoverflow.com/questions/78033458/using-orbax-to-checkpoint-flax-trainstate-with-new-checkpointmanager-api"
    },
    {
        "title": "JAX jax.grad on simple function that takes an array: `ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected`",
        "question": "I'm trying to implement this function and use JAX to automatically build the gradient function:\n$f(x) = \\sum\\limits_{k=1}^{n-1} [100 (x_{k+1} - x_k^2)^2 + (1 - x_k)^2]$\n(sorry, I don't know how to format math on stackoverflow. Some sister sites allow TeX, but apparently this site does not?)\nimport jax\nimport jax.numpy as jnp\n\n# x is an array, which does not handle type hints well.\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].item()\n        xip1 = x[i].item()\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\n\n\n# with n=2.\ndef rosenbrock2(x: any) -> float:\n    return rosenbrock(2, x)\n\n\ngrad_rosenbrock2 = jax.grad(rosenbrock2)\n\nx = jnp.array([-1.2, 1], dtype=jnp.float32).reshape(2,1)\n\n# this line fails with the error given below\ngrad_rosenbrock2(x)\nThis last line results in:\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[1].\nThe problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\nI'm trying to follow the docs, and I'm confused. This is my first time using JAX or Autograd, can someone help me resolve this? Thanks!",
        "answers": [
            "The problem is that the .item() method attempts to convert an array to a static Python scalar, and since you have traced arrays within your grad transformation, conversion to a static value is not possible.\nWhat you need here is to convert a size-1 array to a scalar array, which you can do using .reshape(()):\ndef rosenbrock(n: int, x: any) -> float:\n    f = 0\n    # i is 1-indexed to match document.\n    for i in range(1, n):\n        # adjust 1-based indices to 0-based python indices.\n        xi = x[i-1].reshape(())\n        xip1 = x[i].reshape(())\n\n        fi = 100 * (xip1 - xi**2)**2 + (1 - xi)**2\n        f = f + fi\n    return f\nFor more background on jax transformations and traced arrays, I'd recommend How to think in JAX."
        ],
        "link": "https://stackoverflow.com/questions/78030853/jax-jax-grad-on-simple-function-that-takes-an-array-concretizationtypeerror-a"
    },
    {
        "title": "How to return last index with jnp.where in jit function",
        "question": "Say I have two arrays:\nz = jnp.array([[5.55751118],\n              [5.18212974],\n              [4.35981727],\n              [3.4559711 ],\n              [3.35750248],\n              [2.65199945],\n              [2.02298999],\n              [1.59444971],\n              [0.80865185],\n              [0.77579791]])\n\nz1 = jnp.array([[ 1.58559484],\n               [ 3.79094097],\n               [-0.52712522],\n               [-1.0178286 ],\n               [-3.51076985],\n               [ 1.30108161],\n               [-1.29824303],\n               [-0.19209007],\n               [ 0.37451138],\n               [-2.33619987]])\nI would like to start at the first row in array z and find where in the second matrix a second value is within a threshold of this value.\nexample without @jit: I would like to return the last index of array z1. Value should be -3.51x\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nWhen using JIT (as needed in a larger scale model)\ninit = z[0]\ndistance = 2.6\nnew = init - distance \n\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx)[0][-1]]\n    return val\ntest()\nthis error is produced:\nConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThe error occurred while tracing the function test at /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:9 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:bool[10,1] = ge b c\n    from line /var/folders/ss/pfgdfm2x7_s4cyw2v0b_t7q80000gn/T/ipykernel_85273/75296347.py:11:10 (test)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
        "answers": [
            "The problem is that jnp.where returns a dynamically-sized array, and JAX transformations like jit are not compatible with dynamically-sized arrays (See JAX Sharp Bits: Dynamic Shapes). You can pass a size argument to jnp.where to make the result statically sized. Since we don't know how many elements will be returned, we can choose the maximum possible number of returned elements, which is idx.shape[0]. Since the result will be padded with zeros, the maximum index will give what you're looking for:\n@jit\ndef test():\n    idx = z>=new\n    val = z1[jnp.where(idx, size=idx.shape[0])[0].max()]\n    return val\ntest()"
        ],
        "link": "https://stackoverflow.com/questions/78030406/how-to-return-last-index-with-jnp-where-in-jit-function"
    },
    {
        "title": "Can jax.vmap() do a hstack()?",
        "question": "As the title says, I currently manually hstack() the first axis of a 3D array returned by jax.vmap(). In my code, the copy operation in hstack() is a currently a speed bottleneck. Can I avoid this by instructing jax.vmap() to do this directly?\nHere is a simplified example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(a, b, c):\n  return jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n\ndef arr(m, n):\n  return jnp.arange(m*n).reshape((m, n))\n\nm = 3\n\na = arr(m, 2)\nb = arr(m, 5)\nc = arr(m, 7)\n\nfv = jax.vmap(f)\n\nvmap_output = fv(a, b, c)\ndesired_output = jnp.hstack(fv(a, b, c))\n\nprint(vmap_output)\nprint(desired_output)\nThis yields:\n# vmap() output\n[[[  1.  10.]\n  [ 21.   0.]]\n\n [[  5.  35.]\n  [ 70.   0.]]\n\n [[  9.  60.]\n  [119.   0.]]]\n# Desired output\n[[  1.  10.   5.  35.   9.  60.]\n [ 21.   0.  70.   0. 119.   0.]]\nIf this is not possible, I would resort to pre-allocating an array and simply writing to the columns manually, but I hope to avoid this. Thanks for any clue!\nUpdate from @jakevdp's answer\nAlright, it isn't possible. So I resort to writing to the columns, but this fails as well:\ndef g(output, idx, a, b, c):\n  block = jnp.array([[a.sum(), b.sum()], [c.sum(), 0.]]) # Returns a 2x2 array\n  jax.lax.dynamic_update_slice_in_dim(output, block, idx*2, axis=1)\n\n# Defined above: jax, jnp, m, a, b, c\n\ng_output = jnp.zeros((2, 2*m))\nidxs = jnp.arange(m)\n\ngv = jax.vmap(g, in_axes=(None, 0, 0, 0, 0))\n\ngv(g_output, idxs, a, b, c)\n\nprint(g_output)\nThis yields:\n[[0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0.]]\nSo writing to g_output in the function g is not retained. Is there a way around this?",
        "answers": [
            "No, vmap does not have any built-in capability to stack outputs differently than the batching semantics would imply. But if you're interested in fusing the hstack operation with the vmap operation to the extent possible, you could do so by wrapping it in jit. For example:\n@jax.jit\ndef do_the_thing(a, b, c):\n  return jnp.hstack(fv(a, b, c))\n\nprint(do_the_thing(a, b, c))\nEdit: responding to your edited question: the reason the result is all zeros is because your function doesn't do anything: it returns None, so there's no way for it to affect the input array called g_output. JAX requires pure functions so side-effecting code like what you wrote above is not compatible. If you wanted to replace the hstack with an indexed update, you could do something like this:\ni = jnp.arange(2).reshape(1, 2, 1)\nj = jnp.arange(6).reshape(3, 1, 2)\ng_output = jnp.zeros((2, 2*m)).at[i, j].set(fv(a, b, c))\nbut a nontrivial scatter operation like this will not typically be faster than a simple reshape, especially if you're running on an accelerator like GPU.\nIf your arrays are large enough that reshapes are costly, you might find that a more direct implementation is better; for example:\n@jax.jit\ndef g(a, b, c):\n  output = jnp.zeros((2, 6))\n  output = output.at[0, 0::2].set(a.sum(1))\n  output = output.at[0, 1::2].set(b.sum(1))\n  output = output.at[1, 0::2].set(c.sum(1))\n  return output\n\ng_output = g(a, b, c)"
        ],
        "link": "https://stackoverflow.com/questions/78027629/can-jax-vmap-do-a-hstack"
    },
    {
        "title": "How can I use PyTorch 2.2 with Google Colab TPUs?",
        "question": "I'm having trouble getting PyTorch 2.2 running with TPUs on Google Colab. I'm getting an error about a JAX bug, but I'm confused about this because I'm not doing anything with JAX.\nMy setup process is very simple:\n!pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html\nAnd then\nimport torch\nimport torch_xla.core.xla_model as xm\nwhich gives the error\n/usr/local/lib/python3.10/dist-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')\n This a JAX bug; please report an issue at https://github.com/google/jax/issues\n  _warn(f\"cloud_tpu_init failed: {repr(exc)}\\n This a JAX bug; please report \"\n/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nThen trying\nt1 = torch.tensor(100, device=xm.xla_device())\nt2 = torch.tensor(200, device=xm.xla_device())\nprint(t1 + t2)\ngives the error\n2 frames\n/usr/local/lib/python3.10/dist-packages/torch_xla/runtime.py in xla_device(n, devkind)\n    121 \n    122   if n is None:\n--> 123     return torch.device(torch_xla._XLAC._xla_get_default_device())\n    124 \n    125   devices = xm.get_xla_supported_devices(devkind=devkind)\n\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: No ba16c7433 device found.",
        "answers": [
            "Colab currently only provides an older generation of TPUs which is not compatible with recent JAX or PyTorch releases. It’s possible that may change in the future, but I don’t know of any official timeline of when that might happen. In the meantime, you can access recent-generation TPUs via Kaggle or Google Cloud."
        ],
        "link": "https://stackoverflow.com/questions/78014487/how-can-i-use-pytorch-2-2-with-google-colab-tpus"
    },
    {
        "title": "Does jax save the jaxpr of jit compiled functions?",
        "question": "Consider the following example:\nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n    \nprint(test(jnp.ones(8,)))\nprint(test(jnp.ones(3,)))\nThe output is\n1\n-1\nHowever, I thought that on the first call jax compiles a function to use in subsequent calls. Shouldn't this then give the output 1 and 1, because jax traces through an if and does not use a conditional here? In the jaxpr of the first call is no conditional:\n{ lambda ; a:f32[8]. let\n    b:i32[] = pjit[name=test jaxpr={ lambda ; c:f32[8]. let  in (1,) }] a\n  in (b,) }\nSo how exactly does this work under the hood. Is the jaxpr unique for every call. Does jax only reuse jaxprs if the shape matches? Does jax recompile functions if the shape is different?",
        "answers": [
            "JAX does cache the jaxpr and compiled artifact for each compatible call of the function. This compatibility is determined via the cache key, which contains the shape and dtype of array arguments, as well as the hash of any static arguments and some additional information such as global flags that may affect the computation. Any time something in the cache key changes, it results in a new tracing & compilation of the function. You can see this by printing the _cache_size() of the compiled function. For example:\n@jax.jit\ndef test(x):\n    if x.shape[0] > 4:\n        return 1\n    else:\n        return -1\n\nx8 = jnp.ones(8)\nx3 = jnp.ones(3)\n\nprint(test._cache_size())  # no calls yet, so no cache\n# 0\n\ntest(x8)\nprint(test._cache_size())  # first call caches the jaxpr\n# 1\n\ntest(x8)\nprint(test._cache_size())  # repeated call, so size doesn't change\n# 1\n\ntest(x3)\nprint(test._cache_size())  # new call, so size increases\n# 2\n\ntest(x8)\nprint(test._cache_size())  # repeated call -> size doesn't change\n# 2\nBy keeping track of these static attributes, jit-compiled functions can change their output based on static attributes, but still avoid recompilation for compatible inputs."
        ],
        "link": "https://stackoverflow.com/questions/78011718/does-jax-save-the-jaxpr-of-jit-compiled-functions"
    },
    {
        "title": "multivariate derivatives in jax - efficiency question",
        "question": "I have the following code which computes derivatives of the function:\nimport jax\nimport jax.numpy as jnp\n\n\ndef f(x):\n    return jnp.prod(x)\n\n\ndf1 = jax.grad(f)\ndf2 = jax.jacobian(df1)\ndf3 = jax.jacobian(df2)\nWith this, all the partial derivatives are available, for example (with vmap additionally):\nx = jnp.array([[ 1.,  2.,  3.,  4.,  5.],\n               [ 6.,  7.,  8.,  9., 10.],\n               [11., 12., 13., 14., 15.],\n               [16., 17., 18., 19., 20.],\n               [21., 22., 23., 24., 25.],\n               [26., 27., 28., 29., 30.]])\ndf3_x0_x2_x4 = jax.vmap(df3)(x)[:, 0, 2, 4]\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nThe question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?",
        "answers": [
            "The question is how can I compute df3_x0_x2_x4 only, avoiding all the unnecessary derivative calculations (and leaving f with a single vector argument)?\nEssentially, you're asking for a way to compute sparse Hessians and Jacobians; JAX does not have general support for this (see previous issue threads; e.g https://github.com/google/jax/issues/1032).\nEdit\nIn this particular case, though, since you're effectively computing the gradient/jaacobian with respect to a single element per derivative pass, you can do better by just applying the JVP to a single one-hot vector in each transformation. For example:\ndef deriv(f, x, v):\n  return jax.jvp(f, [x], [v])[1]\n\ndef one_hot(i):\n  return jnp.zeros(x.shape[1]).at[i].set(1)\n\ndf_x0 = lambda x: deriv(f, x, one_hot(0))\ndf2_x0_x2 = lambda x: deriv(df_x0, x, one_hot(2))\ndf3_x0_x2_x4 = lambda x: deriv(df2_x0_x2, x, one_hot(4))\nprint(jax.vmap(df3_x0_x2_x4)(x))\n# [  8.  63. 168. 323. 528. 783.]\nPrevious answer\nIf you're willing to relax your \"leaving f with a single argument\" criterion, you could do something like this:\ndef f(*x):\n  return jnp.prod(jnp.asarray(x))\n\ndf1 = jax.grad(f, argnums=4)\ndf2 = jax.jacobian(df1, argnums=2)\ndf3 = jax.jacobian(df2, argnums=0)\n\ndf3_x0_x2_x4 = jax.vmap(df3)(*(x.T))\nprint(df3_x0_x2_x4)\n# [  8.  63. 168. 323. 528. 783.]\nHere rather than computing all gradients and slicing out the result, you are only computing the gradients with respect to the specific three elements you are interested in."
        ],
        "link": "https://stackoverflow.com/questions/78001753/multivariate-derivatives-in-jax-efficiency-question"
    },
    {
        "title": "JAX `custom_vjp` for functions with multiple outputs",
        "question": "In the JAX documentation, custom derivatives for functions with a single output are covered. I'm wondering how to implement custom derivatives for functions with multiple outputs such as this one?\n# want to define custom derivative of out_2 with respect to *args\ndef test_func(*args, **kwargs):\n    ...\n    return out_1, out_2",
        "answers": [
            "You can define custom derivatives for functions with any number of inputs and outputs: just add the appropriate number of elements to the primals and tangents tuples in the custom_jvp rule. For example:\nimport jax\nimport jax.numpy as jnp\n\n@jax.custom_jvp\ndef f(x, y):\n  return x * y, x / y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primals_out = f(x, y)\n  tangents_out = (x_dot * y + y_dot * x, \n                  x_dot / y - y_dot * x / y ** 2)\n  return primals_out, tangents_out\n\nx = jnp.float32(0.5)\ny = jnp.float32(2.0)\n\njax.jacobian(f, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))\nComparing this with the result computed using the standard non-custom derivative rule for the same function shows that the results are equivalent:\ndef f2(x, y):\n  return x * y, x / y\n\njax.jacobian(f2, argnums=(0, 1))(x, y)\n# ((Array(2., dtype=float32), Array(0.5, dtype=float32)),\n#  (Array(0.5, dtype=float32), Array(-0.125, dtype=float32)))"
        ],
        "link": "https://stackoverflow.com/questions/77952302/jax-custom-vjp-for-functions-with-multiple-outputs"
    },
    {
        "title": "Can't calculate matrix exponential in python",
        "question": "I want to calculate:\nfrom jax.scipy.linalg import expm\nimport jax.numpy as jnp\nfrom functools import lru_cache, reduce\n\nnum_qubits=2\ntheta = jnp.asarray(np.pi*np.random.random((15,2,2,2,2,2,2,2,2)))\n\ndef pauli_matrix(num_qubits):\n    _pauli_matrices = jnp.array(\n    [[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]]\n    )\n    return reduce(jnp.kron, (_pauli_matrices for _ in range(num_qubits)))[1:]\n\ndef SpecialUnitary(num_qubits,theta):\n    assert theta.shape[0] == 15\n    A = jnp.tensordot(theta, pauli_matrix(num_qubits), axes=[[0], [0]])\n    print(f'{A.shape= }{pauli_matrix(num_qubits).shape=}{theta.shape=}')\n    return expm(1j*A/2)\n\nSpecialUnitary(num_qubits,theta)\nShapes: A.shape= (2, 2, 2, 2, 2, 2, 2, 2, 4, 4)pauli_matrix(num_qubits).shape=(15, 4, 4)theta.shape=(15, 2, 2, 2, 2, 2, 2, 2, 2) Error: ValueError: expected A to be a square matrix\nI'm stuck because the documentation says that the expm is calculated on the last two axes, which must be square, which is done.",
        "answers": [
            "Batched expm is supported in recent JAX versions, and you should find that this works fine in JAX v0.4.7 or newer:\nimport jax.numpy as jnp\nimport jax.scipy.linalg\n\nX = jnp.arange(128.0).reshape(2, 2, 2, 4, 4)\n\nresult = jax.scipy.linalg.expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)\nIf for some reason you must use an older JAX version, you can work around this by using jax.numpy.vectorize. For example:\nexpm = jnp.vectorize(jax.scipy.linalg.expm, signature='(n,n)->(n,n)')\n\nresult = expm(X)\nprint(result.shape)\n# (2, 2, 2, 4, 4)"
        ],
        "link": "https://stackoverflow.com/questions/77946266/cant-calculate-matrix-exponential-in-python"
    },
    {
        "title": "Jax Implementation of function similar to Torch's 'Scatter'",
        "question": "For graph learning purposes, I am trying to implement a global sum batching function, that takes as inputs batched graph representations 'x' of size (n x d) and a corresponding vector of batches (n x 1). I then want to compute the sum over all graph representations for each batch. Here is a graphical representation: torch's scatter function\nThis is my current attempt:\ndef global_sum_pool(x, batch):\n    graph_reps = []\n    i = 0\n    n = jnp.max(batch)\n    while True:\n        ind = jnp.where(batch == i, True, False).reshape(-1, 1)\n        ind = jnp.tile(ind, x.shape[1])\n        x_ind = jnp.where(ind == True, x, 0.0)\n        graph_reps.append(jnp.sum(x_ind, axis=0))\n        if i == n:\n            break\n        i += 1\n    return jnp.array(graph_reps)\nI get the following exception on the line if i == n:\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function make_step at /venvs/jax_env/lib/python3.11/site-packages/equinox/_jit.py:37 for jit. \nI understand this is due to the fact that at compile time, Jax does not a priori know the max value of the 'batch' array and hence cannot allocate memory. Does anyone know a workaround or different implementation?",
        "answers": [
            "Rather than implementing this via a for loop, you should use JAX's built-in scatter operator. The most convenient interface for this is the Array.at syntax. If I understand your goal correctly, it might look something like this:\nimport jax.numpy as jnp\nimport numpy as np\n\n# Generate some data\nnum_batches = 4\nn = 10\nd = 3\nx = np.random.randn(n, d)\nind = np.random.randint(low=0, high=num_batches, size=(n,))\n\n#Compute the result with jax.lax.scatter\nresult = jnp.zeros((num_batches, d)).at[ind].add(x)\nprint(result.shape)\n# (4, 3)"
        ],
        "link": "https://stackoverflow.com/questions/77932146/jax-implementation-of-function-similar-to-torchs-scatter"
    },
    {
        "title": "JAX `vjp` fails for vmapped function with `custom_vjp`",
        "question": "Below is an example where a function with a custom-defined vector-Jacobian product (custom_vjp) is vmapped. For a simple function like this, invoking vjp fails:\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef test_func(f: Callable[..., float],\n              R: Array\n              ) -> float:\n\n    return f(jnp.dot(R, R))\n\n\ndef test_func_fwd(f, primal):\n\n    primal_out = test_func(f, primal)\n    residual = 2. * primal * primal_out\n    return primal_out, residual\n\n\ndef test_func_bwd(f, residual, cotangent):\n\n    cotangent_out = residual * cotangent\n    return (cotangent_out, )\n\n\ntest_func.defvjp(test_func_fwd, test_func_bwd)\n\ntest_func = vmap(test_func, in_axes=(None, 0))\n\n\nif __name__ == \"__main__\":\n\n    def f(x):\n        return x\n\n    # vjp\n    primal, f_vjp = vjp(partial(test_func, f),\n                        jnp.ones((10, 3))\n                        )\n\n    cotangent = jnp.ones(10)\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[0].shape)\nThe error message says:\nValueError: Shape of cotangent input to vjp pullback function (10,) must be the same as the shape of corresponding primal input (10, 3).\nHere, I think the error message is misleading, because the cotangent input should have the same shape as the primal output, which should be (10, ) in this case. Still, it's not clear to me why this error occurs.",
        "answers": [
            "The problem is that in test_func_fwd, you recursively call test_func, but you've overwritten test_func in the global namespace with its vmapped version. If you leave the original test_func unchanged in the global namespace, your code will work as expected:\n...\n\ntest_func_mapped = vmap(test_func, in_axes=(None, 0))\n\n... \n\nprimal, f_vjp = vjp(partial(test_func_mapped, f),\n                    jnp.ones((10, 3))\n                    )"
        ],
        "link": "https://stackoverflow.com/questions/77930920/jax-vjp-fails-for-vmapped-function-with-custom-vjp"
    },
    {
        "title": "JAX `vjp` does not recognize cotangent argument with `custom_vjp`",
        "question": "I have a JAX function cart_deriv() which takes another function f and returns the Cartesian derivative of f, implemented as follows:\n@partial(custom_vjp, nondiff_argnums=0)\ndef cart_deriv(f: Callable[..., float],\n               l: int,\n               R: Array\n               ) -> Array:\n\n    df = lambda R: f(l, jnp.dot(R, R))\n\n    for i in range(l):\n        df = jacrev(df)\n\n    return df(R)\n\n\ndef cart_deriv_fwd(f, l, primal):\n\n    primal_out = cart_deriv(f, l, primal)\n    residual = cart_deriv(f, l+1, primal)  ## just a test\n\n    return primal_out, residual\n\n\ndef cart_deriv_bwd(f, residual, cotangent):\n\n    cotangent_out = jnp.ones(3)  ## just a test\n\n    return (None, cotangent_out)\n\n\ncart_deriv.defvjp(cart_deriv_fwd, cart_deriv_bwd)\n\n\n\nif __name__ == \"__main__\":\n\n    def test_func(l, r2):\n        return l + r2\n\n    primal_out, f_vjp = vjp(cart_deriv, \n                            jax.tree_util.Partial(test_func),\n                            2,\n                            jnp.array([1., 2., 3.])\n                            )\n\n    cotangent = jnp.ones((3, 3))\n    cotangent_out = f_vjp(cotangent)\n\n    print(cotangent_out[1].shape)\nHowever this code produces the error:\nTypeError: cart_deriv_bwd() missing 1 required positional argument: 'cotangent'\nI have checked that the syntax agrees with that in the documentation. I'm wondering why the argument cotangent is not recognized by vjp, and how to fix this error?",
        "answers": [
            "The issue is that nondiff_argnums is expected to be a sequence:\n@partial(custom_vjp, nondiff_argnums=(0,))\nWith this properly defined, it's better to avoid wrapping the function in Partial, and just pass it as a static argument by closing over it in the vjp call:\nprimal_out, f_vjp = vjp(partial(cart_deriv, test_func),\n                        2,\n                        jnp.array([1., 2., 3.])\n                        )\ncotangent_out = f_vjp(jnp.ones((3, 3)))\n\nprint(*cotangent_out)\n# (b'',) [1. 1. 1.]"
        ],
        "link": "https://stackoverflow.com/questions/77924142/jax-vjp-does-not-recognize-cotangent-argument-with-custom-vjp"
    },
    {
        "title": "How to implement the next function (the use of Dynamic Shapes) in JAX?",
        "question": "I have a simple function that takes an jax Array as input, searches for the first occurrence of 1, and replaces it with another jax Array (specified as a second input):\nrules_int = [\n    jnp.array([0,0]),\n    jnp.array([1,1,1]),\n]\n# Even with the same size of inputs, the sizes of outputs can be different\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1)[0]\n    if index.size == 0:\n        return arr\n    index = index[0]\n    new_arr = jnp.concatenate([arr[:index], rules_int[action], arr[index+1:]])\n    return new_arr\n\nreplace_first_one(jnp.array([1]), 0)\n# result is Array([0, 0], dtype=int32)\nBut when I use vmap a get an exception:\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])  # Corresponding actions for each array\n\n# Vectorize the function\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nresult = vectorized_replace_first_one(batch_arr, batch_actions)\nindex = jnp.where(arr == 1)[0] The size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations. This BatchTracer with object id 140260750414512 was created on line:\nI read on JAX docs:\nJAX code used within transforms like jax.jit, jax.vmap, jax.grad, etc. requires all output arrays and intermediate arrays to have static shape: that is, the shape cannot depend on values within other arrays.\nPlease suggest how to make it work?\nIdeally, these rules should be applied recursively until there are no rules to apply. (string rewriting system)",
        "answers": [
            "As written, it is impossible to do this with vmap because the output of your function has a shape that depends on the value of action, and so the output would have to be a ragged array, which JAX does not support (see JAX Sharp Bits: Dynamic Shapes).\nTo make the function compatible with vmap, you'll have to adjust it so that it has static shape semantics: in particular, every entry of rules_int must have the same length, and you cannot return arr alone in cases where arr doesn't have any 1 entries. Making these changes and adjusting the logic to avoid dynamically-shaped intermediates, you could write something like this:\nimport jax\n\nrules_int = jnp.array([\n    [0,0],\n    [1,1],\n])\n\ndef replace_first_one(arr, action):\n    index = jnp.where(arr == 1, size=1)[0][0]\n    arr_to_insert = rules_int[action]\n    output_size = len(arr) - 1 + len(arr_to_insert)\n    new_arr = jnp.where(jnp.arange(output_size) < index,\n                        jnp.concatenate([arr[:-1], arr_to_insert]),\n                        jnp.concatenate([arr_to_insert, arr[1:]]))\n    return jax.lax.dynamic_update_slice(new_arr, arr_to_insert, (index,))\n\nreplace_first_one(jnp.array([1]), 0)\n# Array([0, 0], dtype=int32)\nbatch_arr = jnp.array([\n    jnp.array([1, 4, 5, 1]),\n    jnp.array([6, 1, 8, 1])\n])\n\nbatch_actions = jnp.array([0, 1])\n\nvectorized_replace_first_one = vmap(replace_first_one, in_axes=(0, 0))\nvectorized_replace_first_one(batch_arr, batch_actions)\n# Array([[0, 0, 4, 5, 1],\n#        [6, 1, 1, 8, 1]], dtype=int32)\nIf adjusting the semantics of your function in this way to avoid dynamic shapes is not viable given your use-case, then your use-case is unfortunately not compatible with vmap or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77915540/how-to-implement-the-next-function-the-use-of-dynamic-shapes-in-jax"
    },
    {
        "title": "Vectorizing power of `jax.grad`",
        "question": "I'm trying to vectorize the following \"power-of-grad\" function so that it accepts multiple orders: (see here)\ndef grad_pow(f, order, argnum):\n\n    for i in jnp.arange(order):\n        f = grad(f, argnums=argnum)\n\n    return f\nThis function produces the following error after applying vmap on the argument order:\njax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nIt arose in the jnp.arange argument 'stop'\nI have tried writing a static version of grad_pow using jax.lax.cond and jax.lax.scan, following the logic here:\ndef static_grad_pow(f, order, argnum):\n\n    order_max = 3  ## maximum order\n\n    def grad_pow(f, i):\n        return cond(i <= order, grad(f, argnum), f), None\n\n    return scan(grad_pow, f, jnp.arange(order_max+1))[0]\n\n\nif __name__ == \"__main__\":\n\n    test_func = lambda x: jnp.exp(-2*x)\n    test_func_grad_pow = static_grad_pow(jax.tree_util.Partial(test_func), 1, 0)\n    print(test_func_grad_pow(1.))\nNevertheless, this solution still produces an error:\n    return cond(i <= order, grad(f, argnum), f), None\nTypeError: differentiating with respect to argnums=0 requires at least 1 positional arguments to be passed by the caller, but got only 0 positional arguments.\nJust wondering how this issue can be resolved?",
        "answers": [
            "The fundamental issue with your question is that a vmapped function cannot return a function, it can only return arrays. All other details aside, that precludes any possibility of writing a valid function that does what you intend.\nThere are alternatives: for example, rather than attempting to create a function that will return a function, you could instead create a function that accepts arguments and applies that function to those arguments.\nIn that case, you'll run into another issue: if n is traced, there is no way to apply grad n times. JAX transformations like grad are evaluated at trace-time, and traced values like n are not available until runtime. One way to work around this is to pre-define all the functions you're interested in, and to use lax.switch to choose between them at runtime. The result would look something like this:\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=[0], static_argnames=['argnum', 'max_order'])\ndef apply_multi_grad(f, order, *args, argnum=0, max_order=10):\n  funcs = [f]\n  for i in range(max_order):\n    funcs.append(jax.grad(funcs[-1], argnum))\n  return jax.lax.switch(order, funcs, *args)\n\n\norder = jnp.arange(3)\nx = jnp.ones(3)\nf = jnp.sin\n\nprint(jax.vmap(apply_multi_grad, in_axes=(None, 0, 0))(f, order, x))\n# [ 0.84147096  0.5403023  -0.84147096]\n\n# Compare by doing it manually:\nprint(jnp.array([f(x[0]), jax.grad(f)(x[1]), jax.grad(jax.grad(f))(x[2])]))\n# [ 0.84147096  0.5403023  -0.84147096]"
        ],
        "link": "https://stackoverflow.com/questions/77913154/vectorizing-power-of-jax-grad"
    },
    {
        "title": "jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed",
        "question": "I am trying to run multiple sbx programs (that use JAX) concurrently using joblib. Here is my program -\n'''\nFor installation please do -\npip install gym\npip install sbx-rl\npip install mujoco\npip install shimmy\n'''\nfrom joblib import Parallel, delayed\n\nimport gym\nfrom sbx import SAC\n\n# from stable_baselines3 import SAC\ndef train():\n\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env, verbose=1)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\ndef train_model():\n\n    train()\n\n\n\nif __name__ == '__main__':\n    Parallel(n_jobs=10)(delayed(train)() for i in range(3))\nThis is the error that I am getting -\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/dgthomas/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n2024-01-30 11:19:12.354168: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:\nINTERNAL: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory\n2024-01-30 11:19:12.354264: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\njoblib.externals.loky.process_executor._RemoteTraceback: \n\"\"\"\njax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/work/LAS/usr/tbd/5_test.py\", line 23, in my_func\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 109, in __init__\n    self._setup_model()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/sac.py\", line 126, in _setup_model\n    self.key = self.policy.build(self.key, self.lr_schedule, self.qf_learning_rate)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/sbx/sac/policies.py\", line 143, in build\n    key, actor_key, qf_key, dropout_key = jax.random.split(key, 4)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 303, in split\n    return _return_prng_keys(wrapped, _split(typed_key, num))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/random.py\", line 289, in _split\n    return prng.random_split(key, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 769, in random_split\n    return random_split_p.bind(keys, shape=shape)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 444, in bind\n    return self.bind_with_trace(find_top_trace(args), args, params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 447, in bind_with_trace\n    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/core.py\", line 935, in process_primitive\n    return primitive.impl(*tracers, **params)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 781, in random_split_impl\n    base_arr = random_split_impl_base(\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 787, in random_split_impl_base\n    return split(base_arr)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 786, in <lambda>\n    split = iterated_vmap_unary(keys_ndim, lambda k: impl.split(k, shape))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/jax/_src/prng.py\", line 1291, in threefry_split\n    return _threefry_split(key, shape)\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/work/LAS/usr/tbd/5_test.py\", line 27, in <module>\n    Parallel(n_jobs=3)(delayed(my_func)() for i in range(3))\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1952, in __call__\n    return output if self.return_generator else list(output)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\n    yield from self._retrieve()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1699, in _retrieve\n    self._raise_error_fast()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1734, in _raise_error_fast\n    error_job.get_result(self.timeout)\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 736, in get_result\n    return self._return_or_raise()\n  File \"/home/dgthomas/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 754, in _return_or_raise\n    raise self._result\njaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory; current tracing scope: custom-call.11; current profiling annotation: XlaModule:#prefix=jit(_threefry_split)/jit(main),hlo_module=jit__threefry_split,program_id=2#.\nI am using a 40 GB GPU (a100-pcie). Therefore I doubt that my GPU is running out of memory. Please let me know if any clarification is needed.\nEdit 1: This is how I call my program - export XLA_PYTHON_CLIENT_PREALLOCATE=false && python 5_test.py (The name of my program is 5_test.py)",
        "answers": [
            "It appears you are using multiple processes targeting the same GPU. In each process, JAX will attempt to reserve 75% of the available GPU memory (see GPU memory allocation), so attempting this with two or more processes will exhaust the available memory.\nYou could fix this by turning off pre-allocation as mentioned in that doc, by setting the environment variables XLA_PYTHON_CLIENT_PREALLOCATE=false or XLA_PYTHON_CLIENT_MEM_FRACTION=.XX (with .XX set to .08 or something suitable), but I suspect the end result will be less efficient than if you had just run your full program from a single JAX process: multiple host processes targeting a single GPU device concurrently will just compete with each other for resources and lead to suboptimal results."
        ],
        "link": "https://stackoverflow.com/questions/77908236/jaxlib-xla-extension-xlaruntimeerror-internal-failed-to-execute-xla-runtime-ex"
    },
    {
        "title": "JAX `grad` error for function with `jax.lax.switch` and compound boolean conditions",
        "question": "I have encountered a scenario where applying jax.grad to a function with jax.lax.switch and compound boolean conditions yields jax.errors.TracerBoolConversionError. A minimal program to reproduce this behavior is the following:\nfrom jax.lax import switch\nimport jax.numpy as jnp\nfrom jax import grad\n\nfunc_0 = lambda x: jnp.where(0. < x < 1., x, 0.)\nfunc_1 = lambda x: jnp.where(0. < x < 1., x, 1.)\n\nfunc_list = [func_0, func_1]\n\nfunc = lambda index, x: switch(index, func_list, x)\n\ndf = grad(func, argnums=1)(1, 2.)\nprint(df)\nThe error is the following:\nTraceback (most recent call last):\n  File \"***/grad_test.py\", line 12, in <module>\n    df = grad(func, argnums=1)(1, 0.5)\n  File \"***/grad_test.py\", line 10, in <lambda>\n    func = lambda index, x: switch(index, func_list, x)\n  File \"***/grad_test.py\", line 5, in <lambda>\n    func_0 = lambda x: jnp.where(0 < x < 1., x, 0.)\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function <lambda> at ***/grad_test.py:5 for switch. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\nHowever, if the boolean condition is changed to a single condition (for example, x < 1), then no error occurs. I'm wondering if this could be a bug, or otherwise, how the original program should be changed.",
        "answers": [
            "You cannot use chained inequalities with JAX or NumPy arrays. Instead of 0 < x < 1, you should write (0 < x) & (x < 1) (note that due to operator precedence, the parentheses are not optional here)."
        ],
        "link": "https://stackoverflow.com/questions/77900299/jax-grad-error-for-function-with-jax-lax-switch-and-compound-boolean-conditi"
    },
    {
        "title": "Getting derivatives of NNs according to its inputs by batches in JAX",
        "question": "There is a neural network that takes as an input a two variables: net(x, t), where x is usually d-dim, and t is a scalar. The NN outputs a vector of length d. x and t might be batches, so x is of shape (b, d), and t is (b, 1), and the output is (b,d). I need to find\nderivative d out/dt of the NN output. It should be d dim vector (or (batch, d));\nderivative d out/dx of the NN\ngradient of divergence of the NN output according to x, it still should be (batch, d) vector\nSince the NN doesn’t output a scalar, I don’t think Jax grad would help here. I know how to do what I described in torch, but I’m totally new to JAX. I’d really appreciate your help with this question!\nThere is an example:\nimport jaxlib\nimport jax\nfrom jax import numpy as jnp\nimport flax.linen as nn\nfrom flax.training import train_state\n\n\n\nclass NN(nn.Module):\n    hid_dim : int # Number of hidden neurons\n    output_dim : int # Number of output neurons\n\n    @nn.compact  \n    def __call__(self, x, t):\n        out = jnp.hstack((x, t))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.tanh(nn.Dense(features=self.hid_dim)(out))\n        out = nn.Dense(features=self.output_dim)(out)\n        return out\n\nd = 3\nbatch_size = 10\nnet = NN(hid_dim=100, output_dim=d)\n\nrng_nn, rng_inp1, rng_inp2 = jax.random.split(jax.random.PRNGKey(100), 3)\ninp_x = jax.random.normal(rng_inp1, (1, d)) # batch, d\ninp_t = jax.random.normal(rng_inp2, (1, 1))\nparams_net = net.init(rng_nn, inp_x, inp_t)\n\nx = jax.random.normal(rng_inp2, (batch_size, d)) # batch, d\nt = jax.random.normal(rng_inp1, (batxh_size, 1))\n\nout_net = net.apply(params_net, x, t)\n\noptimizer = optax.adam(1e-3)\n\nmodel_state = train_state.TrainState.create(apply_fn=net.apply,\n                                            params= params_net,\n                                            tx=optimizer)\nI'd like to calculate an $L_2$ loss based on some derivatives of the NN's outputs according to its inputs. For example, I'd like to have d f/dx or d f/dt where f is the NN. ALso the gradient of the divergence by x. I assume it'd be something like\ndef find_derivatives(net, params, X, t):\n    d_dt = lambda net, params, X, t: jax.jvp(lambda time: net(params, X, t), (t, ), (jnp.ones_like(t), ))\n    d_dx = lambda net, params, X, t: jax.jvp(lambda X: net(params, X, t), (Xs_all, ), (jnp.ones_like(X), ))\n    out_f, df_dt = d_dt(net.apply, params, X, t)\n\n    d_ddx = lambda net, params, X, t: d_dx(lambda params, X, t: d_dx(net, params, X, t)[1], params, X, t)\n    df_dx, df_ddx = d_ddx(net.apply, params, X, t)\n    \n    return out_f, df_dt, df_dx, df_ddx\n\n\nout_f, df_dt, df_dx, df_ddx = find_derivatives(net, params_net, x, t)",
        "answers": [
            "I would avoid using jax.jvp here, because this is meant as a lower-level API. You can use jax.jacobian to compute the Jacobian (since your function has multiple outputs), and vmap for batching. For example:\ndf_dx = jax.vmap(\n    jax.jacobian(net.apply, argnums=1),\n    in_axes=(None, 0, 0)\n  )(params_net, x, t)\nprint(df_dx.shape)  # (10, 3, 3)\n\ndf_dt = jax.vmap(\n    jax.jacobian(net.apply, argnums=2),\n    in_axes=(None,0, 0)\n  )(params_net, x, t).reshape(10, 3)\nprint(df_dt.shape)  # (10, 3)\nHere df_dx is the batch-wise Jacobian of the 3-dimensional output vector with respect to the 3-dimensional x input vector, and df_dt is the batch-wise gradient of the 3-dimensional output vector with respect to the input t."
        ],
        "link": "https://stackoverflow.com/questions/77897419/getting-derivatives-of-nns-according-to-its-inputs-by-batches-in-jax"
    },
    {
        "title": "`jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].`",
        "question": "I am trying to perform multiprocessing to parallelize my program (that uses JAX) using pmap. I am a newbie with JAX and realize that maybe pmap isn't the right approach. The documentation here, said that pmap can express SPMD programs (which is the case here) and therefore I decided to use it.\nHere's my program. I am basically trying to run a reinforcement learning program (that uses JAX too) twice, using parallel execution -\n'''\nFor installation please do -\npip install gym\npip install sbx\npip install mujoco\npip install shimmy\n'''\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\nimport gym\nfrom sbx import SAC\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda _: my_func())(jnp.arange(2))\nI get the following error -\n(tbd) thoma@thoma-Lenovo-Legion-5-15IMH05H:~/PycharmProjects/tbd$ python new.py\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n  warnings.warn(\n/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 678, in parallel_callable\n    pmap_computation = lower_parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 825, in lower_parallel_callable\n    jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 748, in stage_parallel_callable\n    jaxpr, out_sharded_avals, consts = pe.trace_to_jaxpr_final(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2233, in trace_to_jaxpr_final\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/partial_eval.py\", line 2177, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/core.py\", line 605, in __array__\n    raise TracerArrayConversionError(self)\njax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 17, in <module>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 17, in <lambda>\n    out = pmap(lambda _: my_func())(jnp.arange(2))\n  File \"new.py\", line 12, in my_func\n    model.learn(total_timesteps=7e5, progress_bar=True)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/sac/sac.py\", line 173, in learn\n    return super().learn(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 328, in learn\n    rollout = self.collect_rollouts(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 557, in collect_rollouts\n    actions, buffer_actions = self._sample_action(learning_starts, action_noise, env.num_envs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 390, in _sample_action\n    unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 553, in predict\n    return self.policy.predict(observation, state, episode_start, deterministic)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\njax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[1,17].\nThe error occurred while tracing the function <lambda> at new.py:17 for pmap. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i32[] = convert_element_type[new_dtype=int32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:u32[] = convert_element_type[new_dtype=uint32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b\n    from line new.py:11 (my_func)\n\n  operation a:f32[376,256] = pjit[\n  jaxpr={ lambda ; b:key<fry>[] c:i32[] d:i32[]. let\n      e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n      f:f32[] = convert_element_type[new_dtype=float32 weak_type=False] d\n      g:f32[] = div e 1.4142135381698608\n      h:f32[] = erf g\n      i:f32[] = div f 1.4142135381698608\n      j:f32[] = erf i\n      k:f32[376,256] = pjit[\n        jaxpr={ lambda ; l:key<fry>[] m:f32[] n:f32[]. let\n            o:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] m\n            p:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] n\n            q:u32[376,256] = random_bits[bit_width=32 shape=(376, 256)] l\n            r:u32[376,256] = shift_right_logical q 9\n            s:u32[376,256] = or r 1065353216\n            t:f32[376,256] = bitcast_convert_type[new_dtype=float32] s\n            u:f32[376,256] = sub t 1.0\n            v:f32[1,1] = sub p o\n            w:f32[376,256] = mul u v\n            x:f32[376,256] = add w o\n            y:f32[376,256] = max o x\n          in (y,) }\n        name=_uniform\n      ] b h j\n      z:f32[376,256] = erf_inv k\n      ba:f32[376,256] = mul 1.4142135381698608 z\n      bb:f32[] = stop_gradient e\n      bc:f32[] = nextafter bb inf\n      bd:f32[] = stop_gradient f\n      be:f32[] = nextafter bd -inf\n      bf:f32[376,256] = pjit[\n        jaxpr={ lambda ; bg:f32[376,256] bh:f32[] bi:f32[]. let\n            bj:f32[376,256] = max bh bg\n            bk:f32[376,256] = min bi bj\n          in (bk,) }\n        name=clip\n      ] ba bc be\n    in (bf,) }\n  name=_truncated_normal\n] bl bm bn\n    from line new.py:11 (my_func)\n\n(Additional originating lines are not shown.)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\n   0% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0/700,000  [ 0:00:00 < -:--:-- , ? it/s ]Exception ignored in: <function tqdm.__del__ at 0x7fa875eb8af0>\nTraceback (most recent call last):\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/std.py\", line 1149, in __del__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/tqdm/rich.py\", line 120, in close\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1177, in __exit__\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/progress.py\", line 1163, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/live.py\", line 155, in stop\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1137, in line\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1674, in print\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/console.py\", line 1535, in _collect_renderables\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/rich/protocol.py\", line 28, in rich_cast\nImportError: sys.meta_path is None, Python is likely shutting down\nBasically I am trying to replace the parallelization performed by joblib with JAX. Here's my original program that I am changing -\nfrom joblib import Parallel, delayed\nimport gym\nimport os\nfrom sbx import SAC\nimport multiprocessing\n\ndef my_func():\n\n    env = gym.make(\"Humanoid-v4\")\n\n    model = SAC(\"MlpPolicy\", env,verbose=0)\n    model.learn(total_timesteps=7e5, progress_bar=True)\n\n\nParallel(n_jobs=2)(delayed(my_func)() for i in range(2))",
        "answers": [
            "The problem is here:\n File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/sbx/common/policies.py\", line 58, in predict\n    actions = np.array(actions).reshape((-1, *self.action_space.shape))\nThe sbx package is calling np.array on the inputs – this tells me that sbx is built on NumPy, not on JAX. JAX transformations like pmap are not compatible with NumPy functions, they require functions written with JAX operations. Unless sbx is substantially re-designed, you won't be able to use it with pmap, vmap, jit, grad, or other JAX transformations."
        ],
        "link": "https://stackoverflow.com/questions/77892458/jax-errors-tracerarrayconversionerror-the-numpy-ndarray-conversion-method-ar"
    },
    {
        "title": "How to use JAX pmap with CPU cores",
        "question": "I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible - Here's my code -\nimport jax.numpy as jnp\nimport os\nfrom jax import pmap\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\nHere's the error -\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py\", line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n    ans = call(fun, *args)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 682, in parallel_callable\n    pmap_executable = pmap_computation.compile()\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 923, in compile\n    executable = UnloadedPmapExecutable.from_hlo(\n  File \"/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py\", line 993, in from_hlo\n    raise ValueError(msg.format(shards.num_global_shards,\njax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"new.py\", line 6, in <module>\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\nValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\nBased on this and this discussion, I did this os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8', but it doesn't seem to work.\nEdit 1:\nI tried this but it still doesn't work -\nimport os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n\nimport jax\n\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)",
        "answers": [
            "XLA flags are read when JAX is imported, so you need to set them before importing JAX if you want the flags to have an effect.\nYou should also make sure you're in a clean runtime (i.e. not using a Jupyter kernel where you have previously imported jax).\nAdditionally, keep in mind that --xla_force_host_platform_device_count=8 only affects the host (CPU) device count, so the code as written above won't work if you're using GPU-enabled JAX with a single GPU device. If this is the case, you can force pmap to run on the non-default CPU devices using the devices argument:\nout = pmap(lambda x: x ** 2, devices=jax.devices('cpu')(jnp.arange(8))"
        ],
        "link": "https://stackoverflow.com/questions/77889712/how-to-use-jax-pmap-with-cpu-cores"
    },
    {
        "title": "Unexpected behavior of JAX `vmap` for multiple arguments",
        "question": "I have found that vmap in JAX does not behave as expected when applied to multiple arguments. For example, consider the function below:\ndef f1(x, y, z):\n    f = x[:, None, None] * z[None, None, :] + y[None, :, None]\n    return f\nFor x = jnp.arange(7), y = jnp.arange(5), z = jnp.arange(3), the output of this function has shape (7, 5, 3). However, for the vmap version below:\n@partial(vmap, in_axes=(None, 0, 0), out_axes=(1, 2))\ndef f2(x, y, z):\n    f = x*z + y\n    return f\nIt outputs this error:\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 5: axis 0 of argument y of type int32[5];\n  * one axis had size 3: axis 0 of argument z of type int32[3]\nCould someone kindly explain what's behind this error?",
        "answers": [
            "The semantics of vmap are that it does a single batching operation along one or more arrays. When you specify in_axes=(None, 0, 0), the meaning is \"map simultaneously along the leading dimension of y and z\": the error you're seeing is telling you that the leading dimensions of y and z have different sizes, and so they are not compatible for batching.\nYour function f1 essentially uses broadcasting to encode three batching operations, so to replicate that logic with vmap you'll need three applications of vmap. You can express that as follows:\n@partial(vmap, in_axes=(0, None, None))\n@partial(vmap, in_axes=(None, 0, None))\n@partial(vmap, in_axes=(None, None, 0))\ndef f2(x, y, z):\n    f = x*z + y\n    return f"
        ],
        "link": "https://stackoverflow.com/questions/77886057/unexpected-behavior-of-jax-vmap-for-multiple-arguments"
    },
    {
        "title": "How is it possible that jax vmap returns not iterable?",
        "question": "import jax\nimport pgx\nfrom jax import vmap, jit\nimport jax.numpy as jnp\n\nenv = pgx.make(\"tic_tac_toe\")\nkey = jax.random.PRNGKey(42)\n\nstates = jax.jit(vmap(env.init))(jax.random.split(key, 4))\ntype(states)\nstates has a type pgx.tic_tac_toe.State. I was expecting an Iterable object with a size 4. Somehow iterable results are inside pgx.tic_tac_toe.State.\nCan you please explain how is it possible that jax vmap returns not iterable?\nHow to force vmap to return the next result:\nstates = [env.init(key) for key in jax.random.split(key, 4)]\nNote, this code works as expected:\ndef square(x):\n    return x ** 2\ninputs = jnp.array([1, 2, 3, 4])\nresult = jax.vmap(square)(inputs)\nprint(result) # list object",
        "answers": [
            "Can you please explain how is it possible that jax vmap returns not iterable?\nWhen passed a non-array object, vmap will map the leading axes of each array in its flattened pytree representation. You can see the shapes in the flattened object here:\nprint([arr.shape for arr in jax.tree_util.tree_flatten(states)[0]])\n# [(4,), (4, 3, 3, 2), (4, 2), (4,), (4,), (4, 9), (4,), (4,), (4, 9)]\nThis is an example of the struct-of-arrays pattern used by vmap, where it sounds like you were expececting an array-of-structs pattern.\nHow to force vmap to return the next result\nIf you wanted to convert this output into the list of state objects you were expecting, you could do so using utilities in jax.tree_util:\nleaves, treedef = jax.tree_util.tree_flatten(states)\nstates_list = [treedef.unflatten(leaf) for leaf in zip(*leaves)]\nprint(len(states_list))\n# 4\nThat said, it appears that pgx is built to work natively with the original struct-of-arrays pattern, so you may find that you won't actually need this unstacked version in practice."
        ],
        "link": "https://stackoverflow.com/questions/77881821/how-is-it-possible-that-jax-vmap-returns-not-iterable"
    },
    {
        "title": "Why do I get different values from jnp.round and np.round?",
        "question": "I'm writing tests for some jax code and using np.testing.assert_array...-type functions and came across this difference in values that I didn't expect:\nimport jax.numpy as jnp\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\na = jnp.array([-0.78073686, -0.7908204 ,  2.174842])\nb = np.array(a, dtype='float32')\nassert_array_equal(a, b)\n\nprint(a.round(2), a.dtype)\nprint(b.round(2), b.dtype)\nOutput:\n[-0.78       -0.78999996  2.1699998 ] float32\n[-0.78 -0.79  2.17] float32\nTest:\nassert_array_equal(a.round(2), b.round(2))\nOutput:\nAssertionError: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference: 2.3841858e-07\nMax relative difference: 1.0987031e-07\n x: array([-0.78, -0.79,  2.17], dtype=float32)\n y: array([-0.78, -0.79,  2.17], dtype=float32)\nFootnote:\nI get exactly the same results if I define b as follows, so it's not a problem with the conversion of the array from jax to numpy:\nb = np.array([-0.78073686, -0.7908204 ,  2.174842], dtype='float32')",
        "answers": [
            "This is an example of a general property of floating point computations: two different ways of expressing the same computation will not always produce bitwise-equivalent outputs (see e.g. Is floating point math broken?).\nJAX and NumPy use identical implementations for x.round(2); essentially it is round_to_int(x * 100) / 100 (compare the JAX implementation and the NumPy implementation).\nThe difference is that JAX jit-compiles jnp.round by default. When you disable compilation and perform these operations in sequence, the results are identical:\nimport jax\nwith jax.disable_jit():\n  assert_array_equal(a.round(2), b.round(2))  # passes!\nBut JAX's JIT optimizes the implementation by fusing some operations – this leads to faster computation but in general you should not expect the result to be bitwise-equivalent to the unoptimized version.\nTo address this, whenever you are comparing floating point values, you should avoid exact equality checks in favor of checks that take this floating point roundoff error into account. For example:\nnp.testing.assert_allclose(a.round(2), b.round(2), rtol=1E-6)  # passes!"
        ],
        "link": "https://stackoverflow.com/questions/77868226/why-do-i-get-different-values-from-jnp-round-and-np-round"
    },
    {
        "title": "finding the maximum of a function using jax",
        "question": "I have a function which I would like to find its maximum by optimizing two of its variables using Jax.\nThe current code that I have currently, which does not work, reads\nimport jax.numpy as jnp\nimport jax \nimport scipy\nimport numpy as np\n\ndef temp_func(x,y,z):\n    tmp = x + jnp.dot( jnp.power(y,3), jnp.tanh(z) )\n    return -tmp\ndef obj_func(xy, z):\n    x,y = xy[:2], xy[2:].reshape(2,2)\n    return jnp.sum(temp_func(jnp.array(x),jnp.array(y),z))\n\ngrad_tmp = jax.grad(obj_func, argnums=0) # x,y\n\nxy = jnp.concatenate([np.random.rand(2), np.random.rand(2*2) ])\nz= jnp.array( np.random.rand(2,2) )\nprint(obj_func(xy,z))\n\nresult = scipy.optimize.minimize(obj_func,\n                                 xy,\n                                 args=(z,),\n                                 method='L-BFGS-B',\n                                 jac=grad_tmp\n                                )\nWith this code, I get the error ValueError: failed in converting 7th argument g' of _lbfgsb.setulb to C/Fortran array` Do you have any suggestions to resolve the issue?",
        "answers": [
            "You might think about using the jax version of scipy.optimize.minimize, which will automatically compute and use the derivative:\nimport jax.scipy.optimize\nresult = jax.scipy.optimize.minimize(obj_func, xy, args=(z,), method='BFGS')\nThat said, the results in either case are not going to be very meaningful, because your objective function is linearly decreasing in x and y, so it will be minimized when x, y → ∞"
        ],
        "link": "https://stackoverflow.com/questions/77860052/finding-the-maximum-of-a-function-using-jax"
    },
    {
        "title": "Custom JVP and VJP for higher order functions in JAX",
        "question": "I find custom automatic differentiation capabilities (JVP, VJP) very useful in JAX, but am having a hard time applying it to higher order functions. A minimal example of this sort is as follows: given a higher order function:\ndef parent_func(x):\n    def child_func(y):\n        return x**2 * y\n    return child_func\nI would like to define custom gradients of child_func with respect to x and y. What would be the correct syntax to achieve this?",
        "answers": [
            "Gradients in JAX are defined with respect to a function’s explicit inputs. Your child_func does not take x as an explicit input, so you cannot directly differentiate child_func with respect to x. However, you could do so indirectly by calling it from another function that takes x. For example:\ndef func_to_differentiate(x, y):\n  child_func = parent_func(x)\n  return child_func(y)\n\njax.grad(func_to_differentiate, argnums=0)(1.0, 1.0)  # 2.0\nThen if you wish, you could define standard custom derivative rules for func_to_differentiate."
        ],
        "link": "https://stackoverflow.com/questions/77859418/custom-jvp-and-vjp-for-higher-order-functions-in-jax"
    },
    {
        "title": "JAX python C callbacks",
        "question": "Numba allows to create C-callbacks directly in python with the @cfunc-decorator ( https://numba.pydata.org/numba-doc/0.42.0/user/cfunc.html ):\n@cfunc(\"float64(float64)\") \ndef square(x):\n    return x**2\nTo clarify, the resulting function is a pure C-function, which can then be called directly from C-code.\nIs there an equivalent functionality available in JAX ( https://jax.readthedocs.io/en/latest/# )?\nI have been searching for a while but couldn't find anything. I would appreciate any tips.",
        "answers": [
            "No, JAX doesn't provide any API similar to Numba's cfunc."
        ],
        "link": "https://stackoverflow.com/questions/77855169/jax-python-c-callbacks"
    },
    {
        "title": "How to loop a random number of times in jax with jit compilation?",
        "question": "I am using jax in python, and I want to loop over some code for a random number of times. This is part of a function which is jit compiled later. I have a small example below which should explain what I want to do.\nnum_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\narr = []\nfor i in range(num_iters):\n  arr += [i*i]\nThis works without any error and gives arr=[0,1,4] at the end of the loop (with the fixed seed of 0 that we're using in PRNGKey).\nHowever, if this is part of a jit-compiled function:\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nI get a TracerIntegerConversionError on num_iters. The function works fine without the jit decorator. How to get this to work with jit? I basically just want to construct the list arr whose length depends on a random number. Alternatively, I can also use a list with the maximum possible size, but then I'd have to loop over it a random number of times.\nFurther context\nIt's possible to make it not throw an error using a numpy random number generator instead:\n@jax.jit\ndef do_stuff(start):\n  np_rng = np.random.default_rng()\n  num_iters = np_rng.integers(1, 10)\n  arr = []\n  for i in range(num_iters):\n    arr += [i*i]\n  for value in arr:\n    start += value\n  return start\nHowever, this is not what I want. There is a jax rng which is passed to my function which I wish to use to generate num_iters. Otherwise, arr always has the same length since the numpy seed is fixed to what was available at jit-compile time, and I always get the same result without any randomness. However, if I use that rng key as seed for numpy (like np.random.default_rng(seed=rng[0])) it again gives the following error:\nTypeError: SeedSequence expects int or sequence of ints for entropy not Traced<ShapedArray(uint32[])>with<DynamicJaxprTrace(level=1/0)>",
        "answers": [
            "You could use jax.lax.fori_loop for this:\nimport jax\n\n@jax.jit\ndef do_stuff(start):\n  num_iters = jax.random.randint(jax.random.PRNGKey(0), (1,), 1, 10)[0]\n  return jax.lax.fori_loop(0, num_iters, lambda i, val: val + i * i, start)\n\nprint(do_stuff(10))\n# 15",
            "Jax complains in this case, because you try to use a traced value as a static integer. See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError for more information.\nAs one possible solution you could pass the num_iters as an argument to do_stuff, declare it as static and create the keys outside, along the lines of:\nimport jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnums=(1,))\ndef do_stuff(start, num_iters):    \n  arr = []\n  \n  for i in range(num_iters):\n    arr += [i*i]\n  \n  for value in arr:\n    start += value\n  \n  return start\n\nkey = jax.random.PRNGKey(238)\n\nfor _ in range(4):\n  key, _ = jax.random.split(key)\n  num_iters = int(jax.random.randint(key, (1,), 1, 10))\n  print(do_stuff(0, num_iters))\nWhich prints:\n5\n0\n140\n30\nOther alternative solutions are proposed in the link I listed above.\nI hope this helps!"
        ],
        "link": "https://stackoverflow.com/questions/77844256/how-to-loop-a-random-number-of-times-in-jax-with-jit-compilation"
    },
    {
        "title": "Update JAX array based on values in another array",
        "question": "I have a Jax array X like this:\n[[[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]\n\n [[0. 0. 0.]\n [0. 0. 0.]]]\nHow do I set the values of this array to 1, whose indices are given by array Y:\n[[[1 2]\n [1 2]]\n\n [[0 2]\n [0 1]]\n\n [[1 0]\n [1 0]]]\nDesired output:\n([[[0., 1., 1.],\n        [0., 1., 1.]],\n\n       [[1., 0., 1.],\n        [1., 1., 0.]],\n\n       [[1., 1., 0.],\n        [1., 1., 0.]]]",
        "answers": [
            "There are a couple ways to approach this. First let's define the arrays:\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.zeros((3, 2, 3))\nindices = jnp.array([[[1, 2],\n                      [1, 2]],\n                     [[0, 2],\n                      [0, 1]],\n                     [[1, 0],\n                      [1, 0]]])\nOne way to do this is to use typical numpy-style broadcasting of indices. It might look like this:\ni = jnp.arange(3).reshape(3, 1, 1)\nj = jnp.arange(2).reshape(2, 1)\nx = x.at[i, j, indices].set(1)\nprint(x)\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]\nAnother option is to use a double-vmap transformation to compute the batched indices:\nf = jax.vmap(jax.vmap(lambda x, i: x.at[i].set(1)))\nprint(f(x, indices))\n[[[0. 1. 1.]\n  [0. 1. 1.]]\n\n [[1. 0. 1.]\n  [1. 1. 0.]]\n\n [[1. 1. 0.]\n  [1. 1. 0.]]]"
        ],
        "link": "https://stackoverflow.com/questions/77799930/update-jax-array-based-on-values-in-another-array"
    },
    {
        "title": "Occurence of NaN in softmax & JIT issues",
        "question": "I'm trying to implement the Transformer architecture from scratch using Jax. I find three issues while training:\njax.disable_jit() does not remove implicit jit compilations.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nclass SelfAttention(eqx.Module):\n    def __call__(self, query, key, value, mask):\n        scaled_dot_prod = query @ jnp.transpose(key, (0, 2, 1)) / jnp.sqrt(query.shape[-1])\n        scaled_dot_prod = mask + scaled_dot_prod\n        return (jax.nn.softmax(scaled_dot_prod) @ value)\n\ndef create_mask(arr):\n    return jnp.where(arr == 0, np.NINF, 0)\n\ndef loss(model, X, y, X_mask, y_mask, labels):\n    y_pred = jnp.log(predict(model, X, y, X_mask, y_mask))\n    y_pred = jnp.where(labels==0, 0, jnp.take(y_pred, labels, axis=-1))\n    count = jnp.count_nonzero(y_pred)\n    return -jnp.sum(y_pred)/count\n\nwith jax.disable_jit():\n    for e in range(EPOCHS):\n        total_loss = 0\n        num_batches = 0\n        total_tokens = 0\n        for i, (Xbt, ybt, labelbt) in enumerate(dataloader(Xtr, ytr, SEQ_LEN)):\n            total_tokens += len([token for seq in labelbt for token in list(filter(lambda x: x!=0, seq))])\n            Xbt, ybt, labelbt = [jnp.array(x) for x in (Xbt, ybt, labelbt)]\n            Xmask, ymask = [create_mask(x) for x in (Xbt, ybt)]\n\n            model, opt_state, batch_loss = step(model, opt_state, Xbt, ybt, Xmask, ymask, labelbt)\n            total_loss += batch_loss\n            num_batches += 1\n\n            if num_batches % 20 == 0:\n                print(f\"Batches trained: {num_batches} | Avg. Batch loss: {total_loss/num_batches}\")\n\n        epoch_loss = total_loss / num_batches\n        print(f\"Epoch {e} | loss: {epoch_loss}\")\nError:\ndef _softmax_deprecated(\n    478     x: ArrayLike,\n    479     axis: Optional[Union[int, tuple[int, ...]]] = -1,\n    480     where: Optional[ArrayLike] = None,\n    481     initial: Optional[ArrayLike] = None) -> Array:\n    482   x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)\n--> 483   unnormalized = jnp.exp(x - lax.stop_gradient(x_max))\n    484   result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)\n    485   if where is not None:\n\nFloatingPointError: invalid value (nan) encountered in jit(sub)\nThe above problem is encountered only after crossing 200 batches of training. I haven't checked that skipping the specific batch where the error occurs. Maybe I sohuld inspect if some specific inputs are the resason to this error.\nBut I can't find the answer to the above 3 questions :(",
        "answers": [
            "To answer your questions:\njax.disable_jit() does not remove implicit jit compilations.\nIf this is true, it is a bug and you should report it on the JAX issue tracker. It's unclear from your question what makes you believe this is the case.\nWhy does jax.nn.softmax calls _softmax_deprecated by default?\nBecause _softmax_deprecated is the old default algorithm, that will someday be deprecated but the deprecation has not happened yet. See https://github.com/google/jax/pull/15677 for details. To use the newer algorithm, you can set the jax_softmax_custom_jvp=True configuration.\nI'm encountering NaNs in subtraction inside _softmax_deprecated: unnormalized = jnp.exp(x - lax.stop_gradient(x_max)) I'll attach the code for your reference if needed:\nYou didn't include enough code to reproduce your issue (next time, try to add a minimal reproducible example to allow others to answer your question without guesswork). But it would be worth setting jax_softmax_custom_jvp=True to see if that addresses your issue. The pull request linked above has details."
        ],
        "link": "https://stackoverflow.com/questions/77677455/occurence-of-nan-in-softmax-jit-issues"
    },
    {
        "title": "Why does installing JAX with Docker create such a large image?",
        "question": "I am trying to pip install JAX using Docker and I found that using it just blows up the size of Docker image. The size of image currently is 4.82 GB.\nI made sure to bypass caching while installing packages by doing --no-cache-dir. While that did reduce the size, the size is still unreasonable huge.\nHere is my Dockerfile -\nFROM ubuntu:22.04\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    libosmesa6-dev \\\n    sudo \\\n    wget \\\n    curl \\\n    unzip \\\n    gcc \\\n    g++\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\nRUN mkdir -p ~/miniconda3\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nRUN bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nRUN rm -rf ~/miniconda3/miniconda.sh\nRUN ~/miniconda3/bin/conda init bash\nRUN conda init\n\nRUN pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nThis is how I built it -\ndocker build -t tbd_jax .\nWhen I do docker images, I get this -\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\ntbd_jax      latest    812292e2264e   7 minutes ago    4.82GB\nAfter doing docker history --no-trunc tbd_jax:latest -\nSIZE      COMMENT\nsha256:812292e2264e4340b7715956824055d7409f9546f8dfa54ccad1da056febf300   8 minutes ago    RUN |1 PATH=/root/miniconda3/bin:/root/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin /bin/sh -c pip install --no-cache-dir --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # buildkit   3.54GB    buildkit.dockerfile.v0\nIs there something I can do to reduce the size? I am a bit of a Docker and Linux newbie so pardon my slowness.",
        "answers": [
            "Note that jax[cuda12_pip] installs all the cuda drivers listed here:\n'cuda12_pip': [\n  ...\n  \"nvidia-cublas-cu12>=12.2.5.6\",\n  \"nvidia-cuda-cupti-cu12>=12.2.142\",\n  \"nvidia-cuda-nvcc-cu12>=12.2.140\",\n  \"nvidia-cuda-runtime-cu12>=12.2.140\",\n  \"nvidia-cudnn-cu12>=8.9\",\n  \"nvidia-cufft-cu12>=11.0.8.103\",\n  \"nvidia-cusolver-cu12>=11.5.2\",\n  \"nvidia-cusparse-cu12>=12.1.2.141\",\n  \"nvidia-nccl-cu12>=2.18.3\",\nThese nvidia driver packages are quite large: for example the nvidia_cublas_cu12 wheel is over 400MB, and nvidia-cudnn-cu12 is over 700MB. You may be able to do better by setting up your docker image with system-native CUDA & CUDNN drivers, installed via apt. You can find a description of the requirements here. You can also use NVIDIA's pre-defined GPU containers, as mentioned here."
        ],
        "link": "https://stackoverflow.com/questions/77669055/why-does-installing-jax-with-docker-create-such-a-large-image"
    },
    {
        "title": "How to understand and debug memory usage with JAX?",
        "question": "I am new to JAX and trying to learn use it for running some code on a GPU. In my example I want to search for regular grids in a point cloud (for indexing X-ray diffraction data).\nWith test_mats[4_000_000,3,3] the memory usage seems to be 15 MB. But with test_mats[5_000_000,3,3] I get an error about it wanting to allocate 19 GB.\nI can't tell whether this is a glitch in JAX, or because I am doing something wrong. My example code and output are below. I guess the problem is that it wants to create a temporary array of (N, 3, gvec.shape[1]) before doing the reduction, but I don't know how to see the memory profile for what happens inside the jitted/vmapped function.\nimport sys\nimport os\nimport jax\nimport jax.random\nimport jax.profiler\n\nprint('jax.version.__version__',jax.version.__version__)\n\nimport scipy.spatial.transform\nimport numpy as np\n\n# (3,N) integer grid spot positions\nhkls = np.mgrid[-3:4, -3:4, -3:4].reshape(3,-1)\n\nUmat = scipy.spatial.transform.Rotation.random( 10, random_state=42 ).as_matrix()\na0 = 10.13\ngvec = np.swapaxes( Umat.dot(hkls)/a0, 0, 1 ).reshape(3,-1)\n\ndef count_indexed_peaks_hkl( ubi, gve, tol ):\n    \"\"\" See how many gve this ubi can account for \"\"\"\n    hkl_real = ubi.dot( gve )\n    hkl_int = jax.numpy.round( hkl_real )\n    drlv2 = ((hkl_real - hkl_int)**2).sum(axis=0)\n    npks = jax.numpy.where( drlv2 < tol*tol, 1, 0 ).sum()\n    return npks\n\ndef testsize( N ):\n    print(\"Testing size\",N)\n    jfunc = jax.vmap( jax.jit(count_indexed_peaks_hkl), in_axes=(0,None,None))\n    key = jax.random.PRNGKey(0)\n    test_mats = jax.random.orthogonal(key, 3, (N,) )*a0\n    dev_gvec = jax.device_put( gvec )\n    scores = jfunc( test_mats, gvec, 0.01 )\n    jax.profiler.save_device_memory_profile(f\"memory_{N}.prof\")\n    os.system(f\"~/go/bin/pprof -top {sys.executable} memory_{N}.prof\")\n\ntestsize(400000)\ntestsize(500000)\nOutput is:\ngpu4-03:~/Notebooks/JAXFits % python mem.py \njax.version.__version__ 0.4.16\nTesting size 400000\nFile: python\nType: space\nShowing nodes accounting for 15.26MB, 99.44% of 15.35MB total\nDropped 25 nodes (cum <= 0.08MB)\n      flat  flat%   sum%        cum   cum%\n   15.26MB 99.44% 99.44%    15.26MB 99.44%  __call__\n         0     0% 99.44%    15.35MB   100%  [python]\n         0     0% 99.44%     1.53MB 10.00%  _pjit_batcher\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl\n         0     0% 99.44%    15.30MB 99.70%  _pjit_call_impl_python\n         0     0% 99.44%    15.30MB 99.70%  _python_pjit_helper\n         0     0% 99.44%    15.35MB   100%  bind\n         0     0% 99.44%    15.35MB   100%  bind_with_trace\n         0     0% 99.44%    15.30MB 99.70%  cache_miss\n         0     0% 99.44%    15.30MB 99.70%  call_impl_cache_miss\n         0     0% 99.44%     1.53MB 10.00%  call_wrapped\n         0     0% 99.44%    13.74MB 89.51%  deferring_binary_op\n         0     0% 99.44%    15.35MB   100%  process_primitive\n         0     0% 99.44%    15.30MB 99.70%  reraise_with_filtered_traceback\n         0     0% 99.44%    15.35MB   100%  testsize\n         0     0% 99.44%     1.53MB 10.00%  vmap_f\n         0     0% 99.44%    15.31MB 99.74%  wrapper\nTesting size 500000\n2023-12-14 10:26:23.630474: W external/tsl/tsl/framework/bfc_allocator.cc:296] Allocator\n(GPU_0_bfc) ran out of memory trying to allocate 19.18GiB with freed_by_count=0. The caller\nindicates that this is not a failure, but this may mean that there could be performance \ngains if more memory were available.\nTraceback (most recent call last):\n  File \"~/Notebooks/JAXFits/mem.py\", line 38, in <module>\n    testsize(500000)\n  File \"~/Notebooks/JAXFits/mem.py\", line 33, in testsize\n    scores = jfunc( test_mats, gvec, 0.01 )\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\njaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to \nallocate 20596777216 bytes.\n--------------------\nFor simplicity, JAX has removed its internal frames from the traceback of the following \nexception. Set JAX_TRACEBACK_FILTERING=off to include these.```",
        "answers": [
            "The vmapped function is attempting to create an intermediate array of shape [N, 3, 3430]. For N=400_000, with float32 this amounts to 15GB, and for N=500_000 this amounts to 19GB.\nYour best option in this situation is probably to split your computation into sequentially-executed batches using lax.map or similar. Unfortunately there's not currently any automatic way to do that kind of chunked vmao, but there is a relevant feature request at https://github.com/google/jax/issues/11319, and there are some useful suggestions in that thread."
        ],
        "link": "https://stackoverflow.com/questions/77659069/how-to-understand-and-debug-memory-usage-with-jax"
    },
    {
        "title": "JAX dynamic slice inside of control flow function",
        "question": "I would like to do dynamic slicing inside of lax.while_loop() using a variable carried over, getting an error as below. I know in the case of a simple function, I can pass the variable as a static value, using partial , but how can I handle the case in which the variable (in my case length) is carried over?\nnew_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got (0, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThis is how I coded. This code is just to illustrate the problem. What I would like to do is to extract a part of u and do some operations. Thank you.\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import lax\nfrom functools import partial\nimport jax\n\nu = jnp.array([[1,2,3,4,5],[0,0,0,0,0]])\n\ndef body_fun(carry):\n    length, sum_u = carry\n    new_u = lax.dynamic_slice(u,(0,0),(0,length-1))\n    #new_u = lax.dynamic_slice(u,(0,0),(2,4))\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n\ndef cond_fun(carry):\n    length, sum_u = carry\n    keep_condition = sum_u < 5\n    return keep_condition\n\ninit_carry = (5,10)\nout = lax.while_loop(cond_fun, body_fun, init_carry)\nprint(out)",
        "answers": [
            "The problem is that you are attempting to construct a dynamically-shaped array, and JAX does not support dynamically-shaped arrays (length is a dynamic variable in your loop). See JAX Sharp Bits: Dynamic Shapes for more.\nA typical strategy in these cases is to use a statically-sized array while masking out a dynamic range of values; in your case, you could use a value of 0 for the masked values so that they don't contribute to the sum. It might look like this:\ndef body_fun(carry):\n    length, sum_u = carry\n    idx = jnp.arange(u.shape[1])\n    new_u = jnp.where(idx < length, u, 0)\n    jax.debug.print(\"new_u:{}\", new_u)\n    new_sum_u = jnp.sum(new_u)\n    new_length = length -1\n    return (new_length, new_sum_u)\n(Side-note: it seems like you were using dynamic_slice in hopes that you could generate dynamic array shapes, but the dynamic in dynamic_slice refers to the dynamic offset, not a dynamic size)."
        ],
        "link": "https://stackoverflow.com/questions/77603954/jax-dynamic-slice-inside-of-control-flow-function"
    },
    {
        "title": "Efficient copying of an ensemble in JAX",
        "question": "I have an ensemble of models and want to assign the same parameters to each of the models. Both the models' parameters as well as the new parameters have the same underlying structure. Currently I use the following approach that uses a for-loop.\nimport jax\nimport jax.numpy as jnp\n\nmodel1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nmodel2 = [\n    [jnp.asarray([2]), jnp.asarray([3, 4])],\n    [jnp.asarray([5]), jnp.asarray([6, 7])],\n]\n\nmodels = [model1, model2]\n\nparams = [\n    [jnp.asarray([3]), jnp.asarray([4, 5])],\n    [jnp.asarray([6]), jnp.asarray([7, 8])],\n]\n\nmodels = [jax.tree_map(jnp.copy, params) for _ in range(len(models))]\nIs there a more efficient way in JAX to assign the parameters from params to each model in models?",
        "answers": [
            "Since JAX arrays are immutable, there's no need to copy the parameter arrays, and you could achieve the same result like this:\nmodels = len(models) * [params]"
        ],
        "link": "https://stackoverflow.com/questions/77595758/efficient-copying-of-an-ensemble-in-jax"
    },
    {
        "title": "Parallelize inference of ensemble",
        "question": "I used the this tutorial from JAX to create an ensemble of networks. Currently I compute the loss of each network in a for-loop which I would like to avoid:\nfor params in ensemble_params:\n    loss = mse_loss(params, inputs=x, targets=y)\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\nHere ensemble_params is a list of pytrees (lists of tuples holding JAX parameter arrays). The parameter structure of each network is the same.\nI tried to get rid of the for-loop by applying jax.vmap:\nensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\nHowever, I keep getting the following error message which I do not understand.\nValueError: vmap got inconsistent sizes for array axes to be mapped:\n  * most axes (8 of them) had size 3, e.g. axis 0 of argument params[0][0][0] of type float32[3,2];\n  * some axes (8 of them) had size 4, e.g. axis 0 of argument params[0][1][0] of type float32[4,3]\nHere is a minimal reproducible example:\nimport jax\nfrom jax import Array\nfrom jax import random\nimport jax.numpy as jnp\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\ndef relu(x):\n  return jnp.maximum(0, x)\n\ndef predict(params, image):\n  activations = image\n  for w, b in params[:-1]:\n    outputs = jnp.dot(w, activations) + b\n    activations = relu(outputs)\n  final_w, final_b = params[-1]\n  logits = jnp.dot(final_w, activations) + final_b\n  return logits\n\nbatched_predict = jax.vmap(predict, in_axes=(None, 0))\n\ndef mse_loss(params, inputs, targets):\n    preds = batched_predict(params, inputs)\n    loss = jnp.mean((targets - preds) ** 2)\n    return loss\n\nif __name__ == \"__main__\":\n\n    num_models = 4\n    dim_in = 2\n    dim_out = 4\n    layer_dims = [dim_in, 3, dim_out]\n    batch_size = 2\n\n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble_params = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    key_x, key_y = random.split(key)\n    x = random.normal(key=key_x, shape=(batch_size, dim_in))\n    y = random.normal(key=key_y, shape=(batch_size, dim_out))\n\n    for params in ensemble_params:\n        loss = mse_loss(params, inputs=x, targets=y)\n        print(f\"{loss = }\")\n\n    ensemble_loss = jax.vmap(fun=mse_loss, in_axes=(0, None, None))\n    losses = ensemble_loss(ensemble_params, x, y)\n    print(f\"{losses = }\")  # Same losses expected as above.",
        "answers": [
            "The main issue here is that vmap maps over arrays, not over lists.\nYou are passing a list of parameter objects, expecting vmap to map over the elements of that list. However, the semantics of vmap are that it maps over the first axis of each tree leaf in the argument, and the leaves in your argument differ in their leading axis.\nTo fix this, instead of passing a list of parameter objects containing unbatched arrays, you need to pass a single parameter object containing batched arrays; in other words you need a struct-of-arrays pattern rather than a list-of-structs pattern.\nIn your case, you can create your batched ensemble parameters this way:\nensemble_params = jax.tree_map(lambda *args: jnp.stack(args), *ensemble_params)\nIf you pass this to the ensemble_loss function, you get the expected output:\nlosses = Array([3.762451 , 4.39846  , 4.1425314, 6.045669 ], dtype=float32)"
        ],
        "link": "https://stackoverflow.com/questions/77581033/parallelize-inference-of-ensemble"
    },
    {
        "title": "Weighted sum of pytrees in JAX",
        "question": "I have a pytree represented by a list of lists holding parameter tuples. The sub-lists all have the same structure (see example).\nNow I would like to create a weighted sum so that the resulting pytree has the same structure as one of the sub-lists. The weights for each sub-list are stored in a separate array / list.\nSo far I have the following code that seems to works but requires several steps and for-loop that I would like avoid for performance reasons.\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n    [jnp.asarray([[1, 2], [3, 4]]), jnp.asarray([2, 3])],\n]\n\nlist_2 = [\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n    [jnp.asarray([[2, 3], [3, 4]]), jnp.asarray([5, 3])],\n]\n\nlist_3 = [\n    [jnp.asarray([[7, 1], [4, 4]]), jnp.asarray([6, 2])],\n    [jnp.asarray([[6, 4], [3, 7]]), jnp.asarray([7, 3])],\n]\n\nweights = [1, 2, 3] \npytree = [list_1, list_2, list_3]\n\nweighted_pytree = [jax.tree_map(lambda tree: weight * tree, tree) for weight, tree in zip(weights, pytree)]\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *weighted_pytree)",
        "answers": [
            "I think this will do what you have in mind:\ndef wsum(*args, weights=weights):\n  return jnp.asarray(weights) @ jnp.asarray(args)\n\nreduced = jax.tree_util.tree_map(wsum, *pytree)\nFor the edited question, where tree elements have more general shapes, you can define wsum like this instead:\ndef wsum(*args, weights=weights):\n  return sum(weight * arg for weight, arg in zip(weights, args))"
        ],
        "link": "https://stackoverflow.com/questions/77550969/weighted-sum-of-pytrees-in-jax"
    },
    {
        "title": "Reduce list of lists in JAX",
        "question": "I have a list holding many lists of the same structure (Usually, there are much more than two sub-lists inside the list, the example shows two lists for the sake of simplicity). I would like to create the sum or product over all sub-lists so that the resulting list has the same structure as one of the sub-lists. So far I tried the following using the tree_reduce method but I get errors that I don't understand.\nI could need some guidance on how to use tree_reduce() in such a case.\nimport jax\nimport jax.numpy as jnp\n\nlist_1 = [\n    [jnp.asarray([1]), jnp.asarray([2, 3])],\n    [jnp.asarray([4]), jnp.asarray([5, 6])],\n]\n\nlist_2 = [\n    [jnp.asarray([7]), jnp.asarray([8, 9])],\n    [jnp.asarray([10]), jnp.asarray([11, 12])],\n]\n    \nlist_of_lists = [list_1, list_2]\n   \nreduced = jax.tree_util.tree_reduce(lambda x, y: x + y, list_of_lists, 0, is_leaf=True)\n    \n# Expected\n# reduced = [\n#     [jnp.asarray([8]), jnp.asarray([10, 12])],\n#     [jnp.asarray([14]), jnp.asarray([16, 18])],\n# ]",
        "answers": [
            "You can do this with tree_map of a sum over the splatted list:\nreduced = jax.tree_util.tree_map(lambda *args: sum(args), *list_of_lists)\nprint(reduced)\n[[Array([8], dtype=int32), Array([10, 12], dtype=int32)],\n [Array([14], dtype=int32), Array([16, 18], dtype=int32)]]"
        ],
        "link": "https://stackoverflow.com/questions/77548225/reduce-list-of-lists-in-jax"
    },
    {
        "title": "Add noise to parameters of ensemble in JAX",
        "question": "I use the following code to create parameters for an ensemble of models stored as list of lists holding tuples of weights and biases. How do I efficiently add random noise to all parameters of the ensemble with JAX? I tried to use tree_map() but run into many errors probably caused by the nested structure.\nCould you please provide guidance on how to use tree_map() in this case or point to other methods that JAX provides for such a case?\nfrom jax import Array\nfrom jax import random\n\ndef layer_params(dim_in: int, dim_out: int, key: Array) -> tuple[Array]:\n    w_key, b_key = random.split(key=key)\n    weights = 0 * random.normal(key=w_key, shape=(dim_out, dim_in))\n    biases = 0 * random.normal(key=w_key, shape=(dim_out,))\n    return weights, biases\n\ndef init_params(layer_dims: list[int], key: Array) -> list[tuple[Array]]:\n    keys = random.split(key=key, num=len(layer_dims))\n    params = []\n    for dim_in, dim_out, key in zip(layer_dims[:-1], layer_dims[1:], keys):\n        params.append(layer_params(dim_in=dim_in, dim_out=dim_out, key=key))\n    return params\n\ndef init_ensemble(key: Array, num_models: int, layer_dims: list[int]) -> list:\n    keys = random.split(key=key, num=num_models)\n    models = [init_params(layer_dims=layer_dims, key=key) for key in keys]\n    return models\n\nif __name__ == \"__main__\":\n    num_models = 2\n    layer_dims = [2, 3, 4]\n    \n    key = random.PRNGKey(seed=1)\n    key, subkey = random.split(key)\n    ensemble = init_ensemble(key=subkey, num_models=num_models, layer_dims=layer_dims)\n\n    # Add noise to ensemble.",
        "answers": [
            "Here's one way you could do this:\nfrom jax import tree_util\nleaves, tree = tree_util.tree_flatten(ensemble)\nkey, *subkeys = random.split(key, len(leaves) + 1)\nsubkeys = tree_util.tree_unflatten(tree, subkeys)\n\ndef add_noise(val, key, eps=0.1):\n  return val + eps * random.normal(key, val.shape)\n\nensemble_with_noise = tree_util.tree_map(add_noise, ensemble, subkeys)\nEssentially, you create a tree of subkeys with the same structure as the tree of parameters, then use tree_map to apply the noise function to the tree.",
            "Another possiblity is to use ravel_pytree to first flatten the parameters into a single vector, then add noise, then unravel the noised vector back to the original tree structure:\nimport jax\nimport jax.flatten_util\n\nvector, unravel = jax.flatten_util.ravel_pytree(ensemble)\nnoisy_vector = vector + jax.random.normal(key, vector.shape)\nnoisy_ensemble = unravel(vector)"
        ],
        "link": "https://stackoverflow.com/questions/77539206/add-noise-to-parameters-of-ensemble-in-jax"
    },
    {
        "title": "Jax vmap limit memory",
        "question": "I'm wondering if there is a good way to limit the memory usage for Jax's VMAP function? Equivalently, to vmap in batches at a time if that makes sense?\nIn my specific use case, I have a set of images and I'd like to calculate the affinity between each pair of images; so ~order((num_imgs)^2 * (img shape)) bytes of memory used all at once if I'm understanding vmap correctly (which gets huge since in my real example I have 10,000 100x100 images).\nA basic example is:\ndef affininty_matrix_ex(n_arrays=10, img_size=5, key=jax.random.PRNGKey(0), gamma=jnp.array([0.5])):\n    arr_of_imgs = jax.random.normal(jax.random.PRNGKey(0), (n_arrays, img_size, img_size))\n    arr_of_indices = jnp.arange(n_arrays)\n    inds_1, inds_2 = zip(*combinations(arr_of_indices, 2))\n    v_cPA = jax.vmap(calcPairAffinity2, (0, 0, None, None), 0)\n    affinities = v_cPA(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma)\n    print()\n    print(jax.make_jaxpr(v_cPA)(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma))\n    \n    affinities = affinities.reshape(-1)\n    \n    arr = jnp.zeros((n_arrays, n_arrays), dtype=jnp.float16)\n    arr = arr.at[jnp.triu_indices(arr.shape[0], k=1)].set(affinities)\n    arr = arr + arr.T\n    arr = arr + jnp.identity(n_arrays, dtype=jnp.float16)\n    \n    return arr\n\n\ndef calcPairAffinity2(ind1, ind2, imgs, gamma):\n    #Returns a jnp array of 1 float, jnp.sum adds all elements together\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\nI suppose I could just say something like \"only feed into vmap X pairs at a time, and loop through n_chunks = n_arrays/X, appending each groups results to a list\" but that doesn't seem to be ideal. My understanding is vmap does not like generators, not sure if that would be an alternative way around the issue.",
        "answers": [
            "Edit, Aug 13 2024\nAs of JAX version 0.4.31, what you're asking for is possible using the batch_size argument of lax.map. For an iterable of size N, this will perform a scan with N // batch_size steps, and within each step will vmap the function over the batch. lax.map has less flexible semantics than jax.vmap, but for the simplest cases they look relatively similar. Here's an example using your calcPairAffinity function:\nFor example\nimport jax\nimport jax.numpy as jnp\n\ndef calcPairAffinity(ind1, ind2, imgs, gamma=0.5):\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\n\nimgs = jax.random.normal(jax.random.key(0), (100, 5, 5))\ninds = jnp.arange(imgs.shape[0])\ninds1, inds2 = map(jnp.ravel, jnp.meshgrid(inds, inds))\n\ndef f(inds):\n  return calcPairAffinity(*inds, imgs, 0.5)\n\n\nresult_vmap = jax.vmap(f)((inds1, inds2))\nresult_batched = jax.lax.map(f, (inds1, inds2), batch_size=1000)\nassert jnp.allclose(result_vmap, result_batched)\nOriginal answer\nThis is a frequent request, but unfortunately there's not yet (as of JAX version 0.4.20) any built-in utility to do chunked/batched vmap (xmap does have some functionality along these lines, but is experimental/incomplete and I wouldn't recommend relying on it).\nAdding chunking to vmap is tracked in https://github.com/google/jax/issues/11319, and there's some code there that does a limited version of what you have in mind. Hopefully something like what you describe will be possible with JAX's built-in vmap soon. In the meantime, you might think about applying vmap to chunks manually in the way you describe in your question."
        ],
        "link": "https://stackoverflow.com/questions/77527847/jax-vmap-limit-memory"
    },
    {
        "title": "JAX grad: derivate with respect an specific variable in a matrix",
        "question": "I am using Jax to do the grad of a matrix. For example I have a function f(A) where A is a matrix like A = \\[\\[a,b\\], \\[c,d\\]\\]. I want to just do the grad of f(A) for a,c and d (more specific for the lower-triangular part). How can I do that? also for a general NxN matrix not just the 2x2.\nI tried to convert the regular grad in a lower-triangular, but I am not sure if that is the same of if the output is correct.",
        "answers": [
            "JAX does not offer any way to take the gradient with respect to individual matrix elements. There are two ways you could proceed; first, you could take the gradient with respect to the entire array and extract the elements you're interested in; for example:\nimport jax\nimport jax.numpy as jnp\n\ndef f(A):\n  return (A ** 2).sum()\n\nA = jnp.array([[1.0, 2.0], [3.0, 4.0]])\ndf_dA = jax.grad(f)(A)\nprint(df_dA[0, 0], df_dA[0, 1], df_dA[1, 2])\n2.0 4.0 8.0\nAlternatively, you could split the entries of the array into individual function arguments, and then use argnums to take the gradient with respect to just the ones you're interested in:\ndef f(a, b, c, d):\n  A = jnp.array([[a, b], [c, d]])\n  return (A ** 2).sum()\n\ndf_da, df_db, df_dc = jax.grad(f, argnums=(0, 1, 2))(1.0, 2.0, 3.0, 4.0)\nprint(df_da, df_db, df_dc)\n2.0 4.0 8.0\nIn general you'll probably find the first approach to be both easier to use in practice, and also more efficient. It does have some wasted computation, but sticking with vectorized computations will generally be a net win, especially if you're running on accelerators like GPU or TPU."
        ],
        "link": "https://stackoverflow.com/questions/77517357/jax-grad-derivate-with-respect-an-specific-variable-in-a-matrix"
    }
]